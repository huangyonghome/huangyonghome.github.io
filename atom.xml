<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jesse&#39;s home</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jesse.top/"/>
  <updated>2020-06-29T13:11:55.850Z</updated>
  <id>https://jesse.top/</id>
  
  <author>
    <name>Jesse</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>docker学习笔记---理解swarm集群</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0-5.%E5%AD%A6%E4%B9%A0swarm%E9%9B%86%E7%BE%A4/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习-5.学习swarm集群/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:11:55.850Z</updated>
    
    <content type="html"><![CDATA[<h3 id="理解swarm集群"><a href="#理解swarm集群" class="headerlink" title="理解swarm集群"></a>理解swarm集群</h3><p>一个swarm是一组运行docker服务器的的集群,docker服务器可以是物理机也可以是虚拟机.</p><p>swarm manager可以使用多种策略来运行容器.比如”emptiest node”—部署容器到压力最小的服务器上,或者”global”—确保每台服务器都只允许一个容器实例.你可以在Compose文件中指示swarm manager去选择何种策略</p><p>swarm managers是swarm进群中唯一可以执行命令,或者授权其他服务器以”workers”身份加入swarm集群的服务器.</p><hr><h4 id="初始化swarm-加入节点"><a href="#初始化swarm-加入节点" class="headerlink" title="初始化swarm,加入节点"></a>初始化swarm,加入节点</h4><p>试验环境:</p><p>1.10.0.0.50 —swarm manager<br>2.10.0.0.12 —worker 节点</p><ul><li>初始化swarm,并且指定通告的IP</li></ul><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker swarm init --advertise-addr 10.0.0.50</span><br><span class="line">Swarm initialized: current node (zlvl9l94blu3rfcaaptdvo9u1) is now a manager.</span><br><span class="line"></span><br><span class="line">To add a worker to this swarm, run the following command:</span><br><span class="line"></span><br><span class="line">    docker swarm join --token SWMTKN-1-2i5fyjf2niw81tudcvpw33yuni277vz45lt6tyi5bvcnhvuwea-bj091dpe6e69ph9kt3lmsthgp 10.0.0.50:2377</span><br><span class="line"></span><br><span class="line">To add a manager to this swarm, run &apos;docker swarm join-token manager&apos; and follow the instructions.</span><br></pre></td></tr></table></figure><p>根据上面提示,在第二台服务器上以worker身份加入swarm集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@php ~]$docker swarm join --token SWMTKN-1-2i5fyjf2niw81tudcvpw33yuni277vz45lt6tyi5bvcnhvuwea-bj091dpe6e69ph9kt3lmsthgp 10.0.0.50:2377</span><br><span class="line">This node joined a swarm as a worker.</span><br><span class="line">[root@php ~]$</span><br></pre></td></tr></table></figure><p>执行docker node ls命令可以管理和查看swarm集群的所有节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker node ls</span><br><span class="line">ID                            HOSTNAME                STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class="line">zlvl9l94blu3rfcaaptdvo9u1 *   localhost.localdomain   Ready               Active              Leader              18.09.2</span><br><span class="line">ud5ztqzvvfwg3d3hwmts5y9ct     php                     Ready               Active                                  18.09.3</span><br><span class="line">[root@localhost ~]$</span><br></pre></td></tr></table></figure><p>执行docker swarm leave命令将某个节点退出swarm集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@php ~]$docker swarm leave</span><br><span class="line">Node left the swarm.</span><br><span class="line">[root@php ~]$</span><br><span class="line"></span><br><span class="line">此时这个节点在swarm集群中状态为down</span><br><span class="line"></span><br><span class="line">[root@localhost ~]$docker node ls</span><br><span class="line">ID                            HOSTNAME                STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class="line">zlvl9l94blu3rfcaaptdvo9u1 *   localhost.localdomain   Ready               Active              Leader              18.09.2</span><br><span class="line">ud5ztqzvvfwg3d3hwmts5y9ct     php                     Down                Active                                  18.09.3</span><br><span class="line">[root@localhost ~]$</span><br></pre></td></tr></table></figure><hr><h4 id="在swarm集群部署app"><a href="#在swarm集群部署app" class="headerlink" title="在swarm集群部署app"></a>在swarm集群部署app</h4><p>现在可以把上一小节的docker compose部署在swarm集群上了.执行命令和上一小节一样.但是需要注意的是只能在swarm manager节点服务器上执行命令.</p><p>在第一台服务器上执行如下命令:(确保docker compose文件和镜像文件在这台服务器上)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@localhost ~]$cd /data/compose</span><br><span class="line">[root@localhost compose]$ls</span><br><span class="line">docker-compose.yml</span><br><span class="line">[root@localhost compose]$docker stack deploy -c docker-compose.yml getstartedlab</span><br><span class="line">Creating network getstartedlab_webnet</span><br><span class="line">Creating service getstartedlab_web</span><br><span class="line">[root@localhost compose]$</span><br></pre></td></tr></table></figure><p>APP已经成功部署到swarm集群上,现在可以使用上一小节中的同样的命令来管理app集群,只不过这次services和容器已经部署到两台服务器上:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack ps getstartedlab</span><br><span class="line">ID                  NAME                      IMAGE                  NODE                    DESIRED STATE       CURRENT STATE              ERROR                              PORTS</span><br><span class="line">4borwslue8k5        getstartedlab_web.1       friendlyhello:latest   localhost.localdomain   Running             Preparing 6 seconds ago</span><br><span class="line">ixmkldh16otx        getstartedlab_web.2       friendlyhello:latest   php                     Ready               Preparing 22 seconds ago</span><br><span class="line">i2yhimkst4iq         \_ getstartedlab_web.2   friendlyhello:latest   php                     Shutdown            Rejected 23 seconds ago    &quot;No such image: friendlyhello:…&quot;</span><br><span class="line">gytqpcwnzvrm        getstartedlab_web.3       friendlyhello:latest   localhost.localdomain   Running             Preparing 6 seconds ago</span><br><span class="line">j94tblo4qjwa        getstartedlab_web.4       friendlyhello:latest   php                     Ready               Preparing 22 seconds ago</span><br><span class="line">b7r9xkf4glh6         \_ getstartedlab_web.4   friendlyhello:latest   php                     Shutdown            Rejected 22 seconds ago    &quot;No such image: friendlyhello:…&quot;</span><br><span class="line">i8sv4c293ata        getstartedlab_web.5       friendlyhello:latest   localhost.localdomain   Running             Preparing 6 seconds ago</span><br></pre></td></tr></table></figure><blockquote><p>需要在另外一台服务器上pull同样的镜像,否则容器无法启动</p></blockquote><ul><li><p>在第二台服务器上下载我阿里云私有仓库的镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">docker login --username=jessehuang408 registry.cn-hangzhou.aliyuncs.com</span><br><span class="line">Password:</span><br><span class="line"></span><br><span class="line">[root@php ~]$docker pull registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:frendlyhello-v1.0</span><br><span class="line">frendlyhello-v1.0: Pulling from jesse_images/jesse_images</span><br><span class="line">f7e2b70d04ae: Pull complete</span><br><span class="line">1e9214730e83: Pull complete</span><br><span class="line">5bd4ec081f7b: Pull complete</span><br><span class="line">be26b369a1e7: Pull complete</span><br><span class="line">236be9d80905: Pull complete</span><br><span class="line">1bf8a3675b0b: Pull complete</span><br><span class="line">5752f9477f0c: Pull complete</span><br><span class="line">Digest: sha256:8e8b57ef6e22c8c04c1c80cfab9f336928cffabacaa4ae4e74ec57e54bcffdb2</span><br><span class="line">Status: Downloaded newer image for registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:frendlyhello-v1.0</span><br><span class="line"></span><br><span class="line">[root@php ~]$docker images</span><br><span class="line">REPOSITORY                                                    TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images   frendlyhello-v1.0   f091d1bb803c        2 days ago          131MB</span><br><span class="line">[root@php ~]$*</span><br></pre></td></tr></table></figure></li><li><p>将镜像修改成和第一台服务器一样:frendlyhello:latest</p></li></ul><p>命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker tag 镜像ID REPOSITORY:TAG</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@php ~]$docker tag f091d1bb803c frendlyhello:latest</span><br><span class="line">[root@php ~]$docker images</span><br><span class="line">REPOSITORY                                                    TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">frendlyhello                                                  latest              f091d1bb803c        2 days ago          131MB</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images   frendlyhello-v1.0   f091d1bb803c        2 days ago          131MB</span><br><span class="line">[root@php ~]$</span><br></pre></td></tr></table></figure><ul><li>将第一台服务器的docker-compose文件拷贝到同样的目录下</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@php ~]$mkdir /data/compose</span><br><span class="line">[root@php ~]$scp root@10.0.0.50:/data/compose/docker-compose.yml /data/compose/</span><br></pre></td></tr></table></figure><ul><li>回到第一台服务器上删除刚才创建的getstartedlab</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack rm getstartedlab</span><br><span class="line">Removing service getstartedlab_web</span><br><span class="line">Removing network getstartedlab_webnet</span><br><span class="line"></span><br><span class="line">[root@localhost compose]$docker stack ps getstartedlab</span><br><span class="line">nothing found in stack: getstartedlab</span><br></pre></td></tr></table></figure><ul><li>重新部署docker compose</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack deploy -c docker-compose.yml getstartedlab</span><br><span class="line">Creating network getstartedlab_webnet</span><br><span class="line">Creating service getstartedlab_web</span><br><span class="line">[root@localhost compose]$</span><br></pre></td></tr></table></figure><ul><li>成功部署</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack ps getstartedlab</span><br><span class="line">ID                  NAME                  IMAGE                  NODE                    DESIRED STATE       CURRENT STATE              ERROR               PORTS</span><br><span class="line">uqsj8mim0sac        getstartedlab_web.1   friendlyhello:latest   localhost.localdomain   Running             Preparing 3 seconds ago</span><br><span class="line">shjiwlnj12sp        getstartedlab_web.2   friendlyhello:latest   php                     Running             Preparing 24 seconds ago</span><br><span class="line">8sqllvgid8jp        getstartedlab_web.3   friendlyhello:latest   php                     Running             Preparing 24 seconds ago</span><br><span class="line">v7fsecgcg504        getstartedlab_web.4   friendlyhello:latest   php                     Running             Preparing 24 seconds ago</span><br><span class="line">np8utmyvk5px        getstartedlab_web.5   friendlyhello:latest   localhost.localdomain   Running             Preparing 3 seconds ago</span><br><span class="line">[root@localhost compose]$</span><br></pre></td></tr></table></figure><p>在第二台的worker节点上执行命令会提示失败:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@php compose]$docker stack ps getstartedlab</span><br><span class="line">Error response from daemon: This node is not a swarm manager. Worker nodes can&apos;t be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager.</span><br><span class="line">[root@php compose]$</span><br></pre></td></tr></table></figure><p>现在,在两台服务器上都能访问刚才部署的app</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.12:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 926f433b3896&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; &lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;% </span><br><span class="line"></span><br><span class="line">huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 1535f17586ea&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; &lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;%                                                            huangyong@huangyong-Macbook-Pro  ~ </span><br></pre></td></tr></table></figure><h4 id="扩展app"><a href="#扩展app" class="headerlink" title="扩展app"></a>扩展app</h4><p>扩展app还是直接编辑docker-compose.yml文件.然后重新docker stack deploy部署即可.</p><p>如果是需要将其他虚拟机或者物理服务器加入进swarm集群,就像第二台服务器一样使用docker swarm join命令加入即可,</p><h4 id="停止swarm"><a href="#停止swarm" class="headerlink" title="停止swarm"></a>停止swarm</h4><p>命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker stack rm getstartedlab</span><br></pre></td></tr></table></figure><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 初始化一个swarm集群</span><br><span class="line"></span><br><span class="line">docker swarm init --advertise-addr IP</span><br><span class="line"></span><br><span class="line">#加入到swarm集群</span><br><span class="line">docker swarm join --token &lt;token&gt; &lt;swarm manager IP&gt;:&lt;port&gt;</span><br><span class="line"></span><br><span class="line">#部署app</span><br><span class="line">docker stack deploy -c docker-compose.yml &lt;services name&gt;</span><br><span class="line">&gt; note:在所有docker服务器节点上都需要有docker-compose.yml文件和相关镜像</span><br><span class="line"></span><br><span class="line"># 查看services </span><br><span class="line">docker stack ps &lt;services name&gt;</span><br><span class="line">docker services ls</span><br><span class="line">docker stack ls</span><br><span class="line"></span><br><span class="line"># 从swarm集群中删除 services</span><br><span class="line"></span><br><span class="line">docker stack rm &lt;service name&gt;</span><br><span class="line"></span><br><span class="line"># 删除swarm集群节点</span><br><span class="line"></span><br><span class="line">docker swarm leave #worker节点</span><br><span class="line">docker swarm leave --force #manager节点</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;理解swarm集群&quot;&gt;&lt;a href=&quot;#理解swarm集群&quot; class=&quot;headerlink&quot; title=&quot;理解swarm集群&quot;&gt;&lt;/a&gt;理解swarm集群&lt;/h3&gt;&lt;p&gt;一个swarm是一组运行docker服务器的的集群,docker服务器可以是物理机也可以是虚拟机.&lt;/p&gt;
&lt;p&gt;swarm manager可以使用多种策略来运行容器.比如”emptiest node”—部署容器到压力最小的服务器上,或者”global”—确保每台服务器都只允许一个容器实例.你可以在Compose文件中指示swarm manager去选择何种策略&lt;/p&gt;
&lt;p&gt;swarm managers是swarm进群中唯一可以执行命令,或者授权其他服务器以”workers”身份加入swarm集群的服务器.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&quot;初始化swarm-加入节点&quot;&gt;&lt;a href=&quot;#初始化swarm-加入节点&quot; class=&quot;headerlink&quot; title=&quot;初始化swarm,加入节点&quot;&gt;&lt;/a&gt;初始化swarm,加入节点&lt;/h4&gt;&lt;p&gt;试验环境:&lt;/p&gt;
&lt;p&gt;1.10.0.0.50 —swarm manager&lt;br&gt;2.10.0.0.12 —worker 节点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始化swarm,并且指定通告的IP&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker使用阿里云私有仓库</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0%E2%80%943.%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F%E5%92%8C%E4%BB%93%E5%BA%93/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习—3.使用阿里云国内镜像和仓库/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:11:08.185Z</updated>
    
    <content type="html"><![CDATA[<h3 id="docker使用阿里云私有仓库"><a href="#docker使用阿里云私有仓库" class="headerlink" title="docker使用阿里云私有仓库"></a>docker使用阿里云私有仓库</h3><p>注册阿里云镜像服务:</p><p>以下是我的阿里云镜像仓库链接:<br><a href="https://cr.console.aliyun.com/cn-hangzhou/repositories" target="_blank" rel="noopener">https://cr.console.aliyun.com/cn-hangzhou/repositories</a></p><p>一.使用阿里云镜像加速器<br><a href="https://cr.console.aliyun.com/cn-hangzhou/mirrors" target="_blank" rel="noopener">https://cr.console.aliyun.com/cn-hangzhou/mirrors</a></p><p>镜像加速地址:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://0w5ygvsg.mirror.aliyuncs.com</span><br></pre></td></tr></table></figure><p>如果是Centos系统,可以通过修改daemon配置文件/etc/docker/daemon.json来使用加速器:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /etc/docker</span><br><span class="line">sudo tee /etc/docker/daemon.json &lt;&lt;-&apos;EOF&apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https://0w5ygvsg.mirror.aliyuncs.com&quot;]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure><hr><a id="more"></a><h4 id="使用阿里云的镜像仓库"><a href="#使用阿里云的镜像仓库" class="headerlink" title="使用阿里云的镜像仓库"></a>使用阿里云的镜像仓库</h4><p>首先在阿里云镜像服务控制台创建镜像仓库和命令空间:</p><p><a href="https://cr.console.aliyun.com/cn-hangzhou/repositories" target="_blank" rel="noopener">https://cr.console.aliyun.com/cn-hangzhou/repositories</a></p><p>我的镜像仓库和命名空间都是:jesse_images<br>这是我的镜像仓库地址:registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images</p><p>下面演示,如何推送本地镜像到阿里云仓库</p><p>1.在本地docker服务器登陆阿里云镜像仓库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost docker_python]$docker login --username=jessehuang408 registry.cn-hangzhou.aliyuncs.com</span><br></pre></td></tr></table></figure><p>2.将本地镜像推送到仓库执行以下两条命令</p><ul><li>为本地镜像打个标签</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker tag [ImageId] registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:[镜像版本号]</span><br></pre></td></tr></table></figure><ul><li>将镜像推送到仓库</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:[镜像版本号]</span><br></pre></td></tr></table></figure><p>下面演示将friendlyhello这个镜像推送到阿里云远程仓库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@localhost docker_python]$docker images</span><br><span class="line">REPOSITORY                                                    TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">friendlyhello                                                 latest              f091d1bb803c        43 minutes ago      131MB</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker tag f091d1bb803c registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:v2.0</span><br><span class="line"></span><br><span class="line">docker push registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:v2.0</span><br></pre></td></tr></table></figure><p>在本机可以看到镜像:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@localhost docker_python]$docker images</span><br><span class="line">REPOSITORY                                                    TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">friendlyhello                                                 latest              f091d1bb803c        43 minutes ago      131MB</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images   v2.0                f091d1bb803c        43 minutes ago      131MB</span><br></pre></td></tr></table></figure><p>登陆阿里云的镜像服务控制台,在镜像仓库的管理界面可以看到上传上去的镜像</p><p>如果是从阿里云镜像仓库拉取镜像,执行以下命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker pull registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:[镜像版本号]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;docker使用阿里云私有仓库&quot;&gt;&lt;a href=&quot;#docker使用阿里云私有仓库&quot; class=&quot;headerlink&quot; title=&quot;docker使用阿里云私有仓库&quot;&gt;&lt;/a&gt;docker使用阿里云私有仓库&lt;/h3&gt;&lt;p&gt;注册阿里云镜像服务:&lt;/p&gt;
&lt;p&gt;以下是我的阿里云镜像仓库链接:&lt;br&gt;&lt;a href=&quot;https://cr.console.aliyun.com/cn-hangzhou/repositories&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://cr.console.aliyun.com/cn-hangzhou/repositories&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一.使用阿里云镜像加速器&lt;br&gt;&lt;a href=&quot;https://cr.console.aliyun.com/cn-hangzhou/mirrors&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://cr.console.aliyun.com/cn-hangzhou/mirrors&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;镜像加速地址:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;https://0w5ygvsg.mirror.aliyuncs.com&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果是Centos系统,可以通过修改daemon配置文件/etc/docker/daemon.json来使用加速器:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sudo mkdir -p /etc/docker&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&amp;apos;EOF&amp;apos;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;quot;registry-mirrors&amp;quot;: [&amp;quot;https://0w5ygvsg.mirror.aliyuncs.com&amp;quot;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;EOF&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo systemctl daemon-reload&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo systemctl restart docker&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker-compose</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94docker-compose/"/>
    <id>https://jesse.top/2020/06/29/docker/docker学习笔记—docker-compose/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:15:08.679Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker学习笔记——docker-compose"><a href="#docker学习笔记——docker-compose" class="headerlink" title="docker学习笔记——docker-compose"></a>docker学习笔记——docker-compose</h2><p>docker compose 定义并且运行多个docker容器.使用YAML风格文件定义一个compose文件.利用compose文件创建和启动所有服务.</p><p>使用docker compose基本只需要3个步骤</p><ul><li>在Dockerfile文件定义app环境</li><li>在docker-compose.yml文件中定义组成app的各个服务</li><li>run docker-compose up 和compose 启动和运行app</li></ul><p>下面文档均可以在docker-compose官方找到详细资料:<a href="https://docs.docker.com/compose/" target="_blank" rel="noopener">docker-compose</a></p><h3 id="docker-compose安装"><a href="#docker-compose安装" class="headerlink" title="docker-compose安装"></a>docker-compose安装</h3><a id="more"></a><p>docker-compose的安装非常简单.下面是Linux上的安装方法.其他平台请自行参考官网</p><p>1.下载最近的1.24版本的二进制文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose</span><br></pre></td></tr></table></figure><p>2.给予执行权限.加入环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod +x /usr/local/bin/docker-compose</span><br><span class="line">sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose</span><br></pre></td></tr></table></figure><p>3.安装完成.查看是否安装成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose --version</span><br><span class="line">docker-compose version 1.24.0, build 1110ad01</span><br></pre></td></tr></table></figure><hr><h2 id="compose例子"><a href="#compose例子" class="headerlink" title="compose例子"></a>compose例子</h2><p>在官网上,或者去github上下载一个例子.这里我参考<docker 深入浅出="">这本书的例子</docker></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir /data/counter-app</span><br><span class="line">cd /data/counter-app</span><br><span class="line">git clone https://github.com/nigelpoulton/counter-app.git</span><br></pre></td></tr></table></figure><h3 id="docker-compose文件"><a href="#docker-compose文件" class="headerlink" title="docker-compose文件"></a>docker-compose文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost counter-app]$cat docker-compose.yml</span><br><span class="line"></span><br><span class="line">version: &quot; 3.5&quot;</span><br><span class="line">services:</span><br><span class="line">   web-fe:</span><br><span class="line">      build:</span><br><span class="line">         command: python app.py</span><br><span class="line">         ports:</span><br><span class="line">            - target: 5000</span><br><span class="line">              published: 5000</span><br><span class="line">         </span><br><span class="line">         networks:</span><br><span class="line">            - counter-net</span><br><span class="line">         </span><br><span class="line">         volumes:</span><br><span class="line">            - type: volume</span><br><span class="line">              source: counter-vol</span><br><span class="line">              target: /code</span><br><span class="line">    </span><br><span class="line">   redis:</span><br><span class="line">      image: &quot;redis:alpine&quot;</span><br><span class="line">      networks:</span><br><span class="line">         counter-net</span><br><span class="line">   </span><br><span class="line">networks:</span><br><span class="line">        counter-net:</span><br><span class="line">   </span><br><span class="line">volumes:</span><br><span class="line">       counter-vol:</span><br></pre></td></tr></table></figure><p><strong>compose文件结构</strong></p><p>包含4个一级key: version.services.network.volumes</p><ul><li>version: 必须指定,定义了compose文件格式版本.这里是3.5最新版</li><li>services: 用于定义不同的应用服务.这个例子中定义了2个服务.一个是web-fe的web前端.一个是redis的内存数据库.docker compose会将每个服务部署在各自的容器中</li><li>networks用于创建新的网络.默认情况下会创建bridge网络</li><li>volume用于创建新的卷</li></ul><p>上面的docker compose文件定义了2个服务.在web-fe的服务定义中.包含如下指令:</p><ul><li>build:  指定docker基于当前目录下的Dockerfile文件构建一个新镜像</li><li>command: 指定在容器中执行app.py脚本作为主程序 (这个指令可以忽略,因为dockerfile镜像中已经配置了CMD指令)</li><li>ports: 将容器(target)的5000端口映射到宿主机(published)5000端口</li><li>networks: docker将此容器连接到指定的网络上</li><li>volumes: 指定docker将宿主机counter-vol卷(source)挂载到容器内的/code(target)上.counter-vol卷是已经存在的,或者是在文件下方的volumes一级key中定义的</li></ul><p>redis服务比较简单,就不再赘述..</p><hr><h2 id="部署docker-compose"><a href="#部署docker-compose" class="headerlink" title="部署docker-compose"></a>部署docker-compose</h2><p>简要介绍counter-app目录内的几个文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost counter-app]$ll</span><br><span class="line">总用量 20</span><br><span class="line">-rw-r--r-- 1 root root 599 6月  18 17:34 app.py    #应用程序代码</span><br><span class="line">-rw-r--r-- 1 root root 475 6月  17 18:46 docker-compose.ymal  #compose文件,定义了如何部署容器</span><br><span class="line">-rw-r--r-- 1 root root 109 6月  18 17:34 Dockerfile  #构建web-fe服务镜像的dockerfile</span><br><span class="line">-rw-r--r-- 1 root root 128 6月  18 17:34 README.md   </span><br><span class="line">-rw-r--r-- 1 root root  11 6月  18 17:34 requirements.txt #列出app.py代码文件中python的依赖包</span><br></pre></td></tr></table></figure><h4 id="启动docker-compose"><a href="#启动docker-compose" class="headerlink" title="启动docker-compose"></a>启动docker-compose</h4><p>在当前目录下执行下列路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose up -d #后台启动</span><br></pre></td></tr></table></figure><p>默认情况下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>docker-compose -f compose_file up -d<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">如果找不到文件则会报错:</span><br></pre></td></tr></table></figure></p><p>ERROR:<br>        Can’t find a suitable configuration file in this directory or any<br>        parent. Are you in the right directory?</p><pre><code>Supported filenames: docker-compose.yml, docker-compose.yaml</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">部署过程中创建或者拉取了3个镜像: counterapp_web-fe,python,redis</span><br><span class="line"></span><br><span class="line">部署完成后,启动了如下2个容器:</span><br></pre></td></tr></table></figure><p>[root@localhost counter-app]$docker ps<br>CONTAINER ID        IMAGE                             COMMAND                  CREATED             STATUS              PORTS                    NAMES<br>474301996ccc        redis:alpine                      “docker-entrypoint.s…”   21 hours ago        Up 21 hours         6379/tcp                 counter-app_redis_1<br>c7a1e28b5e28        counter-app_web-fe                “python app.py”          21 hours ago        Up 21 hours         0.0.0.0:5000-&gt;5000/tcp   counter-app_web-fe_1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">每个容器都以项目名为前缀(所在目录名称).此外,还用一个数字为后缀用于表示容器序列(因为docker-compose允许扩容和缩减服务器数量)</span><br><span class="line"></span><br><span class="line">同时,docker-compose还创建了counter-app_counter-net网络:</span><br></pre></td></tr></table></figure></p><p>[root@localhost counter-app]$docker network ls<br>NETWORK ID          NAME                      DRIVER              SCOPE<br>6d40a81d76e7        bridge                    bridge              local<br>ef71284e9acc        counter-app_counter-net   bridge              local<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">应用部署成功后,可以查看容器的运行效果.每次访问,计数器就+1</span><br></pre></td></tr></table></figure></p><p>[root@localhost counter-app]$curl <a href="http://localhost:5000" target="_blank" rel="noopener">http://localhost:5000</a><br>What’s up Docker Deep Divers! You’ve visited me 1 times.<br>[root@localhost counter-app]$curl <a href="http://localhost:5000" target="_blank" rel="noopener">http://localhost:5000</a><br>What’s up Docker Deep Divers! You’ve visited me 2 times.<br>[root@localhost counter-app]$curl <a href="http://localhost:5000" target="_blank" rel="noopener">http://localhost:5000</a><br>What’s up Docker Deep Divers! You’ve visited me 3 times.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## docker Compose管理</span><br><span class="line"></span><br><span class="line">上面讲到如何部署一个compose应用..接下来讲解一下compose的管理命令.需要注意的是所有的docker-compose命令都需要在相关目录下执行.不然仍然会提示找不到docker-compose.yml(yaml)文件</span><br><span class="line"></span><br><span class="line">如果是停止应用.只需将up换成down即可.</span><br></pre></td></tr></table></figure></p><p>[root@localhost counter-app]$docker-compose down<br>Stopping counter-app_redis_1  … done<br>Stopping counter-app_web-fe_1 … done<br>Removing counter-app_redis_1  … done<br>Removing counter-app_web-fe_1 … done<br>Removing network counter-app_counter-net<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">停止compose经历了如下的过程:</span><br><span class="line"></span><br><span class="line">* 停止所有容器</span><br><span class="line">* 移除容器</span><br><span class="line">* 移除docker网络</span><br><span class="line"></span><br><span class="line">此时,无论是执行```docker ps ```还是```docker ps -a```都看不到容器</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 查看compose各个服务容器的运行的进程</span><br></pre></td></tr></table></figure></p><p>[root@localhost counter-app]$docker-compose top<br>counter-app_redis_1</p><h2 id="UID-PID-PPID-C-STIME-TTY-TIME-CMD"><a href="#UID-PID-PPID-C-STIME-TTY-TIME-CMD" class="headerlink" title="UID    PID    PPID    C   STIME   TTY     TIME         CMD"></a>UID    PID    PPID    C   STIME   TTY     TIME         CMD</h2><p>100   32558   32542   0   13:21   ?     00:00:00   redis-server</p><p>counter-app_web-fe_1</p><h2 id="UID-PID-PPID-C-STIME-TTY-TIME-CMD-1"><a href="#UID-PID-PPID-C-STIME-TTY-TIME-CMD-1" class="headerlink" title="UID     PID    PPID    C   STIME   TTY     TIME                    CMD"></a>UID     PID    PPID    C   STIME   TTY     TIME                    CMD</h2><p>root   32582   32564   6   13:21   ?     00:00:00   python app.py<br>root   32703   32582   4   13:21   ?     00:00:00   /usr/local/bin/python /code/app.py<br>[root@localhost counter-app]$<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; PID是docker宿主机的进程ID</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 停止应用容器.但是并不删除资源</span><br><span class="line"></span><br><span class="line">执行完```docker-compose stop```命令后,容器还存在</span><br></pre></td></tr></table></figure></p><p>[root@localhost counter-app]$docker-compose stop<br>Stopping counter-app_web-fe_1 … done<br>Stopping counter-app_redis_1  … done</p><p>[root@localhost counter-app]$docker-compose ps</p><pre><code>Name                      Command               State    Ports</code></pre><hr><p>counter-app_redis_1    docker-entrypoint.sh redis …   Exit 0<br>counter-app_web-fe_1   python app.py                    Exit 0<br>[root@localhost counter-app]$<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 删除,重启已停止的compose应用容器</span><br></pre></td></tr></table></figure></p><p>docker-compose rm #删除.删除应用相关的容器,但是不会删除卷和镜像和网络.<br>docker-compose restart #重启<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### </span><br><span class="line"></span><br><span class="line">#### 拉取服务镜像</span><br><span class="line"></span><br><span class="line">```docker-compose pull server_name ```这个命令会先拉取服务镜像到本地.例如在本文的docker compose例子中有2个服务:web-fe和redis.如果执行下列命令,会仅仅拉取redis镜像到本地</span><br></pre></td></tr></table></figure></p><p>docker-compose pull redis<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 其他命令</span><br><span class="line"></span><br><span class="line">日常docker管理容器的命令都可以使用```docker-compose```替代.例如:</span><br></pre></td></tr></table></figure></p><p>docker-compose logs service_name #查看服务容器日志<br>docker-compose exec service_name #开启终端登陆容器<br>docker-compose kill -s SIGINT    #杀死docker-compose服务容器<br>docker-compose ps                #列出容器<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### docker-compose配置文件指令解析</span><br><span class="line"></span><br><span class="line">以下配置文件以版本3.x为例.官网参考:&lt;https://docs.docker.com/compose/compose-file/&gt;</span><br><span class="line"></span><br><span class="line">下面是个包含完整指令的样例:</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:</p><p>  redis:<br>    image: redis:alpine<br>    ports:</p><pre><code>  - &quot;6379&quot;networks:  - frontenddeploy:  replicas: 2  update_config:    parallelism: 2    delay: 10s  restart_policy:    condition: on-failure</code></pre><p>  db:<br>    image: postgres:9.4<br>    volumes:</p><pre><code>  - db-data:/var/lib/postgresql/datanetworks:  - backenddeploy:  placement:    constraints: [node.role == manager]</code></pre><p>  vote:<br>    image: dockersamples/examplevotingapp_vote:before<br>    ports:</p><pre><code>  - &quot;5000:80&quot;networks:  - frontenddepends_on:  - redisdeploy:  replicas: 2  update_config:    parallelism: 2  restart_policy:    condition: on-failure</code></pre><p>  result:<br>    image: dockersamples/examplevotingapp_result:before<br>    ports:</p><pre><code>  - &quot;5001:80&quot;networks:  - backenddepends_on:  - dbdeploy:  replicas: 1  update_config:    parallelism: 2    delay: 10s  restart_policy:    condition: on-failure</code></pre><p>  worker:<br>    image: dockersamples/examplevotingapp_worker<br>    networks:</p><pre><code>  - frontend  - backenddeploy:  mode: replicated  replicas: 1  labels: [APP=VOTING]  restart_policy:    condition: on-failure    delay: 10s    max_attempts: 3    window: 120s  placement:    constraints: [node.role == manager]</code></pre><p>  visualizer:<br>    image: dockersamples/visualizer:stable<br>    ports:</p><pre><code>  - &quot;8080:8080&quot;stop_grace_period: 1m30svolumes:  - &quot;/var/run/docker.sock:/var/run/docker.sock&quot;deploy:  placement:    constraints: [node.role == manager]</code></pre><p>networks:<br>  frontend:<br>  backend:</p><p>volumes:<br>  db-data:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Service块级别的配置文件指令</span><br><span class="line"></span><br><span class="line">#### build </span><br><span class="line"></span><br><span class="line">build可以指定一个目录或者在build下还可以指定context上下文环境和docker-file文件名</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  webapp:<br>    build: ./dir</p><p>或者<br>webapp:<br>    build:<br>      context: ./dir<br>      dockerfile: Dockerfile-alternate #如果指定dockerfile,则必须要指定一个build路径,也就是context<br>      args:<br>        buildno: 1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">如果同时指定了Image关键字,那么会构建一个指定的镜像名:tag</span><br></pre></td></tr></table></figure></p><p>build: ./dir<br>image: webapp:tag  #构建webapp:tag的镜像名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; build选项在swarm中部署stack时是无效的,因为docker stack命令只接受已经build好的镜像</span><br><span class="line"></span><br><span class="line">#### CONTEXT</span><br><span class="line"></span><br><span class="line">定义上下文目录.如果是一个相对目录,那么是相对Compose file文件的目录.</span><br><span class="line"></span><br><span class="line">#### ARGS</span><br><span class="line"></span><br><span class="line">在build过程中可以允许使用ARGS变量传递给dockerfile.具体用法参考官网</span><br><span class="line"></span><br><span class="line">#### COMMAND</span><br><span class="line"></span><br><span class="line">重写dockerfile或者镜像中的默认命令.和dockerfile一样可以是shell方式也可以是exec方式执行</span><br></pre></td></tr></table></figure></p><p>command: bundle exec thin -p 3000<br>command: [“bundle”, “exec”, “thin”, “-p”, “3000”]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### configs</span><br><span class="line"></span><br><span class="line">授予每个service的配置文件访问.具体用法参考官网</span><br><span class="line"></span><br><span class="line">#### container_name</span><br><span class="line"></span><br><span class="line">指定一个容器名,而不是使用默认名字</span><br></pre></td></tr></table></figure></p><p>container_name: my-web-container<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 需要注意的是由于容器名必须唯一,所以当扩展多个容器副本时,指定一个具体的容器名会报错,所以这个指令在swarm模式下部署stack时会被忽略</span><br><span class="line"></span><br><span class="line">#### depends_on</span><br><span class="line"></span><br><span class="line">用于在多个services之间指定依赖性.service dependencies会导致以下行为</span><br><span class="line"></span><br><span class="line">* ```docker-compose up``` 启动时会参考depndency顺序.在下面这个例子中.db和redis服务会先于web服务启动</span><br><span class="line">* ```docker-compose up SERVICE``` 会自动启动该SERVICE的依赖服务.在下面例子中```docker-compose up web```命令会自动创建和启动db和redis</span><br><span class="line">* ```docker-compose stop```会参考依赖顺序而停止服务.在下面例子中,web服务会先于db和redis服务停止</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  web:<br>    build: .<br>    depends_on:</p><pre><code>- db- redis</code></pre><p>  redis:<br>    image: redis<br>  db:<br>    image: postgres<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 使用depens_on需要注意以下几点:</span><br><span class="line">&gt;</span><br><span class="line">&gt; 1.denpds_on只会在web依赖的服务启动后就启动web服务,而不是等待db和redis服务启动并且处于ready状态才启动web.这有可能会带来一些问题,比如mysql启动较慢,数据库还没准备好等.如果你需要确定后端的db,redis数据库启动成功,并且可以连接时才启动web服务,可以参考https://docs.docker.com/compose/startup-order/</span><br><span class="line">&gt;</span><br><span class="line">&gt; 2.version3版本不再支持depends_on下的condition指令</span><br><span class="line">&gt;</span><br><span class="line">&gt; 3.version3版本的depends_on选项在swarm模式下部署stack时会被忽略</span><br><span class="line"></span><br><span class="line">depends_on选项的控制启动顺序参考:</span><br><span class="line"></span><br><span class="line">编写一个shell脚本循环判断后端的数据库是否ready.如果ready则执行CMD命令.然后在command指令中指定脚本的后端db数据库服务名,以及CMD命令参数</span><br></pre></td></tr></table></figure></p><p>#!/bin/sh</p><h1 id="wait-for-postgres-sh"><a href="#wait-for-postgres-sh" class="headerlink" title="wait-for-postgres.sh"></a>wait-for-postgres.sh</h1><p>#循环测试db服务($1参数)的状态.一旦可以连接了,执行cmd命令(python app.py)<br>set -e</p><p>host=”$1”<br>shift<br>cmd=”$@”</p><p>until PGPASSWORD=$POSTGRES_PASSWORD psql -h “$host” -U “postgres” -c ‘\q’; do</p><blockquote><p>&amp;2 echo “Postgres is unavailable - sleeping”<br>  sleep 1<br>done</p></blockquote><blockquote><p>&amp;2 echo “Postgres is up - executing command”<br>exec $cmd<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p></blockquote><p>command: [“./wait-for-postgres.sh”, “db”, “python”, “app.py”]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### deploy指令</span><br><span class="line"></span><br><span class="line">该指令用于配置服务相关的配置和部署方式.这个指令只在version3版本支持,而且只在swarm模式下才生效.单机```docker-compose up```方式执行会被忽略</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  redis:<br>    image: redis:alpine<br>    deploy:<br>      replicas: 6<br>      update_config:<br>        parallelism: 2<br>        delay: 10s<br>      restart_policy:<br>        condition: on-failure<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">下面是有关deploy的几个子指令介绍</span><br><span class="line"></span><br><span class="line">* ENDPOINT_MODE</span><br><span class="line"></span><br><span class="line">&gt; version 3.3 only</span><br><span class="line"></span><br><span class="line">```endpoint_mode: VIP```  Docker为service分配一个虚拟IP.作为用户的前端入口.docker路由用户请求到所有可用的worker节点..这也是默认模式</span><br><span class="line"></span><br><span class="line">```endpoint_mode: dnsrr``` DNS轮询服务,Docker发起一个service name的DNS查询,并且返回一个包含多个IP地址的列表.客户端通过轮询方式链接其中一个IP地址.</span><br><span class="line"></span><br><span class="line">* LABLES</span><br><span class="line"></span><br><span class="line">为service指定一个标签.只对service生效,无法为service的具体某个容器生效</span><br><span class="line"></span><br><span class="line">* MODE</span><br><span class="line"></span><br><span class="line">mode定义了在swarm节点上的副本部署模式,有global和replicated两种模式,默认是replicated</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  worker:<br>    image: dockersamples/examplevotingapp_worker<br>    deploy:<br>      mode: global<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">关于两种模式的区别参考:&lt;https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/#replicated-and-global-services&gt;</span><br><span class="line"></span><br><span class="line">* REPLACEMENT</span><br><span class="line"></span><br><span class="line">定义了constrants(约束条件)和preferences.的参数.</span><br><span class="line"></span><br><span class="line">下面是constrants指令的用法:</span><br><span class="line"></span><br><span class="line">constrants指令可以限制某个task在哪些swarm节点上运行.多个constrants指令是逻辑AND的关系来匹配满足条件的nodes.constrants可以匹配swarm节点或者Docker引擎标签:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">| node attribute  | matches                  | example                                       |</span><br><span class="line">| :-------------- | :----------------------- | :-------------------------------------------- |</span><br><span class="line">| `node.id`       | Node ID                  | `node.id==2ivku8v2gvtg4`                      |</span><br><span class="line">| `node.hostname` | Node hostname            | `node.hostname!=node-2`                       |</span><br><span class="line">| `node.role`     | Node role                | `node.role==manager`                          |</span><br><span class="line">| `node.labels`   | user defined node labels | `node.labels.security==high`                  |</span><br><span class="line">| `engine.labels` | Docker Engine&apos;s labels   | `engine.labels.operatingsystem==ubuntu 14.04` |</span><br><span class="line"></span><br><span class="line">例如下面这个例子中限制redis service的task运行在lable标签等于queue的swarm节点</span><br></pre></td></tr></table></figure></p><p>$ docker service create \<br>  –name redis_2 \<br>  –constraint ‘node.labels.type == queue’ \<br>  redis:3.0.6<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">回到刚才REPLACEMENT的例子,下面的例子中表示db service只运行在swarm manager节点,而且docker node节点的操作系统是Ubuntu 14.04</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  db:<br>    image: postgres<br>    deploy:<br>      placement:<br>        constraints:</p><pre><code>  - node.role == manager  - engine.labels.operatingsystem == ubuntu 14.04preferences:  - spread: node.labels.zone</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* REPLICAS</span><br><span class="line"></span><br><span class="line">如果service 是replicated模式(默认模式),定义容器的启动数量.在下面的例子中启动6个worker容器</span><br></pre></td></tr></table></figure><p>version: “3.7”<br>services:<br>  worker:<br>    image: dockersamples/examplevotingapp_worker<br>    networks:</p><pre><code>  - frontend  - backenddeploy:  mode: replicated  replicas: 6</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* RESOURCES</span><br><span class="line"></span><br><span class="line">配置容器限定的使用资源</span><br><span class="line"></span><br><span class="line">在下面这个例子中.redis service被限制只允许使用不超过50M内存,以及0.5的CPU处理器时间(单个CPU内核的50%).并且有20M内存和0.25的CPU处理器时间预留(也就是永远为redis service保留)</span><br></pre></td></tr></table></figure><p>version: “3.7”<br>services:<br>  redis:<br>    image: redis:alpine<br>    deploy:<br>      resources:<br>        limits:<br>          cpus: ‘0.50’<br>          memory: 50M<br>        reservations:<br>          cpus: ‘0.25’<br>          memory: 20M<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">**Out Of Memory Exceptions[OOME]**</span><br><span class="line"></span><br><span class="line">如果service 或者容器使用了超过限制的资源.容器或者docker引擎就会出现OOME错误.docker进程可能会被内核OOM killer给kill掉.关于如何规避这种问题,请参考[understand the risks of running out of memory](&lt;https://docs.docker.com/config/containers/resource_constraints/&gt;)</span><br><span class="line"></span><br><span class="line">* RESTART_POLICY</span><br><span class="line"></span><br><span class="line">配置当容器停止时如何重新启动容器的策略.有以下几种子指令</span><br><span class="line"></span><br><span class="line">1.```condition``` 重启容器的约束条件.有: ```none```,```on-failure```,和```any```(default:any)</span><br><span class="line"></span><br><span class="line">2.```delay```: 尝试重启容器的时间间隔.默认是0</span><br><span class="line"></span><br><span class="line">3.```max_attempts```:如果容器重启失败,重启最大尝试次数,默认是一直尝试</span><br><span class="line"></span><br><span class="line">4.```window```:重启后等待多久认定重启成功.默认是immediately</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  redis:<br>    image: redis:alpine<br>    deploy:<br>      restart_policy:<br>        condition: on-failure<br>        delay: 5s<br>        max_attempts: 3<br>        window: 120s<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### ROLLBACK_CONFIG</span><br><span class="line"></span><br><span class="line">更新失败的回滚指令.</span><br><span class="line"></span><br><span class="line">#### UPDATE_CONFIG</span><br><span class="line"></span><br><span class="line">配置service如何进行滚动更新.</span><br><span class="line"></span><br><span class="line">#### DNS</span><br><span class="line"></span><br><span class="line">自定义DNS地址,可以是单个值或者一个列表</span><br></pre></td></tr></table></figure></p><p>dns: 8.8.8.8</p><p>dns:</p><ul><li>8.8.8.8</li><li>9.9.9.9<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### entrypoint</span><br><span class="line"></span><br><span class="line">重写dockerfile或者镜像中的entrypoint指令</span><br></pre></td></tr></table></figure></li></ul><p>entrypoint: /code/entrypoint.sh</p><p>#也可以是一个列表格式:</p><p>entrypoint:</p><pre><code>- php- -d- zend_extension=/usr/local/lib/php/extensions/no-debug-non-zts-20100525/xdebug.so- -d- memory_limit=-1- vendor/bin/phpunit</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### env_file</span><br><span class="line"></span><br><span class="line">添加环境变量文件,可以是单个值,也可以是个列表.该文件最好是在当前docker-compose文件目录或者子目录下.</span><br><span class="line"></span><br><span class="line">如果是指定多个变量文件,而且有重复的变量且赋值不同,那么以最后一个变量文件的变量为准</span><br></pre></td></tr></table></figure><p>services:<br>  some-service:<br>    env_file:</p><pre><code>- a.env- b.env</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### environment</span><br><span class="line"></span><br><span class="line">添加环境变量.可以使用列表格式,或者字典格式</span><br></pre></td></tr></table></figure><p>environment:<br>  RACK_ENV: development<br>  SHOW: ‘true’<br>  SESSION_SECRET:</p><p>environment:</p><ul><li>RACK_ENV=development</li><li>SHOW=true</li><li>SESSION_SECRET<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### extra_hosts</span><br><span class="line"></span><br><span class="line">添加hostname和IP地址的绑定映射到hosts文件</span><br></pre></td></tr></table></figure></li></ul><p>extra_hosts:</p><ul><li>“somehost:162.242.195.82”</li><li><p>“otherhost:50.31.209.229”</p><p>#会在容器内的/etc/hosts文件生成如何内容<br>162.242.195.82  somehost<br>50.31.209.229   otherhost</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### healthcheck</span><br><span class="line"></span><br><span class="line">检查service的各个容器是否处于&quot;healthy&quot;状态.例如下面的例子</span><br></pre></td></tr></table></figure></li></ul><p>healthcheck:<br>  test: [“CMD”, “curl”, “-f”, “<a href="http://localhost&quot;]" target="_blank" rel="noopener">http://localhost&quot;]</a><br>  interval: 1m30s<br>  timeout: 10s<br>  retries: 3<br>  start_period: 40s<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```test```指令必须为一个字符串或者一个列表.如果是个上例子中的列表格式.则第一个参数必须为```NONE```,```CMD```,或者```CMD-SHELL```.如果是字符串相当于指定了```CMD-SHELL```参数</span><br><span class="line"></span><br><span class="line">下面2个写法和上文的例子效果一样</span><br></pre></td></tr></table></figure></p><p>test: [“CMD-SHELL”, “curl -f <a href="http://localhost" target="_blank" rel="noopener">http://localhost</a> || exit 1”]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>test: curl -f <a href="https://localhost" target="_blank" rel="noopener">https://localhost</a> || exit 1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">要关闭健康检查,可以使用```disable:true```.等同于```test:[&quot;NONE&quot;]</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">healthcheck:</span><br><span class="line">  disable: true</span><br></pre></td></tr></table></figure><h4 id="image"><a href="#image" class="headerlink" title="image"></a>image</h4><p>指定容器的启动镜像.可以是指定的repository/tag 或者一个镜像ID.如果本地不存在该镜像,会尝试去pull镜像到本地.如果指定了<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">下面这几种写法均正确</span><br></pre></td></tr></table></figure></p><p>image: redis<br>image: ubuntu:14.04<br>image: tutum/influxdb<br>image: example-registry.com:4000/postgresql<br>image: a4bc65fd<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### logging</span><br><span class="line"></span><br><span class="line">service的log配置.下面的例子中指定了一个syslog服务器的地址</span><br></pre></td></tr></table></figure></p><p>logging:<br>  driver: syslog<br>  options:<br>    syslog-address: “tcp://192.168.0.42:123”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```driver```为容器指定logging的驱动,一共以下3种驱动方式,默认是json-file</span><br></pre></td></tr></table></figure></p><p>driver: “json-file”<br>driver: “syslog”<br>driver: “none”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">也可以限定json-file驱动的日志转出.例如下列指定了最大的日志文件大小和日志保留份数</span><br></pre></td></tr></table></figure></p><p>options:<br>  max-size: “200k”<br>  max-file: “10”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### network_mode</span><br><span class="line"></span><br><span class="line">指定网络模式,有以下几种网络模式</span><br></pre></td></tr></table></figure></p><p>network_mode: “bridge”<br>network_mode: “host”<br>network_mode: “none”<br>network_mode: “service:[service name]”<br>network_mode: “container:[container name/id]”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### networks</span><br><span class="line"></span><br><span class="line">services加入的网络.这些网络名在顶级```network```指令中有指定</span><br></pre></td></tr></table></figure></p><p>services:<br>  some-service:<br>    networks:</p><pre><code>- some-network- other-network</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### IPV4_ADDRESS,IPV6_ADDRESS</span><br><span class="line"></span><br><span class="line">为容器指定一个静态的IP地址.但是对应的Network顶级指令中必须指定一个ipam块,定义该网络的IP子网范围.例如</span><br><span class="line"></span><br><span class="line">app_net网络的ipam快中指定了172.16.238.0/24的子网.然后为app service指定一个静态IP</span><br></pre></td></tr></table></figure><p>version: “3.7”</p><p>services:<br>  app:<br>    image: nginx:alpine<br>    networks:<br>      app_net:<br>        ipv4_address: 172.16.238.10<br>        ipv6_address: 2001:3984:3989::10</p><p>networks:<br>  app_net:<br>    ipam:<br>      driver: default<br>      config:</p><pre><code>- subnet: &quot;172.16.238.0/24&quot;- subnet: &quot;2001:3984:3989::/64&quot;</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### ports</span><br><span class="line"></span><br><span class="line">暴露端口.下面是几种短格式写法.推荐将端口用双引号括起来</span><br></pre></td></tr></table></figure><p>ports:</p><ul><li>“3000”</li><li>“3000-3005”</li><li>“8000:8000”</li><li>“9090-9091:8080-8081”</li><li>“49100:22”</li><li>“127.0.0.1:8001:8001”</li><li>“127.0.0.1:5000-5010:5000-5010”</li><li>“6060:6060/udp”<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 此外还有完整格式的写法.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### restart</span><br><span class="line"></span><br><span class="line">默认的restart策略是```no```有以下四种重启策略</span><br></pre></td></tr></table></figure></li></ul><p>restart: “no”<br>restart: always<br>restart: on-failure<br>restart: unless-stopped<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### sysctls</span><br><span class="line"></span><br><span class="line">配置容器的内核参数.可以是数组或者字典类型</span><br></pre></td></tr></table></figure></p><p>sysctls:<br>  net.core.somaxconn: 1024<br>  net.ipv4.tcp_syncookies: 0</p><p>sysctls:</p><ul><li>net.core.somaxconn=1024</li><li>net.ipv4.tcp_syncookies=0<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### ulimits</span><br><span class="line"></span><br><span class="line">配置ulimits</span><br></pre></td></tr></table></figure></li></ul><p>ulimits:<br>  nproc: 65535<br>  nofile:<br>    soft: 20000<br>    hard: 40000<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### volumes</span><br><span class="line"></span><br><span class="line">挂载一个路径或者一个卷.</span><br><span class="line"></span><br><span class="line">如果是在service层级挂载宿主机上的路径到容器,那么不需要在顶级指令中定义```volumes```key.但是如果是挂载一个卷到多个service,可以在顶级指令中定义个卷名.然后使用这个卷名去挂载</span><br><span class="line"></span><br><span class="line">下面这个例子在顶级```volumes```指令中定义了2个卷名:mydata和dbdata. mydata被web service挂载.dbdata被db service挂载.下面2个挂载格式都可以.</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  web:<br>    image: nginx:alpine<br>    volumes:</p><pre><code>- type: volume  source: mydata  target: /data  volume:    nocopy: true- type: bind  source: ./static  target: /opt/app/static</code></pre><p>  db:<br>    image: postgres:latest<br>    volumes:</p><pre><code>- &quot;/var/run/postgres/postgres.sock:/var/run/postgres/postgres.sock&quot;- &quot;dbdata:/var/lib/postgresql/data&quot;</code></pre><p>volumes:<br>  mydata:<br>  dbdata:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">简短格式写法</span><br></pre></td></tr></table></figure></p><p>volumes:</p><h1 id="Just-specify-a-path-and-let-the-Engine-create-a-volume"><a href="#Just-specify-a-path-and-let-the-Engine-create-a-volume" class="headerlink" title="Just specify a path and let the Engine create a volume"></a>Just specify a path and let the Engine create a volume</h1><ul><li><p>/var/lib/mysql</p><h1 id="Specify-an-absolute-path-mapping"><a href="#Specify-an-absolute-path-mapping" class="headerlink" title="Specify an absolute path mapping"></a>Specify an absolute path mapping</h1></li><li><p>/opt/data:/var/lib/mysql</p><h1 id="Path-on-the-host-relative-to-the-Compose-file"><a href="#Path-on-the-host-relative-to-the-Compose-file" class="headerlink" title="Path on the host, relative to the Compose file"></a>Path on the host, relative to the Compose file</h1></li><li><p>./cache:/tmp/cache</p><h1 id="User-relative-path"><a href="#User-relative-path" class="headerlink" title="User-relative path"></a>User-relative path</h1></li><li><p>~/configs:/etc/configs/:ro</p><h1 id="Named-volume"><a href="#Named-volume" class="headerlink" title="Named volume"></a>Named volume</h1></li><li>datavolume:/var/lib/mysql<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">完整格式写法</span><br></pre></td></tr></table></figure></li></ul><p>version: “3.7”<br>services:<br>  web:<br>    image: nginx:alpine<br>    ports:</p><pre><code>  - &quot;80:80&quot;volumes:  - type: volume    source: mydata    target: /data    volume:      nocopy: true  - type: bind    source: ./static    target: /opt/app/static</code></pre><p>networks:<br>  webnet:</p><p>volumes:<br>  mydata:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## Volume块级别配置文件指令</span><br><span class="line"></span><br><span class="line">大部分volume的用法在上面都已经解释过了,Volume的顶级指令配置不多.</span><br><span class="line"></span><br><span class="line">#### driver</span><br><span class="line"></span><br><span class="line">指定volume卷的驱动,docker引擎默认指定的驱动是```local```.</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## Network块级别配置文件指令</span><br><span class="line"></span><br><span class="line">详细的docker network特性以及所有network驱动请见:[Network guide](&lt;https://docs.docker.com/compose/networking/&gt;)</span><br><span class="line"></span><br><span class="line">默认情况下Compose启动单一网络,一个services的每个容器加入到默认的网络,并且该网络下的所有容器之间都能互相访问</span><br><span class="line"></span><br><span class="line">假如下面的compose文件在```myapp```目录下.</span><br></pre></td></tr></table></figure></p><p>version: “3”<br>services:<br>  web:<br>    build: .<br>    ports:</p><pre><code>- &quot;8000:8000&quot;</code></pre><p>  db:<br>    image: postgres<br>    ports:</p><pre><code>- &quot;8001:5432&quot;</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">当执行```docker-compose up```命令启动时,会执行以下几个步骤</span><br><span class="line"></span><br><span class="line">1.创建一个```myapp_default```的网络</span><br><span class="line"></span><br><span class="line">2.使用web的配置文件启动一个容器,加入到```myapp_default```网络中</span><br><span class="line"></span><br><span class="line">3.使用db的配置文件启动一个容器.加入到```myapp_default```的网络中</span><br><span class="line"></span><br><span class="line">所有容器成功启动后,每个容器都能访问对方的```hostname```和对方的IP地址.</span><br><span class="line"></span><br><span class="line">另外,需要理解```HOST_PORT```和```COMTAINER_PORT```的区别.在上面这个例子中,db的```host_port```是8001,容器的端口是5432,services之间的容器都是通过```CONTAINER_PORT```也就是容器的IP进行通信的.例如访问数据库地址应该是:```postgres://db:5432</span><br></pre></td></tr></table></figure><h4 id="driver"><a href="#driver" class="headerlink" title="driver"></a>driver</h4><p>指定网络驱动.在单主机下Docker引擎默认使用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>driver: overlay<br><code>`</code></p><hr><h2 id="configs和secrets块级别配置文件指令"><a href="#configs和secrets块级别配置文件指令" class="headerlink" title="configs和secrets块级别配置文件指令"></a>configs和secrets块级别配置文件指令</h2><p>还不是很懂这个怎么用的,以后有空再研究</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker学习笔记——docker-compose&quot;&gt;&lt;a href=&quot;#docker学习笔记——docker-compose&quot; class=&quot;headerlink&quot; title=&quot;docker学习笔记——docker-compose&quot;&gt;&lt;/a&gt;docker学习笔记——docker-compose&lt;/h2&gt;&lt;p&gt;docker compose 定义并且运行多个docker容器.使用YAML风格文件定义一个compose文件.利用compose文件创建和启动所有服务.&lt;/p&gt;
&lt;p&gt;使用docker compose基本只需要3个步骤&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在Dockerfile文件定义app环境&lt;/li&gt;
&lt;li&gt;在docker-compose.yml文件中定义组成app的各个服务&lt;/li&gt;
&lt;li&gt;run docker-compose up 和compose 启动和运行app&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面文档均可以在docker-compose官方找到详细资料:&lt;a href=&quot;https://docs.docker.com/compose/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;docker-compose&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;docker-compose安装&quot;&gt;&lt;a href=&quot;#docker-compose安装&quot; class=&quot;headerlink&quot; title=&quot;docker-compose安装&quot;&gt;&lt;/a&gt;docker-compose安装&lt;/h3&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---Docker镜像篇</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%95%9C%E5%83%8F%E7%AF%87/"/>
    <id>https://jesse.top/2020/06/29/docker/docker学习笔记——镜像篇/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:28:47.322Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker笔记——镜像篇"><a href="#docker笔记——镜像篇" class="headerlink" title="docker笔记——镜像篇"></a>docker笔记——镜像篇</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>docker镜像是一个只读的Docker容器模板.含有启动docker容器所需的文件系统结构以及内容.因此是启动一个容器的基础.docker镜像的文件内容以及一些运行docker容器的配置文件组成了docker容器的静态文件运行环境—rootfs.</p><p>可以这么理解,docker镜像是docker容器的静态视角.docker容器是docker镜像的运行状态</p><hr><p><strong>1.rootfs</strong></p><p>rootfs是docker容器的根目录.如:/dev,/proc,/bin,/etc …….传统的Linux容器操作系统内核启动时,首先挂载一个只读(read-only)的rootfs.当系统检测到完整性后,再将其切换到读写(read-write)模式.而在docker架构中.也沿用了Linux内核的启动方法.在docker为容器挂载rootfs时,将rootfs设置为只读模式,挂载完毕后,在已有的只读rootfs上再挂载一个读写层.</p><p>读写层位于docker容器文件系统的最顶层.下面可能挂载了多个只读层.</p><a id="more"></a><p><strong>2.docker镜像的特点</strong></p><ul><li><strong>分层</strong></li></ul><p>每个镜像都由一系列的”镜像层”组成.当需要修改容器镜像内的某个文件时,只对最上方的读写层进行修改,不覆盖下面的只读层文件系统.例如删除一个只读文件系统中的文件时,只会在读写层标记这个文件”已经被删除”,但是这个文件在只读层中仍然存在.只不过不被用户感知.</p><ul><li><strong>写时复制(copy-on-write)</strong></li></ul><p>每个容器在启动的时候并不需要单独复制一份镜像文件,而是将所有镜像层以只读的方式挂载到一个挂载点,在多个容器之间共享.在未更改镜像文件内容时,所有容器共享一份数据,只有在docker容器运行过程中修改过文件时,才会把变化的文件内容写到读写层.并隐藏只读层中的老版本文件.</p><p>写时复制机制减少了镜像对磁盘空间的占用和容器的启动时间</p><ul><li><strong>联合挂载</strong></li></ul><p>联合挂载技术可以在一个挂载点同时挂载多个文件系统.实现这种联合挂载技术的文件系统被称为联合文件系统(union filesystem).从内核的角度来看,docker容器的文件系统分为只读rootfs层和读写层.但是在用户的视角看来,整个文件系统都是rootfs底层.</p><p>下面这个图可以理解,镜像是由一堆的只读层堆叠起来的统一视角:</p><p><img src="![Docker镜像](https://img1.jesse.top/docker-image1.gif" alt=""></p><p>下面这个图理解了docker镜像和docker容器的区别</p><p><img src="https://img1.jesse.top/docker-container1.png" alt=""></p><hr><h3 id="docker镜像的相关概念"><a href="#docker镜像的相关概念" class="headerlink" title="docker镜像的相关概念"></a>docker镜像的相关概念</h3><p>1.<strong>registry</strong></p><p>registry用来保存docker镜像.可以将registry简单的想象成类似于git仓库之类的实体.当<figure class="highlight docker"><figcaption><span>run```命令启动一个容器时,如果宿主机上并不存在该镜像,那么docker将从registry中下载镜像并保存到宿主机</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">可以使用docker官方的公共registry服务(docker hub),可以可以使用阿里云私有的registry,甚至还可以自己搭建私有的registry</span><br><span class="line"></span><br><span class="line">**<span class="number">2</span>.repository**</span><br><span class="line"></span><br><span class="line">repository是由具有某个功能的docker镜像的所有迭代版本构成的镜像组.repository通常表示镜像所具有的功能,例如ansible/ubunbu14.<span class="number">4</span>-ansible.而顶层仓库则只包含repository名.例如,Ubuntu</span><br><span class="line"></span><br><span class="line">repository是一个镜像集合,包含了多个不同版本的镜像.使用标签进行版本区分,例如ubuntu:<span class="number">14.04</span>,ubuntu12.<span class="number">04</span>.他们均属于ubuntu这个repository</span><br><span class="line"></span><br><span class="line">**总而言之,registry是repository的集合,repository是镜像的集合**</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"><span class="comment">### docker镜像相关的命令</span></span><br><span class="line"></span><br><span class="line">* **拉取镜像**</span><br><span class="line"></span><br><span class="line">```docker pull [OPTIONS] NAME[:TAG|@DIGEST]</span><br></pre></td></tr></table></figure></p><p>如果只指定了镜像名,则默认从docker hub官方拉取该镜像的最新latest版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker pull nginx</span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library/nginx</span><br><span class="line">743f2d6c1f65: Pull complete</span><br><span class="line">6bfc4ec4420a: Pull complete</span><br><span class="line">688a776db95f: Pull complete</span><br><span class="line">Digest: sha256:23b4dcdf0d34d4a129755fc6f52e1c6e23bb34ea011b315d87e193033bcd1b68</span><br><span class="line">Status: Downloaded newer image for nginx:latest</span><br></pre></td></tr></table></figure><p>如果指定了tag,则拉取指定的版本镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker pull nginx:1.15</span><br><span class="line">1.15: Pulling from library/nginx</span><br><span class="line">Digest: sha256:23b4dcdf0d34d4a129755fc6f52e1c6e23bb34ea011b315d87e193033bcd1b68</span><br><span class="line">Status: Downloaded newer image for nginx:1.15</span><br></pre></td></tr></table></figure><p>拉取我阿里云的私人registry下的镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#registry.cn-hangzhou.aliyuncs.com/jesse_images为仓库地址</span><br><span class="line">#php7.1.9为镜像版本</span><br><span class="line">[root@localhost ~]$docker pull registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:php7.1.9</span><br><span class="line"></span><br><span class="line">php7.1.9: Pulling from jesse_images/jesse_images</span><br><span class="line">Digest: sha256:ed9b7326b539f47a81697e51ed8ec698bec49fb62959990c1277d068fc55ff94</span><br><span class="line">Status: Downloaded newer image for registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:php7.1.9</span><br></pre></td></tr></table></figure><hr><ul><li><strong>删除镜像</strong></li></ul><p>命令格式:</p><figure class="highlight docker"><figcaption><span>rmi [OPTIONS] IMAGE [IMAGE…]```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">可以是docker rmi 镜像ID 或者 docker rmi 镜像名:tag</span><br></pre></td></tr></table></figure><p>docker images #查看当前宿主机上的镜像<br>[root@localhost ~]$docker images<br>REPOSITORY                                                    TAG                    IMAGE ID            CREATED             SIZE<br>busybox                                                       latest                 64f5d945efcc        6 days ago          1.2MB<br>nginx                                                         1.15                   53f3fd8007f7        7 days ago          109MB<br>nginx                                                         latest                 53f3fd8007f7        7 days ago          109MB<br>php-swoole                                                    7.1                    aa71c42a22ca        9 days ago          588MB</p><p><none>                                                        <none>                 01f5d7914e61        9 days ago          585MB</none></none></p><p>#删除镜像ID为01f5d7914e61的镜像</p><p>[root@localhost ~]$docker rmi 01f5d7914e61<br>Deleted: sha256:01f5d7914e615b0e2f7cc36a494c876dfc0c678963898374d9ef512d7a762aac<br>Deleted: sha256:b4dd4a057d2561647ff7bf6b299a143c99f66831c129618f49bca5e6ac82f99e<br>Deleted: sha256:c37c880338efd3d340bfa71b35b7653b6cec8eb4f5dfcfab8c7ad0045fef3ce6<br>Deleted: sha256:fb7d015f8921c1244134730b6c21f0bda6c7156ccd421d9e0069d5a1074b48dd<br>Deleted: sha256:ab74760ab0af7680fa9338100c92306392ffeb384b8976045a11dab9a4ebbc57<br>Deleted: sha256:8544a2552375c861955db9034e9c3c5a3e83530b84de9b9bb6d4a7d0d5e5b8ac<br>Deleted: sha256:4eebc2d39a0733b28992a064fc71852297927a3994b01a9d1123d71b042ab729</p><p>#删除nginx.tag为1.15的镜像<br>[root@localhost ~]$docker rmi nginx:1.15<br>Untagged: nginx:1.15<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">* **查看镜像**</span><br><span class="line"></span><br><span class="line">```docker history 镜像名```可以看到镜像的构建分层</span><br></pre></td></tr></table></figure></p><p>[root@localhost ~]$docker history nginx<br>IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT<br>53f3fd8007f7        7 days ago          /bin/sh -c #(nop)  CMD [“nginx” “-g” “daemon…   0B</p><p><missing>           7 days ago          /bin/sh -c #(nop)  STOPSIGNAL SIGTERM           0B</missing></p><p><missing>           7 days ago          /bin/sh -c #(nop)  EXPOSE 80                    0B</missing></p><p><missing>           7 days ago          /bin/sh -c ln -sf /dev/stdout /var/log/nginx…   0B</missing></p><p><missing>           7 days ago          /bin/sh -c set -x  &amp;&amp; apt-get update  &amp;&amp; apt…   54.1MB</missing></p><p><missing>           7 days ago          /bin/sh -c #(nop)  ENV NJS_VERSION=1.15.12.0…   0B</missing></p><p><missing>           7 days ago          /bin/sh -c #(nop)  ENV NGINX_VERSION=1.15.12…   0B</missing></p><p><missing>           7 days ago          /bin/sh -c #(nop)  LABEL maintainer=NGINX Do…   0B</missing></p><p><missing>           8 days ago          /bin/sh -c #(nop)  CMD [“bash”]                 0B</missing></p><p><missing>           8 days ago          /bin/sh -c #(nop) ADD file:fcb9328ea4c115670…   55.3MB<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```docker inspect 镜像名``` 可以看到镜像的具体信息</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">* **创建镜像**</span><br><span class="line"></span><br><span class="line">```docker commit```命令可以基于现有的容器创建出一个镜像</span><br></pre></td></tr></table></figure></missing></p><p>#用法格式:<br>docker commit -m ‘镜像说明信息’   -a  作者  容器ID 镜像名:版本</p><p>[root@localhost ~]$docker commit -h</p><p>Usage:    docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]</p><p>Create a new image from a container’s changes</p><p>Options:<br>  -a, –author string    Author (e.g., “John Hannibal Smith <a href="mailto:&#x68;&#x61;&#110;&#x6e;&#x69;&#98;&#97;&#108;&#64;&#x61;&#45;&#116;&#101;&#97;&#x6d;&#46;&#x63;&#111;&#x6d;" target="_blank" rel="noopener">&#x68;&#x61;&#110;&#x6e;&#x69;&#98;&#97;&#108;&#64;&#x61;&#45;&#116;&#101;&#97;&#x6d;&#46;&#x63;&#111;&#x6d;</a>“)<br>  -c, –change list      Apply Dockerfile instruction to the created image<br>  -m, –message string   Commit message<br>  -p, –pause            Pause container during commit (default true)</p><p>  #例如.将正在运行中的Nginx容器提交为一个新的nginx:test镜像<br>[root@localhost ~]$docker commit -m ‘test’ -a ‘jesse’ nginx nginx:test<br>sha256:028f5e2b21a66a1bf5f70727f20cac04e8918f57d5584cc2aeb09f18791d9680<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">* 导入导出镜像</span><br><span class="line"></span><br><span class="line">命令: </span><br><span class="line"></span><br><span class="line">```docker save -o 保存文件名 镜像名:tag```  ————将某个镜像保存为一个文件</span><br><span class="line"></span><br><span class="line">```docker load &lt; 文件名``` or ```docker load —input 文件名``` ——将某个文件导入到本地镜像</span><br><span class="line"></span><br><span class="line">例如</span><br></pre></td></tr></table></figure></p><p>#将nginx:test这个镜像保存为Nginx_test.tar文件<br>[root@localhost ~]$docker save -o nginx_test.tar nginx:test<br>[root@localhost ~]$ll nginx_test.tar<br>-rw——- 1 root root 113036800 5月  16 10:29 nginx_test.tar</p><p>#删除ningx:test这个镜像.然后再从该文件恢复<br>[root@localhost ~]$docker load &lt; nginx_test.tar<br>67392954caf5: Loading layer [==================================================&gt;]  8.192kB/8.192kB<br>Loaded image: nginx:test</p><p>#镜像已经被导入<br>[root@localhost ~]$docker images | grep nginx<br>nginx                                                         test                   028f5e2b21a6        4 minutes ago       109MB<br><code>`</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker笔记——镜像篇&quot;&gt;&lt;a href=&quot;#docker笔记——镜像篇&quot; class=&quot;headerlink&quot; title=&quot;docker笔记——镜像篇&quot;&gt;&lt;/a&gt;docker笔记——镜像篇&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;docker镜像是一个只读的Docker容器模板.含有启动docker容器所需的文件系统结构以及内容.因此是启动一个容器的基础.docker镜像的文件内容以及一些运行docker容器的配置文件组成了docker容器的静态文件运行环境—rootfs.&lt;/p&gt;
&lt;p&gt;可以这么理解,docker镜像是docker容器的静态视角.docker容器是docker镜像的运行状态&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;1.rootfs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;rootfs是docker容器的根目录.如:/dev,/proc,/bin,/etc …….传统的Linux容器操作系统内核启动时,首先挂载一个只读(read-only)的rootfs.当系统检测到完整性后,再将其切换到读写(read-write)模式.而在docker架构中.也沿用了Linux内核的启动方法.在docker为容器挂载rootfs时,将rootfs设置为只读模式,挂载完毕后,在已有的只读rootfs上再挂载一个读写层.&lt;/p&gt;
&lt;p&gt;读写层位于docker容器文件系统的最顶层.下面可能挂载了多个只读层.&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker容器篇</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E5%AE%B9%E5%99%A8%E7%AF%87/"/>
    <id>https://jesse.top/2020/06/29/docker/docker学习笔记—容器篇/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T14:28:34.926Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker学习笔记—-docker容器篇"><a href="#docker学习笔记—-docker容器篇" class="headerlink" title="docker学习笔记— docker容器篇"></a>docker学习笔记— docker容器篇</h2><p><strong>一.创建并启动容器</strong></p><ul><li><strong>创建容器</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker create -it 镜像名:tag</span><br></pre></td></tr></table></figure><ul><li><strong>查看所有容器</strong>(包括运行中,已退出,错误容器)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps -a</span><br></pre></td></tr></table></figure><ul><li>查看所有容器的ID</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps -qa</span><br></pre></td></tr></table></figure><ul><li><strong>启动容器</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker start container_id</span><br></pre></td></tr></table></figure><ul><li><strong>使用指定的镜像直接创建并启动容器</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -it 镜像名:标签 COMMAND</span><br><span class="line"></span><br><span class="line">jesse@jesse-virtual-machine:~$ docker run -it ubuntu:16.04 /bin/bash</span><br><span class="line">root@66a29f973548:/#   ----------此时已经进入到容器的bash环境</span><br></pre></td></tr></table></figure><a id="more"></a><p>当利用 docker run 来创建容器时，Docker 在后台运行的标准操作包括：</p><p>1.检查本地是否存在指定的镜像，不存在就从公有仓库下载</p><p>2.利用镜像创建并启动一个容器</p><p>3.分配一个文件系统，并在只读的镜像层外面挂载一层可读写层</p><p>4.从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去</p><p>5.从地址池配置一个 ip 地址给容器</p><p>6.执行用户指定的应用程序</p><p>7.执行完毕后容器被终止</p><ul><li><strong>以守护状态运行:</strong></li></ul><p>以守护态运行（加参数-d):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d registry.intra.weibo.com/yushuang3/centos:v1 /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;</span><br></pre></td></tr></table></figure><ul><li><strong>容器终止:</strong></li></ul><p><strong>获取容器输出的信息</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker logs container_id</span><br></pre></td></tr></table></figure><p><strong>停止容器</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker stop container_Id</span><br></pre></td></tr></table></figure><blockquote><p>终止一个容器  加入-t=10 表示等待10秒(不加-t选项则默认就是10秒)再次发送SIGKILL信号终止容器</p></blockquote><p><strong>重启容器</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker restart container_id</span><br></pre></td></tr></table></figure><p><strong>容器状态:</strong></p><ul><li><strong>status表示容器的状态..</strong></li></ul><ul><li><strong>exited 表示容器已经退出</strong></li><li><strong>up 表示容器正在运行</strong></li></ul><p>docker run启动容器时还可以指定其他的配置参数:</p><ul><li>-h HOSTNAME 或者 –hostname=HOSTNAME.设置容器的主机名</li><li>–dns=IP_ADDRESS:设置容器的DNS.写在容器的/etc/resolv.conf文件中.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost test]$docker run -itd --name busybox1 -h dwd-busybox --dns=8.8.8.8 busybox</span><br><span class="line">c25dac9c641705e00f02aefe302987f39f853a1feb8c0d3f32dc1675747edd84</span><br><span class="line"></span><br><span class="line">[root@localhost test]$docker exec -it busybox1 hostname</span><br><span class="line">dwd-busybox</span><br><span class="line"></span><br><span class="line">[root@localhost test]$docker exec -it busybox1 cat /etc/resolv.conf</span><br><span class="line">nameserver 8.8.8.8</span><br></pre></td></tr></table></figure><p>但是需要注意的是.这些修改不会被 docker commit保存,也就是不会保存在镜像中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#将busybox1容器保存为busybox1:test镜像</span><br><span class="line">[root@localhost test]$docker commit -m &apos;test&apos; -a &apos;jesse&apos; busybox1 busybox1:test</span><br><span class="line">sha256:3c8faee532f177cf8bb8736db89694c2c3ff5be1a30a15d604e450130909d123</span><br><span class="line"></span><br><span class="line">#用这个镜像,启动一个新的busybox1-test的容器</span><br><span class="line">[root@localhost test]$docker run --name busybox1-test -itd busybox1:test</span><br><span class="line">9326b615e9e3af64336683f7f82e048929de560d4ad0a5caf2485bbc4a62e18c</span><br><span class="line"></span><br><span class="line">#可以看到hostname和dns信息没有被保留</span><br><span class="line">[root@localhost test]$docker exec -it busybox1-test hostname</span><br><span class="line">9326b615e9e3</span><br><span class="line"></span><br><span class="line">[root@localhost test]$docker exec -it busybox1-test cat /etc/resolv.conf</span><br><span class="line">nameserver 114.114.114.114</span><br><span class="line">nameserver 114.114.115.115</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker学习笔记—-docker容器篇&quot;&gt;&lt;a href=&quot;#docker学习笔记—-docker容器篇&quot; class=&quot;headerlink&quot; title=&quot;docker学习笔记— docker容器篇&quot;&gt;&lt;/a&gt;docker学习笔记— docker容器篇&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;一.创建并启动容器&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;创建容器&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker create -it 镜像名:tag&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;查看所有容器&lt;/strong&gt;(包括运行中,已退出,错误容器)&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker ps -a&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;查看所有容器的ID&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker ps -qa&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;启动容器&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker start container_id&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;使用指定的镜像直接创建并启动容器&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker run -it 镜像名:标签 COMMAND&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;jesse@jesse-virtual-machine:~$ docker run -it ubuntu:16.04 /bin/bash&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;root@66a29f973548:/#   ----------此时已经进入到容器的bash环境&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker网络之overlay</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0--docker%E7%BD%91%E7%BB%9C%E4%B9%8Boverlay/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习--docker网络之overlay/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T14:18:08.089Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker官网学习-7-docker网络之overlay"><a href="#docker官网学习-7-docker网络之overlay" class="headerlink" title="docker官网学习-7.docker网络之overlay"></a>docker官网学习-7.docker网络之overlay</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>overlay网卡在多个docker宿主机之间创建一个分布式的网络,允许多个容器安全通信.</p><p>当初始化一个swarm集群,或者加入docker宿主机到一个swarm集群中.Docker会在该宿主机上创建2个网络:</p><ul><li>ingress: 负责swarm集群的控制以及数据流量</li><li>docker_gwbridge:一个Bridge网络,负责连接swarm集群中的每个docker节点</li></ul><p>overlay网络的创建方式和bridge一样.也是docker network create命令</p><hr><h3 id="创建overlay网络"><a href="#创建overlay网络" class="headerlink" title="创建overlay网络"></a>创建overlay网络</h3><a id="more"></a><p><strong>创建overlay网络的前提条件</strong></p><p>1.防火墙开通以下端口</p><ul><li>TCP 2377——集群管理节点通信</li><li>TCP,UPD 7946—节点间通信</li><li>UDP 4789—overlay网络流量</li></ul><p>2.初始化docker宿主机为swarm集群的manager角色.命令为<figure class="highlight docker"><figcaption><span>swarm init```.或者使用```docker swarm join```命令加入到一个现有的swarm集群.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">这两种方式都会创建默认的ingress overlay网络.</span><br><span class="line"></span><br><span class="line">&gt; 即使你不打算使用swarm服务,但是也要这样做.然后才能创建自定义的overlay网络</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**创建overlay网络命令**</span><br></pre></td></tr></table></figure></p><p>#创建个overlay网络.名字为my-overlay<br>docker network create -d overlay my-overlay<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">如果swarm服务或者独立容器需要和其他docker宿主机上的独立容器通信.需要加上```—attachable```参数</span><br></pre></td></tr></table></figure></p><p>#创建个overlay网络,名字为my-overlay.并且和其他docker宿主机的standalone容器通信.<br>docker network create -d overlay –attachable my-overlay<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 可以自定义地址段,掩码,网关等信息.具体方法见docker network create --help</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 自定义默认Ingress网络</span><br><span class="line"></span><br><span class="line">大部分用户不需要配置ingress网络.但是docker17.05以上版本可以自定义ingress网络.如果默认的Ingress网络iP地址段和已经存在的网络有冲突,则自定义的配置可很有用.</span><br><span class="line"></span><br><span class="line">配置Ingress网络需要先删除ingress,然后重新创建.所以最好是在创建容器服务之前先定义好ingress.如果已经有暴露出端口的服务.则需要先删除服务.</span><br><span class="line"></span><br><span class="line">**自定义默认ingress网络步骤如下:**</span><br><span class="line"></span><br><span class="line">1.查看当前ingress网络.```docker network inspect ingress```.删除所有连接到ingress的容器的服务.</span><br><span class="line"></span><br><span class="line">2.移除现有的ingress网络.```docker network rm ingress</span><br></pre></td></tr></table></figure></p><p>3.使用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p> docker network create \<br>  –driver overlay \<br>  –ingress \<br>  –subnet=10.11.0.0/16 \<br>  –gateway=10.11.0.2 \<br>  –opt com.docker.network.driver.mtu=1200 \<br>  my-ingress<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 可以用自定义名称(my-ingress)来定义ingress网络.但是只允许存在一个自定义Ingress.</span><br><span class="line"></span><br><span class="line">4.重启步骤1中的服务</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 自定义默认docker_gwbridge 接口</span><br><span class="line"></span><br><span class="line">docker_gwbridge是连接overlay网络和docker宿主机物理网卡之间的虚拟网桥.当初始化一个swarm集群,或者将docker宿主机加入到一个swarm集群时,docker会自动创建docker_gwbridge.但是docker_gwbridge不是一个docker服务,而是存在于docker宿主机的内核当中.</span><br><span class="line"></span><br><span class="line">所以需要在加入到swarm集群前先配置好docker_gwbridge.</span><br><span class="line"></span><br><span class="line">**自定义默认docker_gwbridge网络步骤如下:**</span><br><span class="line"></span><br><span class="line">1.停止docker</span><br><span class="line"></span><br><span class="line">2.删除当前```docker_gwbridge</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip link set docker_gwbridge down</span><br><span class="line"></span><br><span class="line">$ sudo ip link del dev docker_gwbridge</span><br></pre></td></tr></table></figure><p>3.开启docker.不要加入或者初始化swarm</p><p>4.手动创建<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 关于docker_gwbridge的更多参数请参考[Bridg dirver options](&lt;https://docs.docker.com/engine/reference/commandline/network_create/#bridge-driver-options&gt;)</span><br></pre></td></tr></table></figure></p><p>$ docker network create \<br>–subnet 10.11.0.0/16 \<br>–opt com.docker.network.bridge.name=docker_gwbridge \<br>–opt com.docker.network.bridge.enable_icc=false \<br>–opt com.docker.network.bridge.enable_ip_masquerade=true \<br>docker_gwbridge<br><code>`</code></p><p>5.初始化或者加入到swarm集群</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker官网学习-7-docker网络之overlay&quot;&gt;&lt;a href=&quot;#docker官网学习-7-docker网络之overlay&quot; class=&quot;headerlink&quot; title=&quot;docker官网学习-7.docker网络之overlay&quot;&gt;&lt;/a&gt;docker官网学习-7.docker网络之overlay&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;overlay网卡在多个docker宿主机之间创建一个分布式的网络,允许多个容器安全通信.&lt;/p&gt;
&lt;p&gt;当初始化一个swarm集群,或者加入docker宿主机到一个swarm集群中.Docker会在该宿主机上创建2个网络:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ingress: 负责swarm集群的控制以及数据流量&lt;/li&gt;
&lt;li&gt;docker_gwbridge:一个Bridge网络,负责连接swarm集群中的每个docker节点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;overlay网络的创建方式和bridge一样.也是docker network create命令&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;创建overlay网络&quot;&gt;&lt;a href=&quot;#创建overlay网络&quot; class=&quot;headerlink&quot; title=&quot;创建overlay网络&quot;&gt;&lt;/a&gt;创建overlay网络&lt;/h3&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>使用goreplay收集线上真实http流量</title>
    <link href="https://jesse.top/2020/06/29/Linux-Web/%E4%BD%BF%E7%94%A8goreplay%E6%94%B6%E9%9B%86%E7%BA%BF%E4%B8%8A%E7%9C%9F%E5%AE%9Ehttp%E6%B5%81%E9%87%8F/"/>
    <id>https://jesse.top/2020/06/29/Linux-Web/使用goreplay收集线上真实http流量/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:45:33.890Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用goreplay收集线上真实http流量"><a href="#使用goreplay收集线上真实http流量" class="headerlink" title="使用goreplay收集线上真实http流量"></a>使用goreplay收集线上真实http流量</h2><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>在很多场景中,我们需要将线上服务器的真实Http请求复制转发到某台服务器中(或者测试环境中),并且前提是不影响线上生产业务进行.</p><p>例如:</p><ol><li>通常可能会通过ab等压测工具来对单一http接口进行压测。但如果是需要http服务整体压测，使用ab来压测工作量大且不方便，通过线上流量复制引流，通过将真实请求流量放大N倍来进行压测，能对服务有一个较为全面的检验.</li><li>将线上流量引入到测试环境中,测试某个中间件或者数据库的压力</li><li>上线前在预发布环境，使用线上真实的请求，检查是否准备发布的版本，是否具备发布标准</li><li>用线上的流量转发到预发，检查相同流量下一些指标的反馈情况，检查核心数据是否有改善、优化.</li></ol><a id="more"></a><hr><h3 id="goreplay介绍"><a href="#goreplay介绍" class="headerlink" title="goreplay介绍"></a>goreplay介绍</h3><p>goreplay项目请参考github:<a href="https://github.com/buger/goreplay" target="_blank" rel="noopener">goreplay介绍</a></p><p>goreplay是一款开源网络监控工具,可以在不影响业务的情况下,记录服务器真实流量,将该流量用来做镜像,压力测试,监控和分析等用途.</p><p>简单来说就是goreplay抓取线上真实的流量，并将捕捉到的流量转发到测试服务器上(或者保存到本地文件中)</p><p>goreplay大致工作流程如下:</p><p><img src="https://img2.jesse.top/20200629110035.png" alt=""></p><hr><h3 id="goreplay常见使用方式"><a href="#goreplay常见使用方式" class="headerlink" title="goreplay常见使用方式"></a>goreplay常见使用方式</h3><p>goreplay使用文档参考:<a href="https://github.com/buger/goreplay/wiki" target="_blank" rel="noopener">goreplay文档</a></p><p>常用的一些命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-input-raw 抓取指定端口的流量 gor --input-raw :8080</span><br><span class="line">-output-stdout 打印到控制台</span><br><span class="line">-output-file 将请求写到文件中 gor --input-raw :80 --output-file ./requests.gor</span><br><span class="line">-input-file 从文件中读取请求，与上一条命令呼应 gor --input-file ./requests.gor</span><br><span class="line">-exit-after 5s 持续时间</span><br><span class="line">-http-allow-url url白名单，其他请求将会被丢弃</span><br><span class="line">-http-allow-method 根据请求方式过滤</span><br><span class="line">-http-disallow-url 遇上一个url相反，黑名单，其他的请求会被捕获到</span><br></pre></td></tr></table></figure><blockquote><p>更多命令可以使用 ./gor –help查看</p></blockquote><hr><h3 id="goreplay安装"><a href="#goreplay安装" class="headerlink" title="goreplay安装"></a>goreplay安装</h3><p>在github上下载Linux的二进制文件: <a href="https://github.com/buger/goreplay/releases" target="_blank" rel="noopener">goreplay安装</a></p><blockquote><p>注意.虽然在github上提供了rpm安装包,但是实际安装发现无法安装:</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# rpm -ivh gor-1.0.0-1.x86_64.rpm</span><br><span class="line">Preparing...                          ################################# [100%]</span><br><span class="line">package goreplay-1.0.0-1.x86_64 is intended for a different operating system</span><br></pre></td></tr></table></figure><p>下载github上的二进制文件,解压后是一个gor的二进制可执行文件.复制到PATH变量路径下即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# wget https://github.com/buger/goreplay/releases/download/v1.0.0/gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# ls</span><br><span class="line"> gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# tar -xf gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# ls</span><br><span class="line">gor </span><br><span class="line">[root@dwd-tongji-3 ~]# ll gor</span><br><span class="line">-rwxr-xr-x 1 501 games 17779040 Mar 30  2019 gor</span><br><span class="line">[root@dwd-tongji-3 ~]# cp gor /usr/local/bin/</span><br></pre></td></tr></table></figure><hr><h3 id="goreplay简单实践"><a href="#goreplay简单实践" class="headerlink" title="goreplay简单实践"></a>goreplay简单实践</h3><h4 id="1-将本地http的流量保存到本地文件中"><a href="#1-将本地http的流量保存到本地文件中" class="headerlink" title="1.将本地http的流量保存到本地文件中."></a>1.将本地http的流量保存到本地文件中.</h4><p>为了简便起见,以下命令都在root用户下执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## 1.开启一个screen窗口</span><br><span class="line">[root@dwd-tongji-3 ~]# screen -S GOR</span><br><span class="line">## 2.将80流量保存到本地的文件</span><br><span class="line">[root@dwd-tongji-3 ~]# gor --input-raw :80 --output-file /data/requests.gor</span><br><span class="line">Version: 1.0.0</span><br></pre></td></tr></table></figure><p>默认情况下goreplay会以块文件存储,将流量保存为多个块文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# ls /data/requests_* | more</span><br><span class="line">/data/requests_0.gor</span><br><span class="line">/data/requests_100.gor</span><br><span class="line">/data/requests_101.gor</span><br><span class="line">/data/requests_102.gor</span><br><span class="line">/data/requests_103.gor</span><br><span class="line">/data/requests_104.gor</span><br><span class="line">/data/requests_105.gor</span><br><span class="line">/data/requests_106.gor</span><br><span class="line">/data/requests_107.gor</span><br><span class="line">/data/requests_108.gor</span><br><span class="line">/data/requests_109.gor</span><br><span class="line">/data/requests_10.gor</span><br></pre></td></tr></table></figure><p>使用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>[root@dwd-tongji-3 ~]#./gor –input-raw :80 –output-file /data/gor.gor –output-file-append</p><p>[root@dwd-tongji-3 ~]# ll /data -h<br>total 1.4M<br>-rw-r—– 1 root root 1.4M Jun 29 15:13 gor.gor<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 2.将http的请求打印到终端</span><br></pre></td></tr></table></figure></p><p>[root@dwd-tongji-3 ~]#gor –input-raw :8000 –output-stdout<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 3.将http的请求转发到测试环境</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-http=”<a href="http://beta:80&quot;" target="_blank" rel="noopener">http://beta:80&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在测试服务器上的nginx查看日志,.发现流量已经进来了</span><br></pre></td></tr></table></figure></p><p>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik.php HTTP/1.1” 200 5 “<a href="https://2021001151691008.hybrid.alipay-eco.com/2021001151691008/0.2.2006111453.18/index.html#pages/index/index?appid=2021001151691008&amp;taskId=415&quot;" target="_blank" rel="noopener">https://2021001151691008.hybrid.alipay-eco.com/2021001151691008/0.2.2006111453.18/index.html#pages/index/index?appid=2021001151691008&amp;taskId=415&quot;</a> “Mozilla/5.0 (iPhone; CPU iPhone OS 13_3_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/17D50 Ariver/1.0.12 AliApp(AP/10.1.95.7030) Nebula WK RVKType(0) AlipayDefined(nt:4G,ws:375|667|2.0) AlipayClient/10.1.95.7030 Language/zh-Hans Region/CN NebulaX/1.0.0” “112.96.179.238”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik.php HTTP/1.1” 200 5 “<a href="https://servicewechat.com/wxa090d3923fde0d4b/132/page-frame.html&quot;" target="_blank" rel="noopener">https://servicewechat.com/wxa090d3923fde0d4b/132/page-frame.html&quot;</a> “Mozilla/5.0 (iPhone; CPU iPhone OS 13_5_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 MicroMessenger/7.0.13(0x17000d29) NetType/4G Language/zh_CN” “14.106.171.11”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">####  也可以将流量输出到多个终端</span><br><span class="line"></span><br><span class="line">* 输出到多个http服务器</span><br></pre></td></tr></table></figure></p><p>gor –input-tcp :28020 –output-http “<a href="http://staging.com&quot;" target="_blank" rel="noopener">http://staging.com&quot;</a>  –output-http “<a href="http://dev.com&quot;" target="_blank" rel="noopener">http://dev.com&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 输出到文件或者Http服务器</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-file requests.log –output-http “<a href="http://staging.com&quot;" target="_blank" rel="noopener">http://staging.com&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">####  4.将流量从文件重放到http服务器</span><br><span class="line"></span><br><span class="line">1.首先将请求流量保存到本地文件</span><br></pre></td></tr></table></figure></p><p>sudo ./gor –input-raw :8000 –output-file=requests.gor<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2.再开一个窗口,运行gor,将请求流量从文件中重放</span><br></pre></td></tr></table></figure></p><p>./gor –input-file requests.gor –output-http=”<a href="http://localhost:8001&quot;" target="_blank" rel="noopener">http://localhost:8001&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 压力测试</span><br><span class="line"></span><br><span class="line">goreplay支持将捕获到的生产实际请求流量减少或者放大重播以用于测试环境的压力测试.压力测试一般针对Input流量减少或者放大.例如下面的例子</span><br></pre></td></tr></table></figure></p><h1 id="Replay-from-file-on-2x-speed"><a href="#Replay-from-file-on-2x-speed" class="headerlink" title="Replay from file on 2x speed"></a>Replay from file on 2x speed</h1><p>#将请求流量以2倍的速度放大重播<br>gor –input-file “requests.gor|200%” –output-http “staging.com”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">当然也也支持10%,20%等缩小请求流量</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 限速</span><br><span class="line"></span><br><span class="line">如果受限于测试环境的服务器资源压力,只想重播一部分流量到测试环境中,而不需要所有的实际生产流量,那么就可以用限速功能.有两种策略可以实现限流</span><br><span class="line"></span><br><span class="line">1.随机丢弃请求流量</span><br><span class="line"></span><br><span class="line">2.基于Header或者URL丢弃一定的流量(百分比)</span><br><span class="line"></span><br><span class="line">#####  随机丢弃请求流量</span><br><span class="line"></span><br><span class="line">input和output两端都支持限速,有两种限速算法:**百分比**或者**绝对值**</span><br><span class="line"></span><br><span class="line">* 百分比: input端支持缩小或者放大请求流量,基于指定的策略随机丢弃请求流量</span><br><span class="line">* 绝对值: 如果单位时间(秒)内达到临界值,则丢弃剩余请求流量,下一秒临界值还原</span><br><span class="line"></span><br><span class="line">**用法**:</span><br><span class="line"></span><br><span class="line">在output终端使用&quot;|&quot;运算符指定限速阈值,例如:</span><br><span class="line"></span><br><span class="line">* 使用绝对值限速</span><br></pre></td></tr></table></figure></p><h1 id="staging-server-will-not-get-more-than-ten-requests-per-second"><a href="#staging-server-will-not-get-more-than-ten-requests-per-second" class="headerlink" title="staging.server will not get more than ten requests per second"></a>staging.server will not get more than ten requests per second</h1><p>#staging服务每秒只接收10个请求<br>gor –input-tcp :28020 –output-http “<a href="http://staging.com|10&quot;" target="_blank" rel="noopener">http://staging.com|10&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 使用百分比限速</span><br></pre></td></tr></table></figure></p><h1 id="replay-server-will-not-get-more-than-10-of-requests"><a href="#replay-server-will-not-get-more-than-10-of-requests" class="headerlink" title="replay server will not get more than 10% of requests"></a>replay server will not get more than 10% of requests</h1><h1 id="useful-for-high-load-environments"><a href="#useful-for-high-load-environments" class="headerlink" title="useful for high-load environments"></a>useful for high-load environments</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##### 基于Header或者URL参数限速</span><br><span class="line"></span><br><span class="line">如果header或者URL参数中有唯一值,例如(API key),则可以转发指定百分比的流量到后端,例如:</span><br></pre></td></tr></table></figure></p><h1 id="Limit-based-on-header-value"><a href="#Limit-based-on-header-value" class="headerlink" title="Limit based on header value"></a>Limit based on header value</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%” –http-header-limiter “X-API-KEY: 10%”</p><h1 id="Limit-based-on-URL-param-value"><a href="#Limit-based-on-URL-param-value" class="headerlink" title="Limit based on URL param value"></a>Limit based on URL param value</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%” –http-param-limiter “api_key: 10%”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">###  过滤</span><br><span class="line"></span><br><span class="line">如果只想捕获指定的URL路径请求,或者http头部,或者Http方法,则可以使用过滤功能</span><br><span class="line"></span><br><span class="line">下面是几个例子</span><br><span class="line"></span><br><span class="line">* 只捕获某个URL</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-being-sent-to-the-api-endpoint"><a href="#only-forward-requests-being-sent-to-the-api-endpoint" class="headerlink" title="only forward requests being sent to the /api endpoint"></a>only forward requests being sent to the /api endpoint</h1><p>gor –input-raw :8080 –output-http staging.com –http-allow-url /api<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 拒绝某个URL</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-NOT-being-sent-to-the-api…-endpoint"><a href="#only-forward-requests-NOT-being-sent-to-the-api…-endpoint" class="headerlink" title="only forward requests NOT being sent to the /api… endpoint"></a>only forward requests NOT being sent to the /api… endpoint</h1><p>gor –input-raw :8080 –output-http staging.com –http-disallow-url /api<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 基于正则表达式过滤头部</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-with-an-api-version-of-1-0x"><a href="#only-forward-requests-with-an-api-version-of-1-0x" class="headerlink" title="only forward requests with an api version of 1.0x"></a>only forward requests with an api version of 1.0x</h1><p>gor –input-raw :8080 –output-http staging.com –http-allow-header api-version:^1.0\d</p><h1 id="only-forward-requests-NOT-containing-User-Agent-header-value-“Replayed-by-Gor”"><a href="#only-forward-requests-NOT-containing-User-Agent-header-value-“Replayed-by-Gor”" class="headerlink" title="only forward requests NOT containing User-Agent header value “Replayed by Gor”"></a>only forward requests NOT containing User-Agent header value “Replayed by Gor”</h1><p>gor –input-raw :8080 –output-http staging.com –http-disallow-header “User-Agent: Replayed by Gor”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 过滤HTTP请求方法</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-http “<a href="http://staging.server&quot;" target="_blank" rel="noopener">http://staging.server&quot;</a> \<br>    –http-allow-method GET \<br>    –http-allow-method OPTIONS<br><code>`</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;使用goreplay收集线上真实http流量&quot;&gt;&lt;a href=&quot;#使用goreplay收集线上真实http流量&quot; class=&quot;headerlink&quot; title=&quot;使用goreplay收集线上真实http流量&quot;&gt;&lt;/a&gt;使用goreplay收集线上真实http流量&lt;/h2&gt;&lt;h3 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;背景介绍&lt;/h3&gt;&lt;p&gt;在很多场景中,我们需要将线上服务器的真实Http请求复制转发到某台服务器中(或者测试环境中),并且前提是不影响线上生产业务进行.&lt;/p&gt;
&lt;p&gt;例如:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通常可能会通过ab等压测工具来对单一http接口进行压测。但如果是需要http服务整体压测，使用ab来压测工作量大且不方便，通过线上流量复制引流，通过将真实请求流量放大N倍来进行压测，能对服务有一个较为全面的检验.&lt;/li&gt;
&lt;li&gt;将线上流量引入到测试环境中,测试某个中间件或者数据库的压力&lt;/li&gt;
&lt;li&gt;上线前在预发布环境，使用线上真实的请求，检查是否准备发布的版本，是否具备发布标准&lt;/li&gt;
&lt;li&gt;用线上的流量转发到预发，检查相同流量下一些指标的反馈情况，检查核心数据是否有改善、优化.&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
    
      <category term="goreplay" scheme="https://jesse.top/tags/goreplay/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---学习Docker Stack</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0-6.%E5%AD%A6%E4%B9%A0stack/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习-6.学习stack/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:12:58.803Z</updated>
    
    <content type="html"><![CDATA[<h2 id="学习Docker-Stack"><a href="#学习Docker-Stack" class="headerlink" title="学习Docker Stack"></a>学习Docker Stack</h2><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>一个stack是一组共享依赖包的多个相关的services,并且可以编排和扩展.其实从第4小节开始,在利用compose文件部署app时,就已经开始一直使用stack.但是还只是运行在一个单一服务器的单一service.<br>现在,你可以学习在多个服务器上,运行多个相关的services.</p><hr><ul><li>使用下面的docker-compose.yml文件替换第4小节中的docker-compose.yml</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line">services:</span><br><span class="line">  web:</span><br><span class="line">    # replace username/repo:tag with your name and image details</span><br><span class="line">    image: ianch/friendlyhello:v1</span><br><span class="line">    deploy:</span><br><span class="line">      replicas: 5</span><br><span class="line">      resources:</span><br><span class="line">        limits:</span><br><span class="line">          cpus: &quot;0.1&quot;</span><br><span class="line">          memory: 50M</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;4000:80&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - webnet</span><br><span class="line">  Visualizer:</span><br><span class="line">    image: dockersamples/visualizer:stable</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8080:8080&quot;</span><br><span class="line">    volumes:</span><br><span class="line">      - &quot;/var/run/docker.sock:/var/run/docker.sock&quot;</span><br><span class="line">    deploy:</span><br><span class="line">      placement:</span><br><span class="line">        constraints: [node.role == manager]</span><br><span class="line">    networks:</span><br><span class="line">      - webnet</span><br><span class="line">networks:</span><br><span class="line">  webnet:</span><br></pre></td></tr></table></figure><a id="more"></a><p>docker-compose文件稍微做了点改动.添加一个Visualizer服务,placement指令确保这个Visualizer服务仅仅运行在swarm manager节点.</p><hr><h4 id="部署compose文件"><a href="#部署compose文件" class="headerlink" title="部署compose文件"></a>部署compose文件</h4><ul><li>初始化swarm</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker swarm init</span><br></pre></td></tr></table></figure><ul><li>第二台服务器加入swarm集群</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@php compose]$docker swarm join --token SWMTKN-1-5qr6e90o52h5licxatuvmft65kji5qf1roujebf16auoe5xgam-3d0fuzr8818r6330n88dm1fcu 10.0.0.50:2377</span><br><span class="line">This node joined a swarm as a worker.</span><br></pre></td></tr></table></figure><ul><li>部署app</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack deploy -c docker-compose.yml getstartedlab</span><br><span class="line">Creating network getstartedlab_webnet</span><br><span class="line">Creating service getstartedlab_web</span><br><span class="line">Creating service getstartedlab_Visualizer</span><br><span class="line">[root@localhost compose]$</span><br></pre></td></tr></table></figure><blockquote><p>添加了2个服务.web和Visualizer</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack ps getstartedlab</span><br><span class="line">ID                  NAME                         IMAGE                             NODE                    DESIRED STATE       CURRENT STATE                ERROR               PORTS</span><br><span class="line">ds77cn4hgd9w        getstartedlab_web.1          friendlyhello:latest              php                     Running             Running about a minute ago</span><br><span class="line">agzj3veqno63        getstartedlab_Visualizer.1   dockersamples/visualizer:stable   localhost.localdomain   Running             Running about a minute ago</span><br><span class="line">3hq0if79g3gk        getstartedlab_web.2          friendlyhello:latest              php                     Running             Running about a minute ago</span><br><span class="line">iuxh7qjfpikw        getstartedlab_web.3          friendlyhello:latest              localhost.localdomain   Running             Running 53 seconds ago</span><br><span class="line">ba9hcq5zmlbd        getstartedlab_web.4          friendlyhello:latest              php                     Running             Running about a minute ago</span><br><span class="line">o600yqmqets7        getstartedlab_web.5          friendlyhello:latest              localhost.localdomain   Running             Running 55 seconds ago</span><br><span class="line">[root@localhost compose]$</span><br></pre></td></tr></table></figure><p>访问任意一台服务器的8080端口,可以看到Visualizer服务正在运行</p><p><img src="https://docs.docker.com/get-started/images/get-started-visualizer1.png" alt=""></p><blockquote><p>这是我借用的官网的图片.</p></blockquote><p>可以看到,visualizer运行在swarm manager节点上,5个web服务运行在swarm集群上.visualizer是一个不需要任何依赖,而可以运行在任何app的独立服务.现在尝试一下创建一个具有依赖项的服务:提供访问计数器的Redis服务</p><hr><h3 id="编辑docker-compose文件"><a href="#编辑docker-compose文件" class="headerlink" title="编辑docker-compose文件"></a>编辑docker-compose文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line">services:</span><br><span class="line">  web:</span><br><span class="line">    # replace username/repo:tag with your name and image details</span><br><span class="line">    image: ianch/friendlyhello:v1</span><br><span class="line">    deploy:</span><br><span class="line">      replicas: 5</span><br><span class="line">      resources:</span><br><span class="line">        limits:</span><br><span class="line">          cpus: &quot;0.1&quot;</span><br><span class="line">          memory: 50M</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;4000:80&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - webnet</span><br><span class="line">  Visualizer:</span><br><span class="line">    image: dockersamples/visualizer:stable</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8080:8080&quot;</span><br><span class="line">    volumes:</span><br><span class="line">      - &quot;/var/run/docker.sock:/var/run/docker.sock&quot;</span><br><span class="line">    deploy:</span><br><span class="line">      placement:</span><br><span class="line">        constraints: [node.role == manager]</span><br><span class="line">    networks:</span><br><span class="line">      - webnet</span><br><span class="line"></span><br><span class="line">  redis:</span><br><span class="line">    image: redis</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;6379:6379&quot;</span><br><span class="line">    volumes:</span><br><span class="line">      - &quot;/home/docker/data:/data&quot;</span><br><span class="line">    deploy:</span><br><span class="line">      placement:</span><br><span class="line">        constraints: [node.role == manager]</span><br><span class="line">    command: redis-server --appendonly yes</span><br><span class="line">    networks:</span><br><span class="line">      - webnet</span><br><span class="line">networks:</span><br><span class="line">  webnet:</span><br></pre></td></tr></table></figure><p> 这里我们添加了一个redis服务.在Docker HUB上有redis官方镜像,并且已经暴露了6379端口.所以这里只需要指定redis镜像即可..同样redis也只运行在manager节点服务器.</p><p> 这里为了持久化数据,在启动redis容器的时候指定了appendonly参数,并且挂载了本机的/home/docker/data目录映射到容器的/data.(redis容器默认保存数据路径)</p><ul><li>在manager节点创建/home/docker/data目录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$mkdir -pv /home/docker/data</span><br><span class="line">mkdir: 已创建目录 &quot;/home/docker&quot;</span><br><span class="line">mkdir: 已创建目录 &quot;/home/docker/data&quot;</span><br></pre></td></tr></table></figure><ul><li>部署compose</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack deploy -c docker-compose.yml getstartedlab</span><br><span class="line">Updating service getstartedlab_web (id: mtgafxttekwfh0tkhkaespa1v)</span><br><span class="line">image friendlyhello:latest could not be accessed on a registry to record</span><br><span class="line">its digest. Each node will access friendlyhello:latest independently,</span><br><span class="line">possibly leading to different nodes running different</span><br><span class="line">versions of the image.</span><br><span class="line"></span><br><span class="line">Updating service getstartedlab_Visualizer (id: zmim1kj44afsr9xay8ppxker6)</span><br><span class="line">Creating service getstartedlab_redis</span><br></pre></td></tr></table></figure><p>可以看到3个services都启动起来了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker service ls</span><br><span class="line">ID                  NAME                       MODE                REPLICAS            IMAGE                             PORTS</span><br><span class="line">zmim1kj44afs        getstartedlab_Visualizer   replicated          1/1                 dockersamples/visualizer:stable   *:8080-&gt;8080/tcp</span><br><span class="line">elomaiu5go9p        getstartedlab_redis        replicated          1/1                 redis:latest                      *:6379-&gt;6379/tcp</span><br><span class="line">mtgafxttekwf        getstartedlab_web          replicated          5/5                 friendlyhello:latest              *:4000-&gt;80/tcp</span><br></pre></td></tr></table></figure><p>在浏览器访问服务器的4000端口可以看到有一个访问计数器在增加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; eed56ca4164b&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 5%                                                                                                           huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 777d2cab6468&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 6%                                                                                                           huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 213e6a729c6a&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 7%                                                                                                           huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 85ccc6b1cb18&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 8%                                                                                                           huangyong@huangyong-Macbook-Pro  ~ </span><br></pre></td></tr></table></figure><p>访问另外一台服务器也可以看到同样结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.12:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; e77e36db18be&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 10%                                                                                                          huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; eed56ca4164b&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 11%                                                                                                          huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.12:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; eed56ca4164b&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 12%</span><br></pre></td></tr></table></figure><p>访问visulizer容器的8080端口,可以看到redis服务运行</p><p><img src="https://docs.docker.com/get-started/images/visualizer-with-redis.png" alt=""></p><hr><h3 id="管理命令"><a href="#管理命令" class="headerlink" title="管理命令"></a>管理命令</h3><p>使用docker node ls 列出swarm集群的所有节点<br>使用docker service ls 列出所有服务<br>docker service ps &lt;service_name&gt; 列出某个服务的所有tasks</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker node ls</span><br><span class="line">ID                            HOSTNAME                STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class="line">056r5hr2xb6jjzmbw64m3btd2 *   localhost.localdomain   Ready               Active              Leader              18.09.2</span><br><span class="line">r58bkpsxgi4mjaxrn8octuxw1     php                     Ready               Active                                  18.09.3</span><br><span class="line">[root@localhost ~]$docker service ls</span><br><span class="line">ID                  NAME                       MODE                REPLICAS            IMAGE                             PORTS</span><br><span class="line">zmim1kj44afs        getstartedlab_Visualizer   replicated          1/1                 dockersamples/visualizer:stable   *:8080-&gt;8080/tcp</span><br><span class="line">elomaiu5go9p        getstartedlab_redis        replicated          1/1                 redis:latest                      *:6379-&gt;6379/tcp</span><br><span class="line">mtgafxttekwf        getstartedlab_web          replicated          5/5                 friendlyhello:latest              *:4000-&gt;80/tcp</span><br><span class="line"></span><br><span class="line">[root@localhost ~]$docker service ps getstartedlab_web</span><br><span class="line">ID                  NAME                  IMAGE                  NODE                    DESIRED STATE       CURRENT STATE          ERROR               PORTS</span><br><span class="line">ds77cn4hgd9w        getstartedlab_web.1   friendlyhello:latest   php                     Running             Running 14 hours ago</span><br><span class="line">3hq0if79g3gk        getstartedlab_web.2   friendlyhello:latest   php                     Running             Running 14 hours ago</span><br><span class="line">iuxh7qjfpikw        getstartedlab_web.3   friendlyhello:latest   localhost.localdomain   Running             Running 14 hours ago</span><br><span class="line">ba9hcq5zmlbd        getstartedlab_web.4   friendlyhello:latest   php                     Running             Running 14 hours ago</span><br><span class="line">o600yqmqets7        getstartedlab_web.5   friendlyhello:latest   localhost.localdomain   Running             Running 14 hours ago</span><br><span class="line">[root@localhost ~]$</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;学习Docker-Stack&quot;&gt;&lt;a href=&quot;#学习Docker-Stack&quot; class=&quot;headerlink&quot; title=&quot;学习Docker Stack&quot;&gt;&lt;/a&gt;学习Docker Stack&lt;/h2&gt;&lt;h4 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h4&gt;&lt;p&gt;一个stack是一组共享依赖包的多个相关的services,并且可以编排和扩展.其实从第4小节开始,在利用compose文件部署app时,就已经开始一直使用stack.但是还只是运行在一个单一服务器的单一service.&lt;br&gt;现在,你可以学习在多个服务器上,运行多个相关的services.&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;使用下面的docker-compose.yml文件替换第4小节中的docker-compose.yml&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;version: &amp;quot;3&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;services:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  web:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    # replace username/repo:tag with your name and image details&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    image: ianch/friendlyhello:v1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    deploy:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      replicas: 5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      resources:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        limits:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          cpus: &amp;quot;0.1&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          memory: 50M&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      restart_policy:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        condition: on-failure&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ports:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      - &amp;quot;4000:80&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    networks:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      - webnet&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  Visualizer:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    image: dockersamples/visualizer:stable&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ports:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      - &amp;quot;8080:8080&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    volumes:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      - &amp;quot;/var/run/docker.sock:/var/run/docker.sock&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    deploy:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      placement:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        constraints: [node.role == manager]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    networks:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      - webnet&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;networks:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  webnet:&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker网络之bridge</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0-7.docker%E7%BD%91%E7%BB%9C%E4%B9%8Bbridge/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习-7.docker网络之bridge/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T14:08:42.118Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker官网学习–docker网络之bridge"><a href="#docker官网学习–docker网络之bridge" class="headerlink" title="docker官网学习–docker网络之bridge"></a>docker官网学习–docker网络之bridge</h2><p>本节介绍docker基础网络概念.以便能认识和利用各种不同的网络类型功能.</p><p>docker的网络支持插件化,驱动化定制.有一些网络驱动已经默认集成到docker中.docker网络主要有以下类型</p><ul><li>bridge</li><li>host</li><li>overlay</li><li>macvlan</li><li>none</li><li>其他网络插件</li></ul><hr><a id="more"></a><h4 id="bridge"><a href="#bridge" class="headerlink" title="bridge"></a>bridge</h4><p>​    bridge是docker默认的网络驱动.如果在<figure class="highlight docker"><figcaption><span>run```启动一个容器时没有指定任何网络驱动.那么默认就是bridge桥接网络.桥接网络通常适用于应用进程部署在多个独立的容器中,并且容器之间需要互相通信的场景中.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">   在docker环境中.bridge使用软件桥接允许容器之间通过同一个bridge互联.隔离没有连到这个bridge网络的其他容器网络.docker网络自动创建iptables规则阻止其他网络的容器访问.</span><br><span class="line"></span><br><span class="line">当Docker进程启动时，会在主机上创建一个名为docker0的虚拟网桥，此主机上启动的Docker容器会连接到这个虚拟网桥上。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。</span><br><span class="line"></span><br><span class="line">从docker0子网中分配一个IP给容器使用，并设置docker0的IP地址为容器的默认网关。在主机上创建一对虚拟网卡veth pair设备，Docker将veth pair设备的一端放在新创建的容器中，并命名为eth0（容器的网卡），另一端放在主机中，以vethxxx这样类似的名字命名，并将这个网络设备加入到docker0网桥中。可以通过brctl show命令查看。</span><br><span class="line"></span><br><span class="line">在Linux中.可以使用brctl命令查看和管理网桥(需要先安装bridge-utils软件包).例如查看本机上的网桥及其端口</span><br></pre></td></tr></table></figure></p><p>[work@docker conf.d]$sudo brctl show<br>bridge name    bridge id        STP enabled    interfaces<br>br-17ace6d9d81a        8000.024236cdd4d7    no        veth82ed0e5<br>br-d9897c225d25        8000.024237d1c9f6    no        veth6981090<br>                            veth8e29dbf<br>                            vethd511728<br>docker0        8000.0242322e2e42    no        veth0a0e27d<br>                            veth0c50104<br>                            veth3fe7f4d<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">docker 0网桥下关联了很多vethxxxxx规范命名的interfaces.每一个vethxxxx接口对应一个docker容器.在docker容器中一般是eth0的网卡</span><br></pre></td></tr></table></figure></p><p>[work@docker conf.d]$docker exec -it nginx ifconfig<br>eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 172.17.0.8  netmask 255.255.0.0  broadcast 0.0.0.0<br>        inet6 fe80::42:acff:fe11:8  prefixlen 64  scopeid 0x20<link><br>        ether 02:42:ac:11:00:08  txqueuelen 0  (Ethernet)<br>        RX packets 1428365  bytes 189142687 (180.3 MiB)<br>        RX errors 0  dropped 0  overruns 0  frame 0<br>        TX packets 1403620  bytes 287806317 (274.4 MiB)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">bridge网桥就是这样通信的,docker服务端通过vethxxxx端口和容器的eth0虚拟网卡进行通信.docker容器将宿主机的docker 0虚拟网卡的IP作为它的网关:</span><br></pre></td></tr></table></figure></p><p>[work@docker conf.d]$docker exec -it nginx route -n<br>Kernel IP routing table<br>Destination     Gateway         Genmask         Flags Metric Ref    Use Iface<br>0.0.0.0         172.17.0.1      0.0.0.0         UG    0      0        0 eth0<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bridge模式是docker的默认网络模式，不写--net参数，就是bridge模式。使用docker run -p时，docker实际是在iptables做了DNAT规则，实现端口转发功能。可以使用iptables -t nat -vnL查看。</span><br><span class="line"></span><br><span class="line">bridge网络模式如下所示:</span><br><span class="line"></span><br><span class="line">![](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUViTWFab1lmbU9pYk56NDZwc0FMcDk0bHR1MllTNVZHMmZtNGUxTTNwM0tOUmVQN04xZVh2OHlBLzA/d3hfZm10PXBuZw==)</span><br><span class="line"></span><br><span class="line">​    bridge网桥是docker的默认网络驱动.如果用户在创建容器时自定义了Bridge网络.那么自定义Bridge要优于docker默认的Bridge</span><br><span class="line"></span><br><span class="line"> **用户定义的bridge和默认bridge的区别**</span><br><span class="line"></span><br><span class="line">* 用户定义的bridge在多个容器之间提供更好的隔离性和协调性.</span><br><span class="line"></span><br><span class="line">  连到同一个自定义的bridge的容器之间的所有端口互通.而无需通过-p参数暴露到宿主机.这让容器之间的通信更简单,而且提供更好的安全性.例如:</span><br><span class="line"></span><br><span class="line">  连到同一个自定义的bridge网络的Nginx容器和mysql容器.及时mysql容器没有暴露任何端口.nginx也可以访问mysql容器的3306端口.</span><br><span class="line"></span><br><span class="line">  而默认的Bridge网络,则需要将mysql容器通过```-p```参数暴露3306端口给宿主机.</span><br><span class="line"></span><br><span class="line">* 自定义bridge网络提供容器的主机名DNS解析</span><br><span class="line"></span><br><span class="line">​        默认的bridge网络下的容器间不能通过主机名互相访问,只能通过IP地址.(除非使用—link参数,但是这个参数已经废弃).而用户自定义的bridge网络则可以直接访问对方的主机名.</span><br><span class="line"></span><br><span class="line">* 自定义bridge网络配置更方便</span><br><span class="line"></span><br><span class="line">​        配置一个默认bridge网络,会影响到全局所有容器.而且需要重启docker服务.使用```docker network create```可以创建一个自定义bridge网络.,而且可以分别配置</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 创建和管理自定义bridge网络</span><br><span class="line"></span><br><span class="line">创建命令:</span><br></pre></td></tr></table></figure></p><p>#创建自定义网络名称为my-net<br>docker network create my-net</p><p>#还可以指定网络号,子网掩码等信息<br>[root@localhost ~]$docker network create –help</p><p>Usage:    docker network create [OPTIONS] NETWORK</p><p>Create a network</p><p>Options:<br>      –attachable           Enable manual container attachment<br>      –aux-address map      Auxiliary IPv4 or IPv6 addresses used by Network driver (default map)<br>      –config-from string   The network from which copying the configuration<br>      –config-only          Create a configuration only network<br>  -d, –driver string        Driver to manage the Network (default “bridge”)<br>      –gateway strings      IPv4 or IPv6 Gateway for the master subnet<br>      –ingress              Create swarm routing-mesh network<br>      –internal             Restrict external access to the network<br>      –ip-range strings     Allocate container ip from a sub-range<br>      –ipam-driver string   IP Address Management Driver (default “default”)<br>      –ipam-opt map         Set IPAM driver specific options (default map)<br>      –ipv6                 Enable IPv6 networking<br>      –label list           Set metadata on a network<br>  -o, –opt map              Set driver specific options (default map)<br>      –scope string         Control the network’s scope<br>      –subnet strings       Subnet in CIDR format that represents a network segment<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">删除自定义网络:</span><br></pre></td></tr></table></figure></p><p>#如果有容器正在使用该网络,需要先断开容器<br>docker network rm my-net<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 管理自定义网络下的容器</span><br><span class="line"></span><br><span class="line">创建一个自定义网络下的容器</span><br></pre></td></tr></table></figure></p><p>#在创建容器的时候指定自定义网络名</p><p>docker create –name my-nginx \<br>  –network my-net \<br>  –publish 8080:80 \<br>  nginx:latest<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">将一个正在运行的容器关联(移除)自定义网络</span><br></pre></td></tr></table></figure></p><p>#关联容器和自定义网络命令格式:<br>docker network connect</p><p>#例如,将一个正在运行的mysql容器关联到my-net网络下<br>docker network connect my-net mysql</p><p>#相反从自定义网络下移除一个容器命令:<br>docker network disconnect</p><p>#例如,将一个正在运行的mysql容器从my-net网络下移除<br>docker network disconnect my-net mysql<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 实验:</span><br><span class="line"></span><br><span class="line">* **默认的bridge网络,容器之间无法互相访问对方的主机名.只能通过iP地址通信**</span><br></pre></td></tr></table></figure></p><p>[root@localhost ~]$docker run -itd –rm –name=busybox busybox<br>b3c8be3b3b716579caf11d3852f6c6e04a41b4dc020d9478be1b1f3a4d76cf1f</p><p>[root@localhost ~]$docker run -itd –rm –name=busybox1 busybox<br>a54d962f6a692d96f0bc2fbca37e1b47e59e6b61541f42b8d6872a5008a46b87</p><p>[root@localhost ~]$docker exec busybox ping busybox1<br>ping: bad address ‘busybox1’<br>[root@localhost ~]$</p><p>#通过IP地址可以通信</p><p>[root@localhost ~]$docker exec busybox ping 172.17.0.8<br>PING 172.17.0.8 (172.17.0.8): 56 data bytes<br>64 bytes from 172.17.0.8: seq=0 ttl=64 time=0.136 ms<br>64 bytes from 172.17.0.8: seq=1 ttl=64 time=0.079 ms<br>64 bytes from 172.17.0.8: seq=2 ttl=64 time=0.131 ms<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* **自定义的Bridge网络,可以在容器之间互相访问主机名**</span><br></pre></td></tr></table></figure></p><p>#创建一个自定义网络<br>[root@localhost ~]$docker network create jesse<br>e10a936177681cbfc321f67f961f2a717079ef1790c50e82381296fa77bd7d5f</p><p>[root@localhost ~]$docker network ls | grep jesse<br>e10a93617768        jesse                  bridge              local</p><p>#创建容器,使用network参数指定网络<br>[root@localhost ~]$docker run –name busybox1 -itd –network jesse –rm busybox<br>aedf164ea1769741ae6480583abdc838022f49506761d4054daabdb7fffcd852</p><p>[root@localhost ~]$docker run –name busybox2 -itd –network jesse –rm busybox<br>523bdd54fa12423e25a8ac84f9faef1679ee6031f2d5d2a87c0cacd64f8650ad</p><p>[root@localhost ~]$docker exec busybox1 ping busybox2<br>PING busybox2 (172.20.0.3): 56 data bytes<br>64 bytes from 172.20.0.3: seq=0 ttl=64 time=0.116 ms<br>64 bytes from 172.20.0.3: seq=1 ttl=64 time=0.078 ms</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 连接到不同的bridge网络下的容器互相之间网络隔离</span><br></pre></td></tr></table></figure><p>docker network create bridge<br>docker network create kong-net</p><p>[work@docker conf.d]$docker run –name busybox1 –network bridge -itd busybox<br>f95a229aa7eb5d7022bef4441a075b0ad37ecc50e2a02015f09790d23b28dc33</p><p>[work@docker conf.d]$docker run –name busybox2 –network kong-net -itd busybox<br>a9236a25199cc43a899285462afa851a52cdaf871776a93829897676fc7dd82c</p><p>#busybox1和busybox2不在同一个IP网段</p><p>#busybox1的IP<br>[work@docker conf.d]$docker inspect busybox1<br>172.17.0.11</p><p>#busybox2的IP<br>[work@docker conf.d]$docker exec -it busybox2 ifconfig eth0<br>eth0      Link encap:Ethernet  HWaddr 02:42:AC:12:00:05<br>          inet addr:172.18.0.5  Bcast:0.0.0.0  Mask:255.255.0.0</p><p>#主机名无法访问<br>[work@docker conf.d]$docker exec -it busybox1 ping busybox2<br>ping: bad address ‘busybox2’</p><p>#busybox1容器也无法ping busybox2容器的IP<br>[work@docker conf.d]$docker exec -it busybox1 ping 172.18.0.5<br>PING 172.18.0.5 (172.18.0.5): 56 data bytes<br>^C<br>— 172.18.0.5 ping statistics —<br>32 packets transmitted, 0 packets received, 100% packet loss<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 由于博客不能识别go模板语法,所以省去了go模板,直接用docker inspect busybox1命令来代替.实际场景中该命令无法直接获取容器IP</span><br><span class="line"></span><br><span class="line">#### 将容器从自定义bridge网络中移除</span><br></pre></td></tr></table></figure></p><p>#将jesse从jesse网络移除<br>[root@localhost ~]$docker network disconnect jesse busybox1</p><p>#此时jesse网络下只有busybox2容器<br>[root@localhost ~]$docker network inspect jesse</p><p>#此时busybox1容器的网卡被移除了<br>[root@localhost ~]$docker inspect busybox1 </p><no value=""><p>root@localhost ~]$docker exec -it busybox1 ifconfig<br>lo        Link encap:Local Loopback<br>          inet addr:127.0.0.1  Mask:255.0.0.0<br>          UP LOOPBACK RUNNING  MTU:65536  Metric:1<br>          RX packets:11 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:11 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:1000<br>          RX bytes:618 (618.0 B)  TX bytes:618 (618.0 B)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 将一个正在运行的容器加入到自定义bridge网络</span><br></pre></td></tr></table></figure></p><p>#将jesse加回到jesse网络<br>[root@localhost ~]$docker network connect jesse busybox1</p><p>#获得新的IP地址<br>[root@localhost ~]$docker inspect busybox1<br>172.20.0.2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 宿主机转发容器端口</span><br><span class="line"></span><br><span class="line">默认情况下,bridge网络不会转发外部的请求到容器.开启转发需要更改2个设置:</span><br><span class="line"></span><br><span class="line">1.修改内容,开启转发</span><br></pre></td></tr></table></figure></p><p>sysctl net.ipv4.conf.all.forwarding=1</p><p>sysctl -p<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2.更改iptables的转发的默认规则</span><br></pre></td></tr></table></figure></p><p>sudo iptables -P FORWARD ACCEPT<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 更改默认Bridge网络配置</span><br><span class="line"></span><br><span class="line">docker0网桥是在docker daemon启动时自动创建的.IP默认为172.17.0.1/16.所有连到docker 0网桥的docker容器都会在这个IP范围内选取一个未占用的IP使用.并连接到docker 0网桥上</span><br><span class="line"></span><br><span class="line">docker提供了一些参数帮助用户自定义docker0网桥的设置</span><br><span class="line"></span><br><span class="line">* —bip=CIDR: 设置Docker0的IP地址和子网范围.使用CIDR格式.例如192.168.0.1/24.需要注意的是这个参数仅仅是配置docker0的,对其他自定义的网桥无效.</span><br><span class="line">* —fixed-cidr=CIDR:限制docker容器获取iP的范围.默认情况下docker容器获取的IP范围为整个docker0网桥的IP地址段,也就是—bip指定的地址范围.此参数可以将docker容器缩小到某个子网范围.</span><br><span class="line">* —mtu=BYTES: 指定docker0的最大传输单元(MTU)</span><br></pre></td></tr></table></figure></p><p>#更改daemon.json配置文件.下面这个例子修改了docker0网络的网段地址<br>vim /etc/docker/daemon.json</p><p>{<br>  “registry-mirrors”: [“<a href="https://registry.docker-cn.com&quot;]" target="_blank" rel="noopener">https://registry.docker-cn.com&quot;]</a>,<br>  “bip”: “192.168.1.5/24”,<br>  “mtu”: 1500,<br>  “dns”: [“114.114.114.114”,”114.114.115.115”]</p><p>}</p><p>#重启docker服务<br>[root@localhost ~]$systemctl restart docker</p><p>[root@localhost ~]$ifconfig | grep -A 5 docker0<br>docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 192.168.1.5  netmask 255.255.255.0  broadcast 192.168.1.255<br>        ether 02:42:89:26:d1:c9  txqueuelen 0  (Ethernet)<br>        RX packets 1072032  bytes 61915874 (59.0 MiB)<br>        RX errors 0  dropped 0  overruns 0  frame 0<br>        TX packets 1997027  bytes 1934238839 (1.8 GiB)<br><code>`</code></p><hr></no>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker官网学习–docker网络之bridge&quot;&gt;&lt;a href=&quot;#docker官网学习–docker网络之bridge&quot; class=&quot;headerlink&quot; title=&quot;docker官网学习–docker网络之bridge&quot;&gt;&lt;/a&gt;docker官网学习–docker网络之bridge&lt;/h2&gt;&lt;p&gt;本节介绍docker基础网络概念.以便能认识和利用各种不同的网络类型功能.&lt;/p&gt;
&lt;p&gt;docker的网络支持插件化,驱动化定制.有一些网络驱动已经默认集成到docker中.docker网络主要有以下类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bridge&lt;/li&gt;
&lt;li&gt;host&lt;/li&gt;
&lt;li&gt;overlay&lt;/li&gt;
&lt;li&gt;macvlan&lt;/li&gt;
&lt;li&gt;none&lt;/li&gt;
&lt;li&gt;其他网络插件&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker网络之host,Container,None网络</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0-7.docker%E7%BD%91%E7%BB%9C%E4%B9%8Bhost,Container,None%E7%BD%91%E7%BB%9C/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习-7.docker网络之host,Container,None网络/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:14:06.747Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker官网学习-7-docker网络之host-Container-None网络"><a href="#docker官网学习-7-docker网络之host-Container-None网络" class="headerlink" title="docker官网学习-7.docker网络之host,Container,None网络"></a>docker官网学习-7.docker网络之host,Container,None网络</h2><h3 id="host网络介绍"><a href="#host网络介绍" class="headerlink" title="host网络介绍"></a>host网络介绍</h3><p>如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。</p><hr><h3 id="创建host网络"><a href="#创建host网络" class="headerlink" title="创建host网络"></a>创建host网络</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">docker run -tid --net=host --name busybox busybox1</span><br><span class="line"></span><br><span class="line">#host网络下的容器没有虚拟网卡,而是和宿主机共享网络</span><br><span class="line">[root@localhost ~]$docker exec  -it busybox1 ifconfig</span><br><span class="line">docker0   Link encap:Ethernet  HWaddr 02:42:89:26:D1:C9</span><br><span class="line">          inet addr:192.168.1.5  Bcast:192.168.1.255  Mask:255.255.255.0</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</span><br><span class="line">          RX packets:1072032 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:1997027 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0</span><br><span class="line">          RX bytes:61915874 (59.0 MiB)  TX bytes:1934238839 (1.8 GiB)</span><br></pre></td></tr></table></figure><blockquote><p>注:host网络只能工作在Linux主机上.</p><p>如果容器没有暴露任何端口,那host网络没有任何效果</p></blockquote><a id="more"></a><p>host网络示意图</p><p><img src="http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVObjRZOVg0Q2tRRVZMV2dFZGt4MWljeThKY3VERXFhTGFZZUhiaDB1TWJZeVVtTjhQQ1l0bDl3LzA/d3hfZm10PXBuZw==" alt=""></p><hr><h3 id="Container网络介绍"><a href="#Container网络介绍" class="headerlink" title="Container网络介绍"></a>Container网络介绍</h3><p>这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备通信。</p><p>Container网络示意图</p><p><img src="http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVtaWM5elRoU1d1UmdSc2xVT2oyeHpmeUljZXdpY2E3VkpibE03Nnc5N01PZFRLVEl2TkdpYTBPd2cvMD93eF9mbXQ9cG5n" alt=""></p><hr><h3 id="None网络"><a href="#None网络" class="headerlink" title="None网络"></a>None网络</h3><p>使用none模式，Docker容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置。也就是说，这个Docker容器没有网卡、IP、路由等信息。需要我们自己为Docker容器添加网卡、配置IP等。</p><p>Node模式示意图:</p><p><img src="http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVMeFREaWNxUVFYQ0dObVlUNFlRdVdQYkxBRk1TVmhvRFlrcUtENFVLczVXbWtqbTM1THNpY1FZUS8wP3d4X2ZtdD1wbmc=" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker官网学习-7-docker网络之host-Container-None网络&quot;&gt;&lt;a href=&quot;#docker官网学习-7-docker网络之host-Container-None网络&quot; class=&quot;headerlink&quot; title=&quot;docker官网学习-7.docker网络之host,Container,None网络&quot;&gt;&lt;/a&gt;docker官网学习-7.docker网络之host,Container,None网络&lt;/h2&gt;&lt;h3 id=&quot;host网络介绍&quot;&gt;&lt;a href=&quot;#host网络介绍&quot; class=&quot;headerlink&quot; title=&quot;host网络介绍&quot;&gt;&lt;/a&gt;host网络介绍&lt;/h3&gt;&lt;p&gt;如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;创建host网络&quot;&gt;&lt;a href=&quot;#创建host网络&quot; class=&quot;headerlink&quot; title=&quot;创建host网络&quot;&gt;&lt;/a&gt;创建host网络&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker run -tid --net=host --name busybox busybox1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#host网络下的容器没有虚拟网卡,而是和宿主机共享网络&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[root@localhost ~]$docker exec  -it busybox1 ifconfig&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;docker0   Link encap:Ethernet  HWaddr 02:42:89:26:D1:C9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          inet addr:192.168.1.5  Bcast:192.168.1.255  Mask:255.255.255.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          RX packets:1072032 errors:0 dropped:0 overruns:0 frame:0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          TX packets:1997027 errors:0 dropped:0 overruns:0 carrier:0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          collisions:0 txqueuelen:0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          RX bytes:61915874 (59.0 MiB)  TX bytes:1934238839 (1.8 GiB)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;注:host网络只能工作在Linux主机上.&lt;/p&gt;
&lt;p&gt;如果容器没有暴露任何端口,那host网络没有任何效果&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---Dockerfile</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0%E2%80%942.dockerfile/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习—2.dockerfile/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T14:31:51.094Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h3><p>Dockerfile可以用来编译一个docker镜像.Dockerfile是一个包含一系列指令的文本文档,使用docker build命令,用户可以依据dockerfile和上下文编译一个镜像.</p><p>使用dockerfile需要注意一些事项</p><p><strong>1.上下文</strong></p><p>docker build编译镜像时,会将当前目录下的Dockerfile和所有文件打包添加发送到docker daemon服务端.所以一般情况下创建一个空目录编辑dockerfile文件.然后将需要copy和add的文件放进和dockerfile同一目录下.</p><p>dockerfile中的copy以及add命令,添加文件到docker镜像中时.不要使用绝对路径.例如/home/work/a.txt..docker deamon只能识别到当前上下文环境,无法识别到其他目录.但是可以使用当前上下文的相对路径.</p><p><strong>2.分层</strong></p><p>dockerfile编译镜像时,每条指令都是一个镜像层.除了From指令外,每一行指令都是基于上一行生成的临时镜像运行一个容器.执行一条指令就类似于docker commit命令生成一个新的镜像.所以两条指令之间互不关联.</p><a id="more"></a><p>例如,下列的dockerfile并不能在/data/目录下创建files文件.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu</span><br><span class="line">RUN mkdir /data</span><br><span class="line">RUN cd /data</span><br><span class="line">RUN touch files</span><br></pre></td></tr></table></figure><p>下列的dockerfile甚至不会创建/data/file文件,也不会修改/data/目录权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu</span><br><span class="line">RUN useradd foo</span><br><span class="line">VOLUME /data</span><br><span class="line">RUN touch /data/file</span><br><span class="line">RUN chown -R foo:foo /data</span><br></pre></td></tr></table></figure></p><p>想要实现这个需求,可以这样写:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu</span><br><span class="line">RUN useradd foo</span><br><span class="line">RUN mkdir /data &amp;&amp; touch /data/file</span><br><span class="line">RUN chown -R foo:foo /data</span><br><span class="line">VOLUME /data</span><br></pre></td></tr></table></figure><hr><p><strong>3.精简</strong></p><p>由于dockerfile在构建镜像时,dockerfile文本中每一行语句会产生每一层镜像.</p><p>例如下面这个dockerfile:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">From ubuntu</span><br><span class="line">RUN apt-get update</span><br><span class="line">RUN apt-get -y install vim git wget net-tools</span><br><span class="line">RUN useradd foo</span><br><span class="line">RUN mkdir /data</span><br><span class="line">RUN touch /data/file</span><br><span class="line">RUN chown -R foo:foo /data</span><br></pre></td></tr></table></figure><p>在编译时,每一个RUN语句都会构建一层镜像.(实际上所有指令都是这样,不仅仅是RUN)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost test]$docker build -t test:v1 .</span><br><span class="line">Sending build context to Docker daemon  2.048kB</span><br><span class="line">Step 1/7 : From ubuntu</span><br><span class="line"> ---&gt; 94e814e2efa8</span><br><span class="line">Step 2/7 : RUN apt-get update</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 5520126e7fcc</span><br><span class="line">Step 3/7 : RUN apt-get -y install vim git wget net-tools</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; cb24e170539c</span><br><span class="line">Step 4/7 : RUN useradd foo</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; ca31aeba0309</span><br><span class="line">Step 5/7 : RUN mkdir /data</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; d5c6e0f32f6b</span><br><span class="line">Step 6/7 : RUN touch /data/file</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 9c4b06e9b25d</span><br><span class="line">Step 7/7 : RUN chown -R foo:foo /data</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 75ecea0b0795</span><br><span class="line">Successfully built 75ecea0b0795</span><br><span class="line">Successfully tagged test:v1</span><br></pre></td></tr></table></figure><p>这种写法会导致镜像层非常多,镜像文件也会相对较大.所以一般推荐更精简的语法,每一条功能相同的语句,尽量写在一行.上面的dockerfile可以优化成:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">From ubuntu</span><br><span class="line">RUN apt-get update \</span><br><span class="line">    &amp;&amp; apt-get -y install \</span><br><span class="line">       vim \</span><br><span class="line">       git \</span><br><span class="line">       wget \</span><br><span class="line">       net-tools</span><br><span class="line"></span><br><span class="line">RUN useradd foo</span><br><span class="line">RUN mkdir /data &amp;&amp;  touch /data/file &amp;&amp;  chown -R foo:foo /data</span><br></pre></td></tr></table></figure><p>这次编译只需构建4层镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost test]$docker build -t test:v1 .</span><br><span class="line">Sending build context to Docker daemon  2.048kB</span><br><span class="line">Step 1/4 : From ubuntu</span><br><span class="line"> ---&gt; 94e814e2efa8</span><br><span class="line">Step 2/4 : RUN apt-get update     &amp;&amp; apt-get -y install        vim        git        wget        net-tools</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 7aa2bc9041e0</span><br><span class="line">Step 3/4 : RUN useradd foo</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 5a13764414e6</span><br><span class="line">Step 4/4 : RUN mkdir /data &amp;&amp;  touch /data/file &amp;&amp;  chown -R foo:foo /data</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; bd61817d7526</span><br><span class="line">Successfully built bd61817d7526</span><br><span class="line">Successfully tagged test:v1</span><br></pre></td></tr></table></figure><p>4.使用no-install-recommends</p><p>如果是使用APT包管理器,则应该在执行apt-get install 命令时加上no-install-recommends参数.这样ATP就仅安装核心依赖.而不安装其他推荐和建议的包,这会显著减少不必要包的下载数量</p><hr><h3 id="Dockerfile指令介绍"><a href="#Dockerfile指令介绍" class="headerlink" title="Dockerfile指令介绍"></a>Dockerfile指令介绍</h3><p>介绍完Dockerfile的概念和特点后,接下来了解一下Dockerfile语法中的具体指令的介绍和用法</p><p>下面是一个例子:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># Use an official Python runtime as a parent image</span><br><span class="line">FROM python:2.7-slim</span><br><span class="line"></span><br><span class="line"># Set the working directory to /app</span><br><span class="line">WORKDIR /app</span><br><span class="line"></span><br><span class="line"># Copy the current directory contents into the container at /app</span><br><span class="line">COPY . /app</span><br><span class="line"></span><br><span class="line"># Install any needed packages specified in requirements.txt</span><br><span class="line">RUN pip install --trusted-host pypi.python.org -r requirements.txt</span><br><span class="line"></span><br><span class="line"># Make port 80 available to the world outside this container</span><br><span class="line">EXPOSE 80</span><br><span class="line"></span><br><span class="line"># Define environment variable</span><br><span class="line">ENV NAME World</span><br><span class="line"></span><br><span class="line"># Run app.py when the container launches</span><br><span class="line">CMD [&quot;python&quot;, &quot;app.py&quot;]</span><br></pre></td></tr></table></figure><hr><ul><li>FROM </li></ul><p>格式: FROM image 或者 FROM image:tag</p><p>表示从一个基础镜像构建.Dockerfile必须以FROM语句作为第一条非注释语句.</p><ul><li><p>WORKDIR: 表示工作目录,后续的相对路径也是基于这个目录</p></li><li><p>COPY</p></li></ul><p>格式: copy src dest</p><p>复制宿主机上的文件到镜像中.src是当前上下文中的文件或者目录.dest是容器中的目标文件或者目录.src指定的源可以有多个.此外 src还支持通配符.例如: COPY hom* /mydir/ 表示添加所有当前目录下的hom开头的文件到目录/mydir/下</p><p><dest>可以是文件或者目录.但是必须是镜像中的绝对路径,或者是WORKDIR的相对路径.若<dest>以反斜杠/结尾,则指向的是目录,否则指向文件.当 src 有多个源时, dest必须是目录.如果 dest 目录不存在,则会自动被创建</dest></dest></p><ul><li>ADD</li></ul><p>格式: ADD src dest</p><p> ADD和COPY命令有相同功能,都支持复制本地文件到镜像里.但ADD能从互联网的URL下载文件到镜像..src还可以是一个本地的压缩归档文件.ADD会自动将tar,gz等压缩包上传到镜像后进行解压.</p><p> 但是如果src是一个URL的归档格式文件,则不会自动解压.</p><ul><li>RUN</li></ul><p>RUN命令有两种格式:</p><p>RUN <command> (shell格式)<br>RUN [“executable”,”param1”,”param2”] (exec格式)</p><p>RUN指令的两种格式表示命令在容器中的两种运行方式.当使用shell格式时,命令通过 /bin/sh -c 运行.当使用exec格式时.命令直接运行,不调用shell程序.exec格式中的参数会被当成JSON数组被Docker解析.所以必须使用双引号,不能使用单引号. </p><p>另外由于exec格式不会在shell中运行.所以无法识别ENV环境变量.例如当执行CMD [“echo”,”$HOME”]时,$HOME不会被变量替换.如果希望运行shell程序.可以写成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CMD [&quot;sh&quot;,&quot;-c&quot;,&quot;echo&quot;,&quot;$HOME&quot;]</span><br></pre></td></tr></table></figure><ul><li>EXPOSE: 镜像需要暴露出来的端口. </li></ul><blockquote><p>要注意的是,这里只是说明镜像需要暴露哪些端口,在镜像构建完毕,启动容器时,仍然需要-p参数来映射端口,否则端口不会自动映射</p></blockquote><ul><li><p>ENV</p><p>格式: ENV <key> <value> 或者 ENV <key>=<value></value></key></value></key></p><p>ENV指令用来声明环境变量,并且可以被(ADD,COPY,WORKDIR等)指令调用.调用ENV环境变量的格式和shell一样:\$variable_name或者 \${variable_name}</p></li><li><p>CMD </p></li></ul><p>CMD命令有3种格式:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CMD &lt;command&gt; (shell格式)  </span><br><span class="line">CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] (exec格式)  </span><br><span class="line">CMD [&quot;param1&quot;,&quot;param2&quot;] (为ENTRYPOINT命令提供参数)</span><br></pre></td></tr></table></figure><p>CMD提供容器启动后执行的命令.或者是为ENTRYPOINT传递一些参数.一个dockerfile文件只允许存在一条CMD指令.如果存在多条CMD指令,以最后一条为准.但是如果用户在 docker run 时指定了命令,则会覆盖CMD中的指令</p><ul><li>ENTRYPOINT</li></ul><p>ENTRYPOINT有两种格式.和上文CMD一样分为shell格式和exec格式.</p><p>ENTRYPOINT和CMD类似,指定容器启动时执行的命令.和CMD一样一个Dockerfile文件中可以有多个ENTRYPOINT命令.但只有最后一条生效.但是又有一些区别.当使用shell格式时,ENTRYPOINT会忽略任何CMD指令和 docker run启动容器时手动输入的指令.并且会运行在 /bin/sh -c环境中,成为它的子进程.进程在容器中PID不是1,也不能接收UNIX信号.(也就是在执行 docker stop <container>时,进程接收不到SIGTERM指令)</container></p><p>当使用exec格式时, docker run 手动指定的命令,将作为参数覆盖CMD指定的参数传递到ENTRYPOINT.(也就是说 docker run启动容器时指定的不再是具体命令,而是命令的参数).</p><hr><p>创建上面dockerfile中所需要的app.py和requirements.txt文件,并且将他们和Dockerfile文件放在同一目录下:</p><p>requirements.txt:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost docker_python]$cat requirements.txt</span><br><span class="line">Flask</span><br><span class="line">Redis</span><br></pre></td></tr></table></figure><p>app.py:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">root@localhost docker_python]$cat app.py</span><br><span class="line">from flask import Flask</span><br><span class="line">from redis import Redis, RedisError</span><br><span class="line">import os</span><br><span class="line">import socket</span><br><span class="line"></span><br><span class="line"># Connect to Redis</span><br><span class="line">redis = Redis(host=&quot;redis&quot;, db=0, socket_connect_timeout=2, socket_timeout=2)</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line">@app.route(&quot;/&quot;)</span><br><span class="line">def hello():</span><br><span class="line">    try:</span><br><span class="line">        visits = redis.incr(&quot;counter&quot;)</span><br><span class="line">    except RedisError:</span><br><span class="line">        visits = &quot;&lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;&quot;</span><br><span class="line"></span><br><span class="line">    html = &quot;&lt;h3&gt;Hello &#123;name&#125;!&lt;/h3&gt;&quot; \</span><br><span class="line">           &quot;&lt;b&gt;Hostname:&lt;/b&gt; &#123;hostname&#125;&lt;br/&gt;&quot; \</span><br><span class="line">           &quot;&lt;b&gt;Visits:&lt;/b&gt; &#123;visits&#125;&quot;</span><br><span class="line">    return html.format(name=os.getenv(&quot;NAME&quot;, &quot;world&quot;), hostname=socket.gethostname(), visits=visits)</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    app.run(host=&apos;0.0.0.0&apos;, port=80)</span><br><span class="line">[root@localhost docker_python]$</span><br></pre></td></tr></table></figure><hr><h4 id="开始构建镜像"><a href="#开始构建镜像" class="headerlink" title="开始构建镜像"></a>开始构建镜像</h4><p>首先确保Dockerfile里所需的文件,以及Dockerfile都在同一目录下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost docker_python]$ls</span><br><span class="line">app.py  Dockerfile  requirements.txt</span><br></pre></td></tr></table></figure><p>运行以下命令来构建一个镜像.使用–tag参数(或者-t),可以为镜像打个标签:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build --tag=friendlyhello .</span><br></pre></td></tr></table></figure><p>构建过程略..在构建过程中,注意以下现象:</p><p>1.构建镜像层:</p><p>部分指令会创建一个新的镜像层,而有些指令则不会.关于如何区分命令是否会新建镜像层,一个基本的原则是:</p><p>如果指令的作用是像镜像中添加新的文件或者程序,那么就会新建镜像层.(例如:RUN,COPY,ADD,FROM等)</p><p>如果只是告诉docker如何构建或者运行应用程序,增加或者修改容器的元数据,那么不会构建新的镜像层.(例如:WORKDIR,EXPOSE,ENV,ENTERPOINT等)</p><p>2.构建步骤:</p><p>基本等过程大致为:</p><p>运行临时容器—-&gt;在该容器中运行Dockerfile指令—-&gt;将运行结果保存为一个新等镜像层——&gt; 删除临时容器</p><p>构建完成后,通过以下命令可以看到构建的镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost docker_python]$docker images</span><br><span class="line">REPOSITORY                 TAG                 IMAGE ID            CREATED              SIZE</span><br><span class="line">friendlyhello              latest              f091d1bb803c        About a minute ago   131MB</span><br></pre></td></tr></table></figure><blockquote><p>或者也可以是输入以下命令 docker image ls</p></blockquote><blockquote><p>可能会疑惑,为什么tag标签是latest..镜像的完整标签格式应该是:friendlyhello:lastest.<br>如果需要在构建镜像时指定版本.可以使用: –tag=friendlyhello:v0.0.1</p></blockquote><hr><h4 id="使用构建的镜像启动一个容器"><a href="#使用构建的镜像启动一个容器" class="headerlink" title="使用构建的镜像启动一个容器"></a>使用构建的镜像启动一个容器</h4><p>输入以下命令,利用刚才的镜像启动一个容器:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 4000:80 friendlyhello</span><br></pre></td></tr></table></figure><p>执行结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost docker_python]$docker run -p 4000:80 friendlyhello</span><br><span class="line"> * Serving Flask app &quot;app&quot; (lazy loading)</span><br><span class="line"> * Environment: production</span><br><span class="line">   WARNING: Do not use the development server in a production environment.</span><br><span class="line">   Use a production WSGI server instead.</span><br><span class="line"> * Debug mode: off</span><br><span class="line"> * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)</span><br></pre></td></tr></table></figure><ul><li>docker run 表示启动一个容器</li><li>-p 宿主机端口:容器端口  表示将宿主机的端口映射给容器.如果是-P 80 表示随机映射一个宿主机的端口给容器</li></ul><p>此时可以在其他电脑上访问这个容器的80端口,下面是在我的PC上访问宿主机的4000端口,也就是刚才启动的容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> ✘ huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; f9b1b804404f&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; &lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;%</span><br></pre></td></tr></table></figure><p>容器默认是在前台执行,加上-d参数可以时容器运行在后台:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 4000:80 friendlyhello</span><br></pre></td></tr></table></figure><p>docker ps命令可以显示正在运行中的容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost docker_python]$docker ps</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND             CREATED             STATUS              PORTS                    NAMES</span><br><span class="line">fcf7d29ac627        friendlyhello              &quot;python app.py&quot;     5 seconds ago       Up 1 second         0.0.0.0:4000-&gt;80/tcp     stoic_colden</span><br></pre></td></tr></table></figure><blockquote><p>docker container ls命令也有同样的效果</p></blockquote><hr><p>这一节(包括第3小节)涉及到的基础命令如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">docker build -t friendlyhello .  # Create image using this directory&apos;s Dockerfile</span><br><span class="line">docker run -p 4000:80 friendlyhello  # Run &quot;friendlyname&quot; mapping port 4000 to 80</span><br><span class="line">docker run -d -p 4000:80 friendlyhello         # Same thing, but in detached mode</span><br><span class="line">docker container ls                                # List all running containers</span><br><span class="line">docker container ls -a             # List all containers, even those not running</span><br><span class="line">docker container stop &lt;hash&gt;           # Gracefully stop the specified container</span><br><span class="line">docker container kill &lt;hash&gt;         # Force shutdown of the specified container</span><br><span class="line">docker container rm &lt;hash&gt;        # Remove specified container from this machine</span><br><span class="line">docker container rm $(docker container ls -a -q)         # Remove all containers</span><br><span class="line">docker image ls -a                             # List all images on this machine</span><br><span class="line">docker image rm &lt;image id&gt;            # Remove specified image from this machine</span><br><span class="line">docker image rm $(docker image ls -a -q)   # Remove all images from this machine</span><br><span class="line">docker login             # Log in this CLI session using your Docker credentials</span><br><span class="line">docker tag &lt;image&gt; username/repository:tag  # Tag &lt;image&gt; for upload to registry</span><br><span class="line">docker push username/repository:tag            # Upload tagged image to registry</span><br><span class="line">docker run username/repository:tag                   # Run image from a registry</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Dockerfile&quot;&gt;&lt;a href=&quot;#Dockerfile&quot; class=&quot;headerlink&quot; title=&quot;Dockerfile&quot;&gt;&lt;/a&gt;Dockerfile&lt;/h3&gt;&lt;p&gt;Dockerfile可以用来编译一个docker镜像.Dockerfile是一个包含一系列指令的文本文档,使用docker build命令,用户可以依据dockerfile和上下文编译一个镜像.&lt;/p&gt;
&lt;p&gt;使用dockerfile需要注意一些事项&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.上下文&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;docker build编译镜像时,会将当前目录下的Dockerfile和所有文件打包添加发送到docker daemon服务端.所以一般情况下创建一个空目录编辑dockerfile文件.然后将需要copy和add的文件放进和dockerfile同一目录下.&lt;/p&gt;
&lt;p&gt;dockerfile中的copy以及add命令,添加文件到docker镜像中时.不要使用绝对路径.例如/home/work/a.txt..docker deamon只能识别到当前上下文环境,无法识别到其他目录.但是可以使用当前上下文的相对路径.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.分层&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;dockerfile编译镜像时,每条指令都是一个镜像层.除了From指令外,每一行指令都是基于上一行生成的临时镜像运行一个容器.执行一条指令就类似于docker commit命令生成一个新的镜像.所以两条指令之间互不关联.&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---Docker存储驱动篇</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E7%AC%94%E8%AE%B0%E2%80%94%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8%E7%AF%87/"/>
    <id>https://jesse.top/2020/06/29/docker/docker笔记—存储驱动篇/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:42:46.456Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker笔记——存储驱动篇"><a href="#docker笔记——存储驱动篇" class="headerlink" title="docker笔记——存储驱动篇"></a>docker笔记——存储驱动篇</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>通过前一篇的镜像笔记,我们知道docker的镜像是只读的,而且通过同一个镜像启动的docker容器,他们共享同一份底层镜像文件.</p><p>这里主要说一说.这些分层的多个只读Image镜像是如何在磁盘中存储的.</p><hr><h3 id="docker存储驱动"><a href="#docker存储驱动" class="headerlink" title="docker存储驱动"></a>docker存储驱动</h3><p>docker提供了多种存储驱动来实现不同的方式存储镜像，下面是常用的几种存储驱动：</p><ul><li>AUFS</li><li>OverlayFS</li><li>Devicemapper</li><li>Btrfs</li><li>ZFS</li></ul><p>下面说一说AUFS、OberlayFS及Devicemapper，更多的存储驱动说明可参考：<a href="http://dockone.io/article/1513" target="_blank" rel="noopener">http://dockone.io/article/1513</a></p><a id="more"></a><h4 id="AUFS"><a href="#AUFS" class="headerlink" title="AUFS"></a>AUFS</h4><p>AUFS（AnotherUnionFS）是一种Union FS，是文件级的存储驱动。AUFS是一个能透明覆盖一个或多个现有文件系统的层状文件系统，把多层合并成文件系统的单层表示。简单来说就是支持将不同目录挂载到同一个虚拟文件系统下的文件系统。这种文件系统可以一层一层地叠加修改文件。无论底下有多少层都是只读的，只有最上层的文件系统是可写的。当需要修改一个文件时，AUFS创建该文件的一个副本，使用CoW(写时复制)将文件从只读层复制到可写层进行修改，结果也保存在可写层。</p><p>通常来说最上层是可读写层,下层是只读层.当需要读取一个文件A时,会从最顶层的读写层开始向下寻找.本层没有则根据层关系到下一层开始找.直到找到第一个文件A</p><p>当需要写入一个文件A时,如果这个文件不存在,则在读写层新建一个.否则会像上面的步骤一样从顶层开始寻找,找到A文件后,复制到读写层进行修改</p><p>当需要删除一个文件A时,如果这个文件仅仅存在读写层,则直接删除.否则就需要先在读写层删除,然后再在读写层创建一个whiteout文件来标志这个文件不存在,而不是真正删除底层文件.</p><p>当新建一个文件A.如果这个文件在读写层存在对应的whiteout文件,则先将whiteout文件删除再新建.否则直接读写层新建</p><p>在Docker中，底下的只读层就是image，可写层就是Container。结构如下图所示：</p><p><img src="https://img1.jesse.top/docker-aufs.jpg" alt=""></p><p>如果你正在使用aufs作为存储驱动,那么在Docker的工作目录(/var/lib/docker)和image下发现aufs目录:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">root@docker:~# tree /var/lib/docker -d -L 1</span><br><span class="line">/var/lib/docker</span><br><span class="line">├── aufs</span><br><span class="line">├── containers</span><br><span class="line">├── image</span><br><span class="line">├── network</span><br><span class="line">├── plugins</span><br><span class="line">├── swarm</span><br><span class="line">├── tmp</span><br><span class="line">├── trust</span><br><span class="line">└── volumes</span><br><span class="line"></span><br><span class="line">root@docker:~# tree /var/lib/docker/image -d -L 1</span><br><span class="line">/var/lib/docker/image</span><br><span class="line">└── aufs</span><br><span class="line"></span><br><span class="line">root@docker:~# tree /var/lib/docker/aufs/ -d -L 1</span><br><span class="line">/var/lib/docker/aufs/</span><br><span class="line">├── diff</span><br><span class="line">├── layers</span><br><span class="line">└── mnt</span><br></pre></td></tr></table></figure><p>在docker工作目录的aufs目录下有3个目录.其中mnt为aufs的挂载目录,diff为实际数据,包括只读层和读写层.layers为每层依赖有关的层描述文件</p><hr><h3 id="Device-mapper"><a href="#Device-mapper" class="headerlink" title="Device mapper"></a>Device mapper</h3><p>Device mapper是Linux内核2.6.9后支持的，提供的一种从逻辑设备到物理设备的映射框架机制，在该机制下，用户可以很方便的根据自己的需要制定实现存储资源的管理策略。前面讲的AUFS和OverlayFS都是文件级存储，而Device mapper是块级存储，所有的操作都是直接对块进行操作，而不是文件。</p><p>Device mapper驱动会先在块设备上创建一个资源池，然后在资源池上创建一个带有文件系统的基本设备，所有镜像都是这个基本设备的快照，而容器则是镜像的快照。所以在容器里看到文件系统是资源池上基本设备的文件系统的快照，并不有为容器分配空间。当要写入一个新文件时，在容器的镜像内为其分配新的块并写入数据，这个叫用时分配。</p><p>当要修改已有文件时，再使用CoW为容器快照分配块空间，将要修改的数据复制到在容器快照中新的块里再进行修改。Device mapper 驱动默认会创建一个100G的文件包含镜像和容器。每一个容器被限制在10G大小的卷内，可以自己配置调整。结构如下图所示：</p><p><img src="https://img1.jesse.top/docker-devicemapper.jpg" alt=""></p><p>在Centos 7发行版上最新版的docker中,默认的存储驱动就是device mapper.但是提示已经被弃用了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker info</span><br><span class="line">Containers: 3</span><br><span class="line"> Running: 3</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 0</span><br><span class="line">Images: 138</span><br><span class="line">Server Version: 18.09.2</span><br><span class="line">Storage Driver: devicemapper #这一行</span><br><span class="line">......略......</span><br><span class="line">WARNING: the devicemapper storage-driver is deprecated, and will be removed in a future release. #最后这一行提示devicemapper已经被弃用</span><br></pre></td></tr></table></figure><p>和aufs一样,在docker的工作目录下也能看到device mapper目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$ll /var/lib/docker/devicemapper/</span><br><span class="line">总用量 32</span><br><span class="line">drwx------ 2 root root    32 2月  23 16:25 devicemapper</span><br><span class="line">drwx------ 2 root root 24576 5月  16 10:30 metadata</span><br><span class="line">drwxr-xr-x 5 root root  4096 5月  16 10:30 mnt</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker笔记——存储驱动篇&quot;&gt;&lt;a href=&quot;#docker笔记——存储驱动篇&quot; class=&quot;headerlink&quot; title=&quot;docker笔记——存储驱动篇&quot;&gt;&lt;/a&gt;docker笔记——存储驱动篇&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;通过前一篇的镜像笔记,我们知道docker的镜像是只读的,而且通过同一个镜像启动的docker容器,他们共享同一份底层镜像文件.&lt;/p&gt;
&lt;p&gt;这里主要说一说.这些分层的多个只读Image镜像是如何在磁盘中存储的.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;docker存储驱动&quot;&gt;&lt;a href=&quot;#docker存储驱动&quot; class=&quot;headerlink&quot; title=&quot;docker存储驱动&quot;&gt;&lt;/a&gt;docker存储驱动&lt;/h3&gt;&lt;p&gt;docker提供了多种存储驱动来实现不同的方式存储镜像，下面是常用的几种存储驱动：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AUFS&lt;/li&gt;
&lt;li&gt;OverlayFS&lt;/li&gt;
&lt;li&gt;Devicemapper&lt;/li&gt;
&lt;li&gt;Btrfs&lt;/li&gt;
&lt;li&gt;ZFS&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面说一说AUFS、OberlayFS及Devicemapper，更多的存储驱动说明可参考：&lt;a href=&quot;http://dockone.io/article/1513&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://dockone.io/article/1513&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>kong+postgresql+konga集群环境部署</title>
    <link href="https://jesse.top/2020/06/26/Linux-Web/kong%20postgresql%20konga/"/>
    <id>https://jesse.top/2020/06/26/Linux-Web/kong postgresql konga/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T03:09:18.121Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kong-postgresql-konga集群环境部署"><a href="#kong-postgresql-konga集群环境部署" class="headerlink" title="kong+postgresql+konga集群环境部署"></a>kong+postgresql+konga集群环境部署</h2><h3 id="kong简介"><a href="#kong简介" class="headerlink" title="kong简介"></a>kong简介</h3><p>Kong是Mashape开源的一款API网关，起初是用来管理 Mashape 公司15000个微服务的，后来在2015年开源,现在已经在很多创业公司、大型企业和政府机构中广泛使用。基于nginx,Lua和Cassandra或PostgreSQL，支持分布式操作，有很强的可移植性和可扩展性。可以在任何一种基础设施上运行,作为应用和API之间的中间层，加上众多功能强大的插件，可以实现认证授权、访问控制等功能。并且提供易于使用的RESTful API来操作和配置系统。</p><p>有关kong的详细介绍请参考官网.</p><p>–</p><h3 id="postgreSQL简介"><a href="#postgreSQL简介" class="headerlink" title="postgreSQL简介"></a>postgreSQL简介</h3><p><a href="https://baike.baidu.com/item/PostgreSQL/530240" target="_blank" rel="noopener">PostgreSQL</a> 是一个免费的对象-关系数据库服务器(数据库管理系统)，它在灵活的 BSD-风格许可证下发行。它提供了相对其他开放源代码数据库系统(比如 MySQL 和 Firebird)，和专有系统(比如 Oracle、Sybase、IBM 的 DB2 和 Microsoft SQL Server)之外的另一种选择。</p><p>–</p><h3 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h3><ul><li><strong>kong cluster</strong></li></ul><p>kong 集群并不意味着客户端请求将会负载均衡到kong集群中的每个节点上，kong集群并不是开箱即用，仍然需要在kong集群多节点上层搭建负载均衡，以便分发请求。 一个kong集群只是意味着集群内的节点，都共享同样的配置。</p><p>有关Kong cluster集群的详细介绍请参考官网:<a href="https://docs.konghq.com/0.14.x/clustering/" target="_blank" rel="noopener">Kong cluser document</a></p><p>为了提高冗余性和健壮性.我们对kong的每个环节都进行了冗余设计.一个基本的kong集群架构大概如下图所示:</p><p><img src="https://img1.jesse.top/kong-flow.png" alt=""></p><a id="more"></a><p>–</p><h3 id="部署步骤"><a href="#部署步骤" class="headerlink" title="部署步骤"></a>部署步骤</h3><p><strong>环境</strong>: </p><p>阿里云ECS Centos7.4操作系统<br>Kong: 1.0最新版<br>postgresql 9.6</p><p>konga 最新版</p><p><strong>集群架构说明:</strong></p><p>dwd-kong-node1节点服务器部署postgresql master主库</p><p>dwd-kong-node2节点服务器部署postgresql slave从库</p><p>在两个Kong节点服务器上都部署konga,但是konga指向postgresql master主库</p><p>–</p><h4 id="安装postgresql"><a href="#安装postgresql" class="headerlink" title="安装postgresql"></a>安装postgresql</h4><p>安装方式官网参考: <a href="https://www.postgresql.org/" target="_blank" rel="noopener">Installing Postgresql</a></p><p><strong>安装步骤</strong></p><p>1.安装Postgre9.6版本的Yum源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-centos96-9.6-3.noarch.rpm</span><br></pre></td></tr></table></figure><p>2.安装postgresql</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install postgresql96 postgresql96-server -y</span><br></pre></td></tr></table></figure><p>3.切换到root用户下,初始化数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /usr/pgsql-9.6/bin/postgresql96-setup initdb</span><br></pre></td></tr></table></figure><p>4.修改数据库的配置文件,监听所有接口</p><p>postgresql的配置文件默认路径在:/var/lib/pgsql/9.6/data/postgresql.conf</p><p>修改下列配置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">listen_addresses = &apos;*&apos; #监听所有接口.由于我服务器只有内网地址,所有可以侦听在所有接口,(如果有公网地址,最好不要只样做)</span><br><span class="line">log_directory = &apos;/data/logs/postgre&apos; #指定日志文件的父目录</span><br></pre></td></tr></table></figure><p>5.修改数据库远程访问配置文件.开启远程访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim /var/lib/pgsql/9.6/data/postgresql.conf/pg_hba.conf</span><br><span class="line"></span><br><span class="line">修改下列两项:</span><br><span class="line">#修改为md5认证,下列10.0.0.0/8是内网地址段</span><br><span class="line">host    all             all             127.0.0.1/32            md5</span><br><span class="line">host    all             all             10.0.0.0/8           md5</span><br></pre></td></tr></table></figure><p>6.创建日志目录,并且赋权给postgres用户(安装postgresql后默认会创建这个用户)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-kong-node1 data]# mkdir /data/logs/postgre</span><br><span class="line">[root@dwd-kong-node1 data]# chown -R postgre.postgre /data/logs/postgre</span><br></pre></td></tr></table></figure><p>7.启动postgresql数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable postgresql-9.6</span><br><span class="line">systemctl start postgresql-9.6</span><br></pre></td></tr></table></figure><hr><h3 id="创建数据库用户密码"><a href="#创建数据库用户密码" class="headerlink" title="创建数据库用户密码"></a>创建数据库用户密码</h3><p>1.切换到postgres用户,输入psql登陆数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-kong-node1 data]# su - postgres</span><br><span class="line">Last login: Mon Apr  8 09:27:12 CST 2019 on pts/0</span><br><span class="line">-bash-4.2$</span><br><span class="line">-bash-4.2$ psql</span><br><span class="line">psql (11.2, server 9.6.12)</span><br><span class="line">Type &quot;help&quot; for help.</span><br><span class="line"></span><br><span class="line">postgres=#</span><br></pre></td></tr></table></figure><p>2.创建kong用户和kong数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CREATE USER kong; </span><br><span class="line">CREATE DATABASE kong OWNER kong;</span><br></pre></td></tr></table></figure><p>3.创建konga的数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE konga OWNER kong;</span><br></pre></td></tr></table></figure><p>4.为kong用户创建一个密码,密码也是kong</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter user kong with password &apos;kong&apos;;</span><br></pre></td></tr></table></figure><p>5.输入\l,可以看到当前的数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">postgres=# \l</span><br><span class="line">                                  List of databases</span><br><span class="line">   Name    |  Owner   | Encoding |   Collate   |    Ctype    |   Access privileges</span><br><span class="line">-----------+----------+----------+-------------+-------------+-----------------------</span><br><span class="line"> kong      | kong     | UTF8     | en_US.UTF-8 | en_US.UTF-8 |</span><br><span class="line"> konga     | kong     | UTF8     | en_US.UTF-8 | en_US.UTF-8 |</span><br><span class="line"> postgres  | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |</span><br><span class="line"> template0 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +</span><br><span class="line">           |          |          |             |             | postgres=CTc/postgres</span><br><span class="line"> template1 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +</span><br><span class="line">           |          |          |             |             | postgres=CTc/postgres</span><br><span class="line">(5 rows)</span><br></pre></td></tr></table></figure><p>6.查看当前数据库有哪些用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">postgres=# select * from pg_roles;</span><br><span class="line">      rolname      | rolsuper | rolinherit | rolcreaterole | rolcreatedb | rolcanlogin | rolreplication | rolconnlimit | rolpassword | rolvaliduntil | rolbypassrls | rolconfig |  oid</span><br><span class="line">-------------------+----------+------------+---------------+-------------+-------------+----------------+--------------+-------------+---------------+--------------+-----------+-------</span><br><span class="line"> postgres          | t        | t          | t             | t           | t           | t              |           -1 | ********    |               | t            |           |    10</span><br><span class="line"> pg_signal_backend | f        | t          | f             | f           | f           | f              |           -1 | ********    |               | f            |           |  4200</span><br><span class="line"> kong              | f        | t          | f             | f           | t           | f              |           -1 | ********    |               | f            |           | 16384</span><br><span class="line">(3 rows)</span><br></pre></td></tr></table></figure><hr><h4 id="安装Kong"><a href="#安装Kong" class="headerlink" title="安装Kong"></a>安装Kong</h4><p>安装方法可以参考官网:<a href="https://docs.konghq.com/install/centos/?_ga=2.110797315.728319704.1539597667-917309945.1539077269#packages" target="_blank" rel="noopener">Install Kong</a></p><p>1.下载,安装rpm安装包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget -O kong-community-edition-1.0.0.el7.noarch.rpm  https://kong.bintray.com/kong-community-edition-rpm/centos/7/:kong-community-edition-1.0.0.el7.noarch.rpm</span><br><span class="line"></span><br><span class="line">sudo yum install epel-release</span><br><span class="line">sudo yum install kong-community-edition-1.0.0.el7.noarch.rpm</span><br></pre></td></tr></table></figure><p>2.dwd-kong-node1修改kong配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kong</span><br><span class="line">sudo cp kong.conf.default kong.conf</span><br><span class="line"></span><br><span class="line">[root@dwd-kong-node1 data]#  sed -r &apos;/^[[:space:]]+#*/d&apos; /etc/kong/kong.conf | sed &apos;/^#/d&apos; | sed &apos;/^$/d&apos;</span><br><span class="line"></span><br><span class="line">prefix = /data/logs/kong/       # Working directory. Equivalent to Nginx&apos;s</span><br><span class="line">proxy_access_log = access.log       # Path for proxy port request access</span><br><span class="line">proxy_error_log = error.log         # Path for proxy port request error</span><br><span class="line">admin_listen = 0.0.0.0:8001     # Address and port on which Kong will expose</span><br><span class="line">database = postgres             # Determines which of PostgreSQL or Cassandra</span><br><span class="line">pg_host = 127.0.0.1             # The PostgreSQL host to connect to.</span><br><span class="line">pg_port = 5432                  # The port to connect to.</span><br><span class="line">pg_user = kong                  # The username to authenticate if required.</span><br><span class="line">pg_password = kong                 # The password to authenticate if required.</span><br><span class="line">pg_database = kong              # The database name to connect to.</span><br><span class="line">cluster_listen = 0.0.0.0:7946   # Address and port used to communicate with</span><br><span class="line">cluster_listen_rpc = 127.0.0.1:7373  # Address and port used to communicate</span><br></pre></td></tr></table></figure><p>3.dwd-kong-node2修改kong配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-kong-node2 system]# clear</span><br><span class="line">[root@dwd-kong-node2 system]# sed -r &apos;/^[[:space:]]+#*/d&apos; /etc/kong/kong.conf | sed &apos;/^#/d&apos; | sed &apos;/^$/d&apos;</span><br><span class="line">prefix = /data/logs/kong/       # Working directory. Equivalent to Nginx&apos;s</span><br><span class="line">proxy_access_log = access.log       # Path for proxy port request access</span><br><span class="line">proxy_error_log = error.log         # Path for proxy port request error</span><br><span class="line">database = postgres             # Determines which of PostgreSQL or Cassandra</span><br><span class="line">pg_host = 10.111.30.174             # The PostgreSQL host to connect to.</span><br><span class="line">pg_port = 5432                  # The port to connect to.</span><br><span class="line">pg_user = kong                  # The username to authenticate if required.</span><br><span class="line">pg_password = kong                 # The password to authenticate if required.</span><br><span class="line">pg_database = kong              # The database name to connect to.</span><br><span class="line">pg_ssl = off                    # Toggles client-server TLS connections</span><br><span class="line">pg_ssl_verify = off             # Toggles server certificate verification if</span><br><span class="line">cluster_listen = 0.0.0.0:7946   # Address and port used to communicate with</span><br><span class="line">cluster_listen_rpc = 127.0.0.1:7373  # Address and port used to communicate</span><br></pre></td></tr></table></figure><blockquote><p>唯一区别就是这里的数据库指向dwd-kong-node1上的postgresql(IP:10.111.30.174),而非本机.</p></blockquote><p>3.创建kong目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /data/logs/kong/</span><br></pre></td></tr></table></figure><p>4.在dwd-kong-node1上准备数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kong migrations bootstrap -c /etc/kong/kong.conf</span><br></pre></td></tr></table></figure><blockquote><p>由于dwd-kong-node2上指向了node1的数据库,所以在node2上不需要执行这个命令</p></blockquote><p>5.启动kong</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kong start -c /etc/kong/kong.conf</span><br></pre></td></tr></table></figure><p>查看端口.可以看到postgresql和kong的侦听端口都已经成功启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-kong-node1 data]# netstat -tulpn</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp        0      0 0.0.0.0:5432            0.0.0.0:*               LISTEN      26580/postmaster</span><br><span class="line">tcp        0      0 0.0.0.0:8443            0.0.0.0:*               LISTEN      24746/nginx: master</span><br><span class="line">tcp        0      0 0.0.0.0:5822            0.0.0.0:*               LISTEN      29688/sshd</span><br><span class="line">tcp        0      0 0.0.0.0:8000            0.0.0.0:*               LISTEN      24746/nginx: master</span><br><span class="line">tcp        0      0 0.0.0.0:8001            0.0.0.0:*               LISTEN      24746/nginx: master</span><br><span class="line">tcp6       0      0 :::5432                 :::*                    LISTEN      26580/postmaster</span><br><span class="line">udp        0      0 0.0.0.0:68              0.0.0.0:*                           748/dhclient</span><br><span class="line">udp        0      0 10.111.30.174:123       0.0.0.0:*                           6290/ntpd</span><br><span class="line">udp        0      0 127.0.0.1:123           0.0.0.0:*                           6290/ntpd</span><br><span class="line">udp        0      0 0.0.0.0:123             0.0.0.0:*                           6290/ntpd</span><br><span class="line">udp        0      0 0.0.0.0:34019           0.0.0.0:*                           748/dhclient</span><br><span class="line">udp6       0      0 :::31421                :::*                                748/dhclient</span><br><span class="line">udp6       0      0 :::123                  :::*                                6290/ntpd</span><br></pre></td></tr></table></figure><hr><h3 id="搭建konga"><a href="#搭建konga" class="headerlink" title="搭建konga"></a>搭建konga</h3><p>konga是管理kong各个组件(serveice,route,plugin,upstream,consumer)的可视化UI管理工具.在增删改查各个组件的配置时非常方便.</p><p>个人觉得UI界面比kong-dashboard要漂亮</p><p>konga的github参考:<a href="https://github.com/pantsel/konga" target="_blank" rel="noopener">konga</a></p><hr><p>1.确保需要有node.js环境.如果没有npm工具,必须先安装nodejs</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@DWD-BETA kong]$ npm -v</span><br><span class="line">6.4.1</span><br><span class="line">[work@DWD-BETA kong]$ node -v</span><br><span class="line">v10.15.3</span><br></pre></td></tr></table></figure><p>2.安装bower,gulp包.安装git软件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm install bower</span><br><span class="line">npm install gulp</span><br><span class="line">yum install git</span><br></pre></td></tr></table></figure><p>3.work用户下安装konga</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/pantsel/konga.git</span><br><span class="line">$ cd konga</span><br><span class="line">$ npm i</span><br></pre></td></tr></table></figure><p>4.编辑.env环境文件(dwd-kong-node2的文件内容中将下列的localhost修改为node1服务器的IP:10.111.30.174)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">work@dwd-kong-node1 konga]$ cat .env_example</span><br><span class="line">PORT=1337</span><br><span class="line">NODE_ENV=production</span><br><span class="line">KONGA_HOOK_TIMEOUT=120000</span><br><span class="line">DB_ADAPTER=postgres</span><br><span class="line">DB_URI=postgresql://localhost:5432/konga</span><br><span class="line">KONGA_LOG_LEVEL=warn</span><br><span class="line">TOKEN_SECRET=some_secret_token</span><br><span class="line">DB_USER=kong</span><br><span class="line">DB_PASSWORD=kong</span><br><span class="line">DB_DATABASE=konga</span><br></pre></td></tr></table></figure><p>5.启动konga</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">work@dwd-kong-node1 konga]$ npm start</span><br></pre></td></tr></table></figure><p>6.如果启动报错,则安装依赖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">work@dwd-kong-node1 konga]$ npm run bower-deps</span><br></pre></td></tr></table></figure><p>这个程序默认是前台启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[work@dwd-kong-node1 konga]$ npm start</span><br><span class="line"></span><br><span class="line">&gt; kongadmin@0.14.3 start /home/work/konga</span><br><span class="line">&gt; node --harmony app.js</span><br><span class="line"></span><br><span class="line">No DB Adapter defined. Using localDB...</span><br><span class="line">debug: Hook:api_health_checks:process() called</span><br><span class="line">debug: Hook:health_checks:process() called</span><br><span class="line">debug: Hook:start-scheduled-snapshots:process() called</span><br><span class="line">debug: Hook:upstream_health_checks:process() called</span><br><span class="line">debug: Hook:user_events_hook:process() called</span><br><span class="line">debug: User had models, so no seed needed</span><br><span class="line">debug: Kongnode had models, so no seed needed</span><br><span class="line">debug: Emailtransport seeds updated</span><br><span class="line">debug: -------------------------------------------------------</span><br><span class="line">debug: :: Mon Apr 08 2019 18:34:31 GMT+0800 (China Standard Time)</span><br><span class="line">debug: Environment : development</span><br><span class="line">debug: Port        : 1337</span><br><span class="line">debug: -------------------------------------------------------</span><br></pre></td></tr></table></figure><p>此时在浏览器输入:<a href="http://10.111.30.174:1337" target="_blank" rel="noopener">http://10.111.30.174:1337</a> 就能访问konga了.</p><hr><h3 id="systemctl管理kong和konga进程"><a href="#systemctl管理kong和konga进程" class="headerlink" title="systemctl管理kong和konga进程"></a>systemctl管理kong和konga进程</h3><ul><li>kong</li></ul><p>在/usr/lib/systemd/system路径下编辑kong.service文件.内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">work@dwd-kong-node1 system]$ pwd</span><br><span class="line">/usr/lib/systemd/system</span><br><span class="line">[work@dwd-kong-node1 system]$ cat kong.service</span><br><span class="line">[Unit]</span><br><span class="line">Description= kong service</span><br><span class="line">After=syslog.target network.target postgresql-9.6.target</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">User=work</span><br><span class="line">Group=work</span><br><span class="line">Type=forking</span><br><span class="line">ExecStart=/usr/local/bin/kong start -c /etc/kong/kong.conf</span><br><span class="line">ExecReload=/usr/local/bin/kong reload -c /etc/kong/kong.conf</span><br><span class="line">ExecStop=/usr/local/bin/kong stop</span><br><span class="line">Restart=always</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><ul><li>konga</li></ul><p>在/usr/lib/systemd/system路径下编辑konga.service文件.内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[work@dwd-kong-node1 system]$ cat konga.service</span><br><span class="line">[Unit]</span><br><span class="line">Description= konga service</span><br><span class="line">After=syslog.target network.target postgresql-9.6.target  kong.target</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">User=work</span><br><span class="line">Group=work</span><br><span class="line">Type=forking</span><br><span class="line">#需要指定工作目录,因为npm命令要在konga的目录下执行</span><br><span class="line">WorkingDirectory=/home/work/konga</span><br><span class="line">ExecStart=/usr/local/bin/npm start</span><br><span class="line">ExecStop=kill $(netstat -tlnp |grep 1337|  awk &apos;&#123;print $NF&#125;&apos; | awk -F &quot;/&quot; &apos;&#123;print $1&#125;&apos;)</span><br><span class="line">Restart=always</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><ul><li>启动进程,且设置开机启动</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[work@dwd-kong-node2 ~]$ sudo systemctl enable kong</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/kong.service to /usr/lib/systemd/system/kong.service.</span><br><span class="line">[work@dwd-kong-node2 ~]$ sudo systemctl enable konga.service</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/konga.service to /usr/lib/systemd/system/konga.service.</span><br><span class="line">[work@dwd-kong-node2 ~]$ sudo systemctl start kong.service</span><br><span class="line">[work@dwd-kong-node2 ~]$ sudo systemctl start konga.service</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kong-postgresql-konga集群环境部署&quot;&gt;&lt;a href=&quot;#kong-postgresql-konga集群环境部署&quot; class=&quot;headerlink&quot; title=&quot;kong+postgresql+konga集群环境部署&quot;&gt;&lt;/a&gt;kong+postgresql+konga集群环境部署&lt;/h2&gt;&lt;h3 id=&quot;kong简介&quot;&gt;&lt;a href=&quot;#kong简介&quot; class=&quot;headerlink&quot; title=&quot;kong简介&quot;&gt;&lt;/a&gt;kong简介&lt;/h3&gt;&lt;p&gt;Kong是Mashape开源的一款API网关，起初是用来管理 Mashape 公司15000个微服务的，后来在2015年开源,现在已经在很多创业公司、大型企业和政府机构中广泛使用。基于nginx,Lua和Cassandra或PostgreSQL，支持分布式操作，有很强的可移植性和可扩展性。可以在任何一种基础设施上运行,作为应用和API之间的中间层，加上众多功能强大的插件，可以实现认证授权、访问控制等功能。并且提供易于使用的RESTful API来操作和配置系统。&lt;/p&gt;
&lt;p&gt;有关kong的详细介绍请参考官网.&lt;/p&gt;
&lt;p&gt;–&lt;/p&gt;
&lt;h3 id=&quot;postgreSQL简介&quot;&gt;&lt;a href=&quot;#postgreSQL简介&quot; class=&quot;headerlink&quot; title=&quot;postgreSQL简介&quot;&gt;&lt;/a&gt;postgreSQL简介&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://baike.baidu.com/item/PostgreSQL/530240&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PostgreSQL&lt;/a&gt; 是一个免费的对象-关系数据库服务器(数据库管理系统)，它在灵活的 BSD-风格许可证下发行。它提供了相对其他开放源代码数据库系统(比如 MySQL 和 Firebird)，和专有系统(比如 Oracle、Sybase、IBM 的 DB2 和 Microsoft SQL Server)之外的另一种选择。&lt;/p&gt;
&lt;p&gt;–&lt;/p&gt;
&lt;h3 id=&quot;集群架构&quot;&gt;&lt;a href=&quot;#集群架构&quot; class=&quot;headerlink&quot; title=&quot;集群架构&quot;&gt;&lt;/a&gt;集群架构&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;kong cluster&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;kong 集群并不意味着客户端请求将会负载均衡到kong集群中的每个节点上，kong集群并不是开箱即用，仍然需要在kong集群多节点上层搭建负载均衡，以便分发请求。 一个kong集群只是意味着集群内的节点，都共享同样的配置。&lt;/p&gt;
&lt;p&gt;有关Kong cluster集群的详细介绍请参考官网:&lt;a href=&quot;https://docs.konghq.com/0.14.x/clustering/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kong cluser document&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;为了提高冗余性和健壮性.我们对kong的每个环节都进行了冗余设计.一个基本的kong集群架构大概如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img1.jesse.top/kong-flow.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>docker安装最新版Kong(v1.0)+konga</title>
    <link href="https://jesse.top/2020/06/26/Linux-Web/docker%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0%E7%89%88Kong(v1.0)%20konga/"/>
    <id>https://jesse.top/2020/06/26/Linux-Web/docker安装最新版Kong(v1.0) konga/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T07:32:53.722Z</updated>
    
    <content type="html"><![CDATA[<h3 id="docker安装最新版Kong-v1-0-konga"><a href="#docker安装最新版Kong-v1-0-konga" class="headerlink" title="docker安装最新版Kong(v1.0)+konga"></a>docker安装最新版Kong(v1.0)+konga</h3><p>参考以下文档:</p><p><a href="https://docs.konghq.com/install/docker/?_ga=2.167535422.1288669860.1553147426-917309945.1539077269" target="_blank" rel="noopener">Kong installation</a></p><p><a href="https://github.com/pantsel/konga#installation" target="_blank" rel="noopener">konga github</a></p><hr><h4 id="docker安装kong-postgresql"><a href="#docker安装kong-postgresql" class="headerlink" title="docker安装kong+postgresql"></a>docker安装kong+postgresql</h4><p>1.创建一个docker网络用于docker,postgresql和konga容器间通信</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network create kong-net</span><br></pre></td></tr></table></figure><p>2.启动posgtresql容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name kong-database \</span><br><span class="line">               --network=kong-net \</span><br><span class="line">               -p 5432:5432 \</span><br><span class="line">               -e &quot;POSTGRES_USER=kong&quot; \</span><br><span class="line">               -e &quot;POSTGRES_DB=kong&quot; \</span><br><span class="line">               postgres:9.6</span><br></pre></td></tr></table></figure><p>3.初始化postgresql数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm \</span><br><span class="line">    --network=kong-net \</span><br><span class="line">    -e &quot;KONG_DATABASE=postgres&quot; \</span><br><span class="line">    -e &quot;KONG_PG_HOST=kong-database&quot; \</span><br><span class="line">    -e &quot;KONG_CASSANDRA_CONTACT_POINTS=kong-database&quot; \</span><br><span class="line">    kong:latest kong migrations bootstrap</span><br></pre></td></tr></table></figure><blockquote><p>注意两点:</p><p>1.最好是先删除本地的kong镜像.因为本地的Kong:lastest镜像不一定就是最新版</p><p>2.如果本地的kong:latest镜像地域0.15版本,则不支持bootstrap命令.可以将bootstrap命令替换成up</p></blockquote><a id="more"></a><p>4.启动kong容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name kong \</span><br><span class="line">     --network=kong-net \</span><br><span class="line">     -v /data/logs/kong:/var/log/kong \</span><br><span class="line">     -v /data/apps/kong/plugins/:/usr/local/share/lua/5.1/kong/plugins/ \</span><br><span class="line">     -e &quot;KONG_DATABASE=postgres&quot; \</span><br><span class="line">     -e &quot;KONG_PG_HOST=kong-database&quot; \</span><br><span class="line">     -e &quot;KONG_CASSANDRA_CONTACT_POINTS=kong-database&quot; \</span><br><span class="line">     -e &quot;KONG_PROXY_ACCESS_LOG=/var/log/kong/access.log&quot; \</span><br><span class="line">     -e &quot;KONG_ADMIN_ACCESS_LOG=/var/log/kong/admin_access.log&quot; \</span><br><span class="line">     -e &quot;KONG_PROXY_ERROR_LOG=/var/log/kong/error.log&quot; \</span><br><span class="line">     -e &quot;KONG_ADMIN_ERROR_LOG=/var/log/kong/admin_error.log&quot; \</span><br><span class="line">     -e &quot;KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl&quot; \</span><br><span class="line">     -p 8000:8000 \</span><br><span class="line">     -p 8443:8443 \</span><br><span class="line">     -p 8001:8001 \</span><br><span class="line">     -p 8444:8444 \</span><br><span class="line">     kong:latest</span><br></pre></td></tr></table></figure><blockquote><p>这里我映射了kong的插件目录和日志目录. </p></blockquote><ul><li>注意:要先吧kong容器里的/usr/local/share/lua/5.1/kong/plugins/目录下内容复制到宿主机的/data/apps/kong/plugins/目录下.否则宿主机的空目录会覆盖容器的插件目录,导致容器无法启动.</li></ul><p>kong容器目录拷贝到宿主机方法如下:</p><p>1.先不挂载目录启动kong容器</p><p>2.执行命令拷贝kong容器的/usr/local/share/lua/5.1/kong/plugins/ 目录到宿主机/data/apps/kong/plugins/目录下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[work@docker ~]$docker cp kong:/usr/local/share/lua/5.1/kong/plugins/  /data/apps/kong/plugins/</span><br></pre></td></tr></table></figure><blockquote><p>如果不需要将容器的kong插件目录映射到宿主机的话,这一步可以不需要做</p></blockquote><p>容器已经成功启动:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@docker conf.d]$docker ps | grep -E &quot;kong|postgre&quot;</span><br><span class="line">10fc881cca4b        kong:latest                                 &quot;/docker-entrypoin...&quot;   About an hour ago   Up About an hour    0.0.0.0:8000-8001-&gt;8000-8001/tcp, 0.0.0.0:8443-8444-&gt;8443-8444/tcp                                             kong</span><br><span class="line"></span><br><span class="line">afd1487e29a0        postgres:9.6                                &quot;docker-entrypoint...&quot;   3 hours ago         Up 3 hours          0.0.0.0:5432-&gt;5432/tcp                                                                                         kong-database</span><br></pre></td></tr></table></figure><hr><h4 id="安装konga"><a href="#安装konga" class="headerlink" title="安装konga"></a>安装konga</h4><p>konga是管理kong的一个dashboard界面.</p><p>1.先初始化数据库.这里也是用后端的postgresql数据库.官方命令如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm pantsel/konga:latest -c prepare -a &#123;&#123;adapter&#125;&#125; -u &#123;&#123;connection-uri&#125;&#125;</span><br></pre></td></tr></table></figure><table><thead><tr><th>argument</th><th>description</th><th>default</th></tr></thead><tbody><tr><td>-c</td><td>command</td><td>-</td></tr><tr><td>-a</td><td>adapter (can be <code>postgres</code> or <code>mysql</code>)</td><td>-</td></tr><tr><td>-u</td><td>full database connection url</td></tr></tbody></table><p>执行以下命令,初始化数据库:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm pantsel/konga:latest -c prepare -a postgres -u  postgres://kong@10.0.0.250:5432/konga</span><br></pre></td></tr></table></figure><blockquote><p>这里稍微有点疑问的是数据库的connection url ..完整的connection url地址是: postgres://user:password@host:port/konga</p><p> postgres:<a href="mailto://kong@10.0.0.250" target="_blank" rel="noopener">//kong@10.0.0.250</a>:5432/konga —— 这里kong代表用户名,由于没有密码所以没有指定密码.10.0.0.250是postgresql的host主机名.konga表示初始化一个数据库</p></blockquote><p>执行结果如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[work@docker ~]$docker run --rm pantsel/konga:latest -c prepare -a postgres -u  postgres://kong@10.0.0.250:5432/konga</span><br><span class="line">debug: Preparing database...</span><br><span class="line">Using postgres DB Adapter.</span><br><span class="line">Database `konga` does not exist. Creating...</span><br><span class="line">Database `konga` created! Continue...</span><br><span class="line">debug: Hook:api_health_checks:process() called</span><br><span class="line">debug: Hook:health_checks:process() called</span><br><span class="line">debug: Hook:start-scheduled-snapshots:process() called</span><br><span class="line">debug: Hook:upstream_health_checks:process() called</span><br><span class="line">debug: Hook:user_events_hook:process() called</span><br><span class="line">debug: Seeding User...</span><br><span class="line">debug: User seed planted</span><br><span class="line">debug: Seeding Kongnode...</span><br><span class="line">debug: Kongnode seed planted</span><br><span class="line">debug: Seeding Emailtransport...</span><br><span class="line">debug: Emailtransport seed planted</span><br><span class="line">debug: Database migrations completed!</span><br></pre></td></tr></table></figure><ol start="2"><li>启动konga</li></ol><p>命令格式如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -p 1337:1337 </span><br><span class="line">             --network &#123;&#123;kong-network&#125;&#125; \ // optional</span><br><span class="line">             -e &quot;TOKEN_SECRET=&#123;&#123;somerandomstring&#125;&#125;&quot; \</span><br><span class="line">             -e &quot;DB_ADAPTER=the-name-of-the-adapter&quot; \ // &apos;mongo&apos;,&apos;postgres&apos;,&apos;sqlserver&apos;  or &apos;mysql&apos;</span><br><span class="line">             -e &quot;DB_HOST=your-db-hostname&quot; \</span><br><span class="line">             -e &quot;DB_PORT=your-db-port&quot; \ // Defaults to the default db port</span><br><span class="line">             -e &quot;DB_USER=your-db-user&quot; \ // Omit if not relevant</span><br><span class="line">             -e &quot;DB_PASSWORD=your-db-password&quot; \ // Omit if not relevant</span><br><span class="line">             -e &quot;DB_DATABASE=your-db-name&quot; \ // Defaults to &apos;konga_database&apos;</span><br><span class="line">             -e &quot;DB_PG_SCHEMA=my-schema&quot;\ // Optionally define a schema when integrating with prostgres</span><br><span class="line">             -e &quot;NODE_ENV=production&quot; \ // or &apos;development&apos; | defaults to &apos;development&apos;</span><br><span class="line">             --name konga \</span><br><span class="line">             pantsel/konga</span><br></pre></td></tr></table></figure><p>执行如下命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 1337:1337 -d \</span><br><span class="line">             --network=kong-net \</span><br><span class="line">             -e &quot;DB_ADAPTER=postgres&quot; \</span><br><span class="line">             -e &quot;DB_HOST=10.0.0.250&quot; \</span><br><span class="line">             -e &quot;DB_PORT=5432&quot; \</span><br><span class="line">             -e &quot;DB_USER=kong&quot; \</span><br><span class="line">             -e &quot;DB_DATABASE=konga&quot; \</span><br><span class="line">             -e &quot;NODE_ENV=production&quot; \</span><br><span class="line">             --name konga \</span><br><span class="line">             pantsel/konga</span><br></pre></td></tr></table></figure><p>容器成功启动:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beb70407b417        pantsel/konga                               &quot;/app/start.sh&quot;          2 hours ago         Up 2 hours          0.0.0.0:1337-&gt;1337/tcp                                                                                         konga</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;docker安装最新版Kong-v1-0-konga&quot;&gt;&lt;a href=&quot;#docker安装最新版Kong-v1-0-konga&quot; class=&quot;headerlink&quot; title=&quot;docker安装最新版Kong(v1.0)+konga&quot;&gt;&lt;/a&gt;docker安装最新版Kong(v1.0)+konga&lt;/h3&gt;&lt;p&gt;参考以下文档:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.konghq.com/install/docker/?_ga=2.167535422.1288669860.1553147426-917309945.1539077269&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kong installation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/pantsel/konga#installation&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;konga github&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&quot;docker安装kong-postgresql&quot;&gt;&lt;a href=&quot;#docker安装kong-postgresql&quot; class=&quot;headerlink&quot; title=&quot;docker安装kong+postgresql&quot;&gt;&lt;/a&gt;docker安装kong+postgresql&lt;/h4&gt;&lt;p&gt;1.创建一个docker网络用于docker,postgresql和konga容器间通信&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker network create kong-net&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;2.启动posgtresql容器&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker run -d --name kong-database \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               --network=kong-net \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               -p 5432:5432 \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               -e &amp;quot;POSTGRES_USER=kong&amp;quot; \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               -e &amp;quot;POSTGRES_DB=kong&amp;quot; \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               postgres:9.6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;3.初始化postgresql数据库&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ docker run --rm \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    --network=kong-net \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    -e &amp;quot;KONG_DATABASE=postgres&amp;quot; \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    -e &amp;quot;KONG_PG_HOST=kong-database&amp;quot; \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    -e &amp;quot;KONG_CASSANDRA_CONTACT_POINTS=kong-database&amp;quot; \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    kong:latest kong migrations bootstrap&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;注意两点:&lt;/p&gt;
&lt;p&gt;1.最好是先删除本地的kong镜像.因为本地的Kong:lastest镜像不一定就是最新版&lt;/p&gt;
&lt;p&gt;2.如果本地的kong:latest镜像地域0.15版本,则不支持bootstrap命令.可以将bootstrap命令替换成up&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>k8s挂载NFS网络磁盘</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/k8s%E6%8C%82%E8%BD%BDNFS%E7%BD%91%E7%BB%9C%E7%A3%81%E7%9B%98/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/k8s挂载NFS网络磁盘/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T03:04:46.695Z</updated>
    
    <content type="html"><![CDATA[<h2 id="k8s挂载NFS网络磁盘"><a href="#k8s挂载NFS网络磁盘" class="headerlink" title="k8s挂载NFS网络磁盘"></a>k8s挂载NFS网络磁盘</h2><p>按照<kubernetes in="" action="">这本书NFS做持久化存储的例子,发现了一个坑.,启动pod失败,报如下错误</kubernetes></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown: changing ownership of &apos;/data/db&apos;: Operation not permitted</span><br></pre></td></tr></table></figure><p>网上也有人遇到这个问题.可以参考这篇文档: <a href="https://blog.csdn.net/herhun_chen/article/details/90247123" target="_blank" rel="noopener">Kubernetes 集群挂载NFS Volume</a></p><a id="more"></a><p>以下是pod的yaml文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m01 ~]# cat mongodb-pod-nfs.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: mongodb</span><br><span class="line">spec:</span><br><span class="line">  volumes:</span><br><span class="line">     - name: mongodb-data</span><br><span class="line">       nfs:</span><br><span class="line">         server: 10.111.5.184</span><br><span class="line">         path: /data/k8s/</span><br><span class="line"></span><br><span class="line">  containers:</span><br><span class="line">     - image: mongo</span><br><span class="line">       name: mongodb</span><br><span class="line">       volumeMounts:</span><br><span class="line">         - name: mongodb-data</span><br><span class="line">           mountPath: /data/db</span><br><span class="line">       ports:</span><br><span class="line">         - containerPort: 27017</span><br><span class="line">           protocol: TCP</span><br></pre></td></tr></table></figure><p>出现这种<figure class="highlight plain"><figcaption><span>not permitted```的权限类问题,肯定是NFS的挂载有问题.但是在所有k8s的节点上往NFS共享磁盘写文件,又是正常的.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">解决这个问题需要在NFS服务器的```/etx/exports```配置文件修改成如下配置</span><br></pre></td></tr></table></figure></p><p>[work@hsq-beta-rpc ~]$cat /etc/exports<br>/data/k8s/ 10.111.0.0/16(rw,fsid=0,async,no_subtree_check,no_auth_nlm,insecure,no_root_squash)<br><code>`</code></p><blockquote><p>注意,以上配置其实是一行.没有换行</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;k8s挂载NFS网络磁盘&quot;&gt;&lt;a href=&quot;#k8s挂载NFS网络磁盘&quot; class=&quot;headerlink&quot; title=&quot;k8s挂载NFS网络磁盘&quot;&gt;&lt;/a&gt;k8s挂载NFS网络磁盘&lt;/h2&gt;&lt;p&gt;按照&lt;kubernetes in=&quot;&quot; action=&quot;&quot;&gt;这本书NFS做持久化存储的例子,发现了一个坑.,启动pod失败,报如下错误&lt;/kubernetes&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;chown: changing ownership of &amp;apos;/data/db&amp;apos;: Operation not permitted&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;网上也有人遇到这个问题.可以参考这篇文档: &lt;a href=&quot;https://blog.csdn.net/herhun_chen/article/details/90247123&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kubernetes 集群挂载NFS Volume&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>harbor私有仓库部署</title>
    <link href="https://jesse.top/2020/06/26/docker/harbor%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E9%83%A8%E7%BD%B2/"/>
    <id>https://jesse.top/2020/06/26/docker/harbor私有仓库部署/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T03:03:12.763Z</updated>
    
    <content type="html"><![CDATA[<h2 id="harbor私有仓库部署"><a href="#harbor私有仓库部署" class="headerlink" title="harbor私有仓库部署"></a>harbor私有仓库部署</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>harbor是docker的私有仓库,可以部署在局域网服务器上,用来管理docker镜像.虽然docker官方也提供公共镜像仓库,但是由于是境外网站,拉取镜像速度非常慢,而且有被墙的可能.部署私有仓库非常有必要</p><p>harbor是vmware公司开源的企业级的docker registry管理项目.</p><blockquote><p>处于数据脱敏需要,以下内容中隐藏了真实域名.而使用hub.xxxxxx.com替代</p></blockquote><hr><h3 id="社区"><a href="#社区" class="headerlink" title="社区"></a>社区</h3><p>harbor github: <a href="https://github.com/goharbor/harbor" target="_blank" rel="noopener">goharbor/harbor</a></p><p>官网文档介绍: <a href="https://goharbor.io/docs/1.10/" target="_blank" rel="noopener">harbor doc</a></p><p>在部署中遇到的各种坑,都可以通过查阅文档,或者搜索github的issue解决</p><hr><h3 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h3><p>harbor是docker-compose部署的.包括一系列组件:nginx.core,log,register等等.</p><p>但是由于本机已经存在一个Nginx镜像.所以用Nginx代理到harbor的Nginx.<strong>如果是独立的服务器部署Harbor的话,则不会存在这个问题,可以直接跳过这一章节.</strong></p><p>nginx代理框架大概是:</p><p><strong>nginx—-&gt;harbor-nginx—–&gt;habor</strong></p><p>由于docker提交镜像需要Https协议,所以:</p><p><strong>nginx—301跳转到nginx https—–&gt;harbor-nginx http—-&gt; habor</strong></p><p>但是这样的部署方式,有一个问题:</p><p>私有仓库可以正常login但是push镜像的时候,又提示未验证.</p><p>该问题尝试过很多解决方案,但是均无法解决</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-docker ~]# docker login --username=admin -pHarbor12345 hub.xxxxxx.com</span><br><span class="line">WARNING! Using --password via the CLI is insecure. Use --password-stdin.</span><br><span class="line">WARNING! Your password will be stored unencrypted in /root/.docker/config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/login/#credentials-store</span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br><span class="line">[root@idc-function-docker ~]# docker push hub.xxxxxx.com/master/nginx:latest</span><br><span class="line">The push refers to repository [hub.xxxxxx.com/master/nginx]</span><br><span class="line">d37eecb5b769: Pushing [==================================================&gt;]  3.584kB</span><br><span class="line">99134ec7f247: Preparing</span><br><span class="line">c3a984abe8a8: Preparing</span><br><span class="line">unauthorized: authentication required</span><br></pre></td></tr></table></figure><p>所以现在的架构是</p><p><strong>nginx—301跳转到Nginx https——&gt; harbor-nginx https——&gt;harbor</strong></p><hr><h3 id="harbor部署前提条件"><a href="#harbor部署前提条件" class="headerlink" title="harbor部署前提条件"></a>harbor部署前提条件</h3><ul><li>安装docker-ce</li><li>安装docker-composer</li><li>准备一个空目录.比如/data.或者/data/harbor (注意,最好是空目录,不要和其他项目混杂一起)</li><li>安装好https域名证书</li></ul><hr><h3 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h3><p>docker和docker-compose的安装就略过了.这里提一句,我用的是acme.sh部署letsencrypt的证书</p><p>接下来开始部署harbor</p><ul><li>1.去github下载离线安装包.离线安装包虽然比较大,但是安装过程快速,且不会中断</li></ul><p>这里安装的是最新版,v1.10.1.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在github下载 harbor-offline-installer-v1.10.1.tgz</span><br><span class="line">tar xvf 解压</span><br></pre></td></tr></table></figure><ul><li>2.解压后,进入harbor文件,编辑harbor.yaml配置文件.需要改动以下几个地方</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hostname hub.xxxxxx.com  #定义主机名,可以使用IP,也可以用域名</span><br><span class="line"></span><br><span class="line">#定义https的服务器证书和秘钥的文件路径</span><br><span class="line">https:</span><br><span class="line">  # https port for harbor, default is 443</span><br><span class="line">  port: 443</span><br><span class="line">  # The path of cert and key files for nginx</span><br><span class="line">  certificate: /data/letsencrypt/hub.xxxxxx.com/fullchain.cer</span><br><span class="line">  private_key: /data/letsencrypt/hub.xxxxxx.com/hub.xxxxxx.com.key</span><br><span class="line">  </span><br><span class="line">#这是harbor命令行和浏览器登陆的初始密码..用户名是admin</span><br><span class="line">harbor_admin_password: Harbor12345</span><br><span class="line"></span><br><span class="line">#这是数据目录,最好是一个空目录</span><br><span class="line">data_volume: /data/apps/harbor</span><br><span class="line"></span><br><span class="line">#日志文件保存路径</span><br><span class="line">log:</span><br><span class="line">    location: /data/logs/harbor</span><br></pre></td></tr></table></figure><ul><li>3.执行install.sh文件.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#该脚本会检查主机的环境,拉取相关镜像.以及根据配置文件生成docker-compose文件.</span><br><span class="line">[root@idc-function-docker harbor]#./install.sh</span><br></pre></td></tr></table></figure><blockquote><p> 这个脚本执行到最后会报错,提示80端口和nginx容器已经被占用了.但是没关系.</p></blockquote><ul><li>4.修改docker-composer文件.(如果本机上没有nginx或者80端口没有被占用,这一步可以不做)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#将Nginx容器名改成hub-nginx</span><br><span class="line">#将80和443的端口映射修改一下,比如我这里</span><br><span class="line">proxy:</span><br><span class="line">    image: goharbor/nginx-photon:v1.10.1</span><br><span class="line">    container_name: hub-nginx</span><br><span class="line">ports:</span><br><span class="line">      - 8081:8080</span><br><span class="line">      - 4443:8443</span><br></pre></td></tr></table></figure><ul><li><ol start="5"><li>启动docker-compose</li></ol></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-docker harbor]#docker-compose up -d</span><br></pre></td></tr></table></figure><ul><li><ol start="6"><li>宿主机的nginx开启代理. (如果不是采用nginx代理的话,可以忽略这一步)</li></ol></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">#我这里是有一个独立的Nginx容器</span><br><span class="line">[root@idc-function-docker harbor]# cat/data/conf/nginx/conf.d/dwd-docker-hub.conf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line"></span><br><span class="line">  server_name hub.xxxxxx.com;</span><br><span class="line">  listen 443 ssl http2;</span><br><span class="line">  ssl_certificate  /data/letsencrypt/hub.xxxxxx.com/fullchain.cer;</span><br><span class="line">  ssl_certificate_key /data/letsencrypt/hub.xxxxxx.com/hub.xxxxxx.com.key;</span><br><span class="line">  include /data/letsencrypt/options-ssl-nginx.conf; # managed by Certbot</span><br><span class="line">  ssl_dhparam /data/letsencrypt/ssl-dhparams.pem; # managed by Certbot</span><br><span class="line"></span><br><span class="line">  add_header      Strict-Transport-Security &quot;max-age=31536000&quot; always;</span><br><span class="line">  location / &#123;</span><br><span class="line"></span><br><span class="line">    proxy_pass https://172.16.20.30:4443; #代理到宿主机的4443端口,宿主机会将4443代理到hub-docer容器的443端口</span><br><span class="line">    client_max_body_size 2000m; #这里要定义大一点,否则提交镜像的时候,会提示 413 Request Entity Too Large</span><br><span class="line">                proxy_buffering off;</span><br><span class="line">                proxy_ssl_verify off;</span><br><span class="line">                proxy_set_header Host $http_host;</span><br><span class="line">                proxy_set_header Upgrade $http_upgrade;</span><br><span class="line">                proxy_set_header Connection &quot;Upgrade&quot;;</span><br><span class="line">                proxy_set_header X-Forwarded-Proto $scheme;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    if ($host = hub.xxxxxx.com)&#123;</span><br><span class="line">        return 301 https://$host$request_uri;</span><br><span class="line">    &#125; # managed by Certbot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  listen 80;</span><br><span class="line">    server_name hub.xxxxxx.com;</span><br><span class="line">    return 404; # managed by Certbot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至此,部署就完成了.</p><p>我之前在部署的过程中遇到无数的坑,,可能都是由于harbor没有使用Https引起的</p><hr><p>浏览器访问<a href="https://hub.xxxxxx.com,用初始的账号密码登陆,可以新建项目,创建用户等" target="_blank" rel="noopener">https://hub.xxxxxx.com,用初始的账号密码登陆,可以新建项目,创建用户等</a>.</p><p>我这里创建了一个master的公开项目</p><hr><p>登陆和push镜像没有任何问题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root@idc-function-docker ~]# docker login --username=admin -pHarbor12345 hub.xxxxxx.com</span><br><span class="line">WARNING! Using --password via the CLI is insecure. Use --password-stdin.</span><br><span class="line">WARNING! Your password will be stored unencrypted in /root/.docker/config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/login/#credentials-store</span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br><span class="line">[root@idc-function-docker ~]# docker push hub.xxxxxx.com/master/nginx:latest</span><br><span class="line">The push refers to repository [hub.xxxxxx.com/master/nginx]</span><br><span class="line">d37eecb5b769: Layer already exists</span><br><span class="line">99134ec7f247: Layer already exists</span><br><span class="line">c3a984abe8a8: Layer already exists</span><br><span class="line">latest: digest: sha256:7ac7819e1523911399b798309025935a9968b277d86d50e5255465d6592c0266 size: 948</span><br><span class="line">[root@idc-function-docker ~]#</span><br></pre></td></tr></table></figure><p>在另外一台客户端上,尝试pull镜像不需要密码.如果需要密码pull镜像,可以将项目设置为私有</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker pull  hub.xxxxxx.com/master/nginx:latest</span><br><span class="line">latest: Pulling from master/nginx</span><br><span class="line">Digest: sha256:7ac7819e1523911399b798309025935a9968b277d86d50e5255465d6592c0266</span><br><span class="line">Status: Downloaded newer image for hub.xxxxxx.com/master/nginx:latest</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;harbor私有仓库部署&quot;&gt;&lt;a href=&quot;#harbor私有仓库部署&quot; class=&quot;headerlink&quot; title=&quot;harbor私有仓库部署&quot;&gt;&lt;/a&gt;harbor私有仓库部署&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;harbor是docker的私有仓库,可以部署在局域网服务器上,用来管理docker镜像.虽然docker官方也提供公共镜像仓库,但是由于是境外网站,拉取镜像速度非常慢,而且有被墙的可能.部署私有仓库非常有必要&lt;/p&gt;
&lt;p&gt;harbor是vmware公司开源的企业级的docker registry管理项目.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;处于数据脱敏需要,以下内容中隐藏了真实域名.而使用hub.xxxxxx.com替代&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&quot;社区&quot;&gt;&lt;a href=&quot;#社区&quot; class=&quot;headerlink&quot; title=&quot;社区&quot;&gt;&lt;/a&gt;社区&lt;/h3&gt;&lt;p&gt;harbor github: &lt;a href=&quot;https://github.com/goharbor/harbor&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;goharbor/harbor&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;官网文档介绍: &lt;a href=&quot;https://goharbor.io/docs/1.10/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;harbor doc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在部署中遇到的各种坑,都可以通过查阅文档,或者搜索github的issue解决&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;框架&quot;&gt;&lt;a href=&quot;#框架&quot; class=&quot;headerlink&quot; title=&quot;框架&quot;&gt;&lt;/a&gt;框架&lt;/h3&gt;&lt;p&gt;harbor是docker-compose部署的.包括一系列组件:nginx.core,log,register等等.&lt;/p&gt;
&lt;p&gt;但是由于本机已经存在一个Nginx镜像.所以用Nginx代理到harbor的Nginx.&lt;strong&gt;如果是独立的服务器部署Harbor的话,则不会存在这个问题,可以直接跳过这一章节.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;nginx代理框架大概是:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nginx—-&amp;gt;harbor-nginx—–&amp;gt;habor&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;由于docker提交镜像需要Https协议,所以:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nginx—301跳转到nginx https—–&amp;gt;harbor-nginx http—-&amp;gt; habor&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;但是这样的部署方式,有一个问题:&lt;/p&gt;
&lt;p&gt;私有仓库可以正常login但是push镜像的时候,又提示未验证.&lt;/p&gt;
&lt;p&gt;该问题尝试过很多解决方案,但是均无法解决&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes API服务器权限(ServiceAccount &amp;&amp; RBAC)</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/kubernetes%20API%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9D%83%E9%99%90(ServiceAccount%20&amp;&amp;%20RBAC)/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/kubernetes API服务器权限(ServiceAccount &amp;&amp; RBAC)/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T08:48:06.747Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kubernetes-API服务器权限-ServiceAccount-amp-amp-RBAC"><a href="#kubernetes-API服务器权限-ServiceAccount-amp-amp-RBAC" class="headerlink" title="kubernetes API服务器权限(ServiceAccount &amp;&amp; RBAC)"></a>kubernetes API服务器权限(ServiceAccount &amp;&amp; RBAC)</h2><h3 id="ServiceAccount介绍"><a href="#ServiceAccount介绍" class="headerlink" title="ServiceAccount介绍"></a>ServiceAccount介绍</h3><p>​        每个Pod都与一个ServiceAccount相关联,它代表了运行在pod中应用程序的身份证明.每个pod在启动的时候kubernetes会自动挂载ServiceAccount的TOKEN到pod容器的<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">​ServiceAccount只不过是一种运行在pod中的应用程序和API服务器身份认证的一种方式.应用程序通过在请求中传递serviceaccount的token和API服务器通信</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 了解ServiceAccount资源</span><br><span class="line"></span><br><span class="line">​ServiceAccount和pod,Secret,ConfigMap等一样,本身也是一种资源.他们作用在单独的命名空间.kubernetes为每个命名空间自动创建一个默认的ServiceAccount(名字是default),可以像查看其它资源一样使用```kubectl get sa```来查看ServiceAccount列表</span><br><span class="line"></span><br><span class="line">&lt;!--more--&gt;</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl get sa<br>NAME      SECRETS   AGE<br>default   1         51d<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 同一个命名空间下的多个pod可以使用同一个ServiceAccount</span><br><span class="line">* pod只能使用同一个命名空间下的ServiceAccount</span><br><span class="line">* 如果在pod的manifest文件中没有显示的指定ServiceAccount名称.则默认使用default ServiceAccount</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 创建ServiceAccount</span><br><span class="line"></span><br><span class="line">​为什么要去创建新的ServiceAccount,而不是所有pod都使用默认的ServiceAccount? 为了集群的安全性,尽量做到权限最小分配的原则,只对有需要的Pod配置有相应较高权限的ServiceAccount,而其他pod使用default默认ServiceAccount应该不允许他们检索或者修改部署在集群中的任何资源.</span><br><span class="line"></span><br><span class="line">​下面实验一下如何创建其他ServiceAccount,并且分配给Pod</span><br><span class="line"></span><br><span class="line">* 创建ServiceAccount很简单,直接使用```kubectl create serviceaccount```命令</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl create serviceaccount foo<br>serviceaccount/foo created</p><p>[root@k8s-master ~]# kubectl describe sa foo<br>Name:                foo<br>Namespace:           default<br>Labels:              <none><br>Annotations:         <none><br>Image pull secrets:  <none><br>Mountable secrets:   foo-token-ktjj5<br>Tokens:              foo-token-ktjj5<br>Events:              <none><br>[root@k8s-master ~]#<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">上面就创建了一个名为foo的serviceaccount.如果查看这个名为foo-token-ktjj5的TOKEN秘钥可以发现这个ServiceAccount也和default默认的ServiceAccount一样具有相同的条目(CA证书,命名空间,token),当然这2个token本身是不一样的</span><br></pre></td></tr></table></figure></none></none></none></none></p><p>[root@k8s-master ~]# kubectl describe secret foo-token-ktjj5<br>Name:         foo-token-ktjj5<br>Namespace:    default<br>Labels:       <none><br>Annotations:  kubernetes.io/service-account.name: foo<br>              kubernetes.io/service-account.uid: 28cb7741-eaf3-46bb-bd58-3f1ddb748c2e</none></p><p>Type:  kubernetes.io/service-account-token</p><h1 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h1><p>ca.crt:     1025 bytes<br>namespace:  7 bytes<br>token:      eyJhbGciOiJSUzI1NiIsImtpZCI6InI3cE50azROV1pseDJqNmxVQTlSWjF6Vk9YVFFnamZEWFF5RG56YTJhaWMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImZvby10b2tlbi1rdGpqNSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJmb28iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyOGNiNzc0MS1lYWYzLTQ2YmItYmQ1OC0zZjFkZGI3NDhjMmUiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVmYXVsdDpmb28ifQ.KdfyYVkb7Ye_sV8yUWqU6I8fe8Pck7cyTDdwh08Y9w0f4cwJKqIwsuVn39VCpYIH3PaDsKoxBv7Bpahn6lfiP3MgA4zcWlX4vtBUxJCAt-GBXBTcHkqQ6BwtxFzkY4rgsXLd5HuqsrqrZbxrSM1zfNovPccRgklN3kbz0BuxTmKQ65E9DFhAt5kmzajO3qvAB_ymGeoJLMul1fZEqLnV48UEKN5KFAxVfwo0b4On2LcKOVkmG_P7yO7X6TsE6zEN03kvoxjOd0vnJrGUCT9fu0KlabRyDsyDWsua-Su2kv-gDg3O6zQxqDGgEwRxTb2-7Cbv6PaFbfCwSRnCC8Y6FA<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 将ServiceAccount分配给pod</span><br><span class="line"></span><br><span class="line">将ServiceAccount赋值给Pod非常简单,进需要在pod的manifest文件中的spec字段下指定serviceAccountName即可.例如下面之前学习过的curl镜像和kubectl-proxy的ambassador镜像</span><br></pre></td></tr></table></figure></p><p>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: curl-custome-sa<br>spec:</p><p>  #指定pod使用哪个serviceaccount<br>  serviceAccountName: foo<br>  containers:</p><pre><code>- name: main  image: tutum/curl  command: [&apos;sleep&apos;,&apos;9999999&apos;]- name: ambassador  image: luksa/kubectl-proxy:1.6.2</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; ServiceAccount必须在pod创建之前新建,后续不能被修改</span><br><span class="line"></span><br><span class="line">* 使用新创建的ServiceAccount访问API服务器.(通过ambassador容器)</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl exec -it curl-custome-sa -c main curl localhost:8001/api/v1/pods<br>{<br>  “kind”: “Status”,<br>  “apiVersion”: “v1”,<br>  “metadata”: {</p><p>  },<br>  “status”: “Failure”,<br>  “message”: “pods is forbidden: User \”system:serviceaccount:default:foo\” cannot list resource \”pods\” in API group \”\” at the cluster scope”,<br>  “reason”: “Forbidden”,<br>  “details”: {<br>    “kind”: “pods”<br>  },<br>  “code”: 403<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">还是无法访问? 这是因为RBAC(基于角色权限控制)插件没有给这个ServiceAccount进行权限授权.</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 介绍RBAC</span><br><span class="line"></span><br><span class="line">​在1.6版本开始,集群安全性显著提高.RBAC在集群中默认开启.RBAC会阻止未授权的用户查看和修改集群状态.API服务器对外暴露了REST接口,用户可以通过向服务器发送HTTP请求来执行动作.</span><br><span class="line"></span><br><span class="line">​REST客户端发送GET,POST,PUT,DELETE和其他类型的HTTP请求到特定的URL资源.这些资源可以是kubernetes的Pod,Service,Secret等等.</span><br><span class="line"></span><br><span class="line">​RBAC支持的支持一些动词(例如,get,create,update)等映射到客户端请求的HTTP方法(GET,POST,PUT).完整的映射方法如下表</span><br><span class="line"></span><br><span class="line">| HTTP方法 |        RBAC动词        |</span><br><span class="line">| :------: | :--------------------: |</span><br><span class="line">| GET,HEAD | get(或者watch用于监听) |</span><br><span class="line">|   POST   |         create         |</span><br><span class="line">|   PUT    |         update         |</span><br><span class="line">|  PATCH   |         patch          |</span><br><span class="line">|  DELETE  |         delete         |</span><br><span class="line"></span><br><span class="line">​除了可以对全部资源类型应用安装权限,RBAC规则还可以引用于特定的资源实例,以及非资源URL路径.(并不是API服务器对外暴露的每个路径都映射到一个资源),例如/api路径,/healthz健康路径</span><br><span class="line"></span><br><span class="line">​RBAC授权插件将用户角色作为决定用户能否执行操作的关键因素主体.(可以是一个人,一个ServiceAccount,或者一组人,一组ServiceAccount).一个用户可以有多个角色.</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 介绍RBAC资源</span><br><span class="line"></span><br><span class="line">RBAC授权规则通过四种资源来进行配置.可以分为两个组:</span><br><span class="line"></span><br><span class="line">* Role(角色)和ClusterRole(集群角色).他们指定了在资源上可以执行哪些动作</span><br><span class="line">* RoleBinding(角色绑定)和ClusterRoleBinding.(集群角色绑定).他们将上述角色绑定到特定的用户,组或者ServiceAccount上</span><br><span class="line"></span><br><span class="line">&gt; 角色定义了可以有什么访问权限,而角色绑定决定了谁可以做这些操作</span><br><span class="line"></span><br><span class="line">角色和集群角色,或者角色绑定和集群角色绑定之间的区别在于角色和角色绑定是命名空间资源.而集群角色和集群角色绑定是集群级别的资源(独立资源,不属于任何命名空间).</span><br><span class="line"></span><br><span class="line">* 单个命名空间可以创建多个角色,以及多个角色绑定</span><br><span class="line">* 尽管角色绑定是在命名空间下的.但是也可以引用其他命名空间下的集群角色.</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 演示RBAC的实践</span><br><span class="line"></span><br><span class="line">* ##### 创建2个命名空间,然后在2个命名空间下运行```kubectl-proxy``` pod.</span><br><span class="line"></span><br><span class="line">下面这个配置创建了```foo```,```bar```这2个名称空间.以及在每个名称空间下创建了一个```kubectl-proxy```的test容器</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl create ns foo</span><br><span class="line">namespace/foo created</span><br><span class="line">[root@k8s-master ~]# kubectl create ns bar</span><br><span class="line">namespace/bar created</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl run test --image=luksa/kubectl-proxy -n foo</span><br><span class="line">kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.</span><br><span class="line">deployment.apps/test created</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl run test --image=luksa/kubectl-proxy -n bar</span><br><span class="line">kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.</span><br><span class="line">deployment.apps/test created</span><br></pre></td></tr></table></figure></p><p>列出各名称空间下的pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get po -n foo</span></span><br><span class="line">NAME                    READY   STATUS    RESTARTS   AGE</span><br><span class="line"><span class="built_in">test</span>-7b4bb6b9ff-qpppf   1/1     Running   0          28m</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get po -n bar</span></span><br><span class="line">NAME                    READY   STATUS    RESTARTS   AGE</span><br><span class="line"><span class="built_in">test</span>-7b4bb6b9ff-jl6v5   1/1     Running   0          42m</span><br></pre></td></tr></table></figure><p>此时由于还没有授权,进入pod容器内部,对API服务器发起http访问请求时.API服务器不允许Pod访问services服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl exec -it test-7b4bb6b9ff-qpppf -n foo sh</span><br><span class="line"></span><br><span class="line">/ # curl localhost:8001/api/v1/namespaces/foo/services</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Status&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;status&quot;: &quot;Failure&quot;,</span><br><span class="line">  &quot;message&quot;: &quot;services is forbidden: User \&quot;system:serviceaccount:foo:default\&quot; cannot list resource \&quot;services\&quot; in API group \&quot;\&quot; in the namespace \&quot;foo\&quot;&quot;,</span><br><span class="line">  &quot;reason&quot;: &quot;Forbidden&quot;,</span><br><span class="line">  &quot;details&quot;: &#123;</span><br><span class="line">    &quot;kind&quot;: &quot;services&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;code&quot;: 403</span><br><span class="line">&#125;/ #</span><br></pre></td></tr></table></figure><hr><ul><li><h5 id="使用Role和RoleBinding"><a href="#使用Role和RoleBinding" class="headerlink" title="使用Role和RoleBinding"></a>使用Role和RoleBinding</h5><h5 id="创建Role"><a href="#创建Role" class="headerlink" title="创建Role"></a>创建Role</h5><p>Role资源定义了哪些操作可以在哪些资源上执行.下面的配置文件定义了一个Role.允许用户获取并列出foo命名空间中的服务</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  #该role所在的命名空间,如果没有填写,则此role应用在默认的命名空间</span><br><span class="line">  namespace: foo</span><br><span class="line">  name: service-reader</span><br><span class="line">#以下是此role定义的规则</span><br><span class="line">rules:</span><br><span class="line">    #Service是核心apiGroup的资源,所以没有apiGroup名,就是&quot;&quot;空</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    #该Role角色授权的规则,允许get,list访问权限</span><br><span class="line">    verbs: [&quot;get&quot;,&quot;list&quot;]</span><br><span class="line">   #这个规则和服务有关,(必须使用复数)</span><br><span class="line">    resources: [&quot;services&quot;]</span><br><span class="line">~</span><br></pre></td></tr></table></figure><p>Role配置文件中定义了以下信息:</p><ul><li>Role所处的名称空间</li><li>Role角色名</li><li>verbs: 角色允许的访问权限</li><li>resources: 角色允许访问的资源</li></ul><p>创建role</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl create -f serviceaccount-service-reader.yaml</span></span><br><span class="line">role.rbac.authorization.k8s.io/service-reader created</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get role -n foo</span></span><br><span class="line">NAME             AGE</span><br><span class="line">service-reader   6m18s</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl describe role -n foo service-reader</span></span><br><span class="line">Name:         service-reader</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources  Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  ---------  -----------------  --------------  -----</span><br><span class="line">  services   []                 []              [get list]</span><br><span class="line">[root@k8s-master ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure><ul><li><h5 id="将Role角色绑定到ServiceAccount"><a href="#将Role角色绑定到ServiceAccount" class="headerlink" title="将Role角色绑定到ServiceAccount"></a>将Role角色绑定到ServiceAccount</h5><p>​      Role角色定义了哪些操作可以执行,但是没有指定谁拥有该角色.要做到这一点,需要将角色绑定到一个主体上.它可以是一个usr(用户),或者一个ServiceAccount.或者一个组.</p><p>​        通过创建一个RoleBinding资源来实现将角色绑定到主体.使用<figure class="highlight plain"><figcaption><span>create rolebinding```.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">完整格式是```kubectl create rolebinding 绑定名 --role=角色名 --serviceaccount=命名空间:ServiceAccount名 -n 命名空间```.下面是具体的例子实现</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl create rolebinding test --role=service-reader --serviceaccount=foo:default -n foo</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/test created</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl get rolebinding test -n foo  -o yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: &quot;2020-05-05T09:45:58Z&quot;</span><br><span class="line">  name: test</span><br><span class="line">  namespace: foo</span><br><span class="line">  resourceVersion: &quot;10719596&quot;</span><br><span class="line">  selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/foo/rolebindings/test</span><br><span class="line">  uid: a400ece9-ceb7-4a57-8687-f385a69b9190</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: service-reader</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: default</span><br><span class="line">  namespace: foo</span><br></pre></td></tr></table></figure></p><p>​        在上面的例子中创建了一个test的rolebinding的角色绑定资源.将<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">​通过yaml文件显示,RoleBinding角色绑定引用了单一的角色(从```roleRef```字段可以看到引用的Role角色信息).RoleBinding可以将单个角色绑定到多个主体.</span><br><span class="line"></span><br><span class="line">​因为这个RoleBinding将service-reader这个Role角色绑定到了foo命名空间下的ServiceAccount下.所以现在foo下的pod可以访问集群的Service资源了</span><br></pre></td></tr></table></figure></p><p>#进入foo命名空间下的Pod容器内部,通过API服务器访问services<br>/ # curl localhost:8001/api/v1/namespaces/foo/services<br>{<br>  “kind”: “ServiceList”,<br>  “apiVersion”: “v1”,<br>  “metadata”: {</p><pre><code>&quot;selfLink&quot;: &quot;/api/v1/namespaces/foo/services&quot;,&quot;resourceVersion&quot;: &quot;10720460&quot;</code></pre><p>  },<br>  “items”: []  #items列表为空很正常,因为foo名称空间下没有任何Serivce资源<br>}/ #</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* ##### 在RoleBinding角色绑定中使用其他名称空间的ServiceAccount</span><br><span class="line"></span><br><span class="line">  ​刚才在foo的名称空间下创建了Role和RoleBinding.并且foo下的pod可以正常通过API服务器访问Service资源了.但是很显然bar名称空间下pod没有任何访问权限.</span><br><span class="line"></span><br><span class="line">  ​可以修改foo名称空间中的RoleBinding,并添加另一个pod的ServiceAccount.即使这个ServiceAccount在另一个不同的名称空间中.</span><br><span class="line"></span><br><span class="line">  ​修改RoleBinding资源名为test的配置文件</span><br></pre></td></tr></table></figure></li></ul><p>[root@k8s-master ~]# kubectl edit rolebinding test -n foo<br>rolebinding.rbac.authorization.k8s.io/test edited</p><p>#在配置文件中最后新增一个subjects配置.引用来自bar名称空间中的default ServiceAccount<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: RoleBinding<br>metadata:<br>  creationTimestamp: “2020-05-05T09:45:58Z”<br>  name: test<br>  namespace: foo<br>  resourceVersion: “10719596”<br>  selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/foo/rolebindings/test<br>  uid: a400ece9-ceb7-4a57-8687-f385a69b9190<br>roleRef:<br>  apiGroup: rbac.authorization.k8s.io<br>  kind: Role<br>  name: service-reader<br>subjects:</p><ul><li>kind: ServiceAccount<br>name: default<br>namespace: foo</li><li>kind: ServiceAccount<br>name: default<br>namespace: bar<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">此时,bar名称空间下的Pod可以访问foo名称空间下的Service资源(但是仍然不能访问bar自己名称空间下的资源)</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">/ # curl localhost:8001/api/v1/namespaces/foo/services</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;ServiceList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;/api/v1/namespaces/foo/services&quot;,</span><br><span class="line">    &quot;resourceVersion&quot;: &quot;10722489&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: []</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">/ # curl localhost:8001/api/v1/namespaces/bar/services</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Status&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;status&quot;: &quot;Failure&quot;,</span><br><span class="line">  &quot;message&quot;: &quot;services is forbidden: User \&quot;system:serviceaccount:bar:default\&quot; cannot list resource \&quot;services\&quot; in API group \&quot;\&quot; in the namespace \&quot;bar\&quot;&quot;,</span><br><span class="line">  &quot;reason&quot;: &quot;Forbidden&quot;,</span><br><span class="line">  &quot;details&quot;: &#123;</span><br><span class="line">    &quot;kind&quot;: &quot;services&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;code&quot;: 403</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="ClusterRole和ClusterRoleBinding"><a href="#ClusterRole和ClusterRoleBinding" class="headerlink" title="ClusterRole和ClusterRoleBinding"></a>ClusterRole和ClusterRoleBinding</h3><p>​    Role和RoleBinding都是名称空间的资源,他们属于一个单一的名称空间资源上.但是如上面那个例子,RoleBinding可以引用来自其他名称空间中的ServiceAccount.</p><p>​    除了这些名称空间里的资源,还存在2个集群级别的RBAC资源:<strong>ClusterRole</strong>和<strong>ClusterRoleBinding</strong>.之所以存在集群级别的 资源是因为常规的<strong>Role</strong>和<strong>RoleBinding</strong>只允许访问和角色在同一个名称空间中的资源.如果希望允许不同名称空间下的Pod可以互相访问资源,则需要在每个名称空间中创建一个相同的<strong>Role</strong>和<strong>RoleBinding</strong>.</p><p>​    另外,一些特定的资源完全不在任何名称空间中(例如.Node,PV,namespace等等).我们也提过API服务器对外暴露的一些不表示资源的URL路径(例如/healthz).常规<strong>Role</strong>和<strong>RoleBinding</strong>不能对这些类型的URL进行授权.但是<strong>ClusterRole</strong>可以.</p><h5 id="创建ClusterRole"><a href="#创建ClusterRole" class="headerlink" title="创建ClusterRole"></a>创建ClusterRole</h5><p>命令格式和Role类似:<figure class="highlight plain"><figcaption><span>create clusterrole 名字```.下面这个例子创建一个pv-reader的集群角色,允许只读访问persistentvolumes资源</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl create clusterrole pv-reader --verb=get,list --resource=persistentvolumes</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/pv-reader created</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl get clusterrole pv-reader -o yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: &quot;2020-05-05T10:26:01Z&quot;</span><br><span class="line">  name: pv-reader</span><br><span class="line">  resourceVersion: &quot;10725342&quot;</span><br><span class="line">  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/pv-reader</span><br><span class="line">  uid: 2c6d3083-3385-46b3-9ea6-b479f57e7db5</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - persistentvolumes</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">[root@k8s-master ~]#</span><br></pre></td></tr></table></figure></p><h5 id="创建ClusterRoleBinding"><a href="#创建ClusterRoleBinding" class="headerlink" title="创建ClusterRoleBinding"></a>创建ClusterRoleBinding</h5><p>命令和RoleBinding使用方法类似: <figure class="highlight plain"><figcaption><span>create clusterrolebind```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">下面的命令创建了个一个pv-test的```ClusterRoleBinding```集群角色绑定对线,指定的角色是```pv-reader```,分配给的ServiceAccount是```foo```名称空间下的default默认ServiceAccount</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl create clusterrolebinding pv-test –clusterrole=pv-reader –serviceaccount=foo:default<br>clusterrolebinding.rbac.authorization.k8s.io/pv-test created<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">现在foo名称空间下的Pod就可以访问集群下的persistentvolume(PV)资源了</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">/ # curl localhost:8001/api/v1/persistentvolumes</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;PersistentVolumeList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;/api/v1/persistentvolumes&quot;,</span><br><span class="line">    &quot;resourceVersion&quot;: &quot;10858720&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;pv-1&quot;,</span><br><span class="line">        &quot;selfLink&quot;: &quot;/api/v1/persistentvolumes/pv-1&quot;,</span><br><span class="line">        &quot;uid&quot;: &quot;544a9c07-1c41-41c5-8ed4-da41b4d11cf5&quot;,</span><br><span class="line">        &quot;resourceVersion&quot;: &quot;10245103&quot;,</span><br><span class="line">        &quot;creationTimestamp&quot;: &quot;2020-05-02T02:54:43Z&quot;,</span><br><span class="line">        .......</span><br></pre></td></tr></table></figure></p><blockquote><p>由于指定了绑定到foo名称空间下的ServiceAccount.所以显而易见,bar名称空间下的pod没有权限访问perssitentvolumes资源</p></blockquote><hr><h3 id="ClusterRole授权访问指定命名空间中的资源"><a href="#ClusterRole授权访问指定命名空间中的资源" class="headerlink" title="ClusterRole授权访问指定命名空间中的资源"></a>ClusterRole授权访问指定命名空间中的资源</h3><p><strong>ClusterRole</strong>不是必须一直和集群级别的<strong>ClusterRoleBinding</strong>绑定使用.他们也可以和常规的<strong>RoleBinding</strong>进行捆绑.</p><p>在kubernetes中,有很多系统预定义的RBAC角色资源.通过<figure class="highlight plain"><figcaption><span>get```命令就可以看到系统预定义的角色.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>kubectl get role -n kube-system<br>kubectl get clusterrole<br>kubectl get rolebinding -n kube-system<br>kubectl get clusterrolebinding<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">下面是一个系统预定义的名为view的集群角色</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl get clusterrole view -o yaml</span><br><span class="line"></span><br><span class="line">  name: view</span><br><span class="line">  resourceVersion: &quot;413&quot;</span><br><span class="line">  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/view</span><br><span class="line">  uid: 0aa05d0e-69b3-45df-8f92-be922386dbfe</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - configmaps</span><br><span class="line">  - endpoints</span><br><span class="line">  - persistentvolumeclaims</span><br><span class="line">  - persistentvolumeclaims/status</span><br><span class="line">  - pods</span><br><span class="line">  - replicationcontrollers</span><br><span class="line">  - replicationcontrollers/scale</span><br><span class="line">  - serviceaccounts</span><br><span class="line">  - services</span><br><span class="line">  - services/status</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  .......</span><br></pre></td></tr></table></figure></p><p>​        这个ClusterRole又很多规则,允许只读(get,list,watch)访问各种资源.这些资源都是有名称空间的.这个名为View的ClusterRole的作用取决于它是和<strong>ClusterRoleBinding</strong>绑定还是和<strong>RoleBinding</strong>绑定.</p><ul><li><p>如果创建了一个<strong>ClusterRoleBinding</strong>,并且引用了View这个<strong>ClusterRole</strong>.那么绑定的主体可以在所有名称空间中查看指定资源</p></li><li><p>如果创建的是一个<strong>RoleBinding</strong>,并且引用了View这个<strong>ClusterRole</strong>,那么在绑定中列出的主体只能查看在RoleBinding名称空间中的资源.</p></li></ul><p>下面观察一下一个ClusterRole绑定到<strong>ClusterRoleBinding</strong>和<strong>RoleBinding</strong>的不同效果</p><p>在实验之前,确认一下默认情况下foo名称空间下的pod无论是访问集群下的pod还是名称空间下的pod都没有权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment"># curl localhost:8001/api/v1/pods</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"kind"</span>: <span class="string">"Status"</span>,</span><br><span class="line">  <span class="string">"apiVersion"</span>: <span class="string">"v1"</span>,</span><br><span class="line">  <span class="string">"metadata"</span>: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"status"</span>: <span class="string">"Failure"</span>,</span><br><span class="line">  <span class="string">"message"</span>: <span class="string">"pods is forbidden: User \"system:serviceaccount:foo:default\" cannot list resource \"pods\" in API group \"\" at the cluster scope"</span>,</span><br><span class="line">  <span class="string">"reason"</span>: <span class="string">"Forbidden"</span>,</span><br><span class="line">  <span class="string">"details"</span>: &#123;</span><br><span class="line">    <span class="string">"kind"</span>: <span class="string">"pods"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"code"</span>: 403</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">/ <span class="comment"># curl localhost:8001/api/v1/namespaces/foo/pods</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"kind"</span>: <span class="string">"Status"</span>,</span><br><span class="line">  <span class="string">"apiVersion"</span>: <span class="string">"v1"</span>,</span><br><span class="line">  <span class="string">"metadata"</span>: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"status"</span>: <span class="string">"Failure"</span>,</span><br><span class="line">  <span class="string">"message"</span>: <span class="string">"pods is forbidden: User \"system:serviceaccount:foo:default\" cannot list resource \"pods\" in API group \"\" in the namespace \"foo\""</span>,</span><br><span class="line">  <span class="string">"reason"</span>: <span class="string">"Forbidden"</span>,</span><br><span class="line">  <span class="string">"details"</span>: &#123;</span><br><span class="line">    <span class="string">"kind"</span>: <span class="string">"pods"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"code"</span>: 403</span><br></pre></td></tr></table></figure><h4 id="1-ClusterRole绑定到ClusterRoleBinding"><a href="#1-ClusterRole绑定到ClusterRoleBinding" class="headerlink" title="1.ClusterRole绑定到ClusterRoleBinding"></a>1.ClusterRole绑定到ClusterRoleBinding</h4><p>下面的命令将view这个ClusterRole绑定到foo名称空间下的ServiceAccount</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl create clusterrolebinding view-test --clusterrole=view --serviceaccount=foo:default</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/view-test created</span><br></pre></td></tr></table></figure><p>现在pod能列出foo名称空间下的Pod了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/ # curl localhost:8001/api/v1/namespaces/foo/pods</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;PodList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;/api/v1/namespaces/foo/pods&quot;,</span><br><span class="line">    &quot;resourceVersion&quot;: &quot;10865044&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;test-7b4bb6b9ff-qpppf&quot;,</span><br></pre></td></tr></table></figure><p>而且也可以列出bar名称空间下的Pod了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/ # curl localhost:8001/api/v1/namespaces/bar/pods</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;PodList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;/api/v1/namespaces/bar/pods&quot;,</span><br><span class="line">    &quot;resourceVersion&quot;: &quot;10865200&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;test-7b4bb6b9ff-jl6v5&quot;</span><br></pre></td></tr></table></figure><p>还可以使用/api/v1/pods的URL路径来检索所有名称空间中的pod</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">/ # curl localhost:8001/api/v1/pods | more</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;PodList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;/api/v1/pods&quot;,</span><br><span class="line">    &quot;resourceVersion&quot;: &quot;10872966&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;test-7b4bb6b9ff-jl6v5&quot;,</span><br><span class="line">        &quot;generateName&quot;: &quot;test-7b4bb6b9ff-&quot;,</span><br><span class="line">        &quot;namespace&quot;: &quot;bar&quot;,</span><br><span class="line">        ......</span><br></pre></td></tr></table></figure><p><strong>总结:</strong> 正如预期的那样.将ClusterRole绑定到ClusterRoleBinding.便可以获取整个集群中(包括所有名称空间)下的所有pod的列表.</p><hr><h4 id="2-将ClusterRole绑定到RoleBinding"><a href="#2-将ClusterRole绑定到RoleBinding" class="headerlink" title="2.将ClusterRole绑定到RoleBinding."></a>2.将ClusterRole绑定到RoleBinding.</h4><p>先删除刚才创建的view-test的绑定关系删除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl delete clusterrolebinding view-test</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io &quot;view-test&quot; deleted</span><br></pre></td></tr></table></figure><p>创建一个RoleBinding替代</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl create rolebinding view-test --clusterrole=view --serviceaccount=foo:default -n foo</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/view-test created</span><br></pre></td></tr></table></figure><p>现在foo名称空间有一个RoleBinding,将同一个名称空间中的default ServiceAccount绑定到view这个<strong>ClusterRole</strong></p><p>现在pod仍然可以访问当前名称空间下的Pod</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/ # curl localhost:8001/api/v1/namespaces/foo/pods</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;PodList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;/api/v1/namespaces/foo/pods&quot;,</span><br><span class="line">    &quot;resourceVersion&quot;: &quot;10876697&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;test-7b4bb6b9ff-qpppf&quot;,</span><br></pre></td></tr></table></figure><p>但是现在已经无法访问bar名称空间下的pod了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">/ # curl localhost:8001/api/v1/namespaces/bar/pods</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Status&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;status&quot;: &quot;Failure&quot;,</span><br><span class="line">  &quot;message&quot;: &quot;pods is forbidden: User \&quot;system:serviceaccount:foo:default\&quot; cannot list resource \&quot;pods\&quot; in API group \&quot;\&quot; in the namespace \&quot;bar\&quot;&quot;,</span><br><span class="line">  &quot;reason&quot;: &quot;Forbidden&quot;,</span><br><span class="line">  &quot;details&quot;: &#123;</span><br><span class="line">    &quot;kind&quot;: &quot;pods&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;code&quot;: 403</span><br></pre></td></tr></table></figure><p>整个集群下的pod资源当然也无法访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">/ # curl localhost:8001/api/v1/pods</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Status&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;status&quot;: &quot;Failure&quot;,</span><br><span class="line">  &quot;message&quot;: &quot;pods is forbidden: User \&quot;system:serviceaccount:foo:default\&quot; cannot list resource \&quot;pods\&quot; in API group \&quot;\&quot; at the cluster scope&quot;,</span><br><span class="line">  &quot;reason&quot;: &quot;Forbidden&quot;,</span><br><span class="line">  &quot;details&quot;: &#123;</span><br><span class="line">    &quot;kind&quot;: &quot;pods&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;code&quot;: 403</span><br></pre></td></tr></table></figure><hr><h3 id="总结Role-ClusterRole-RoleBinding和ClusterRoleBinding的组合"><a href="#总结Role-ClusterRole-RoleBinding和ClusterRoleBinding的组合" class="headerlink" title="总结Role,ClusterRole,RoleBinding和ClusterRoleBinding的组合"></a>总结Role,ClusterRole,RoleBinding和ClusterRoleBinding的组合</h3><table><thead><tr><th>访问资源</th><th>角色类型</th><th>绑定类型</th></tr></thead><tbody><tr><td>集群级别的资源(Nodes,PersistentVolumes,……)</td><td>ClusterRole</td><td>ClusterRoleBinding</td></tr><tr><td>非资源URL(/api,/healthz……)</td><td>ClusterRole</td><td>ClusterRoleBinding</td></tr><tr><td>在任何名称空间中的资源(跨所有名称空间的资源)</td><td>ClusterRole</td><td>ClusterRoleBinding</td></tr><tr><td>具体名称空间中的资源(在多个名称空间中重用这个相同的ClusterRole</td><td>ClusterRole</td><td>RoleBinding</td></tr><tr><td>在具体名称空间中的资源(Role必须在每个名称空间中定义)</td><td>Role</td><td>RoleBinding</td></tr></tbody></table><hr><h3 id="了解默认的ClusterRole和ClusterRoleBinding"><a href="#了解默认的ClusterRole和ClusterRoleBinding" class="headerlink" title="了解默认的ClusterRole和ClusterRoleBinding"></a>了解默认的ClusterRole和ClusterRoleBinding</h3><p>Kubernetes提供了默认的ClusterRole和ClusterRoleBinding.API服务器启动时都会更新他们,这保证了如果你错误地删除角色和绑定,Kubernetes会重新创建默认的角色和绑定</p><figure class="highlight plain"><figcaption><span>get clusterrolebinding```和```kubectl get clusterrole```命令显示了系统默认的ClusterRole和ClusterRoleBinding</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在前面的例子中,使用了默认的view ClusterRole.它允许读取一个名称空间中的大多数资源(除了Role,RoleBinding,Secret).</span><br><span class="line"></span><br><span class="line">下面是一些其他的默认**ClusterRole**:</span><br></pre></td></tr></table></figure><p>#允许修改一个名称空间中的资源,不允许查看和修改Role,RoleBinding<br>ClusterRole: edit</p><p>#一个名称空间中的资源完全控制权.可以读取和修改名称空间中的任何资源.和上面的区别在于是否有权限查看修改Role,RoleBinding<br>ClusterRole: admin</p><p>#集群完全控制权限<br>ClusterRole: cluster-admin<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">显然将admin或者cluster-admin授予pod应用程序或者用户是一个坏主意,和安全问题一样,最好是仅仅给每个人提供他们所需的权限(最小权限原则)</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 总结:</span><br></pre></td></tr></table></figure></p><p>#创建ServiceAccount<br>kubectl create serviceaccount 服务账号名 </p><p>#将ServiceAccount分配给pod<br>在pod的manifest配置文件中指定ServiceAccount名:<br>serviceAccountName: 服务账号名</p><p>#RBAC4种角色资源<br>Role: 常规角色.定义了对某个资源具有某种访问权限<br>RoleBinding: 常规角色绑定.定义了该角色绑定到哪个主体<br>ClusterRole: 集群角色.同上<br>ClusterRoleBinding: 集群角色绑定.同上</p><p>区别在于:<br>常规角色和集群角色绑定作用于某个名称空间下.<br>集群角色和集群角色绑定作用于整个集群,不受限于任何名称空间</p><p>#了解集群默认集群角色和集群角色绑定<br><code>`</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;kubernetes-API服务器权限-ServiceAccount-amp-amp-RBAC&quot;&gt;&lt;a href=&quot;#kubernetes-API服务器权限-ServiceAccount-amp-amp-RBAC&quot; class=&quot;headerlink&quot; title
      
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>使用kubeadmin安装kubernetes集群文档</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/%E4%BD%BF%E7%94%A8kubeadmin%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4%E6%96%87%E6%A1%A3/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/使用kubeadmin安装kubernetes集群文档/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T10:06:08.209Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用kubeadmin安装kubernetes集群文档"><a href="#使用kubeadmin安装kubernetes集群文档" class="headerlink" title="使用kubeadmin安装kubernetes集群文档"></a>使用kubeadmin安装kubernetes集群文档</h2><h3 id="集群环境"><a href="#集群环境" class="headerlink" title="集群环境"></a>集群环境</h3><table><thead><tr><th>主机名</th><th>IP地址</th><th>节点角色</th><th>操作系统</th><th>Service网段</th><th>pod网段</th></tr></thead><tbody><tr><td>k8s-m01</td><td>10.111.2.58</td><td>master</td><td>CentOS7.6</td><td>10.100.0.0/20</td><td>10.96.0.0/12</td></tr><tr><td>k8s-n01</td><td>10.111.2.56</td><td>node</td><td>CentOS7.6</td><td>10.100.0.0/20</td><td>10.96.0.0/12</td></tr><tr><td>k8s-n02</td><td>10.111.2.55</td><td>node</td><td>CentOS7.6</td><td>10.100.0.0/20</td><td>10.96.0.0/12</td></tr></tbody></table><hr><h3 id="服务器-网络要求"><a href="#服务器-网络要求" class="headerlink" title="服务器,网络要求"></a>服务器,网络要求</h3><ul><li><p>service和Pod网段可以任意指定,但是不能和物理机网段,或者当前其他物理网段有冲突</p></li><li><p>服务器配置至少是2核2G</p></li></ul><a id="more"></a><h3 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h3><ul><li><p>kubernetes v1.15.3</p></li><li><p>docker 18.09.7</p></li></ul><h3 id="安装文档参考"><a href="#安装文档参考" class="headerlink" title="安装文档参考:"></a>安装文档参考:</h3><p>大部分文档资料来源: <a href="https://www.kuboard.cn/install/install-k8s.html#introduction" target="_blank" rel="noopener">https://www.kuboard.cn/install/install-k8s.html#introduction</a></p><p>但是我没有参考这个教程安装calico网络插件..而是安装flannal网络插件</p><h3 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h3><blockquote><p> 以下所有步骤都是用root账户执行</p></blockquote><p><strong>一.在master节点执行以下安装步骤</strong></p><p>1.设置三台服务器的主机名:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#master节点:</span><br><span class="line">hostnamectl --static set-hostname  k8s-m01</span><br><span class="line"></span><br><span class="line">#node1节点</span><br><span class="line">hostnamectl --static set-hostname  k8s-n01</span><br><span class="line"></span><br><span class="line">#node2节点</span><br><span class="line">hostnamectl --static set-hostname  k8s-02</span><br></pre></td></tr></table></figure><p>2.添加host解析</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#在3台服务器上执行</span><br><span class="line"></span><br><span class="line">echo &quot;10.111.2.58   k8s-m01</span><br><span class="line">10.111.2.56  k8s-n01</span><br><span class="line">10.111.2.55 k8s-n02</span><br><span class="line">&quot; &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><p>3.安装以下软件:</p><ul><li>docker</li><li>nfs-utils</li><li>kubectl</li><li>kubeadm</li><li>kubelet</li></ul><p>一键安装:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -sSL https://kuboard.cn/install-script/v1.15.3/install-kubelet.sh | sh</span><br></pre></td></tr></table></figure><p>也可以采用手动安装的方式.(以下手动安装步骤实际上就是上面的Install-kubelet.sh文件内容.):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line"># 在 master 节点和 worker 节点都要执行</span><br><span class="line"></span><br><span class="line"># 安装 docker</span><br><span class="line"># 参考文档如下</span><br><span class="line"># https://docs.docker.com/install/linux/docker-ce/centos/ </span><br><span class="line"># https://docs.docker.com/install/linux/linux-postinstall/</span><br><span class="line"></span><br><span class="line"># 卸载旧版本</span><br><span class="line">yum remove -y docker \</span><br><span class="line">docker-client \</span><br><span class="line">docker-client-latest \</span><br><span class="line">docker-common \</span><br><span class="line">docker-latest \</span><br><span class="line">docker-latest-logrotate \</span><br><span class="line">docker-logrotate \</span><br><span class="line">docker-selinux \</span><br><span class="line">docker-engine-selinux \</span><br><span class="line">docker-engine</span><br><span class="line"></span><br><span class="line"># 设置 yum repository</span><br><span class="line">yum install -y yum-utils \</span><br><span class="line">device-mapper-persistent-data \</span><br><span class="line">lvm2</span><br><span class="line">yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line"></span><br><span class="line"># 安装并启动 docker</span><br><span class="line">yum install -y docker-ce-18.09.7 docker-ce-cli-18.09.7 containerd.io</span><br><span class="line">systemctl enable docker</span><br><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"># 安装 nfs-utils</span><br><span class="line"># 必须先安装 nfs-utils 才能挂载 nfs 网络存储</span><br><span class="line">yum install -y nfs-utils</span><br><span class="line"></span><br><span class="line"># 关闭 防火墙</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line"></span><br><span class="line"># 关闭 SeLinux</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config</span><br><span class="line"></span><br><span class="line"># 关闭 swap</span><br><span class="line">swapoff -a</span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"># 修改 /etc/sysctl.conf</span><br><span class="line"># 如果有配置，则修改</span><br><span class="line">sed -i &quot;s#^net.ipv4.ip_forward.*#net.ipv4.ip_forward=1#g&quot;  /etc/sysctl.conf</span><br><span class="line">sed -i &quot;s#^net.bridge.bridge-nf-call-ip6tables.*#net.bridge.bridge-nf-call-ip6tables=1#g&quot;  /etc/sysctl.conf</span><br><span class="line">sed -i &quot;s#^net.bridge.bridge-nf-call-iptables.*#net.bridge.bridge-nf-call-iptables=1#g&quot;  /etc/sysctl.conf</span><br><span class="line"># 可能没有，追加</span><br><span class="line">echo &quot;net.ipv4.ip_forward = 1&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo &quot;net.bridge.bridge-nf-call-ip6tables = 1&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo &quot;net.bridge.bridge-nf-call-iptables = 1&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line"># 执行命令以应用</span><br><span class="line">sysctl -p</span><br><span class="line"></span><br><span class="line"># 配置K8S的yum源</span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">repo_gpgcheck=0</span><br><span class="line">gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg</span><br><span class="line">       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 卸载旧版本</span><br><span class="line">yum remove -y kubelet kubeadm kubectl</span><br><span class="line"></span><br><span class="line"># 安装kubelet、kubeadm、kubectl</span><br><span class="line">yum install -y kubelet-1.15.3 kubeadm-1.15.3 kubectl-1.15.3</span><br><span class="line"></span><br><span class="line"># 修改docker Cgroup Driver为systemd</span><br><span class="line"># # 将/usr/lib/systemd/system/docker.service文件中的这一行 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock</span><br><span class="line"># # 修改为 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd</span><br><span class="line"># 如果不修改，在添加 worker 节点时可能会碰到如下错误</span><br><span class="line"># [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. </span><br><span class="line"># Please follow the guide at https://kubernetes.io/docs/setup/cri/</span><br><span class="line">sed -i &quot;s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g&quot; /usr/lib/systemd/system/docker.service</span><br><span class="line"></span><br><span class="line"># 设置 docker 镜像，提高 docker 镜像下载速度和稳定性</span><br><span class="line"># 如果您访问 https://hub.docker.io 速度非常稳定，亦可以跳过这个步骤</span><br><span class="line">curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io</span><br><span class="line"></span><br><span class="line"># 重启 docker，并启动 kubelet</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br><span class="line">systemctl enable kubelet &amp;&amp; systemctl start kubelet</span><br><span class="line"></span><br><span class="line">docker version</span><br></pre></td></tr></table></figure><p>4.初始化master节点</p><p>一键初始化 .执行以下命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 只在 master 节点执行</span><br><span class="line"># 替换 x.x.x.x 为 master 节点实际 IP（请使用内网 IP）</span><br><span class="line"># export 命令只在当前 shell 会话中有效，开启新的 shell 窗口后，如果要继续安装过程，请重新执行此处的 export 命令</span><br><span class="line"></span><br><span class="line">export MASTER_IP=x.x.x.x</span><br><span class="line"></span><br><span class="line"># 替换 apiserver.demo 为 您想要的 dnsName (不建议使用 master 的 hostname 作为 APISERVER_NAME)</span><br><span class="line"></span><br><span class="line">export APISERVER_NAME=apiserver.demo</span><br><span class="line">export POD_SUBNET=10.100.0.1/20</span><br><span class="line">echo &quot;$&#123;MASTER_IP&#125;    $&#123;APISERVER_NAME&#125;&quot; &gt;&gt; /etc/hosts</span><br><span class="line">curl -sSL https://kuboard.cn/install-script/v1.15.3/init-master.sh | sh</span><br></pre></td></tr></table></figure><p>也可以手工方式初始化:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># 只在 master 节点执行</span><br><span class="line"># 替换 x.x.x.x 为 master 节点实际 IP（请使用内网 IP）</span><br><span class="line"># export 命令只在当前 shell 会话中有效，开启新的 shell 窗口后，如果要继续安装过程，请重新执行此处的 export 命令</span><br><span class="line">export MASTER_IP=x.x.x.x</span><br><span class="line"># 替换 apiserver.demo 为 您想要的 dnsName (不建议使用 master 的 hostname 作为 APISERVER_NAME)</span><br><span class="line">export APISERVER_NAME=apiserver.demo</span><br><span class="line">export POD_SUBNET=10.100.0.1/20</span><br><span class="line">echo &quot;$&#123;MASTER_IP&#125;    $&#123;APISERVER_NAME&#125;&quot; &gt;&gt; /etc/hosts</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 只在 master 节点执行</span><br><span class="line"></span><br><span class="line"># 查看完整配置选项 https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2</span><br><span class="line"></span><br><span class="line">rm -f ./kubeadm-config.yaml</span><br><span class="line">cat &lt;&lt;EOF &gt; ./kubeadm-config.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.15.3</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">controlPlaneEndpoint: &quot;$&#123;APISERVER_NAME&#125;:6443&quot;</span><br><span class="line">networking:</span><br><span class="line">  serviceSubnet: &quot;10.96.0.0/12&quot;</span><br><span class="line">  podSubnet: &quot;$&#123;POD_SUBNET&#125;&quot;</span><br><span class="line">  dnsDomain: &quot;cluster.local&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># kubeadm init</span><br><span class="line"># 根据您服务器网速的情况，您需要等候 3 - 10 分钟</span><br><span class="line">kubeadm init --config=kubeadm-config.yaml --upload-certs</span><br><span class="line"></span><br><span class="line"># 配置 kubectl</span><br><span class="line">rm -rf /root/.kube/</span><br><span class="line">mkdir /root/.kube/</span><br><span class="line">cp -i /etc/kubernetes/admin.conf /root/.kube/config</span><br><span class="line"></span><br><span class="line"># 安装 calico 网络插件</span><br><span class="line"># 参考文档 https://docs.projectcalico.org/v3.8/getting-started/kubernetes/</span><br><span class="line">rm -f calico.yaml</span><br><span class="line">wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml</span><br><span class="line">sed -i &quot;s#192\.168\.0\.0/16#$&#123;POD_SUBNET&#125;#&quot; calico.yaml</span><br><span class="line">kubectl apply -f calico.yaml</span><br></pre></td></tr></table></figure><blockquote><p>如果不像安卓calico网络插件,而是安装flannal插件,可以将上个安装calico网络插件步骤替换成如下flannal部署方式:</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure><p>5.检查初始化结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 只在 master 节点执行</span><br><span class="line"></span><br><span class="line"># 执行如下命令，等待 3-10 分钟，直到所有的容器组处于 Running 状态</span><br><span class="line">watch kubectl get pod -n kube-system -o wide</span><br><span class="line"></span><br><span class="line"># 查看 master 节点初始化结果</span><br><span class="line">kubectl get nodes</span><br></pre></td></tr></table></figure><p>6.获得join命令参数:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 只在 master 节点执行</span><br><span class="line">kubeadm token create --print-join-command</span><br><span class="line"></span><br><span class="line">执行这个命令会输出如下类似信息:</span><br><span class="line"></span><br><span class="line"># kubeadm token create 命令的输出</span><br><span class="line">kubeadm join apiserver.demo:6443 --token mpfjma.4vjjg8flqihor4vt     --discovery-token-ca-cert-hash sha256:6f7a8e40a810323672de5eee6f4d19aa2dbdb38411845a1bf5dd63485c43d303</span><br></pre></td></tr></table></figure><hr><p>二.初始化node节点</p><p>在两台node节点分别执行:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 只在 worker 节点执行</span><br><span class="line"># 替换 $&#123;MASTER_IP&#125; 为 master 节点实际 IP</span><br><span class="line"># 替换 $&#123;APISERVER_NAME&#125; 为初始化 master 节点时所使用的 APISERVER_NAME</span><br><span class="line">echo &quot;$&#123;MASTER_IP&#125;    $&#123;APISERVER_NAME&#125;&quot; &gt;&gt; /etc/hosts</span><br><span class="line"></span><br><span class="line"># 替换为 master 节点上 kubeadm token create 命令的输出</span><br><span class="line">kubeadm join apiserver.demo:6443 --token mpfjma.4vjjg8flqihor4vt     --discovery-token-ca-cert-hash sha256:6f7a8e40a810323672de5eee6f4d19aa2dbdb38411845a1bf5dd63485c43d303</span><br></pre></td></tr></table></figure><p>四.检查初始化结果</p><p>在master节点上执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 只在 master 节点执行</span><br><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line">#输出结果如下:</span><br><span class="line"></span><br><span class="line">[root@k8s-m01 ~]# kubectl get nodes</span><br><span class="line">NAME      STATUS     ROLES    AGE     VERSION</span><br><span class="line">k8s-m01   Ready      master   7m52s   v1.15.3</span><br><span class="line">k8s-n01   Ready      &lt;none&gt;   50s     v1.15.3</span><br><span class="line">k8s-n02   NotReady   &lt;none&gt;   5s      v1.15.3</span><br></pre></td></tr></table></figure><p>至此,已经完成了kubeadm工具安装部署kubernetes集群的工作</p><hr><h2 id="移除node节点"><a href="#移除node节点" class="headerlink" title="移除node节点"></a>移除node节点</h2><p><strong>方式一.</strong></p><ul><li>在node节点上执行</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 只在 worker 节点执行</span><br><span class="line">kubeadm reset</span><br></pre></td></tr></table></figure><p><strong>方式二.</strong></p><ul><li>在master节点上执行</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 只在 master 节点执行</span><br><span class="line">kubectl delete node k8s-n01</span><br></pre></td></tr></table></figure><hr><h2 id="重置k8s集群配置"><a href="#重置k8s集群配置" class="headerlink" title="重置k8s集群配置"></a>重置k8s集群配置</h2><p>在master节点上执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm reset</span><br></pre></td></tr></table></figure><p>然后在node节点上执行reset,再重新添加回集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm reset</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;使用kubeadmin安装kubernetes集群文档&quot;&gt;&lt;a href=&quot;#使用kubeadmin安装kubernetes集群文档&quot; class=&quot;headerlink&quot; title=&quot;使用kubeadmin安装kubernetes集群文档&quot;&gt;&lt;/a&gt;使用kubeadmin安装kubernetes集群文档&lt;/h2&gt;&lt;h3 id=&quot;集群环境&quot;&gt;&lt;a href=&quot;#集群环境&quot; class=&quot;headerlink&quot; title=&quot;集群环境&quot;&gt;&lt;/a&gt;集群环境&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;节点角色&lt;/th&gt;
&lt;th&gt;操作系统&lt;/th&gt;
&lt;th&gt;Service网段&lt;/th&gt;
&lt;th&gt;pod网段&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;k8s-m01&lt;/td&gt;
&lt;td&gt;10.111.2.58&lt;/td&gt;
&lt;td&gt;master&lt;/td&gt;
&lt;td&gt;CentOS7.6&lt;/td&gt;
&lt;td&gt;10.100.0.0/20&lt;/td&gt;
&lt;td&gt;10.96.0.0/12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k8s-n01&lt;/td&gt;
&lt;td&gt;10.111.2.56&lt;/td&gt;
&lt;td&gt;node&lt;/td&gt;
&lt;td&gt;CentOS7.6&lt;/td&gt;
&lt;td&gt;10.100.0.0/20&lt;/td&gt;
&lt;td&gt;10.96.0.0/12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k8s-n02&lt;/td&gt;
&lt;td&gt;10.111.2.55&lt;/td&gt;
&lt;td&gt;node&lt;/td&gt;
&lt;td&gt;CentOS7.6&lt;/td&gt;
&lt;td&gt;10.100.0.0/20&lt;/td&gt;
&lt;td&gt;10.96.0.0/12&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id=&quot;服务器-网络要求&quot;&gt;&lt;a href=&quot;#服务器-网络要求&quot; class=&quot;headerlink&quot; title=&quot;服务器,网络要求&quot;&gt;&lt;/a&gt;服务器,网络要求&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;service和Pod网段可以任意指定,但是不能和物理机网段,或者当前其他物理网段有冲突&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;服务器配置至少是2核2G&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>kong+casssandra集群环境部署</title>
    <link href="https://jesse.top/2020/06/26/Linux-Web/kong%20casssandra%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/"/>
    <id>https://jesse.top/2020/06/26/Linux-Web/kong casssandra集群环境部署/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T03:07:42.986Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kong-casssandra集群环境部署"><a href="#kong-casssandra集群环境部署" class="headerlink" title="kong+casssandra集群环境部署"></a>kong+casssandra集群环境部署</h2><h3 id="kong简介"><a href="#kong简介" class="headerlink" title="kong简介"></a>kong简介</h3><p>Kong是Mashape开源的一款API网关，起初是用来管理 Mashape 公司15000个微服务的，后来在2015年开源,现在已经在很多创业公司、大型企业和政府机构中广泛使用。基于nginx,Lua和Cassandra或PostgreSQL，支持分布式操作，有很强的可移植性和可扩展性。可以在任何一种基础设施上运行,作为应用和API之间的中间层，加上众多功能强大的插件，可以实现认证授权、访问控制等功能。并且提供易于使用的RESTful API来操作和配置系统。</p><p>有关kong的详细介绍请参考官网.</p><p>–</p><h3 id="cassandra简介"><a href="#cassandra简介" class="headerlink" title="cassandra简介"></a>cassandra简介</h3><p>Cassandra 是一个来自 Apache 的分布式数据库，具有高度可扩展性，可用于管理大量的结构化数据。它提供了高可用性，没有单点故障。kong支持PostgreSQL或者Cassandra两种数据库.这里我们选择了cassandra.</p><p>有关cassandra的详细介绍和使用方法.请参考官网</p><a id="more"></a><p>–</p><h3 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h3><ul><li><strong>kong cluster</strong></li></ul><p>kong 集群并不意味着客户端请求将会负载均衡到kong集群中的每个节点上，kong集群并不是开箱即用，仍然需要在kong集群多节点上层搭建负载均衡，以便分发请求。 一个kong集群只是意味着集群内的节点，都共享同样的配置。</p><p>有关Kong cluster集群的详细介绍请参考官网:<a href="https://docs.konghq.com/0.14.x/clustering/" target="_blank" rel="noopener">Kong cluser document</a></p><p>为了提高冗余性和健壮性.我们对kong的每个环节都进行了冗余设计.一个基本的kong集群架构大概如下图所示:</p><p><img src="http://pabkmteb4.bkt.clouddn.com/kong-flow.png" alt=""></p><p>–</p><h3 id="部署步骤"><a href="#部署步骤" class="headerlink" title="部署步骤"></a>部署步骤</h3><p><strong>环境</strong>: </p><p>阿里云ECS Centos7.4操作系统<br>Kong: 0.14 最新版<br>Cassandra: 3.11 最新版</p><p>–</p><h4 id="安装Cassandra"><a href="#安装Cassandra" class="headerlink" title="安装Cassandra"></a>安装Cassandra</h4><p>安装方式官网参考: <a href="http://cassandra.apache.org/doc/latest/getting_started/installing.html#installation-from-binary-tarball-files" target="_blank" rel="noopener">Installing Cassandra</a></p><p><strong>安装前提条件</strong></p><p>1.安装JDK 8版本<br>2.安装2.7以上版本的Python.(cassandra管理工具:cqlsh 需要python2.7以上环境)</p><p><strong>安装步骤</strong></p><p>1.下载cassandra二进制文件.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://mirrors.tuna.tsinghua.edu.cn/apache/cassandra/3.11.3/apache-cassandra-3.11.3-bin.tar.gz</span><br></pre></td></tr></table></figure><p>2.将cassandra目录添加进环境变量.用work用户运行cassandra</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir /usr/local/cassandra</span><br><span class="line">sudo chown -R work:work /usr/local/cassandra</span><br><span class="line">sudo tar -xvf apache-cassandra-3.11.3-bin.tar.gz -C /usr/local/cassandra</span><br><span class="line"></span><br><span class="line">cd /usr/local/cassandra</span><br><span class="line">sudo mv apache-cassandra-3.11.3/* .</span><br><span class="line">sudo rm apache-cassandra-3.11.3/ -rf</span><br></pre></td></tr></table></figure><p>添加进环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/profile</span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/usr/local/java</span><br><span class="line">export CASSANDRA_HOME=/usr/local/cassandra</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$CASSANDRA_HOME/bin</span><br></pre></td></tr></table></figure><p>–</p><h4 id="配置Cassandra-以及cassandra集群"><a href="#配置Cassandra-以及cassandra集群" class="headerlink" title="配置Cassandra,以及cassandra集群"></a>配置Cassandra,以及cassandra集群</h4><p>1.编辑cassandra的cassandra.yml配置文件.修改下列配置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node1 kong]$ vim /usr/local/cassandra/conf/cassandra.yaml</span><br><span class="line"></span><br><span class="line">#定义Cassandra集群名</span><br><span class="line">cluster_name: &apos;dwd_cassandra&apos;</span><br><span class="line">#定义hints路径.可以使用默认路径</span><br><span class="line">hints_directory: /data/cassandra/hints</span><br><span class="line"></span><br><span class="line">#采用密码方式连接数据库.默认情况下不需要任何用户密码就可以登录数据库</span><br><span class="line">authenticator: PasswordAuthenticator</span><br><span class="line"></span><br><span class="line">#定义数据库文件路径.可以使用默认/var/lib路径</span><br><span class="line">data_file_directories:</span><br><span class="line">      - /data/cassandra</span><br><span class="line"></span><br><span class="line">#定义commit日志路径.可以使用默认路径</span><br><span class="line">commitlog_directory: /data/cassandra/commitlog</span><br><span class="line"></span><br><span class="line">#缓存文件路径</span><br><span class="line">saved_caches_directory: /data/cassandra/saved_caches</span><br><span class="line"></span><br><span class="line">#关键配置,定义集群种子服务器地址.这里定义服务器的内网地址.不能使用0.0.0.0或者127的本机地址,可以加入多个集群节点的地址,IP地址之间用逗号分隔</span><br><span class="line">- seeds: &quot;10.25.87.159&quot;</span><br><span class="line"></span><br><span class="line">#listen地址</span><br><span class="line">listen_address: 10.25.87.159</span><br><span class="line"></span><br><span class="line">#rpc地址</span><br><span class="line">rpc_address: 10.25.87.159</span><br></pre></td></tr></table></figure><p>2.创建刚才定义的路径目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -pv /data/cassandra </span><br><span class="line">sudo chown -R work.work /data/</span><br></pre></td></tr></table></figure><p>3.启动cassandra.直接在命令行执行cassandra</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node1 kong]$ cassandra </span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line"></span><br><span class="line">[work@kong-node1 kong]$ /usr/local/cassandra/bin/cassandra</span><br></pre></td></tr></table></figure><ol start="4"><li>使用cqlsh工具登陆cassandra数据库.创建cassandra用户密码,以及创建键空间</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#注意由于cassandra只侦听了内网的地址,因此要指定IP地址.</span><br><span class="line">#默认账号密码都是cassandra</span><br><span class="line">[work@kong-node1 kong]$ cqlsh 10.25.87.159  -ucassandra -pcassandra </span><br><span class="line"></span><br><span class="line">#创建一个kong用户.并且为超级用户</span><br><span class="line">cassandra@cqlsh&gt; create user kong with password &apos;kong&apos; superuser;</span><br><span class="line"></span><br><span class="line">#创建一个keyspace.命名为kong</span><br><span class="line">cassandra@cqlsh&gt; CREATE KEYSPACE kong WITH REPLICATION = &#123; &apos;class&apos; : &apos;SimpleStrategy&apos;, &apos;replication_factor&apos; : 1&#125;;</span><br><span class="line"></span><br><span class="line">cassandra@cqlsh&gt; exit</span><br></pre></td></tr></table></figure><p>5.删除自带的cassandra用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node1 cassandra]$ cqlsh 10.25.87.159  -ukong -pkong</span><br><span class="line">kong@cqlsh&gt; desc kong;</span><br><span class="line"></span><br><span class="line">CREATE KEYSPACE kong WITH replication = &#123;&apos;class&apos;: &apos;SimpleStrategy&apos;, &apos;replication_factor&apos;: &apos;1&apos;&#125;  AND durable_writes = true;</span><br><span class="line"></span><br><span class="line">kong@cqlsh&gt; drop user cassandra;</span><br></pre></td></tr></table></figure><p>–</p><h4 id="安装Kong"><a href="#安装Kong" class="headerlink" title="安装Kong"></a>安装Kong</h4><p>安装方法可以参考官网:<a href="https://docs.konghq.com/install/centos/?_ga=2.110797315.728319704.1539597667-917309945.1539077269#packages" target="_blank" rel="noopener">Install Kong</a></p><p>1.下载,安装rpm安装包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget -O kong-community-edition-0.14.1.el7.noarch.rpm  https://bintray.com/kong/kong-community-edition-rpm/download_file?file_path=centos/7/kong-community-edition-0.14.1.el7.noarch.rpm</span><br><span class="line"></span><br><span class="line">sudo yum install epel-release</span><br><span class="line">sudo yum install kong-community-edition-0.14.1.el7.noarch.rpm</span><br></pre></td></tr></table></figure><p>2.修改配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kong</span><br><span class="line">sudo cp kong.conf.default kong.conf</span><br><span class="line"></span><br><span class="line">sudo vim kong.conf</span><br><span class="line"></span><br><span class="line">#修改日志文件路径</span><br><span class="line">prefix = /data/kong/</span><br><span class="line"></span><br><span class="line">#由于磁盘空间有限,关闭kong的代理日志.后端真实服务器会记录nginx访问日志</span><br><span class="line">proxy_access_log = off</span><br><span class="line">proxy_error_log = off</span><br><span class="line"></span><br><span class="line">#在所有地址侦听管理端口,当然只侦听127地址会更安全.</span><br><span class="line">admin_listen = 0.0.0.0:8001, 0.0.0.0:8444 ssl</span><br><span class="line"></span><br><span class="line">#指定使用cassandra数据库</span><br><span class="line">database = cassandra</span><br><span class="line">#数据库地址,端口</span><br><span class="line">cassandra_contact_points = 10.25.87.159</span><br><span class="line">cassandra_port = 9042</span><br><span class="line"></span><br><span class="line">#上文定义的cassandra数据库的用户密码和键空间</span><br><span class="line">cassandra_keyspace = kong</span><br><span class="line">cassandra_username = kong</span><br><span class="line">cassandra_password = kong</span><br><span class="line"></span><br><span class="line">#kong官方建议的cassandra一致性机制</span><br><span class="line">cassandra_consistency = QUORUM</span><br><span class="line"></span><br><span class="line">#以下是集群的数据库和缓存方面的配置.详细介绍请参考官网</span><br><span class="line">db_update_frequency = 5</span><br><span class="line">db_update_propagation = 2</span><br></pre></td></tr></table></figure><p>3.创建kong目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /data/kong</span><br></pre></td></tr></table></figure><p>4.准备启动工作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kong migrations up -c /etc/kong/kong.conf</span><br></pre></td></tr></table></figure><p>5.启动kong</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kong start -c /etc/kong/kong.conf</span><br></pre></td></tr></table></figure><p>查看端口.可以看到cassandra和kong的侦听端口都已经成功启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">work@kong-node1 kong]$ netstat -tulnp</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp        0      0 0.0.0.0:8443            0.0.0.0:*               LISTEN      29342/nginx: master</span><br><span class="line">tcp        0      0 0.0.0.0:8444            0.0.0.0:*               LISTEN      29342/nginx: master</span><br><span class="line">tcp        0      0 127.0.0.1:7199          0.0.0.0:*               LISTEN      28598/java</span><br><span class="line">tcp        0      0 0.0.0.0:8000            0.0.0.0:*               LISTEN      29342/nginx: master</span><br><span class="line">tcp        0      0 0.0.0.0:8001            0.0.0.0:*               LISTEN      29342/nginx: master</span><br><span class="line">tcp        0      0 127.0.0.1:35503         0.0.0.0:*               LISTEN      28598/java</span><br><span class="line">tcp        0      0 10.25.87.159:9042       0.0.0.0:*               LISTEN      28598/java</span><br><span class="line">tcp        0      0 10.25.87.159:7000       0.0.0.0:*               LISTEN      28598/java</span><br></pre></td></tr></table></figure><hr><h3 id="部署另外一台kong和cassandra"><a href="#部署另外一台kong和cassandra" class="headerlink" title="部署另外一台kong和cassandra"></a>部署另外一台kong和cassandra</h3><p>今天在阿里云镜像了kong-node1的服务器.新的服务器名字为kong-node2.<br>软件已经安装,只需要修改部分配置</p><h5 id="修改node1和node2上的cassandra配置"><a href="#修改node1和node2上的cassandra配置" class="headerlink" title="修改node1和node2上的cassandra配置"></a>修改node1和node2上的cassandra配置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">在node1和node2上:</span><br><span class="line">[work@kong-node2 ~]$ vim /usr/local/cassandra/conf/cassandra.yaml</span><br><span class="line">#修改seeds配置.添加2台服务器的内网IP地址</span><br><span class="line">- seeds: &quot;10.25.87.159, 10.80.229.244&quot; </span><br><span class="line"></span><br><span class="line">在node2上修改侦听地址</span><br><span class="line">[work@kong-node2 ~]$ vim /usr/local/cassandra/conf/cassandra.yaml</span><br><span class="line">#将下列地址改成node2内网地址</span><br><span class="line">listen_address: 10.80.229.244</span><br><span class="line">rpc_address: 10.80.229.244</span><br></pre></td></tr></table></figure><h5 id="在node2上修改kong的配置文件"><a href="#在node2上修改kong的配置文件" class="headerlink" title="在node2上修改kong的配置文件:"></a>在node2上修改kong的配置文件:</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">work@kong-node2 ~]$ vim /etc/kong/kong.conf</span><br><span class="line">#连接本机的cassandra数据库地址</span><br><span class="line">cassandra_contact_points = 10.80.229.244</span><br></pre></td></tr></table></figure><blockquote><p>note:千万不要启动node2上的cassandra.因为node2是从node1镜像过去的.所以数据库的token是一模一样的.</p></blockquote><h5 id="在node2上删除cassandra的数据库-这一步非常重要-因为这是从node1镜像过来的-所以node2上的数据库是node1的数据"><a href="#在node2上删除cassandra的数据库-这一步非常重要-因为这是从node1镜像过来的-所以node2上的数据库是node1的数据" class="headerlink" title="在node2上删除cassandra的数据库.这一步非常重要.因为这是从node1镜像过来的.所以node2上的数据库是node1的数据"></a>在node2上删除cassandra的数据库.这一步非常重要.因为这是从node1镜像过来的.所以node2上的数据库是node1的数据</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /data/cassandra/*</span><br></pre></td></tr></table></figure><h5 id="启动node2上的数据库"><a href="#启动node2上的数据库" class="headerlink" title="启动node2上的数据库"></a>启动node2上的数据库</h5><p>直接在命令行执行:cassandra</p><h5 id="查看cassandra的单台服务器状态"><a href="#查看cassandra的单台服务器状态" class="headerlink" title="查看cassandra的单台服务器状态"></a>查看cassandra的单台服务器状态</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node1 bin]$ nodetool info</span><br><span class="line">ID                     : 4fe1df37-e69e-4a25-acdc-4b2d73a92225</span><br><span class="line">Gossip active          : true</span><br><span class="line">Thrift active          : false</span><br><span class="line">Native Transport active: true</span><br><span class="line">Load                   : 522.87 KiB</span><br><span class="line">Generation No          : 1540372001</span><br><span class="line">Uptime (seconds)       : 63895</span><br><span class="line">Heap Memory (MB)       : 404.89 / 1004.00</span><br><span class="line">Off Heap Memory (MB)   : 0.00</span><br><span class="line">Data Center            : datacenter1</span><br><span class="line">Rack                   : rack1</span><br><span class="line">Exceptions             : 0</span><br><span class="line">Key Cache              : entries 59, size 5.03 KiB, capacity 50 MiB, 7540 hits, 7911 requests, 0.953 recent hit rate, 14400 save period in seconds</span><br><span class="line">Row Cache              : entries 0, size 0 bytes, capacity 0 bytes, 0 hits, 0 requests, NaN recent hit rate, 0 save period in seconds</span><br><span class="line">Counter Cache          : entries 0, size 0 bytes, capacity 25 MiB, 0 hits, 0 requests, NaN recent hit rate, 7200 save period in seconds</span><br><span class="line">Chunk Cache            : entries 28, size 1.75 MiB, capacity 219 MiB, 1237 misses, 26133 requests, 0.953 recent hit rate, NaN microseconds miss latency</span><br><span class="line">Percent Repaired       : 100.0%</span><br><span class="line">Token                  : (invoke with -T/--tokens to see all 256 tokens)</span><br></pre></td></tr></table></figure><h5 id="查看cassandra集群状态-可以看到集群中2台服务器"><a href="#查看cassandra集群状态-可以看到集群中2台服务器" class="headerlink" title="查看cassandra集群状态.可以看到集群中2台服务器"></a>查看cassandra集群状态.可以看到集群中2台服务器</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node1 bin]$ nodetool status</span><br><span class="line">Datacenter: datacenter1</span><br><span class="line">=======================</span><br><span class="line">Status=Up/Down</span><br><span class="line">|/ State=Normal/Leaving/Joining/Moving</span><br><span class="line">--  Address        Load       Tokens       Owns (effective)  Host ID                               Rack</span><br><span class="line">UN  10.80.229.244  339.93 KiB  256          51.3%             04a75f63-be99-4f3e-93ff-937bbe9656d8  rack1</span><br><span class="line">UN  10.25.87.159   522.87 KiB  256          48.7%             4fe1df37-e69e-4a25-acdc-4b2d73a92225  rack1</span><br></pre></td></tr></table></figure><hr><h4 id="以下是我踩过的坑-在没有删除node2上的数据库文件情况下-直接启动cassnadra"><a href="#以下是我踩过的坑-在没有删除node2上的数据库文件情况下-直接启动cassnadra" class="headerlink" title="以下是我踩过的坑.在没有删除node2上的数据库文件情况下,直接启动cassnadra."></a>以下是我踩过的坑.在没有删除node2上的数据库文件情况下,直接启动cassnadra.</h4><p>启动node2的cassandra后.发现集群无法正常启动.使用cassandra自带的nodetool工具可以查看集群状态.这里只有自己一台服务器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node2 ~]$ nodetool status</span><br><span class="line">Datacenter: datacenter1</span><br><span class="line">=======================</span><br><span class="line">Status=Up/Down</span><br><span class="line">|/ State=Normal/Leaving/Joining/Moving</span><br><span class="line">--  Address        Load       Tokens       Owns (effective)  Host ID                               Rack</span><br><span class="line">UN  10.80.229.244  600.05 KiB  256          100.0%            4fe1df37-e69e-4a25-acdc-4b2d73a92225  rack1</span><br></pre></td></tr></table></figure><p>查看cassandra启动日志,发现日志提示和node1有一样的token:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">less /usr/local/cassandra/logs/system.log</span><br><span class="line"></span><br><span class="line">INFO  [GossipStage:1] 2018-10-24 17:07:26,874 StorageService.java:2386 - Nodes /10.25.87.159 and /10.80.229.244 have the same token -9066137612411979055.  Ignoring /10.25.87.159</span><br><span class="line">INFO  [GossipStage:1] 2018-10-24 17:07:26,874 StorageService.java:2386 - Nodes /10.25.87.159 and /10.80.229.244 have the same token -912539082610246005.  Ignoring /10.25.87.159</span><br><span class="line">INFO  [GossipStage:1] 2018-10-24 17:07:26,874 StorageService.java:2386 - Nodes /10.25.87.159 and /10.80.229.244 have the same token -9125687604150710607.  Ignoring /10.25.87.159</span><br><span class="line">INFO  [GossipStage:1] 2018-10-24 17:07:26,874 StorageService.java:2386 - Nodes /10.25.87.159 and /10.80.229.244 have the same token -9186325188411815558.  Ignoring /10.25.87.159</span><br><span class="line">INFO  [GossipStage:1] 2018-10-24 17:07:26,874 StorageService.java:2386 - Nodes /10.25.87.159 and /10.80.229.244 have the same token -934168442605847346.  Ignoring /10.25.87.159</span><br><span class="line">INFO  [GossipStage:1] 2018-10-24 17:07:26,874 StorageService.java:2386 - Nodes /10.25.87.159 and /10.80.229.244 have the same token -937629522304513228.  Ignoring /10.25.87.159</span><br><span class="line">INFO  [GossipStage:1] 2018-10-24 17:07:26,874 StorageService.java:2386 - Nodes /10.25.87.159 and /10.80.229.244 have the same token -983284835358960159.  Ignoring /10.25.87.159</span><br><span class="line">INFO  [GossipStage:1] 2018-10-24 17:07:26,875 StorageService.java:2386 - Nodes /10.25.87.159 and /10.80.229.244 have the same token 1111859401021864246.  Ignoring /10.25.87.159</span><br><span class="line">INFO  [GossipStage:1] 2018-10-24 17:07:26,875 StorageService.java:2386 - Nodes /10.25.87.159 and /10.80.229.244 have the same token 1185525604491731552.  Ignoring /10.25.87.159</span><br><span class="line">INFO  [GossipStage:1] 2018-10-24 17:07:26,875 StorageService.java:2386 - Nodes /10.25.87.159 and /10.80.229.244 have the same token 1209704333924286496.  Ignoring /10.25.87.159</span><br><span class="line">INFO  [GossipStage:1] 2018-10-24 17:07:26,875 StorageService.java:2386 - Nodes /10.25.87.159 and /10.80.229.244 have the same token 1243859262038298713.  Ignoring /10.25.87.159</span><br><span class="line">INFO  [GossipStage:1] 2018-10-24 17:07:26,875 StorageService.java:2386 - Nodes /10.25.87.159 and /10.80.229.244 have the same token 1284321765579584761.  Ignoring /10.25.87.159</span><br><span class="line">INFO  [GossipStage:1] 2018-10-24 17:07:26,875 StorageService.java:2386 - Nodes /10.25.87.159 and /10.80.229.244 have the same token 1472069791929520463.  Ignoring /10.25.87.159</span><br><span class="line">INFO  [GossipStage:1] 2018-10-24 17:07:26,875 StorageService.java:2386 - Nodes /10.25.87.159 and /10.80.229.244 have the same token 1479257042759500258.  Ignoring /10.25.87.159</span><br></pre></td></tr></table></figure><p>不仅如此,在node1上启动kong,提示cassandra数据库验证失败.以及提示kong需要migrations up(只需要在第一次启动kong时才需要migratios up):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node1 bin]$ kong restart -c /etc/kong/kong.conf</span><br><span class="line">Error: /usr/local/share/lua/5.1/kong/cmd/start.lua:37: [cassandra error] the current database schema does not match this version of Kong. Please run `kong migrations up` to update/initialize the database schema. Be aware that Kong migrations should only run from a single node, and that nodes running migrations concurrently will conflict with each other and might corrupt your database schema!</span><br></pre></td></tr></table></figure><p>kong migrations失败</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> work@kong-node1 bin]$ kong migrations up -c /etc/kong/kong.conf</span><br><span class="line">migrating core for keyspace kong</span><br><span class="line">core migrated up to: 2015-01-12-175310_skeleton</span><br><span class="line">Error: [cassandra error] Error during migration 2015-01-12-175310_init_schema: [Invalid] Undefined column name request_host</span><br></pre></td></tr></table></figure><h5 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h5><p>启动node1和node2的cassandra</p><p>1.在node1和node2上drop kong的键空间</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node1 bin]$ cqlsh 10.25.87.159 -ukong -pkong</span><br><span class="line">Connected to dwd_cassandra at 10.25.87.159:9042.</span><br><span class="line">[cqlsh 5.0.1 | Cassandra 3.11.3 | CQL spec 3.4.4 | Native protocol v4]</span><br><span class="line">Use HELP for help.</span><br><span class="line">kong@cqlsh&gt; drop keyspace kong;</span><br><span class="line">kong@cqlsh&gt; exit</span><br><span class="line"></span><br><span class="line">[work@kong-node2 ~]$ cqlsh 10.80.229.244 -ukong -pkong</span><br><span class="line">Connected to dwd_cassandra at 10.80.229.244:9042.</span><br><span class="line">[cqlsh 5.0.1 | Cassandra 3.11.3 | CQL spec 3.4.4 | Native protocol v4]</span><br><span class="line">Use HELP for help.</span><br><span class="line">kong@cqlsh&gt; drop KEYSPACE kong;</span><br><span class="line">ConfigurationException: Cannot drop non existing keyspace &apos;kong&apos;.</span><br><span class="line">kong@cqlsh&gt; exit</span><br></pre></td></tr></table></figure><p>2.删除kong的键空间目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[work@kong-node1 bin]$ rm -rf  /data/cassandra/kong/</span><br><span class="line">[work@kong-node1 bin]$ ll /data/cassandra/kong/</span><br><span class="line">ls: cannot access /data/cassandra/kong/: No such file or directory</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[work@kong-node2 ~]$ rm -rf /data/cassandra/kong/</span><br><span class="line">[work@kong-node2 ~]$ ll /data/cassandra/kong/</span><br><span class="line">ls: cannot access /data/cassandra/kong/: No such file or directory</span><br></pre></td></tr></table></figure><p>3.在node1上创建kong键空间.创建完毕后,应该会同步到node2</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node1 bin]$ cqlsh 10.25.87.159 -ukong -pkong</span><br><span class="line">Connected to dwd_cassandra at 10.25.87.159:9042.</span><br><span class="line">[cqlsh 5.0.1 | Cassandra 3.11.3 | CQL spec 3.4.4 | Native protocol v4]</span><br><span class="line">Use HELP for help.</span><br><span class="line">kong@cqlsh&gt; CREATE KEYSPACE kong WITH REPLICATION = &#123; &apos;class&apos; : &apos;SimpleStrategy&apos;, &apos;replication_factor&apos; : 1&#125;;</span><br><span class="line">kong@cqlsh&gt; exit</span><br></pre></td></tr></table></figure><p>4.在node1上执行 kong migrations up ,执行完后同样会同步到node2</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">work@kong-node1 bin]$ kong migrations up -c /etc/kong/kong.conf</span><br><span class="line">migrating core for keyspace kong</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">66 migrations ran</span><br><span class="line">waiting for Cassandra schema consensus (10000ms timeout)...</span><br><span class="line">Cassandra schema consensus: reached</span><br></pre></td></tr></table></figure><p>5.在node1和node2上启动kong</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node1 bin]$ kong restart -c /etc/kong/kong.conf</span><br><span class="line"></span><br><span class="line">[work@kong-node2 ~]$ kong start -c /etc/kong/kong.conf</span><br><span class="line">Kong started</span><br></pre></td></tr></table></figure><hr><h3 id="搭建kong-dashboard"><a href="#搭建kong-dashboard" class="headerlink" title="搭建kong-dashboard"></a>搭建kong-dashboard</h3><p>kong-dashboard是管理kong各个组件(serveice,route,plugin,upstream,consumer)的可视化UI管理工具.在增删改查各个组件的配置时非常方便.</p><p>现在kong-dashboard也支持到了最新版的kong和kong的最新组件.</p><p>kong-dashboard的github参考:<a href="https://github.com/PGBI/kong-dashboard" target="_blank" rel="noopener">kong-dashboar</a></p><hr><p>1.确保需要有node.js环境.如果没有npm工具,必须先安装nodejs</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@DWD-BETA kong]$ npm -v</span><br><span class="line">5.6.0</span><br><span class="line">[work@DWD-BETA kong]$ node -v</span><br><span class="line">v8.10.0</span><br></pre></td></tr></table></figure><p>2.root用户执行下列安装命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@DWD-BETA ~]# npm install -g kong-dashboard</span><br><span class="line">/usr/local/src/nodejs/bin/kong-dashboard -&gt; /usr/local/src/nodejs/lib/node_modules/kong-dashboard/bin/kong-dashboard.js</span><br><span class="line">+ kong-dashboard@3.5.0</span><br><span class="line">added 184 packages in 28.8s</span><br></pre></td></tr></table></figure><p>3.启动kong-dashboard.启动方式有以下几种</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># Start Kong Dashboard</span><br><span class="line">kong-dashboard start --kong-url http://kong:8001</span><br><span class="line"></span><br><span class="line"># Start Kong Dashboard on a custom port</span><br><span class="line">kong-dashboard start \</span><br><span class="line">  --kong-url http://kong:8001 \</span><br><span class="line">  --port [port]</span><br><span class="line"></span><br><span class="line"># Start Kong Dashboard with basic auth</span><br><span class="line">kong-dashboard start \</span><br><span class="line">  --kong-url http://kong:8001 \</span><br><span class="line">  --basic-auth user1=password1 user2=password2</span><br><span class="line"></span><br><span class="line"># See full list of start options</span><br><span class="line">kong-dashboard start --help</span><br></pre></td></tr></table></figure><p>但是kong-dashboard是前台启动,没有deamnize模式.所以将kong-dashboard加入到supervisor进程管理</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@DWD-BETA ~]# vim /etc/supervisor/conf.d/kong-dashboard.conf</span><br><span class="line"></span><br><span class="line">[program:kong-dashboard]</span><br><span class="line">command=/usr/local/src/nodejs/bin/kong-dashboard start --kong-url http://localhost:8001</span><br><span class="line">numprocs=1</span><br><span class="line">user=work</span><br><span class="line">stdout_logfile = /data/logs/kong/kong-dashboard.log</span><br><span class="line">redirect_stderr = true</span><br><span class="line">autostart=true</span><br><span class="line">autorestart=true</span><br></pre></td></tr></table></figure><p>修改权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@DWD-BETA ~]# chown -R work.work /etc/supervisor/conf.d/</span><br></pre></td></tr></table></figure><p>更新supervisor配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@DWD-BETA ~]# supervisorctl update kong-dashboard</span><br><span class="line">kong-dashboard: added process group</span><br></pre></td></tr></table></figure><p>但是由于我这台服务器上8080端口已经被使用,所以启动kong-dashboard报错:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">work@DWD-BETA kong]$ /usr/local/src/nodejs/bin/kong-dashboard start --kong-url http://localhost:8001</span><br><span class="line">Connecting to Kong on http://localhost:8001 ...</span><br><span class="line">Connected to Kong on http://localhost:8001.</span><br><span class="line">Kong version is 0.14.1</span><br><span class="line">Starting Kong Dashboard on port 8080</span><br><span class="line">events.js:183</span><br><span class="line">      throw er; // Unhandled &apos;error&apos; event</span><br><span class="line">      ^</span><br><span class="line"></span><br><span class="line">Error: listen EADDRINUSE :::8080</span><br><span class="line">    at Object._errnoException (util.js:1022:11)</span><br><span class="line">    at _exceptionWithHostPort (util.js:1044:20)</span><br><span class="line">    at Server.setupListenHandle [as _listen2] (net.js:1367:14)</span><br><span class="line">    at listenInCluster (net.js:1408:12)</span><br><span class="line">    at Server.listen (net.js:1492:7)</span><br><span class="line">    at Application.listen (/usr/local/src/nodejs/lib/node_modules/kong-dashboard/node_modules/koa/lib/application.js:65:19)</span><br><span class="line">    at Server.start (/usr/local/src/nodejs/lib/node_modules/kong-dashboard/lib/server.js:32:9)</span><br><span class="line">    at startKongDashboard (/usr/local/src/nodejs/lib/node_modules/kong-dashboard/bin/kong-dashboard.js:189:10)</span><br><span class="line">    at request.get.then.then (/usr/local/src/nodejs/lib/node_modules/kong-dashboard/bin/kong-dashboard.js:178:5)</span><br><span class="line">    at &lt;anonymous&gt;</span><br></pre></td></tr></table></figure><p>8080端口被jenkins占用了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@DWD-BETA kong]$ netstat -tulnp | grep 8080</span><br><span class="line">(Not all processes could be identified, non-owned process info</span><br><span class="line"> will not be shown, you would have to be root to see it all.)</span><br><span class="line">tcp        0      0 0.0.0.0:8080            0.0.0.0:*               LISTEN      17955/java</span><br></pre></td></tr></table></figure><p>更换kong-dashboard端口为8081</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[program:kong-dashboard]</span><br><span class="line">command=/usr/local/src/nodejs/bin/kong-dashboard  start --kong-url http://localhost:8001 --port 8081</span><br><span class="line">numprocs=1</span><br><span class="line">user=work</span><br><span class="line">stdout_logfile = /data/logs/kong/kong-dashboard.log</span><br><span class="line">redirect_stderr = true</span><br><span class="line">autostart=true</span><br><span class="line">autorestart=true</span><br></pre></td></tr></table></figure><p>更新supervisor后,仍然无法启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[work@DWD-BETA kong]$ supervisorctl update kong-dashboard</span><br><span class="line">kong-dashboard: stopped</span><br><span class="line">kong-dashboard: updated process group</span><br><span class="line">[work@DWD-BETA kong]$ supervisorctl status kong-dashboard</span><br><span class="line">kong-dashboard                   BACKOFF   Exited too quickly (process log may have details)</span><br></pre></td></tr></table></figure><p>手动启动正常</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[work@DWD-BETA kong]$ /usr/local/src/nodejs/bin/kong-dashboard  start --kong-url http://localhost:8001 --port 8081</span><br><span class="line">Connecting to Kong on http://localhost:8001 ...</span><br><span class="line">Connected to Kong on http://localhost:8001.</span><br><span class="line">Kong version is 0.14.1</span><br><span class="line">Starting Kong Dashboard on port 8081</span><br><span class="line">Kong Dashboard has started on port 8081</span><br><span class="line">^C</span><br></pre></td></tr></table></figure><p>查看supervisor启动日志文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[work@DWD-BETA kong]$ less /data/logs/kong/kong-dashboard.log</span><br><span class="line">/usr/bin/env: node: No such file or directory</span><br><span class="line">/usr/bin/env: node: No such file or directory</span><br><span class="line">/usr/bin/env: node: No such file or directory</span><br><span class="line">/usr/bin/env: node: No such file or directory</span><br></pre></td></tr></table></figure><p>网上查找解决方案.说是要链接以下文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -s /usr/bin/nodejs /usr/bin/node</span><br></pre></td></tr></table></figure><p>但是这台服务器上nodejs路径不同,所以执行以下命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[work@DWD-BETA kong]$ sudo ln -s /usr/local/src/nodejs /usr/bin/node</span><br></pre></td></tr></table></figure><p>仍然无法启动,提示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/env: node: Permission denied</span><br><span class="line">/usr/bin/env: node: Permission denied</span><br><span class="line">/usr/bin/env: node: Permission denied</span><br><span class="line">/usr/bin/env: node: Permission denied</span><br><span class="line">/usr/bin/env: node: Permission denied</span><br><span class="line">/usr/bin/env: node: Permission denied</span><br></pre></td></tr></table></figure><p>修改kong-dashboard启动用户为root.仍然无法启动.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[program:kong-dashboard]</span><br><span class="line">command=/usr/local/src/nodejs/bin/kong-dashboard  start --kong-url http://127.0.0.1:8001 --port 8081</span><br><span class="line">numprocs=1</span><br><span class="line">user=root</span><br><span class="line">stdout_logfile = /data/logs/kong/kong-dashboard.log</span><br><span class="line">redirect_stderr = true</span><br><span class="line">autostart=true</span><br><span class="line">autorestart=true</span><br></pre></td></tr></table></figure><hr><p>操作失误在创建软件的时候,删除了nodejs源目录.重新安装了npm和kong-dashboard</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">解压nodejs包到:/usr/local/node/</span><br><span class="line"></span><br><span class="line">#设置环境变量: </span><br><span class="line">[root@DWD-BETA ~]# cat /etc/profile | tail -2</span><br><span class="line">export NODEJS_HOME=/usr/local/node/</span><br><span class="line">export PATH=$PATH:$NODEJS_HOME/bin</span><br><span class="line"></span><br><span class="line">[root@DWD-BETA ~]# node -v</span><br><span class="line">v8.10.0</span><br><span class="line">[root@DWD-BETA ~]# npm -v</span><br><span class="line">5.6.0</span><br></pre></td></tr></table></figure><p>创建链接</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@DWD-BETA ~]# ln -s /usr/local/node/bin/node /usr/bin/node</span><br><span class="line">root@DWD-BETA ~]# ll /usr/bin/node -d</span><br><span class="line">lrwxrwxrwx 1 root root 24 Nov  3 11:25 /usr/bin/node -&gt; /usr/local/node/bin/node</span><br></pre></td></tr></table></figure><p>修改supervisor配置文件中的命令路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[program:kong-dashboard]</span><br><span class="line">command=/usr/local/node/bin/kong-dashboard  start --kong-url http://127.0.0.1:8001 --port 8081</span><br><span class="line">numprocs=1</span><br><span class="line">priority=3</span><br><span class="line">user=root</span><br><span class="line">stdout_logfile = /data/logs/kong/kong-dashboard.log</span><br><span class="line">redirect_stderr = true</span><br><span class="line">autostart=true</span><br><span class="line">autorestart=true</span><br></pre></td></tr></table></figure><p>启动kong-dashboard</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@DWD-BETA ~]# supervisorctl update kong-dashboard</span><br><span class="line">kong-dashboard: stopped</span><br><span class="line">kong-dashboard: updated process group</span><br><span class="line"></span><br><span class="line">[root@DWD-BETA ~]# supervisorctl status kong-dashboard</span><br><span class="line">kong-dashboard                   RUNNING   pid 16635, uptime 0:05:02</span><br></pre></td></tr></table></figure><p>启动完成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@DWD-BETA ~]# netstat -tulnp | grep 8081</span><br><span class="line">tcp6       0      0 :::8081                 :::*                    LISTEN      16635/node</span><br></pre></td></tr></table></figure><hr><h3 id="将cassandra加入到supervisor进程管理"><a href="#将cassandra加入到supervisor进程管理" class="headerlink" title="将cassandra加入到supervisor进程管理"></a>将cassandra加入到supervisor进程管理</h3><p>cassandra加入supervisor进程托管遇到不少问题.踩过以下2个坑:</p><p>1.启动报错,提示需要更高级版本的java.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Cassandra 3.0 and later require Java 8u40 or later.</span><br><span class="line">Cassandra 3.0 and later require Java 8u40 or later.</span><br><span class="line">Cassandra 3.0 and later require Java 8u40 or later.</span><br><span class="line">Cassandra 3.0 and later require Java 8u40 or later.</span><br><span class="line">Cassandra 3.0 and later require Java 8u40 or later.</span><br><span class="line">Cassandra 3.0 and later require Java 8u40 or later.</span><br><span class="line">Cassandra 3.0 and later require Java 8u40 or later.</span><br></pre></td></tr></table></figure><p>我的解决方案:</p><ul><li>在cassandra的supervisor配置文件中加入环境变量:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">environment=JAVA_HOME=&quot;/usr/local/java&quot;  #这样cassandra会识别用户自定义安装的Java.</span><br></pre></td></tr></table></figure><ul><li>配置软链</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -s /usr/local/java/bin/java /usr/bin/java</span><br></pre></td></tr></table></figure><p>2.仍然无法启动,因为命令行是以daemon方式启动.在cassandra的supervisor配置文件中的启动参数加入-f选项.</p><p>最终的cassandra配置文件如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[program:cassandra]</span><br><span class="line">command=/usr/local/cassandra/bin/cassandra -f</span><br><span class="line">directory=/usr/local/cassandra/</span><br><span class="line">environment=JAVA_HOME=&quot;/usr/local/java&quot;</span><br><span class="line">enviroment=PATH=&quot;$JAVA_HOME/bin:$PATH&quot;</span><br><span class="line">priority=0</span><br><span class="line">numprocs=1</span><br><span class="line">user=work</span><br><span class="line">stdout_logfile = /data/logs/cassandra/cassandra_supervisor.log</span><br><span class="line">redirect_stderr = true</span><br><span class="line">autostart=true</span><br><span class="line">autorestart=true</span><br></pre></td></tr></table></figure><hr><h3 id="将kong加入到supervisor"><a href="#将kong加入到supervisor" class="headerlink" title="将kong加入到supervisor"></a>将kong加入到supervisor</h3><p>1.由于默认kong启动是以daemon方式启动.所以修改/etc/kong/kong.conf配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#将下列行修改为off,且取消注释</span><br><span class="line">nginx_daemon = off</span><br></pre></td></tr></table></figure><p>2.编辑kong的supervisor配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[program:kong]</span><br><span class="line">command=/usr/local/bin/kong start -c /etc/kong/kong.conf</span><br><span class="line">numprocs=1</span><br><span class="line">priority=0</span><br><span class="line">user=work</span><br><span class="line">stdout_logfile = /data/logs/kong/kong_supervisor.log</span><br><span class="line">redirect_stderr = true</span><br><span class="line">autostart=true</span><br><span class="line">autorestart=true</span><br></pre></td></tr></table></figure><blockquote><p>注意由于kong启动的时候会连接后端的cassandra数据库,所以需要先启动cassandra,再启动kong.这就是为什么supervisor里要加入priority参数.优先级越小,启动顺序越优先.停止顺序越靠后.     </p></blockquote><p><strong>但是经过我的验证,发现priority参数没什么鸟用.当我start all,stop all时.永远是cassandra进程首先启动和关闭,无论priority优先级是大还是小.而不是supervisor官网介绍的那样效果</strong></p><p>启动没问题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node2 ~]$ supervisorctl status</span><br><span class="line">cassandra                        RUNNING   pid 13531, uptime 0:10:09</span><br><span class="line">kong                             RUNNING   pid 14460, uptime 0:00:35</span><br><span class="line">kong-dashboard                   RUNNING   pid 14496, uptime 0:00:17</span><br></pre></td></tr></table></figure><p>但是发现supervisor管理kong进程有很严重的问题.</p><p>因为kong启动后包括2个进程:kong和nginx</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">work@kong-node1 conf.d]$ ps aux | grep kong</span><br><span class="line">work      3198  0.2  0.0 130104  2700 ?        S    17:12   0:00 perl /usr/local/openresty/bin/resty /usr/local/bin/kong start -c /etc/kong/kong.conf</span><br><span class="line">work      3215  6.3  0.1 259600 11800 ?        S    17:12   0:00 nginx: master process /usr/local/openresty/nginx/sbin/nginx -p /data/kong -c nginx.conf</span><br></pre></td></tr></table></figure><p>.这个时候如果用supervisorctl restart kong进程会出现无法启动的情况.这是因为supervisor kill掉了kong进程.但是没有kill Ningx进程.所以重新启动kong的时候,由于nginx进程仍然存在,故无法启动.例如:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node1 conf.d]$ ps aux | grep kong</span><br><span class="line">work      2917  0.0  0.1 259600 11820 ?        S    17:05   0:00 nginx: master process /usr/local/openresty/nginx/sbin/nginx -p /data/kong -c nginx.conf</span><br><span class="line">work      3191  0.0  0.0 112704   976 pts/0    S+   17:11   0:00 grep --color=auto kong</span><br><span class="line">[work@kong-node1 conf.d]$ kill 2917</span><br><span class="line">[work@kong-node1 conf.d]$ ps aux | grep kong</span><br><span class="line">work      3193  0.0  0.0 112704   976 pts/0    S+   17:11   0:00 grep --color=auto kong</span><br><span class="line">[work@kong-node1 conf.d]$ supervisorctl start kong</span><br><span class="line">kong: started</span><br><span class="line"></span><br><span class="line">#kong启动后包含2个进程</span><br><span class="line">[work@kong-node1 conf.d]$ ps aux | grep kong</span><br><span class="line">work      3198  0.2  0.0 130104  2700 ?        S    17:12   0:00 perl /usr/local/openresty/bin/resty /usr/local/bin/kong start -c /etc/kong/kong.conf</span><br><span class="line">work      3215  6.3  0.1 259600 11800 ?        S    17:12   0:00 nginx: master process /usr/local/openresty/nginx/sbin/nginx -p /data/kong -c nginx.conf</span><br><span class="line">work      3228  0.0  0.0 112704   976 pts/0    R+   17:12   0:00 grep --color=auto kong</span><br><span class="line"></span><br><span class="line">#supervisor关闭了Kong进程后,无法启动.</span><br><span class="line">[work@kong-node1 conf.d]$ supervisorctl restart kong</span><br><span class="line">kong: stopped</span><br><span class="line">kong: ERROR (spawn error)</span><br><span class="line"></span><br><span class="line">#因为虽然kong进程杀死了.但是nginx进程还在.所以kong的8000端口仍然被占用</span><br><span class="line">[work@kong-node1 conf.d]$ ps aux | grep kong</span><br><span class="line">work      3215  0.1  0.1 259600 11800 ?        S    17:12   0:00 nginx: master process /usr/local/openresty/nginx/sbin/nginx -p /data/kong -c nginx.conf</span><br><span class="line">work      3307  0.0  0.0 112704   976 pts/0    S+   17:15   0:00 grep --color=auto kong</span><br><span class="line"></span><br><span class="line">#查看启动日志,提示kong进程已经启动了.</span><br><span class="line">Error: Kong is already running in /data/kong</span><br><span class="line"></span><br><span class="line">  Run with --v (verbose) or --vv (debug) for more details</span><br></pre></td></tr></table></figure><p><strong>暂时就不用supervisor管理了,采用命令行直接启动方式</strong></p><hr><p><strong>命令行启动kong.只有一个Nginx进程.没有Perl进程.不知道何故</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node2 conf.d]$kong start -c /etc/kong/kong.conf</span><br><span class="line">Kong started</span><br><span class="line">[work@kong-node2 conf.d]$ps aux | grep kong</span><br><span class="line">work     15558  0.0  0.0 259600  6540 ?        Ss   17:29   0:00 nginx: master process /usr/local/openresty/nginx/sbin/nginx -p /data/kong -c nginx.conf</span><br></pre></td></tr></table></figure><hr><h3 id="将kong-Cassandra加入到systemctl管理工具"><a href="#将kong-Cassandra加入到systemctl管理工具" class="headerlink" title="将kong,Cassandra加入到systemctl管理工具"></a>将kong,Cassandra加入到systemctl管理工具</h3><ul><li><strong>cassandra</strong></li></ul><p>systemctl文件如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node1 ~]$cd /usr/lib/systemd/system</span><br><span class="line">[work@kong-node1 system]$vim cassandra.service</span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=Cassandra</span><br><span class="line">After=network.target</span><br><span class="line">Before=kong.target </span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">User=work</span><br><span class="line">Group=work</span><br><span class="line">ExecStart=/usr/local/cassandra/bin/cassandra -f</span><br><span class="line">ExecStop=kill $(pgrep -f cassandra)</span><br><span class="line">Restart=always</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><blockquote><p>注意.如果java是二进制包,则需要将Bin文件链接到系统环境变量目录下.因为systemctl不会读取profile环境,所以不能识别到java.</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node1 system]$which java</span><br><span class="line">/usr/bin/java</span><br><span class="line">[work@kong-node1 system]$ll /usr/bin/java</span><br><span class="line">lrwxrwxrwx 1 root root 24 Nov 14 17:46 /usr/bin/java -&gt; /usr/local/java/bin/java</span><br></pre></td></tr></table></figure><p>第二种方法是在systemctl中指定环境变量.指定environment参数(这种方法理论可行,但是没有论证)</p><p>例如:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=Cassandra</span><br><span class="line">After=network.target</span><br><span class="line">Before=kong.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">User=work</span><br><span class="line">Group=work</span><br><span class="line">environment=JAVA_HOME=/usr/local/java</span><br><span class="line">environment=PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">ExecStart=/usr/local/cassandra/bin/cassandra -f</span><br><span class="line">ExecStop=kill $(pgrep -f cassandra)</span><br><span class="line">Restart=always</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><hr><ul><li><strong>kong</strong></li></ul><p>在同目录下编辑kong服务配置.</p><blockquote><p>注意,需要在cassandra后端数据库启动后,才能启动kong服务</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node1 system]$vim kong.service</span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description= kong service</span><br><span class="line">After=syslog.target network.target cassandra.target</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">User=work</span><br><span class="line">Group=work</span><br><span class="line">Type=forking</span><br><span class="line">ExecStart=/usr/local/bin/kong start -c /etc/kong/kong.conf</span><br><span class="line">ExecReload=/usr/local/bin/kong reload -c /etc/kong/kong.conf</span><br><span class="line">ExecStop=/usr/local/bin/kong stop</span><br><span class="line">Restart=always</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><ul><li><strong>在kong-node2上启动kong-dashbaord</strong></li></ul><blockquote><p>dashbaord服务启动需要在cassandra,kong服务启动之后</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[work@kong-node2 system]$vim kong-dashboard.service</span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=kong-dashboard</span><br><span class="line">After=network.target cassandra.target kong.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">User=work</span><br><span class="line">Group=work</span><br><span class="line">ExecStart=/usr/local/bin/kong-dashboard start --kong-url http://10.111.30.158:8001</span><br><span class="line">ExecStop=kill $(pgrep -f kong-dashboard)</span><br><span class="line">Restart=always</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><ul><li>加入到开启自启动</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable cassandra</span><br><span class="line">systemctl enable kong</span><br><span class="line">systemctl enable kong-dashboard</span><br></pre></td></tr></table></figure><p>经过反复论证,systemctl可以管理以上服务.</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kong-casssandra集群环境部署&quot;&gt;&lt;a href=&quot;#kong-casssandra集群环境部署&quot; class=&quot;headerlink&quot; title=&quot;kong+casssandra集群环境部署&quot;&gt;&lt;/a&gt;kong+casssandra集群环境部署&lt;/h2&gt;&lt;h3 id=&quot;kong简介&quot;&gt;&lt;a href=&quot;#kong简介&quot; class=&quot;headerlink&quot; title=&quot;kong简介&quot;&gt;&lt;/a&gt;kong简介&lt;/h3&gt;&lt;p&gt;Kong是Mashape开源的一款API网关，起初是用来管理 Mashape 公司15000个微服务的，后来在2015年开源,现在已经在很多创业公司、大型企业和政府机构中广泛使用。基于nginx,Lua和Cassandra或PostgreSQL，支持分布式操作，有很强的可移植性和可扩展性。可以在任何一种基础设施上运行,作为应用和API之间的中间层，加上众多功能强大的插件，可以实现认证授权、访问控制等功能。并且提供易于使用的RESTful API来操作和配置系统。&lt;/p&gt;
&lt;p&gt;有关kong的详细介绍请参考官网.&lt;/p&gt;
&lt;p&gt;–&lt;/p&gt;
&lt;h3 id=&quot;cassandra简介&quot;&gt;&lt;a href=&quot;#cassandra简介&quot; class=&quot;headerlink&quot; title=&quot;cassandra简介&quot;&gt;&lt;/a&gt;cassandra简介&lt;/h3&gt;&lt;p&gt;Cassandra 是一个来自 Apache 的分布式数据库，具有高度可扩展性，可用于管理大量的结构化数据。它提供了高可用性，没有单点故障。kong支持PostgreSQL或者Cassandra两种数据库.这里我们选择了cassandra.&lt;/p&gt;
&lt;p&gt;有关cassandra的详细介绍和使用方法.请参考官网&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes headless Service</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/kubernetes%20headless%20Service/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/kubernetes headless Service/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T08:48:46.557Z</updated>
    
    <content type="html"><![CDATA[<hr><h3 id="kubernetes-headless-Service"><a href="#kubernetes-headless-Service" class="headerlink" title="kubernetes headless Service"></a>kubernetes headless Service</h3><p>​     我们以前学习过,Service是Kubernetes项目中用来将一组Pod暴露给外界访问的一种机制.外部客户端通过Service地址可以随机访问到某个具体的Pod</p><p>​     之前学过几种Service类型,包括nodeport,loadbalancer等等.所有这类Service都有一个VIP(虚拟IP),访问Service VIP,Service会将请求转发到后端的Pod上,</p><p>​     还有一种Service是Headless Service(无头服务),这类Service自身不需要VIP,当DNS解析该Service时,会解析出Service后端的Pod地址.这样设置的好处是Kubernetes项目为Pod分配唯一的”可解析身份”,只要知道一个pod的名字和对应的Headless Service名字,就可以通过这条DNS访问到后端的Pod</p><a id="more"></a><hr><h3 id="持久存储"><a href="#持久存储" class="headerlink" title="持久存储"></a>持久存储</h3><p>我们知道通过headless Service使Pod有一个稳定的网络标识,那么存储呢?有状态的应用必须有自己独立的存储,即便这个pod被删除,新创建出来的pod(新pod与旧pod拥有相同的网络表示)也必须挂载相同的存储.</p><p>​      之前在学习kubernetes的存储时,我们学习过PV,PVC存储卷,通过pod模板关联一个持久卷声明就可以为pod提供一个持久卷存储.因为持久卷声明(PVC)和持久卷(PV)是一对一关系.但是之前接触过的ReplicationController,ReplicaSet,Deployment等资源创建的pod是同一个模板创建的,所以共享的是同一个持久卷存储.而StatefulSet要求每个pod都需要有独立的持久卷声明和存储.所以StatefulSet要求关联到一个或多个不同的持久卷声明模板.这些持久卷声明会在pod创建之前准备就绪,并且关联到每个pod中.</p><h3 id="持久卷的创建和删除"><a href="#持久卷的创建和删除" class="headerlink" title="持久卷的创建和删除"></a>持久卷的创建和删除</h3><p>​      扩容一个StatefulSet副本时,会创建2个或者多个对象: pod实例已经与之关联的一个或者多个持久卷声明.但是当StatefulSet缩容时,只会删除一个Pod,而留下持久卷声明.这就意味着删除Pod时,与pod关联的持久卷存储数据并不会被删除.如果持久卷声明被手动删除,那么持久卷上的数据则会消失.</p><p>​     因为缩容会保留持久卷声明,所以在随后的扩容操作中,新的pod实例会使用绑定在持久卷上相同的声明和其上的数据.所以如果因为误操作而缩容一个StatefulSet副本后,可以做一次扩容操作,新的pod实例会运行到与之前完全一致的状态,甚至连pod名字也是一样的</p><hr><h3 id="部署StatefulSet应用"><a href="#部署StatefulSet应用" class="headerlink" title="部署StatefulSet应用"></a>部署StatefulSet应用</h3><p>部署StatefulSet应用之前,需要创建几个不同类型的对象.</p><ul><li><p>一个演示用的docker镜像</p></li><li><p>存储数据文件的持久卷(PV)</p></li><li><p>一个Headless Service服务实例</p></li><li><p>Statefulset模板</p></li></ul><h4 id="准备一个docker镜像"><a href="#准备一个docker镜像" class="headerlink" title="准备一个docker镜像"></a>准备一个docker镜像</h4><p>这里使用书上提供的luksa/kubia-pet镜像,这个镜像是一个Node应用,当应用接收到一个POST请求时,将请求中的body写入到某个文件,当接收到一个GET请求时,返回pod主机名以及改文件中的内容.</p><h4 id="创建持久化存储卷-pv"><a href="#创建持久化存储卷-pv" class="headerlink" title="创建持久化存储卷(pv)"></a>创建持久化存储卷(pv)</h4><p>因为稍后会调度StatefulSet创建3个副本.所以这里需要3个持久卷.如果计划调度更多的副本,则需要创建更多的持久卷..</p><p>之前在学习存储知识的时候介绍过存储卷,所以具体不演示,以下是创建3个PV持久卷的配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-master</span> <span class="string">~]#</span> <span class="string">cat</span> <span class="string">statefulset-kubia-pv.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="comment">#创建一个List列表资源,List的items下列出每个PV的配置</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">List</span></span><br><span class="line"><span class="attr">items:</span></span><br><span class="line"><span class="attr">  - apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">    kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">pv-1</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">       capacity:</span></span><br><span class="line"><span class="attr">         storage:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">       accessModes:</span></span><br><span class="line"><span class="bullet">         -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">       persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span></span><br><span class="line"><span class="attr">       storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">       nfs:</span></span><br><span class="line"><span class="attr">         path:</span> <span class="string">/data/k8s/pv-1</span></span><br><span class="line"><span class="attr">         server:</span> <span class="number">172.16</span><span class="number">.20</span><span class="number">.1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  - apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">    kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">pv-2</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">       capacity:</span></span><br><span class="line"><span class="attr">         storage:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">       accessModes:</span></span><br><span class="line"><span class="bullet">         -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">       persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span></span><br><span class="line"><span class="attr">       storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">       nfs:</span></span><br><span class="line"><span class="attr">         path:</span> <span class="string">/data/k8s/pv-2</span></span><br><span class="line"><span class="attr">         server:</span> <span class="number">172.16</span><span class="number">.20</span><span class="number">.1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  - apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">    kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">pv-3</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">       capacity:</span></span><br><span class="line"><span class="attr">         storage:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">       accessModes:</span></span><br><span class="line"><span class="bullet">         -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">       persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span></span><br><span class="line"><span class="attr">       storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">       nfs:</span></span><br><span class="line"><span class="attr">         path:</span> <span class="string">/data/k8s/pv-3</span></span><br><span class="line"><span class="attr">         server:</span> <span class="number">172.16</span><span class="number">.20</span><span class="number">.1</span></span><br></pre></td></tr></table></figure><blockquote><p>以前接触过在yaml文件中添加—3个横杠使的在一个文件中可以区分定义多个资源,这次定义一个List对象,然后把各个资源作为List对象的各个项目.这2种方法均可以在一个YAML文件中定义多个资源</p></blockquote><p>现在已经定义了个3个底层的PV持久卷</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get pv</span><br><span class="line">NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE</span><br><span class="line">mypv1   1Gi        RWO,ROX        Recycle          Available           nfs                     12d</span><br><span class="line">pv-1    1Gi        RWO            Recycle          Available           nfs                     4s</span><br><span class="line">pv-2    1Gi        RWO            Recycle          Available           nfs                     4s</span><br><span class="line">pv-3    1Gi        RWO            Recycle          Available           nfs                     4s</span><br></pre></td></tr></table></figure><hr><h3 id="创建Headless-Service"><a href="#创建Headless-Service" class="headerlink" title="创建Headless Service"></a>创建Headless Service</h3><p>下面是headless service的配置文件,唯一需要注意的是该类型服务的clusterIP属性必须为None</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">statefulset-kubia-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  clusterIP:</span> <span class="string">None</span></span><br><span class="line">  <span class="comment">#所有标签为statefulset-kubia的Pod都属于这个Service</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">statefulset-kubia</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">      port:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p>创建服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get svc</span><br><span class="line">NAME                    TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">statefulset-kubia-svc   ClusterIP      None            &lt;none&gt;        80/TCP         3s</span><br></pre></td></tr></table></figure><hr><h3 id="创建Statefuleset"><a href="#创建Statefuleset" class="headerlink" title="创建Statefuleset"></a>创建Statefuleset</h3><p>​      statefulset资源的配置和RS,deployment等没有太大的区别,这里使用了一个新的组件volumeClaimTemplates.其中定义了一个持久卷声明.该组件会为每个Pod创建一个独立的持久卷声明.</p><p>​      这个组件是在statefulset资源的spec全局对象下,虽然在pod的template模板中并没有创建持久卷声明(而是直接通过volumeMounts属性来挂在).但是Statefulset在创建时,会自动将volumeClaimTemplate定义的持久卷声明关联到pod中.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-master</span> <span class="string">~]#</span> <span class="string">cat</span> <span class="string">statefulset-kubia.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">statefulset-kubia-v1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  serviceName:</span> <span class="string">statefulset-kubia-v1</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">       app:</span> <span class="string">statefulset-kubia</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">statefulset-kubia</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">statefulset-kubia</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">luksa/kubia-pet</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">            containerPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">data</span></span><br><span class="line"><span class="attr">              mountPath:</span> <span class="string">/var/data</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  volumeClaimTemplates:</span></span><br><span class="line"><span class="attr">       - metadata:</span></span><br><span class="line"><span class="attr">           name:</span> <span class="string">data</span></span><br><span class="line"><span class="attr">         spec:</span></span><br><span class="line"><span class="attr">           resources:</span></span><br><span class="line"><span class="attr">              requests:</span></span><br><span class="line"><span class="attr">                 storage:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">           storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">           accessModes:</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">ReadWriteOnce</span></span><br></pre></td></tr></table></figure><blockquote><p>注意:volumeClaimTemplates组件一定要声明存储类型storageClassName,如果没有声明这一点则Pod一直处于Pending状态.并且会有以下报错信息</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl describe po statefulset-kubia-v1-0</span><br><span class="line"></span><br><span class="line">  Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  error while running &quot;VolumeBinding&quot; filter plugin for pod &quot;statefulset-kubia-v1-0&quot;: pod has unbound immediate PersistentVolumeClaims</span><br></pre></td></tr></table></figure><p>查看PVC提示没有找到PV</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl describe pvc data-statefulset-kubia-v1-0</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason         Age                  From                         Message</span><br><span class="line">  ----    ------         ----                 ----                         -------</span><br><span class="line">  Normal  FailedBinding  55s (x182 over 45m)  persistentvolume-controller  no persistent volumes available for this claim and no storage class is set</span><br></pre></td></tr></table></figure><p>   创建statefulset资源,列出pod资源.和rs,rc,deployment不同的是,他们会一次性创建完所有的pod,而statefulset会在每一个pod完全就绪后,才会创建第二个.</p><p>​    statefulset这样做是因为:状态明确的集群应用对同事有2个集群成员启动引起的竞争情况是非常敏感的.所以依次启动每个成员是比较安全可靠的.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-master</span> <span class="string">~]#</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">pods</span></span><br><span class="line"><span class="string">NAME</span>                     <span class="string">READY</span>   <span class="string">STATUS</span>    <span class="string">RESTARTS</span>   <span class="string">AGE</span></span><br><span class="line"><span class="string">statefulset-kubia-v1-0</span>   <span class="number">0</span><span class="string">/1</span>     <span class="string">Pending</span>   <span class="number">0</span>          <span class="number">74</span><span class="string">s</span></span><br></pre></td></tr></table></figure><hr><p>现在3个Pod副本都已经被创建完成.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get pods</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Running   0          12m</span><br><span class="line">statefulset-kubia-v1-1   1/1     Running   0          12m</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running   0          12m</span><br></pre></td></tr></table></figure><p>statefulset自动创建了3个PVC,并且各自与3个pv自动关联</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pvc</span></span><br><span class="line">NAME                          STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">data-statefulset-kubia-v1-0   Bound    pv-2     1Gi        RWO            nfs            11m</span><br><span class="line">data-statefulset-kubia-v1-1   Bound    pv-3     1Gi        RWO            nfs            11m</span><br><span class="line">data-statefulset-kubia-v1-2   Bound    pv-1     1Gi        RWO            nfs            11m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pv</span></span><br><span class="line">NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                 STORAGECLASS   REASON   AGE</span><br><span class="line">pv-1   1Gi        RWO            Recycle          Bound    default/data-statefulset-kubia-v1-2   nfs                     5h18m</span><br><span class="line">pv-2   1Gi        RWO            Recycle          Bound    default/data-statefulset-kubia-v1-0   nfs                     5h18m</span><br><span class="line">pv-3   1Gi        RWO            Recycle          Bound    default/data-statefulset-kubia-v1-1   nfs                     5h18m</span><br><span class="line">[root@k8s-master ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>​       和RS,RC,Deployment等资源不同的是,Statefulset部署的pod名称并非是随机的,而是pod模板名加上一个序号,这个序号从0开始,依次增加.</p><p>​       PVC的名称格式是PVC的名字+pod名.每个pod自动创建一个PVC,并且该PVC自动关联到一个后端的PV持久卷</p><hr><h3 id="访问POD"><a href="#访问POD" class="headerlink" title="访问POD"></a>访问POD</h3><p>​     由于创建的Service类型是Headless service模式,所以不能通过它来访问pod,而是需要直接连接到每个后端单独的pod.(或者是创建一个普通的Service,但是这样也不允许访问指定的pod)</p><p>​    这次介绍如何通过API服务器与pod通信.API服务器可以通过代理直接连接到指定的pod.可以通过如下的URL</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;apiServerHost&gt;:&lt;port&gt;/api/v1/namespaces/default/pods/pods名称/proxy/&lt;path&gt;</span><br></pre></td></tr></table></figure><p> 在k8s的master节点运行下面命令,下面命令运行一个kubectl proxy.从而可以让proxy去API服务器通信,而不必使用麻烦的授权和SSL证书来直接与API服务器通信</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl proxy</span><br></pre></td></tr></table></figure><p>现在就可以直接访问Pod了.开启另一个master服务器终端.通过curl访问某个Pod.比如访问statefulset-kubia-v1-0这个Pod容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-0/proxy/</span><br><span class="line">You&apos;ve hit statefulset-kubia-v1-0</span><br><span class="line">Data stored on this pod: No data posted yet</span><br></pre></td></tr></table></figure><p>这种访问方式经过了2层的中间代理:</p><h5 id="1-curl命令发送给kubectl-proxy"><a href="#1-curl命令发送给kubectl-proxy" class="headerlink" title="1.curl命令发送给kubectl proxy"></a>1.curl命令发送给kubectl proxy</h5><h5 id="2-kubectl-proxy-带上认证TOKEN转发给API服务器"><a href="#2-kubectl-proxy-带上认证TOKEN转发给API服务器" class="headerlink" title="2. kubectl proxy 带上认证TOKEN转发给API服务器"></a>2. kubectl proxy 带上认证TOKEN转发给API服务器</h5><h5 id="3-API服务器再通过pod容器的实际IP地址将请求转发到后端的Pod"><a href="#3-API服务器再通过pod容器的实际IP地址将请求转发到后端的Pod" class="headerlink" title="3. API服务器再通过pod容器的实际IP地址将请求转发到后端的Pod"></a>3. API服务器再通过pod容器的实际IP地址将请求转发到后端的Pod</h5><p>下面是发送一个post请求到statefulset-kubia-v1-0的例子</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# curl -X POST -d &quot;Hey There ! This greeting was submitted to statefulset-kubia-v1-0&quot; \</span><br><span class="line">&gt; localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-0/proxy/</span><br><span class="line">Data stored on pod statefulset-kubia-v1-0</span><br><span class="line"></span><br><span class="line">#再次用GET请求,就可以返回刚才POST提交的数据</span><br><span class="line">[root@k8s-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-0/proxy/</span><br><span class="line">You&apos;ve hit statefulset-kubia-v1-0</span><br><span class="line">Data stored on this pod: Hey There ! This greeting was submitted to statefulset-kubia-v1-0</span><br></pre></td></tr></table></figure><p>当我们访问其他的pod容器时,并没有返回写入的数据,这和期望的一致,说明每个节点都有各自独立的存储状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-1/proxy/</span><br><span class="line">You&apos;ve hit statefulset-kubia-v1-1</span><br><span class="line">Data stored on this pod: No data posted yet</span><br></pre></td></tr></table></figure><hr><h3 id="删除pod-重新调度"><a href="#删除pod-重新调度" class="headerlink" title="删除pod,重新调度"></a>删除pod,重新调度</h3><p>之前我们在statefulset-kubia-v1-0这个pod节点写入了一条数据,这次我们删除这个Pod,等它被重新调度,然后检查它是否还会返回与之前一致的数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl delete po statefulset-kubia-v1-0</span><br><span class="line">pod &quot;statefulset-kubia-v1-0&quot; deleted</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl get po</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Running   0          2m</span><br><span class="line">statefulset-kubia-v1-1   1/1     Running   0          17h</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running   0          17h</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-0/proxy/</span><br><span class="line">You&apos;ve hit statefulset-kubia-v1-0</span><br><span class="line">Data stored on this pod: Hey There ! This greeting was submitted to statefulset-kubia-v1-0</span><br><span class="line">[root@k8s-master ~]#</span><br></pre></td></tr></table></figure><blockquote><p>删除一个Pod,当Pod重新被调度时不一定是原节点,有可能会调度到另外一个节点</p></blockquote><p>从上面的实验中可以得出2个结论:</p><ul><li>statefulset的pod被重新调度时,会新创建一个和之前一模一样的Pod(包括主机名称,pod名,存储)</li><li>当pod被删除,重新调度后持久化数据与之前一模一样.</li></ul><hr><h3 id="statefulSet滚动更新"><a href="#statefulSet滚动更新" class="headerlink" title="statefulSet滚动更新"></a>statefulSet滚动更新</h3><h4 id="1-7版本之前默认的On-Delete更新策略"><a href="#1-7版本之前默认的On-Delete更新策略" class="headerlink" title="1.7版本之前默认的On Delete更新策略"></a>1.7版本之前默认的On Delete更新策略</h4><p>​     statefulset在1.7版本开始支持滚动更新..在1.7版本之前默认的更新测量是<figure class="highlight plain"><figcaption><span>Delete```.这种侧列和ReplicaSet类似.当更新了配置文件后,旧的pod并不会被自动删除,而是需要手动删除.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">​    下面这个例子中,将镜像更换为luksa/kubia-pet-peers.副本数从3个增加到4个.(为此,我们需要提前再创建一个pv-4</span><br><span class="line"></span><br><span class="line">```shell</span><br><span class="line">#编辑pv配置文件,增加pv-4(前提是nfs服务器上实现存在/data/k8s/pv-4目录</span><br><span class="line">[root@k8s-master ~]# vim statefulset-kubia-pv.yaml</span><br><span class="line"></span><br><span class="line">#更新pv配置文件</span><br><span class="line">[root@k8s-master ~]# kubectl apply -f statefulset-kubia-pv.yaml</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">persistentvolume/pv-1 configured</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">persistentvolume/pv-2 configured</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">persistentvolume/pv-3 configured</span><br><span class="line">persistentvolume/pv-4 created</span><br><span class="line"></span><br><span class="line">#查看PV.</span><br><span class="line">[root@k8s-master ~]# kubectl get pv</span><br><span class="line">NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                 STORAGECLASS   REASON   AGE</span><br><span class="line">pv-1   1Gi        RWO            Recycle          Bound       default/data-statefulset-kubia-v1-2   nfs                     23h</span><br><span class="line">pv-2   1Gi        RWO            Recycle          Bound       default/data-statefulset-kubia-v1-0   nfs                     23h</span><br><span class="line">pv-3   1Gi        RWO            Recycle          Bound       default/data-statefulset-kubia-v1-1   nfs                     23h</span><br><span class="line">pv-4   1Gi        RWO            Recycle          Available                                         nfs                     9s</span><br><span class="line">[root@k8s-master ~]#</span><br></pre></td></tr></table></figure></p><p>更新statefulset配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#更新配置文件</span><br><span class="line">[root@k8s-master ~]# vim statefulset-kubia.yaml</span><br><span class="line">#应用配置文件</span><br><span class="line">[root@k8s-master ~]# kubectl apply -f statefulset-kubia.yaml</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">statefulset.apps/statefulset-kubia-v1 configured</span><br><span class="line"></span><br><span class="line">#查看Pod</span><br><span class="line">[root@k8s-master ~]# kubectl get pods</span><br><span class="line">NAME                     READY   STATUS              RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Running             0          70m</span><br><span class="line">statefulset-kubia-v1-1   1/1     Running             0          18h</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running             0          18h</span><br><span class="line">statefulset-kubia-v1-3   0/1     ContainerCreating   0          5s</span><br><span class="line">[root@k8s-master ~]#</span><br></pre></td></tr></table></figure><p>通过Pod的存活字段可以看到之前旧版本的Pod并没有被自动删除,而是新增了一个副本.这和ReplicaSet的机制类似.</p><hr><h4 id="自动滚动更新策略"><a href="#自动滚动更新策略" class="headerlink" title="自动滚动更新策略"></a>自动滚动更新策略</h4><p>编辑statefulset配置文件,将镜像版本改回到luksa/kubia-pet</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  serviceName:</span> <span class="string">statefulset-kubia-v1</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">4</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">       app:</span> <span class="string">statefulset-kubia</span></span><br><span class="line">  <span class="comment">#在spec字段配置更新策略,默认的type是On Delete,修改为RollingUpdate</span></span><br><span class="line"><span class="attr">  updateStrategy:</span></span><br><span class="line"><span class="attr">     type:</span> <span class="string">RollingUpdate</span></span><br></pre></td></tr></table></figure><p>应用新的配置文件,此时会触发自动更新</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl apply -f statefulset-kubia.yaml</span></span><br><span class="line">statefulset.apps/statefulset-kubia-v1 configured</span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods</span></span><br><span class="line">NAME                     READY   STATUS        RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Running       0          5m22s</span><br><span class="line">statefulset-kubia-v1-1   1/1     Running       0          6m12s</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running       0          6m57s</span><br><span class="line">statefulset-kubia-v1-3   1/1     Terminating   0          7m39s</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods</span></span><br><span class="line">NAME                     READY   STATUS        RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Running       0          6m55s</span><br><span class="line">statefulset-kubia-v1-1   1/1     Terminating   0          7m45s</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running       0          14s</span><br><span class="line">statefulset-kubia-v1-3   1/1     Running       0          54s</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods</span></span><br><span class="line">NAME                     READY   STATUS        RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Terminating   0          7m27s</span><br><span class="line">statefulset-kubia-v1-1   1/1     Running       0          7s</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running       0          46s</span><br><span class="line">statefulset-kubia-v1-3   1/1     Running       0          86s</span><br></pre></td></tr></table></figure><p>发现了什么? 当滚动更新时,kubectl会以倒序的方式,从最末尾一个pod开始依次更新.</p><blockquote><p>StatefulSet的滚动更新策略不同于Deployment可以指定maxSuge参数指定一次同时更新的pod数量,而是只能单个方式进行依次更新</p></blockquote><blockquote><p>StatefulSet还支持partition(分区)的更新策略,具体可以查看官网</p></blockquote><p>无论是何种更新策略.Pod的数据(包括主机名,存储)都会持久化.再次访问第0个pod,存储数据依然存在</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-0/proxy/</span><br><span class="line">You&apos;ve hit statefulset-kubia-v1-0</span><br><span class="line">Data stored on this pod: Hey There ! This greeting was submitted to statefulset-kubia-v1-0</span><br></pre></td></tr></table></figure><hr><h3 id="StatefulSet-如何处理节点失效"><a href="#StatefulSet-如何处理节点失效" class="headerlink" title="StatefulSet 如何处理节点失效"></a>StatefulSet 如何处理节点失效</h3><p>在node2上关闭网卡来模拟这台服务器掉线,观察statefulSet处理节点失效的情况</p><blockquote><p>注意关闭节点网卡前请确保可以通过控制台连接服务器,因为这意味着无法ssh远程登录</p></blockquote><p>node2节点已经关闭,状态为notready</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get node</span><br><span class="line">NAME         STATUS     ROLES    AGE   VERSION</span><br><span class="line">k8s-master   Ready      master   49d   v1.17.3</span><br><span class="line">k8s-node1    Ready      &lt;none&gt;   49d   v1.17.3</span><br><span class="line">k8s-node2    NotReady   &lt;none&gt;   49d   v1.17.3</span><br></pre></td></tr></table></figure><p>过一段时间后,所有node2节点上的Pod为Terminating终止状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide</span></span><br><span class="line">NAME                     READY   STATUS        RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">statefulset-kubia-v1-0   1/1     Terminating   0          33m   10.100.169.174   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-1   1/1     Terminating   0          34m   10.100.169.173   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running       0          34m   10.100.36.97     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-3   1/1     Running       0          35m   10.100.36.99     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h5 id="删除不健康的Pod"><a href="#删除不健康的Pod" class="headerlink" title="删除不健康的Pod"></a>删除不健康的Pod</h5><p>当尝试手动删除pod时,发现永远都无法删除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl delete po statefulset-kubia-v1-0</span><br><span class="line">pod &quot;statefulset-kubia-v1-0&quot; deleted</span><br><span class="line">^@</span><br><span class="line">^@</span><br></pre></td></tr></table></figure><p>在另一个终端上查看该pod.发现虽然pod被Terminating挂起,但是容器仍然处于运行状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl describe pods statefulset-kubia-v1-0</span></span><br><span class="line">Name:                      statefulset-kubia-v1-0</span><br><span class="line">Namespace:                 default</span><br><span class="line">Priority:                  0</span><br><span class="line">Node:                      k8s-node2/172.16.20.253</span><br><span class="line">Start Time:                Sun, 03 May 2020 10:54:17 +0800</span><br><span class="line">Labels:                    app=statefulset-kubia</span><br><span class="line">                           controller-revision-hash=statefulset-kubia-v1-74b44bc68b</span><br><span class="line">                           statefulset.kubernetes.io/pod-name=statefulset-kubia-v1-0</span><br><span class="line">Annotations:               cni.projectcalico.org/podIP: 10.100.169.174/32</span><br><span class="line">Status:                    Terminating (lasts 12m)</span><br><span class="line">Termination Grace Period:  30s</span><br><span class="line">IP:                        10.100.169.174</span><br><span class="line">IPs:</span><br><span class="line">  IP:           10.100.169.174</span><br><span class="line">Controlled By:  StatefulSet/statefulset-kubia-v1</span><br><span class="line">Containers:</span><br><span class="line">  statefulset-kubia:</span><br><span class="line">    Container ID:   docker://099628b95ded3644752a3de799ef338794704aaa4ebe4db5a966b821b2e9a71a</span><br><span class="line">    Image:          luksa/kubia-pet</span><br><span class="line">    Image ID:       docker-pullable://luksa/kubia-pet@sha256:4263bc375d3ae2f73fe7486818cab64c07f9cd4a645a7c71a07c1365a6e1a4d2</span><br><span class="line">    Port:           8080/TCP</span><br><span class="line">    Host Port:      0/TCP</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Sun, 03 May 2020 10:54:21 +0800</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/data from data (rw)</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jfrqr (ro)</span><br></pre></td></tr></table></figure><h5 id="强制删除"><a href="#强制删除" class="headerlink" title="强制删除"></a>强制删除</h5><p>带上参数<figure class="highlight plain"><figcaption><span>--grace-period 0```可以强制删除一个pod</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl delete po statefulset-kubia-v1-0 –force –grace-period 0<br>warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.<br>pod “statefulset-kubia-v1-0” force deleted</p><p>[root@k8s-master ~]# kubectl get pods<br>NAME                     READY   STATUS              RESTARTS   AGE<br>statefulset-kubia-v1-0   0/1     ContainerCreating   0          5s<br>statefulset-kubia-v1-1   1/1     Terminating         0          44m<br>statefulset-kubia-v1-2   1/1     Running             0          44m<br>statefulset-kubia-v1-3   1/1     Running             0          45m</p><p>[root@k8s-master ~]# kubectl get pods -o wide<br>NAME                     READY   STATUS        RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES<br>statefulset-kubia-v1-0   1/1     Running       0          12s   10.100.36.101    k8s-node1   <none>           <none><br>statefulset-kubia-v1-1   1/1     Terminating   0          44m   10.100.169.173   k8s-node2   <none>           <none><br>statefulset-kubia-v1-2   1/1     Running       0          44m   10.100.36.97     k8s-node1   <none>           <none><br>statefulset-kubia-v1-3   1/1     Running       0          45m   10.100.36.99     k8s-node1   <none>           <none><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">此时第0个pod已经重新创建,并且运行到node1节点,而另一个pod依然处于Terminating状态</span><br><span class="line"></span><br><span class="line">##### node2节点恢复正常</span><br><span class="line"></span><br><span class="line">当节点恢复正常后,很快node和pod都全部恢复正常.此时第0个pod依然还是挂在在node1节点.第1个pod的状态迅速从Terminating状态变为Running状态</span><br></pre></td></tr></table></figure></none></none></none></none></none></none></none></none></p><p>[root@k8s-master ~]# kubectl get pods -o wide<br>NAME                     READY   STATUS    RESTARTS   AGE     IP               NODE        NOMINATED NODE   READINESS GATES<br>statefulset-kubia-v1-0   1/1     Running   0          3m17s   10.100.36.101    k8s-node1   <none>           <none><br>statefulset-kubia-v1-1   1/1     Running   0          5s      10.100.169.172   k8s-node2   <none>           <none><br>statefulset-kubia-v1-2   1/1     Running   0          47m     10.100.36.97     k8s-node1   <none>           <none><br>statefulset-kubia-v1-3   1/1     Running   0          48m     10.100.36.99     k8s-node1   <none>           <none><br><code>`</code></none></none></none></none></none></none></none></none></p><hr><h3 id="本章总结"><a href="#本章总结" class="headerlink" title="本章总结"></a>本章总结</h3><p>Stateful和RS,deployment的用法总体没有太大区别,下面是这2种资源的对比</p><table><thead><tr><th style="text-align:center">特性</th><th style="text-align:center">Deployment</th><th style="text-align:center">StatefulSet</th></tr></thead><tbody><tr><td style="text-align:center">是否暴露到外网</td><td style="text-align:center">可以</td><td style="text-align:center">一般不</td></tr><tr><td style="text-align:center">请求面向的对象</td><td style="text-align:center">ServiceName</td><td style="text-align:center">指定pod的域名</td></tr><tr><td style="text-align:center">灵活性</td><td style="text-align:center">通过Service(名称或者IP)访问后端Pod</td><td style="text-align:center">可以访问任意一个pod</td></tr><tr><td style="text-align:center">易用性</td><td style="text-align:center">只需要关心Service信息即可</td><td style="text-align:center">需要知道访问pod的名称,Headless Service名称</td></tr><tr><td style="text-align:center">PV/PVC稳定性</td><td style="text-align:center">无法保障绑定关系</td><td style="text-align:center">可以保障</td></tr><tr><td style="text-align:center">pod名称稳定性</td><td style="text-align:center">使用一个随机的名称后缀,重启后会随机生成另外一个.名称不重复</td><td style="text-align:center">稳定,每次都一样</td></tr><tr><td style="text-align:center">升级更新顺序</td><td style="text-align:center">随机启动.如果pod宕机重启,也是随机分配一个Node节点重新启动</td><td style="text-align:center">pod按顺序依次启动,如果pod宕机,依然使用相同的Node节点和名称</td></tr><tr><td style="text-align:center">停止顺序</td><td style="text-align:center">随机停止</td><td style="text-align:center">倒序停止</td></tr><tr><td style="text-align:center">集群内部服务发现</td><td style="text-align:center">只能通过Service访问随机的一个Pod</td><td style="text-align:center">可以打通pod之间的通信</td></tr><tr><td style="text-align:center">性能开销</td><td style="text-align:center">无需维护pod与node,pvc等关系</td><td style="text-align:center">需要维护额外的关系信息</td></tr></tbody></table><p>通过对比发现</p><ul><li>如果不需要额外数据依赖或者状态维护的部署,优先选择Deployment</li><li>如果单纯要做数据持久化,方式pod宕机数据丢失,直接使用PV/PVC就可以</li><li>如果是有多个副本,且每个副本挂载的PV存储数据不同,并且pod宕机重启后仍然关联到之前的PVC,并且数据需要持久化,考虑使用StatefulSet</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;h3 id=&quot;kubernetes-headless-Service&quot;&gt;&lt;a href=&quot;#kubernetes-headless-Service&quot; class=&quot;headerlink&quot; title=&quot;kubernetes headless Service&quot;&gt;&lt;/a&gt;kubernetes headless Service&lt;/h3&gt;&lt;p&gt;​     我们以前学习过,Service是Kubernetes项目中用来将一组Pod暴露给外界访问的一种机制.外部客户端通过Service地址可以随机访问到某个具体的Pod&lt;/p&gt;
&lt;p&gt;​     之前学过几种Service类型,包括nodeport,loadbalancer等等.所有这类Service都有一个VIP(虚拟IP),访问Service VIP,Service会将请求转发到后端的Pod上,&lt;/p&gt;
&lt;p&gt;​     还有一种Service是Headless Service(无头服务),这类Service自身不需要VIP,当DNS解析该Service时,会解析出Service后端的Pod地址.这样设置的好处是Kubernetes项目为Pod分配唯一的”可解析身份”,只要知道一个pod的名字和对应的Headless Service名字,就可以通过这条DNS访问到后端的Pod&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
</feed>

<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jesse&#39;s home</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jesse.top/"/>
  <updated>2021-04-01T13:43:50.803Z</updated>
  <id>https://jesse.top/</id>
  
  <author>
    <name>Jesse</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kubernetes pod原理</title>
    <link href="https://jesse.top/2021/02/06/kubernetes/pod/kubernetes--pod%E5%8E%9F%E7%90%86/"/>
    <id>https://jesse.top/2021/02/06/kubernetes/pod/kubernetes--pod原理/</id>
    <published>2021-02-06T15:39:58.000Z</published>
    <updated>2021-04-01T13:43:50.803Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kubernetes—-pod原理"><a href="#Kubernetes—-pod原理" class="headerlink" title="Kubernetes—-pod原理"></a>Kubernetes—-pod原理</h2><h3 id="开篇"><a href="#开篇" class="headerlink" title="开篇"></a>开篇</h3><p>Pod是kubernetes项目中最小的API对象,是原子调度单位.我们之前学习过很多Linux容器,Docker方面的知识.那Kubernetes为什么不使用容器作为调度单位,而是要将容器封装成一个Pod?</p><p>要探讨这个问题,我们需要深入研究一下Kubernetes的设计思想和工作原理.</p><h3 id="容器本质"><a href="#容器本质" class="headerlink" title="容器本质"></a>容器本质</h3><p>通过之前Docker的原理学习,我们知道容器的本质到底是什么? <strong>容器的本质是进程</strong>.<strong>容器的镜像就像是这个进程的安装包.</strong>一键启动这个镜像,就相当于用这个安装包启动了一个进程(PID为1).那么Kubernetes呢?</p><p>Kubernetes就是操作系统! 负责所有容器的编排和管理</p><p>但是,在一个操作系统里,进程并不是孤苦伶仃的单独运行的,而是以进程组的方式,多个进程同时在一起运行.这些进程存在着”进程和进程组”的关系,他们之间有非常密切的写作关系,使得他们必须部署在同一台机器上.</p><p>由于受限于容器的”单进程模型”.一个进程组下的不同进程可能需要制作成多个不同的容器,而这多个容器在传统的调度工作中(比如像Docker Swarm,Mesos)都没有被妥善处理好,在进程组的调度上要么无法保障一个进程组的多个容器无法调度到同一个节点,要么调度的效率和性能的问题.</p><p>可在Kubernetes里,这个问题通过Pod完美解决了.Pod是kubernetes的原子调度单位,这就意味着Kubernetes是统一按照Pod而非单个容器的资源需求计算的.所以可以将多个容器部署在同一个Pod里,这些容器共享同一个Pod的网络名称,进程间通信,IP地址,共享卷等.而Kubernetes在调度时,会将他们作为一个整体,而非单个容器进程.</p><p>像这样容器间的紧密协作，我们可以称为“超亲密关系”。这些具有“超亲密关系”容器的典型特征包括但不限于：互相之间会发生直接的文件交换、使用 localhost 或者 Socket 文件进行本地通信、会发生非常频繁的远程调用、需要共享某些 Linux Namespace（比如，一个容器要加入另一个容器的 Network Namespace）等等。</p><p>这也就意味着，并不是所有有“关系”的容器都属于同一个 Pod。比如，PHP 应用容器和 MySQL 虽然会发生访问关系，但并没有必要、也不应该部署在同一台机器上，它们更适合做成两个 Pod。</p><p>不过，相信此时你可能会有<strong>第二个疑问：</strong></p><p>对于初学者来说，一般都是先学会了用 Docker 这种单容器的工具，才会开始接触 Pod。</p><p>而如果 Pod 的设计只是出于调度上的考虑，那么 Kubernetes 项目似乎完全没有必要非得把 Pod 作为“一等公民”吧？这不是故意增加用户的学习门槛吗？</p><p>为了理解这一层含义，我就必须先给你介绍一下Pod 的实现原理。</p><h3 id="POD实现原理"><a href="#POD实现原理" class="headerlink" title="POD实现原理"></a>POD实现原理</h3><ul><li><strong>Pod只是一个逻辑的概念</strong></li></ul><p>也就是说，Kubernetes 真正处理的，还是宿主机操作系统上 Linux 容器的 Namespace 和 Cgroups，而并不存在一个所谓的 Pod 的边界或者隔离环境。</p><p>那么，Pod 又是怎么被“创建”出来的呢？</p><p>答案是：Pod，其实是一组共享了某些资源的容器。</p><p>具体的说：<strong>Pod 里的所有容器，共享的是同一个 Network Namespace，并且可以声明共享同一个 Volume。</strong></p><p>那这么来看的话，一个有 A、B 两个容器的 Pod，不就是等同于一个容器（容器 A）共享另外一个容器（容器 B）的网络和 Volume 的玩儿法么？</p><p>这好像通过 docker run –net –volumes-from 这样的命令就能实现嘛，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --net=B --volumes-from=B --name=A image-A ...</span><br></pre></td></tr></table></figure><p>但是，你有没有考虑过，如果真这样做的话，容器 B 就必须比容器 A 先启动，这样一个 Pod 里的多个容器就不是对等关系，而是拓扑关系了。</p><p>所以，在 Kubernetes 项目里，Pod 的实现需要使用一个中间容器，这个容器叫作 Infra 容器。在这个 Pod 中，Infra 容器永远都是第一个被创建的容器，而其他用户定义的容器，则通过 Join Network Namespace 的方式，与 Infra 容器关联在一起。这样的组织关系，可以用下面这样一个示意图来表达：</p><p><img src="https://img2.jesse.top/image-20210328112238279.png" alt="image-20210328112238279"></p><p>如上图所示，这个 Pod 里有两个用户容器 A 和 B，还有一个 Infra 容器。很容易理解，在 Kubernetes 项目里，Infra 容器一定要占用极少的资源，所以它使用的是一个非常特殊的镜像，叫作：<code>k8s.gcr.io/pause</code>。这个镜像是一个用汇编语言编写的、永远处于“暂停”状态的容器，解压后的大小也只有 100~200 KB 左右。</p><p>而在 Infra 容器“Hold 住”Network Namespace 后，用户容器就可以加入到 Infra 容器的 Network Namespace 当中了。所以，如果你查看这些容器在宿主机上的 Namespace 文件（这个 Namespace 文件的路径，我已经在前面的内容中介绍过），它们指向的值一定是完全一样的。</p><p>这也就意味着，对于 Pod 里的容器 A 和容器 B 来说：</p><ul><li>它们可以直接使用 localhost 进行通信；</li><li>它们看到的网络设备跟 Infra 容器看到的完全一样；</li><li>一个 Pod 只有一个 IP 地址，也就是这个 Pod 的 Network Namespace 对应的 IP 地址；</li><li>当然，其他的所有网络资源，都是一个 Pod 一份，并且被该 Pod 中的所有容器共享；</li><li>Pod 的生命周期只跟 Infra 容器一致，而与容器 A 和 B 无关。</li></ul><h3 id="容器设计模式"><a href="#容器设计模式" class="headerlink" title="容器设计模式"></a>容器设计模式</h3><p>Pod 这种“超亲密关系”容器的设计思想，实际上就是希望，当用户想在一个容器里跑多个功能并不相关的应用时，应该优先考虑它们是不是更应该被描述成一个 Pod 里的多个容器。</p><p>为了能够掌握这种思考方式，你就应该尽量尝试使用它来描述一些用单个容器难以解决的问题。</p><p>一个典型的例子就是容器的日志收集</p><p>比如，我现在有一个应用，需要不断地把日志文件输出到容器的 /var/log 目录中。</p><p>这时，我就可以把一个 Pod 里的 Volume 挂载到应用容器的 /var/log 目录上。</p><p>然后，我在这个 Pod 里同时运行一个日志收集客户端 容器，它也声明挂载同一个 Volume 到自己的 /var/log 目录上。</p><p>像这样，我们就用一种“组合”方式,解决了主容器和日志收集容器之间的耦合关系,而日志收集容器我们一般也称之为辅助容器.实际上，这个所谓的“组合”操作，正是容器设计模式里最常用的一种模式，它的名字叫：sidecar。顾名思义，sidecar 指的就是我们可以在一个 Pod 中，启动一个辅助容器，来完成一些独立于主进程（主容器）之外的工作。</p><p>这样，接下来 sidecar 容器就只需要做一件事儿，那就是不断地从自己的 /var/log 目录里读取日志文件，转发到 MongoDB 或者 Elasticsearch 中存储起来。这样，一个最基本的日志收集工作就完成了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Kubernetes—-pod原理&quot;&gt;&lt;a href=&quot;#Kubernetes—-pod原理&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes—-pod原理&quot;&gt;&lt;/a&gt;Kubernetes—-pod原理&lt;/h2&gt;&lt;h3 id=&quot;开篇&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
      <category term="pod" scheme="https://jesse.top/categories/kubernetes/pod/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 网络</title>
    <link href="https://jesse.top/2021/02/06/kubernetes/network/kubernetes%20network/"/>
    <id>https://jesse.top/2021/02/06/kubernetes/network/kubernetes network/</id>
    <published>2021-02-06T15:39:58.000Z</published>
    <updated>2021-02-06T15:48:07.531Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kubernetes-网络"><a href="#kubernetes-网络" class="headerlink" title="kubernetes 网络"></a>kubernetes 网络</h2><h3 id="1-开篇"><a href="#1-开篇" class="headerlink" title="1.开篇"></a>1.开篇</h3><p>​    Docker容器诞生以来,,如何确定合适的网络方案是亟待解决的难题之一.在日趋复杂的业务场景下,网络的复杂性也呈几何级数上升.本篇首先回顾了Docker容器的网络通信,然后介绍Kubernertes的网络模型.在Kubernetes集群中,IP地址的分配对象是以Pod为单位,而非容器.同一个Pod内的所有容器共享同一个网络名称空间</p><a id="more"></a><h3 id="2-容器网络基础"><a href="#2-容器网络基础" class="headerlink" title="2 容器网络基础"></a>2 容器网络基础</h3><p>​    一个Linux容器的网络栈是被隔离在它自己的Network Namespace中，Network Namespace包括了：网卡（Network Interface），回环设备（Lookback Device），路由表（Routing Table）和iptables规则，对于服务进程来讲这些就构建了它发起请求和相应的基本环境。而要实现一个容器网络，离不开以下Linux网络功能：</p><ul><li>网络命名空间：将独立的网络协议栈隔离到不同的命令空间中，彼此间无法通信</li><li>Veth Pair：Veth设备对的引入是为了实现在不同网络命名空间的通信，总是以两张虚拟网卡（veth peer）的形式成对出现的。并且，从其中一端发出的数据，总是能在另外一端收到</li><li>Iptables/Netfilter：Netfilter负责在内核中执行各种挂接的规则（过滤、修改、丢弃等），运行在内核中；Iptables模式是在用户模式下运行的进程，负责协助维护内核中Netfilter的各种规则表；通过二者的配合来实现整个Linux网络协议栈中灵活的数据包处理机制</li><li>网桥：网桥是一个二层网络虚拟设备，类似交换机，主要功能是通过学习而来的Mac地址将数据帧转发到网桥的不同端口上</li><li>路由: Linux系统包含一个完整的路由功能，当IP层在处理数据发送或转发的时候，会使用路由表来决定发往哪里</li></ul><h3 id="2-Docker容器网络模型"><a href="#2-Docker容器网络模型" class="headerlink" title="2.Docker容器网络模型"></a>2.Docker容器网络模型</h3><h4 id="2-1-同节点容器通信"><a href="#2-1-同节点容器通信" class="headerlink" title="2.1 同节点容器通信"></a>2.1 同节点容器通信</h4><p>​    Docker容器网络的原始模型主要用到的就是Bridge桥接网络.Docker守护进程首次启动时,会在当前宿主机节点创建一个名为<code>docker0</code> 的虚拟网桥设备.并默认配置其使用172.17.0.0/16的网络.</p><blockquote><p>Host和Container网络模型使用场景非常少.不再费篇幅介绍 </p></blockquote><p>​    并且为该主机节点上的每一个容器分配一个虚拟的以<code>vethxxx</code> 开头的虚拟网卡.从而使得同一节点下的所有容器都可以在二层网络模式下.利用<code>docker0</code> 虚拟网桥实现容器和容器之间,容器和宿主机节点之间的网络通信.</p><p>​    同一宿主机节点下的容器网络通信方式如下</p><p><img src="https://img2.jesse.top/image-20210206153900657.png" alt="image-20210206153900657"></p><h4 id="2-2-不同节点容器通信"><a href="#2-2-不同节点容器通信" class="headerlink" title="2.2 不同节点容器通信"></a>2.2 不同节点容器通信</h4><p>​    以上是同节点上的容器通信方式.对于不同节点的容器之间进行通信,Docker则无能为力.因为每个节点的docker0网桥分配的虚拟IP都是同一网段,所以不同宿主机节点上的容器可能使用的是同一个IP地址,双方并不清楚对方容器具体在哪台节点.</p><p>​    解决此问题的方式是NAT.所有容器均会被NAT隐藏在节点网络之内.他们发往Docker主机外部的所有流量都会SNAT后出去,容器若要接入Docker主机外部的流量,则需要事先将网络端口暴露到宿主机的端口..然后对方容器的流量达到宿主机后再执行DNAT转发给目的容器.</p><p>不同宿主机节点下的容器网络通信方式如下</p><p><img src="https://img2.jesse.top/image-20210206155556803.png" alt="image-20210206155556803"></p><p>这种解决方式在网络规模庞大的时候兼职就是个灾难.转发效率非常低下,宿主机上端口变成一种稀缺资源.</p><h3 id="3-Kubernetes网络模型"><a href="#3-Kubernetes网络模型" class="headerlink" title="3. Kubernetes网络模型"></a>3. Kubernetes网络模型</h3><p>Kubernetes的网络模型主要用于解决四类通信需求:</p><h4 id="1-容器间通信"><a href="#1-容器间通信" class="headerlink" title="(1) 容器间通信"></a>(1) 容器间通信</h4><p>​    Pod对象内的各容器共享同一个网络名称空间.所有运行于同一个Pod内部的容器与同一主机上的多个进程类似.彼此之间可以通过<code>localhost</code> 或者<code>lo</code> 回环接口进行通信.</p><p>例如下图3-1所示,每个节点上的Container1和container2容器在一个Pod内部,共享同一个IP地址和网络接口</p><p><img src="https://img2.jesse.top/image-20210206160446993.png" alt="image-20210206160446993"></p><p>​                                                                                                                图 3-1 Pod网络</p><h4 id="2-Pod间通信"><a href="#2-Pod间通信" class="headerlink" title="(2) Pod间通信"></a>(2) Pod间通信</h4><p>​    Kubernertes要求每个Pod对象需要运行于同一个平面网络中,并且都拥有一个集群内全局唯一的IP地址,可以直接于其他Pod通信.例如上图3-1中的Pod P和Pod Q之间通信.另外，运行Pod的各节点也会通过桥接设备等持有此平面网络中的一个IP地址，如图3-1中的cbr0接口，这就意味着Node到Pod间的通信也可在此网络上直接进行。因此，Pod间的通信或Pod到Node间的通信比较类似于同一IP网络中主机间进行的通信。</p><h4 id="3-Service与Pod间通信"><a href="#3-Service与Pod间通信" class="headerlink" title="(3) Service与Pod间通信"></a>(3) Service与Pod间通信</h4><p>​    Service资源的专用网络也称为集群网络（Cluster Network），需要在启动kube-apiserver时经由“–service-cluster-ip-range”选项进行指定，如10.96.0.0/12，而每个Service对象在此网络中均拥一个称为Cluster-IP的固定地址。管理员或用户对Service对象的创建或更改操作由API Server存储完成后触发各节点上的kube-proxy，并根据代理模式的不同将其定义为相应节点上的iptables规则或ipvs规则，借此完成从Service的Cluster-IP与Pod-IP之间的报文转发，如图3-2所示。</p><p><img src="https://img2.jesse.top/image-20210206161001841.png" alt="image-20210206161001841"></p><p>​                                                                                                         图 3-2 Service和Pod</p><h4 id="4-集群外部到Pod对象之间的通信"><a href="#4-集群外部到Pod对象之间的通信" class="headerlink" title="(4) 集群外部到Pod对象之间的通信"></a>(4) 集群外部到Pod对象之间的通信</h4><p>​    将集群外部的流量引入到Pod对象的方式有受限于Pod所在的工作节点范围的节点端口（nodePort）和主机网络（hostNetwork）两种，以及工作于集群级别的NodePort或LoadBalancer类型的Service对象。不过，即便是四层代理的模式也要经由两级转发才能到达目标Pod资源：请求流量首先到达外部负载均衡器，由其调度至某个工作节点之上，而后再由工作节点的netfilter（kube-proxy）组件上的规则（iptables或ipvs）调度至某个目标Pod对象。</p><h3 id="4-Kubernetes-CNI插件"><a href="#4-Kubernetes-CNI插件" class="headerlink" title="4. Kubernetes CNI插件"></a>4. Kubernetes CNI插件</h3><p>​    Kubernetes设计了以上四种网络模型.但是Kubernetes自己并不负责网络具体工作,而是交给的了第三方网络插件.为了规范以及兼容各种解决方案.CoreOS和Google联合制定了CNI（Container Network Interface）标准，旨在定义容器网络模型规范。它连接了两个组件：容器管理系统和网络插件。它们之间通过JSON格式的文件进行通信，以实现容器的网络功能.具体的网络工作均由插件来实现，包括创建容器netns、关联网络接口到对应的netns以及为网络接口分配IP等。</p><p>CNI的基本思想是:容器运行时环境在创建容器时，先创建好网络名称空间（netns），然后调用CNI插件为这个netns配置网络，而后再启动容器内的进程。</p><p>Kubernetes要求网络插件需要满足以下基本原则:</p><ul><li>Pod无论运行在任何节点都可以互相直接通信，而不需要借助NAT地址转换实现。</li><li>Node与Pod可以互相通信，在不限制的前提下，Pod可以访问任意网络。</li><li>Pod拥有独立的网络栈，Pod看到自己的地址和外部看见的地址应该是一样的，并且同个Pod内所有的容器共享同个网络栈。 </li></ul><p>CNI本身只是规范，付诸生产还需要有特定的实现。目前，CNI提供的插件分为三类：main、meta和ipam。main一类的插件主要在于实现某种特定的网络功能，例如loopback、bridge、macvlan和ipvlan等；meta一类的插件自身并不提供任何网络实现，而是用于调用其他插件，例如调用flannel；ipam仅用于分配IP地址，而不提供网络实现。</p><p>CNI具有很强的扩展性和灵活性，例如，如果用户对某个插件具有额外的需求，则可以通过输入中的args和环境变量CNI_ARGS进行传递，然后在插件中实现自定义的功能，这大大增加了它的扩展性。另外，CNI插件将main和ipam分开，赋予了用户自由组合它们的机制，甚至一个CNI插件也可以直接调用另外一个CNI插件。CNI目前已经是Kubernetes当前推荐的网络方案。常见的CNI网络插件包含如下这些主流的项目.</p><ul><li><p><strong>Flannel</strong>.</p><p>一个为Kubernetes提供叠加网络的网络插件，它基于Linux TUN/TAP，使用UDP封装IP报文来创建叠加网络，并借助etcd维护网络的分配情况。</p></li><li><p><strong>Calico</strong></p><p>一个基于BGP的三层网络插件，并且也支持网络策略来实现网络的访问控制；它在每台机器上运行一个vRouter，利用Linux内核来转发网络数据包，并借助iptables实现防火墙等功能。</p></li><li><p><strong>Weave Net</strong>：</p><p>Weave Net是一个多主机容器的网络方案，支持去中心化的控制平面，在各个host上的wRouter间建立Full Mesh的TCP连接，并通过Gossip来同步控制信息。</p></li></ul><h3 id="5-Kubernetes-CNI网络通信"><a href="#5-Kubernetes-CNI网络通信" class="headerlink" title="5. Kubernetes CNI网络通信"></a>5. Kubernetes CNI网络通信</h3><p>实际上CNI的容器网络通信流程跟前面的基础网络一样，只是CNI维护了一个单独的网桥来代替 docker0。这个网桥的名字就叫作：CNI 网桥，它在宿主机上的设备名称默认是：cni0。cni的设计思想，就是：Kubernetes在启动Infra容器之后，就可以直接调用CNI网络插件，为这个Infra容器的Network Namespace，配置符合预期的网络栈。</p><p>CNI插件三种网络实现模式：</p><p><img src="https://img2.jesse.top/cni-network.png" alt="img"></p><ul><li>overlay 模式是基于隧道技术实现的，整个容器网络和主机网络独立，容器之间跨主机通信时将整个容器网络封装到底层网络中，然后到达目标机器后再解封装传递到目标容器。不依赖与底层网络的实现。实现的插件有flannel(UDP、vxlan)、calico(IPIP)等等</li><li>三层路由模式中容器和主机也属于不通的网段，他们容器互通主要是基于路由表打通，无需在主机之间建立隧道封包。但是限制条件必须依赖大二层同个局域网内。实现的插件有flannel(host-gw)、calico(BGP)等等</li><li>underlay网络是底层网络，负责互联互通。 容器网络和主机网络依然分属不同的网段，但是彼此处于同一层网络，处于相同的地位。整个网络三层互通，没有大二层的限制，但是需要强依赖底层网络的实现支持.实现的插件有calico(BGP)等等</li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>kubernetes容器网络: <a href="https://tech.ipalfish.com/blog/2020/03/06/kubernetes_container_network/" target="_blank" rel="noopener">https://tech.ipalfish.com/blog/2020/03/06/kubernetes_container_network/</a> (伴鱼团队)</p><p>&lt;kubernetes进阶实战&gt; 11.1 马永亮</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kubernetes-网络&quot;&gt;&lt;a href=&quot;#kubernetes-网络&quot; class=&quot;headerlink&quot; title=&quot;kubernetes 网络&quot;&gt;&lt;/a&gt;kubernetes 网络&lt;/h2&gt;&lt;h3 id=&quot;1-开篇&quot;&gt;&lt;a href=&quot;#1-开篇&quot; class=&quot;headerlink&quot; title=&quot;1.开篇&quot;&gt;&lt;/a&gt;1.开篇&lt;/h3&gt;&lt;p&gt;​    Docker容器诞生以来,,如何确定合适的网络方案是亟待解决的难题之一.在日趋复杂的业务场景下,网络的复杂性也呈几何级数上升.本篇首先回顾了Docker容器的网络通信,然后介绍Kubernertes的网络模型.在Kubernetes集群中,IP地址的分配对象是以Pod为单位,而非容器.同一个Pod内的所有容器共享同一个网络名称空间&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
      <category term="network" scheme="https://jesse.top/categories/kubernetes/network/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Calico</title>
    <link href="https://jesse.top/2021/02/06/kubernetes/network/kubernetes%20Calico/"/>
    <id>https://jesse.top/2021/02/06/kubernetes/network/kubernetes Calico/</id>
    <published>2021-02-06T15:39:58.000Z</published>
    <updated>2021-02-06T15:56:37.275Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kubernetes-Calico"><a href="#Kubernetes-Calico" class="headerlink" title="Kubernetes Calico"></a>Kubernetes Calico</h2><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h3><p>​    Calico是一个非常流行的Kubernetes网络插件和解决方案.Calico是一个开源虚拟化网络方案，用于为云原生应用实现互联及策略控制。与Flannel相比，Calico的一个显著优势是对网络策略（network policy）的支持，它允许用户动态定义ACL规则控制进出容器的数据报文，实现为Pod间的通信按需施加安全策略。事实上，Calico可以整合进大多数主流的编排系统，如Kubernetes、Apache Mesos、Docker和OpenStack等。</p><p>​    Calico本身是一个三层的虚拟网络方案，它将每个节点都当作路由器（router），将每个节点的容器都当作是“节点路由器”的一个终端并为其分配一个IP地址，各节点路由器通过BGP（Border Gateway Protocol）学习生成路由规则，从而将不同节点上的容器连接起来。因此，Calico方案其实是一个纯三层的解决方案，通过每个节点协议栈的三层（网络层）确保容器之间的连通性，这摆脱了flannel host-gw类型的所有节点必须位于同一二层网络的限制，从而极大地扩展了网络规模和网络边界。</p><a id="more"></a><p>​    Calico利用Linux内核在每一个计算节点上实现了一个高效的vRouter（虚拟路由器）进行报文转发，而每个vRouter都通过BGP负责把自身所属的节点上运行的Pod资源的IP地址信息基于节点的agent程序（Felix）直接由vRouter生成路由规则向整个Calico网络内进行传播.</p><p>​    Calico承载的各Pod资源直接通过vRouter经由基础网络进行互联，它非叠加、无隧道、不使用VRF表，也不依赖于NAT，因此每个工作负载都可以直接配置使用公网IP接入互联网，当然，也可以按需使用网络策略控制它的网络连通性。</p><p>​    Calico官网介绍: projectcaclico.org</p><h3 id="2-重要特性"><a href="#2-重要特性" class="headerlink" title="2.重要特性"></a>2.重要特性</h3><h4 id="2-1-经IP路由直连"><a href="#2-1-经IP路由直连" class="headerlink" title="2.1 经IP路由直连"></a>2.1 经IP路由直连</h4><p>Calico中，Pod收发的IP报文由所在节点的Linux内核路由表负责转发，并通过iptables规则实现其安全功能。某Pod对象发送报文时，Calico应确保节点总是作为下一跳MAC地址返回，不管工作负载本身可能配置什么路由，而发往某Pod对象的报文，其最后一个IP跃点就是Pod所在的节点，也就是说，报文的最后一程即由节点送往目标Pod对象，如下图所示。</p><p><img src="https://img2.jesse.top/image-20210206163936241.png" alt="image-20210206163936241"></p><p>需为某Pod对象提供连接时，系统上的专用插件（如Kubernetes的CNI）负责将需求通知给Calico Agent。收到消息后，Calico Agent会为每个工作负载添加直接路径信息到工作负载的TAP设备（如veth）。而运行于当前节点的BGP客户端监控到此类消息后会调用路由reflector向工作于其他节点的BGP客户端进行通告。</p><h4 id="2-2-简单、高效、易扩展"><a href="#2-2-简单、高效、易扩展" class="headerlink" title="2.2 简单、高效、易扩展"></a>2.2 简单、高效、易扩展</h4><p>Calico未使用额外的报文封装和解封装，从而简化了网络拓扑，这也是Calico高性能、易扩展的关键因素。毕竟，小的报文减少了报文分片的可能性，而且较少的封装和解封装操作也降低了对CPU的占用。此外，较少的封装也易于实现报文分析，易于进行故障排查。</p><p>创建、移动或删除Pod对象时，相关路由信息的通告速度也是影响其扩展性的一个重要因素。Calico出色的扩展性缘于与互联网架构设计原则别无二致的方式，它们都使用了BGP作为控制平面。BGP以高效管理百万级的路由设备而闻名于世，Calico自然可以游刃有余地适配大型IDC网络规模。另外，由于Calico各工作负载使用基IP直接进行互联，因此它还支持多个跨地域的IDC之间进行协同。</p><h3 id="3-Calico系统架构"><a href="#3-Calico系统架构" class="headerlink" title="3.Calico系统架构"></a>3.Calico系统架构</h3><p><img src="https://img2.jesse.top/1060878-20190413152300545-538840176.png" alt="img"></p><p>各组件介绍如下:</p><ul><li><p><strong>Felix</strong>：Calico Agent，运行于每个节点。主要负责网络接口管理和监听、路由、ARP 管理、ACL 管理和同步、状态上报等。</p></li><li><p><strong>etcd</strong>：分布式键值存储，主要负责网络元数据一致性，确保Calico网络状态的准确性，可以与kubernetes共用；</p></li><li><p><strong>BGP Client（BIRD）</strong>：Calico 为每一台 Host 部署一个 BGP Client，使用 BIRD 实现，BIRD 是一个单独的持续发展的项目，实现了众多动态路由协议比如 BGP、OSPF、RIP 等。在 Calico 的角色是监听 Host 上由 Felix 注入的路由信息，然后通过 BGP 协议广播告诉剩余 Host 节点，从而实现网络互通。</p></li><li><strong>BGP Route Reflector</strong>：在大型网络规模中，如果仅仅使用 BGP client 形成 mesh 全网互联的方案就会导致规模限制，因为所有节点之间俩俩互联，需要 N^2 个连接，为了解决这个规模问题，可以采用 BGP 的 Router Reflector 的方法，使所有 BGP Client 仅与特定 RR 节点互联并做路由同步，从而大大减少连接数。</li></ul><h4 id="3-1-Felix"><a href="#3-1-Felix" class="headerlink" title="3.1 Felix"></a>3.1 Felix</h4><p>​    Felix运行于各节点的用于支持端点（VM或Container）构建的守护进程，它负责生成路由和ACL，以及其他任何由节点用到的信息，从而为各端点构建连接机制。Felix在各编排系统中主要负责以下任务。</p><p>​    首先是接口管理（Interface Management）功能，负责为接口生成必要的信息并送往内核，以确保内核能够正确处理各端点的流量，尤其是要确保各节点能够响应目标MAC为当前节点上各工作负载的MAC地址的ARP请求，以及为其管理的接口打开转发功能。另外，它还要监控各接口的变动以确保规则能够得到正确的应用。</p><p>​    其次是路由规划（Route Programming）功能，其负责为当前节点运行的各端点在内核FIB（Forwarding Information Base）中生成路由信息，以保证到达当前节点的报文可正确转发给端点。</p><p>​    再次是ACL规划（ACL Programming）功能，负责在Linux内核中生成ACL，用于实现仅放行端点间的合法流量，并确保流量不能绕过Calico的安全措施。</p><p>​    最后是状态报告（State Reporting）功能，负责提供网络健康状态的相关数据，尤其是报告由其管理的节点上的错误和问题。这些报告数据会存储于etcd，供其他组件或网络管理员使用。</p><h4 id="3-2-编排系统插件"><a href="#3-2-编排系统插件" class="headerlink" title="3.2 编排系统插件"></a>3.2 编排系统插件</h4><p>​    编排系统插件（Orchestrator Plugin）依赖于编排系统自身的实现，故此并不存在一个固定的插件以代表此组件。编排系统插件的主要功能是将Calico整合进系统中，并让管理员和用户能够使用Calico的网络功能。它主要负责完成API的转换和反馈输出。</p><p>​    编排系统通常有其自身的网络管理API，网络插件需要负责将对这些API的调用转为Calico的数据模型并存储于Calico的存储系统中。如果有必要，网络插件还要将Calico系统的信息反馈给编排系统，如Felix的存活状态，网络发生错误时设定相应的端点为故障等。</p><h4 id="3-3-etcd存储系统"><a href="#3-3-etcd存储系统" class="headerlink" title="3.3 etcd存储系统"></a>3.3 etcd存储系统</h4><p>​    Calico使用etcd完成组件间的通信，并以之作为一个持久数据存储系统。根据编排系统的不同，etcd所扮演角色的重要性也因之而异，但它贯穿了整个Calico部署全程，并被分为两类主机：核心集群和代理（proxy）。在每个运行着Felix或编排系统插件的主机上都应该运行一个etcd代理以降低etcd集群和集群边缘节点的压力。此模式中，每个运行着插件的节点都会运行着etcd集群的一个成员节点。</p><p>​    etcd是一个分布式、强一致、具有容错功能的存储系统，这一点有助于将Calico网络实现为一个状态确切的系统：要么正常，要么发生故障。另外，分布式存储易于通过扩展应对访问压力的提升，而避免成为系统瓶颈。另外，etcd也是Calico各组件的通信总线，可用于确保让非etcd组件在键空间（keyspace）中监控某些特定的键，以确保它们能够看到所做的任何更改，从而使它们能够及时地响应这些更改。</p><h4 id="3-4-BGP客户端-BIRD"><a href="#3-4-BGP客户端-BIRD" class="headerlink" title="3.4 BGP客户端(BIRD)"></a>3.4 BGP客户端(BIRD)</h4><p>​    Calico要求在每个运行着Felix的节点上同时还要运行一个BGP客户端，负责将Felix生成的路由信息载入内核并通告到整个IDC。在Calico语境中，此组件是通用的BIRD，因此任何BGP客户端（如GoBGP等）都可以从内核中提取路由并对其分发对于它们来说都适合的角色。</p><p>​    BGP客户端的核心功能就是路由分发，在Felix插入路由信息至内核FIB中时，BGP客户端会捕获这些信息并将其分发至其他节点，从而确保了流量的高效路由。</p><h4 id="3-5-BGP路由反射器-BRID"><a href="#3-5-BGP路由反射器-BRID" class="headerlink" title="3.5 BGP路由反射器(BRID)"></a>3.5 BGP路由反射器(BRID)</h4><p>​    在大规模的部署场景中，简易版的BGP客户端易于成为性能瓶颈，因为它要求每个BGP客户端都必须连接至其同一网络中的其他所有BGP客户端以传递路由信息，一个有着N个节点的部署环境中，其存在网络连接的数量为N的二次方，随着N值的逐渐增大，其连接复杂度会急剧上升。因而在较大规模的部署场景中，Calico应该选择部署一个BGP路由反射器，它是由BGP客户端连接的中心点，BGP的点到点通信也就因此转化为与中心点的单路通信模型，如图11-18所示。出于冗余之需，生产实践中应该部署多个BGP路由反射器。对于Calico来说，BGP客户端程序除了作为客户端使用之外，还可以配置成路由反射器。</p><h3 id="4-Calico网络工作模式"><a href="#4-Calico网络工作模式" class="headerlink" title="4.Calico网络工作模式"></a>4.Calico网络工作模式</h3><h4 id="4-1-BGP模式"><a href="#4-1-BGP模式" class="headerlink" title="4.1 BGP模式"></a>4.1 BGP模式</h4><p>边界网关协议（Border Gateway Protocol, BGP）是互联网上一个核心的去中心化自治路由协议，它通过维护IP路由表或“前缀”表来实现自治系统（AS）之间的可达性，属于矢量路由协议。不过，考虑到并非所有的网络都能支持BGP，以及Calico控制平面的设计要求物理网络必须是二层网络，以确保vRouter间均直接可达，路由不能够将物理设备当作下一跳等原因，为了支持三层网络</p><h4 id="4-2-IPIP模式"><a href="#4-2-IPIP模式" class="headerlink" title="4.2 IPIP模式"></a>4.2 IPIP模式</h4><p>​    BGP模式要求Kubernetes的所有物理节点网络必须是二层网络.为了支持三层网络，Calico还推出了IP-in-IP叠加的模型，它也使用Overlay的方式来传输数据。IPIP的包头非常小，而且也是内置在内核中，因此理论上它的速度要比VxLAN快一点，但安全性更差。Calico 3.x的默认配置使用的是IPIP类型的传输方案而非BGP。</p><p>​    工作于IPIP模式的Calico会在每个节点上创建一个tunl0接口（TUN类型虚拟设备）用于封装三层隧道报文。节点上创建的每一个Pod资源，都会由Calico自动创建一对虚拟以太网接口（TAP类型的虚拟设备），其中一个附加于Pod的网络名称空间，另一个（名称以cali为前缀后跟随机字串）留置在节点的根网络名称空间，并经由tunl0封装或解封三层隧道报文。Calico IPIP模式如下图所示。</p><p><img src="https://img2.jesse.top/image-20210206165304293.png" alt="image-20210206165304293"></p><h3 id="5-Calico-网络通信方式"><a href="#5-Calico-网络通信方式" class="headerlink" title="5. Calico 网络通信方式"></a>5. Calico 网络通信方式</h3><h4 id="5-1-Calico网络环境介绍"><a href="#5-1-Calico网络环境介绍" class="headerlink" title="5.1 Calico网络环境介绍"></a>5.1 Calico网络环境介绍</h4><p>当前k8s集群使用的是v1.17.3的版本.有2个node节点.IP地址分别如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl get nodes -o wide | awk &apos;&#123;print $1,$6&#125;&apos; | sed 1,2d</span><br><span class="line">k8s-node1 172.16.20.252</span><br><span class="line">k8s-node2 172.16.20.253</span><br></pre></td></tr></table></figure><p>每个node节点都启动一个<code>tunl0</code> 的虚拟路由器.和许多<code>calixxx</code> 开头的虚拟网卡设备</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-node1 ~]# ifconfig</span><br><span class="line">cali42b086c8543: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1440</span><br><span class="line">        inet6 fe80::ecee:eeff:feee:eeee  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether ee:ee:ee:ee:ee:ee  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 13335563  bytes 928478769 (885.4 MiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 13335563  bytes 928478769 (885.4 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">tunl0: flags=193&lt;UP,RUNNING,NOARP&gt;  mtu 1440</span><br><span class="line">        inet 10.100.36.64  netmask 255.255.255.255</span><br><span class="line">        tunnel   txqueuelen 1000  (IPIP Tunnel)  #默认是IPIP模式</span><br><span class="line">        RX packets 3978810  bytes 345003038 (329.0 MiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 3674392  bytes 613045453 (584.6 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions</span><br></pre></td></tr></table></figure><p>Calico的CNI插件会为每个容器设置一个veth pair设备，然后把另一端接入到宿主机网络空间，由于没有网桥，CNI插件还需要在宿主机上为每个容器的veth pair设备配置一条路由规则，用于接收传入的IP包.</p><p>了这样的veth pair设备以后，容器发出的IP包就会通过veth pair设备到达宿主机，这些路由规则都是Felix维护配置的，而路由信息则是calico bird组件基于BGP分发而来。Calico实际上是将集群里所有的节点都当做边界路由器来处理，他们一起组成了一个全互联的网络，彼此之间通过BGP交换路由，这些节点我们叫做BGP Peer。</p><p>为了下面试验Calico的网络工作.当前集群使用daemonSet控制器运行了2个<code>busybox:1.28.4</code> 镜像的容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl get pods -o wide</span><br><span class="line">NAME            READY   STATUS    RESTARTS   AGE    IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">busybox-g5rkr   1/1     Running   0          130m   10.100.36.103    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">busybox-zdwsc   1/1     Running   0          130m   10.100.169.176   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>在<code>k8s-node1</code>节点上可以看到两条相关路由</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">10.100.36.103   0.0.0.0         255.255.255.255 UH    0      0        0 cali96df9f67b52</span><br><span class="line">10.100.169.128  172.16.20.253   255.255.255.192 UG    0      0        0 tunl0</span><br></pre></td></tr></table></figure><p>第一条路由是访问该节点下的Busybox容器.它的下一跳是<code>calixxxx</code>开头的虚拟网卡.这种通信方式和docker的Bridge网桥模式其实并没有任何区别.</p><p>第二条路由的目的网络是10.100.169.128,子网掩码是255.255.255.192.它代表了IP范围为10.100.169.128-190的地址.而运行于另外一个节点下的<code>busybox-zdwsc</code>Pod的IP地址就位于这个范围之内.所以这条路由可以使node1节点借助于tunl0可以直接和node2节点下的pod进行通信.</p><blockquote><p>在<code>k8s-node2</code> 服务器可以看到类似的这2条路由</p></blockquote><h4 id="5-2-Calico网络模型解密"><a href="#5-2-Calico网络模型解密" class="headerlink" title="5.2 Calico网络模型解密"></a>5.2 Calico网络模型解密</h4><p>登录<code>k8s-node1</code>节点下的Pod容器内部.查看Pod容器的IP地址,以及路由条目.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl exec -it busybox-g5rkr -- sh</span><br><span class="line">/ # ifconfig</span><br><span class="line">eth0      Link encap:Ethernet  HWaddr 4A:7C:E7:FA:4B:CC</span><br><span class="line">          inet addr:10.100.36.103  Bcast:0.0.0.0  Mask:255.255.255.255</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1440  Metric:1</span><br><span class="line">          RX packets:14 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0</span><br><span class="line">          RX bytes:1322 (1.2 KiB)  TX bytes:426 (426.0 B)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/ # route -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">0.0.0.0         169.254.1.1     0.0.0.0         UG    0      0        0 eth0</span><br><span class="line">169.254.1.1     0.0.0.0         255.255.255.255 UH    0      0        0 eth0</span><br></pre></td></tr></table></figure><p>通过<code>k8s-node</code>节点上的下面的路由条目,我们可以知道节点主机和Pod容器的IP地址<code>10.100.36.103</code>通信使用的是<code>cali96df9f67b52</code>这个虚拟网卡</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10.100.36.103   0.0.0.0         255.255.255.255 UH    0      0        0 cali96df9f67b52</span><br></pre></td></tr></table></figure><p>路由条目显示<code>169.254.1.1</code> 是Pod容器的默认网关.但是有网络常识的我们都知道这个IP是个保留的IP地址,不存在于互联网或者任何设备中.那Pod如何和网关通信呢?</p><p>回顾一下网络课程,我们知道任何网络设备和网关设备都是在一个二层局域网中,而二层数据链路层使用MAC地址进行通信,不需要双方的IP地址信息.通信方(这里是Pod容器)会通过ARP协议获取网关的MAC地址,然后通过MAC地址将数据包发送给网关..也就是说网络设备不关心对方的IP是否可达,只要能找到对应的MAC地址就可以.</p><p>通过<code>ip neigh</code>命令查看Pod容器的ARP缓存</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/ # ip neigh</span><br><span class="line">169.254.1.1 dev eth0 lladdr ee:ee:ee:ee:ee:ee ref 1 used 0/0/0 probes 4 REACHABLE</span><br></pre></td></tr></table></figure><blockquote><p>如果是新的Pod容器可能无法获得ARP缓存,此时只需要随便发生一个网络交互(例如ping百度)即可</p></blockquote><p>这个MAC地址(ee:ee:ee:ee:ee:ee)也是Calico的虚拟<code>cali96df9f67b52</code>网卡的虚拟MAC地址.下放是宿主机网卡信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cali96df9f67b52: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1440</span><br><span class="line">        inet6 fe80::ecee:eeff:feee:eeee  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether ee:ee:ee:ee:ee:ee  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure><p>所有虚拟网卡默认开启了ARP代理协议</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-node1 ~]# cat /proc/sys/net/ipv4/conf/cali96df9f67b52/proxy_arp</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p>所以Calico 通过一个巧妙的方法将 Pod 的所有流量引导到一个特殊的网关 169.254.1.1，从而引流到主机的 calixxx 网络设备上，最终将二三层流量全部转换成三层流量来转发。</p><h3 id="6-Calico-IPIP网络模式"><a href="#6-Calico-IPIP网络模式" class="headerlink" title="6.Calico IPIP网络模式"></a>6.Calico IPIP网络模式</h3><p>登录<code>busybox-g5rkr</code>Pod容器内部.ping位于另外一台<code>k8s-node2</code> 下的<code>busybox-zdwsc</code>Pod容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl get pods -o wide</span><br><span class="line">NAME            READY   STATUS    RESTARTS   AGE    IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">busybox-g5rkr   1/1     Running   0          130m   10.100.36.103    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">busybox-zdwsc   1/1     Running   0          130m   10.100.169.176   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>两个Pod之前可以直接访问对方的IP地址.而不需要像Docker容器那样暴露端口,然后利用对方宿主机的IP进行通信</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl exec -it busybox-g5rkr -- sh</span><br><span class="line">/ # ifconfig</span><br><span class="line">eth0      Link encap:Ethernet  HWaddr 4A:7C:E7:FA:4B:CC</span><br><span class="line">          inet addr:10.100.36.103  Bcast:0.0.0.0  Mask:255.255.255.255</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1440  Metric:1</span><br><span class="line">          RX packets:14 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0</span><br><span class="line">          RX bytes:1322 (1.2 KiB)  TX bytes:426 (426.0 B)</span><br><span class="line"></span><br><span class="line">/ # ping 10.100.169.176</span><br><span class="line">PING 10.100.169.176 (10.100.169.176): 56 data bytes</span><br><span class="line">64 bytes from 10.100.169.176: seq=0 ttl=62 time=0.622 ms</span><br><span class="line">64 bytes from 10.100.169.176: seq=1 ttl=62 time=0.552 ms</span><br><span class="line">64 bytes from 10.100.169.176: seq=2 ttl=62 time=0.597 ms</span><br></pre></td></tr></table></figure><p>在<code>k8s-node2</code> 节点抓包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-node2 ~]# tcpdump -i ens192 -nn  -w imcp.cap</span><br></pre></td></tr></table></figure><p>用wireshark软件打开抓包文件.发现如下ICMP的报文</p><p><img src="https://img2.jesse.top/image-20210206220720374.png" alt="image-20210206220720374"></p><p>可以看到每个数据报文共有两个IP网络层,内层是Pod容器之间的IP网络报文,外层是宿主机节点的网络报文(2个node节点).之所以要这样做是因为tunl0是一个隧道端点设备，在数据到达时要加上一层封装，便于发送到对端隧道设备中。 </p><p>Pod间的通信经由IPIP的三层隧道转发,相比较VxLAN的二层隧道来说，IPIP隧道的开销较小，但其安全性也更差一些。</p><p>IPIP的通信方式如下:</p><p><img src="https://img2.jesse.top/1060878-20190415165144848-1984358878.png" alt="img"></p><h4 id="6-1-Pod和Service网络通信"><a href="#6-1-Pod和Service网络通信" class="headerlink" title="6.1 Pod和Service网络通信"></a>6.1 Pod和Service网络通信</h4><p>经过测试.在k8s集群内部物理节点和pod容器内部访问Service的http服务.仍然使用的是Ipip通信模式.</p><p>下面是在容器内部通过Service访问busybox pod容器的http服务的抓包报文</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl exec -it busybox-6hnvc -- sh</span><br><span class="line">/ # curl http://10.96.166.242</span><br><span class="line">sh: curl: not found</span><br><span class="line">/ # wget -O - -q http://10.96.166.242</span><br><span class="line">wget: server returned error: HTTP/1.0 404 Not Found</span><br><span class="line">/ # wget -O - -q http://10.96.166.242</span><br><span class="line">wget: server returned error: HTTP/1.0 404 Not Found</span><br></pre></td></tr></table></figure><p><img src="https://img2.jesse.top/image-20210206222911793.png" alt="image-20210206222911793"></p><h3 id="7-BGP网络模式"><a href="#7-BGP网络模式" class="headerlink" title="7. BGP网络模式"></a>7. BGP网络模式</h3><p>Calico网络部署时,默认安装就是IPIP网络.通过修改calico.yaml部署文件中的<code>CALICO_IPV4POOL_IPIP</code> 值修改成<code>off</code> 就切换到BGP网络模式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Enable IPIP</span><br><span class="line">- name: CALICO_IPV4POOL_IPIP</span><br><span class="line">  value: &quot;Always&quot;  #改成Off</span><br></pre></td></tr></table></figure><p>重新部署calico</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl apply -f calico-3.10.2.yaml</span><br></pre></td></tr></table></figure><p>然后关闭ipipMode.把ipipMode从Always修改成为Never</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 target]# kubectl edit ippool</span><br><span class="line"></span><br><span class="line">  ipipMode: Never</span><br></pre></td></tr></table></figure><h4 id="7-1-和Ipip的区别"><a href="#7-1-和Ipip的区别" class="headerlink" title="7.1 和Ipip的区别"></a>7.1 和Ipip的区别</h4><p>BGP网络相比较IPIP网络，最大的不同之处就是没有了隧道设备 tunl0。 前面介绍过IPIP网络pod之间的流量发送tunl0，然后tunl0发送对端设备。BGP网络中，pod之间的流量直接从网卡发送目的地，减少了tunl0这个环节。</p><h4 id="7-2-通信方式"><a href="#7-2-通信方式" class="headerlink" title="7.2 通信方式"></a>7.2 通信方式</h4><p>删除原来的pod.重新启动新的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl create -f deployment-kubia-v1.yaml</span><br><span class="line">daemonset.apps/busybox created</span><br><span class="line">service/busybox created</span><br><span class="line">[root@k8s-master ~]$kubectl get pods -o wide</span><br><span class="line">NAME            READY   STATUS    RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">busybox-bd566   1/1     Running   0          16s   10.100.36.97     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">busybox-fntv9   1/1     Running   0          16s   10.100.169.129   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>再次查看路由表.发现节点和pod容器通信直接通过宿主机的物理网卡,而不是tunl0设备了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$route -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">0.0.0.0         172.16.20.254   0.0.0.0         UG    100    0        0 ens192</span><br><span class="line">10.100.36.64    172.16.20.252   255.255.255.192 UG    0      0        0 ens192</span><br><span class="line">10.100.169.128  172.16.20.253   255.255.255.192 UG    0      0        0 ens192</span><br></pre></td></tr></table></figure><p>此时,再次2个Pod容器互ping抓包分析.发现两个Pod像物理机一样直接通信,而不需要进行任何数据包封装和解封装.并且数据报文的MAC地址也是node1和node2物理网卡的MAC地址</p><p><img src="https://img2.jesse.top/image-20210206224938109.png" alt="image-20210206224938109"></p><p>BGP的网络连接方式:</p><p><img src="https://img2.jesse.top/1060878-20190415165320714-135136611.png" alt="img"></p><h3 id="8-BGP和ipip网络模式对比"><a href="#8-BGP和ipip网络模式对比" class="headerlink" title="8. BGP和ipip网络模式对比"></a>8. BGP和ipip网络模式对比</h3><ul><li><p><strong>IPIP</strong>:</p><p>特点: tunl0封装数据.形成隧道.所有Pod和pod.pod和节点之间进行三层网络传输</p><p>优点: 适用所有网络类型.能够解决跨网段的路由问题.</p></li><li><p><strong>BGP</strong>:</p><p>特点: 适用BGP路由导向流量</p><p>优点: Pod之间直接通信.省去了隧道,封装,解封装等任何中间环节,传输效率非常高.</p><p>缺点: 需要确保所有物理节点在同一个二层网络,否则Pod无法跨节点网段通信</p></li></ul><h3 id="9-Calico网络优化"><a href="#9-Calico网络优化" class="headerlink" title="9. Calico网络优化"></a>9. Calico网络优化</h3><h4 id="9-1-MTU"><a href="#9-1-MTU" class="headerlink" title="9.1 MTU"></a>9.1 MTU</h4><p>Calico 的IPIP网络模型下tunl0接口的MTU默认为1440，这种设置主要是为适配Google的GCE环境，在非GCE的物理环境中，其最佳值为1480。因此，对于非GCE环境的部署，建议将配置清单calico.yaml下载至本地修改后，再将其应用到集群中。要修改的内容是DaemonSet资源calico-node的Pod模板，将容器calico-node的环境变量“FELIX_INPUTMTU”的值修改为1480即可</p><blockquote><p>因为IPIP多了一层IP报文封装,而IP报文头部一般是20个字节.所以MUT的值应该是最大1500-20.</p></blockquote><h4 id="9-2-Calico-typha"><a href="#9-2-Calico-typha" class="headerlink" title="9.2 Calico-typha"></a>9.2 Calico-typha</h4><p>对于50个节点以上规模的集群来说，所有Calico节点均基于Kubernetes API存取数据会为API Server带来不小的通信压力，这就应该使用calico-typha进程将所有Calico的通信集中起来与API Server进行统一交互。calico-typha以Pod资源的形式托管运行于Kubernetes系统之上，启用的方法为下载前面步骤中用到的Calico的部署清单文件至本地，修改其calico-typha的Pod资源副本数量为所期望的值并重新应用配置清单即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-typha</span><br><span class="line">  ...</span><br><span class="line">spec:</span><br><span class="line">  ...</span><br><span class="line">  replicas: &lt;number of replicas&gt;</span><br></pre></td></tr></table></figure><p>每个calico-typha Pod资源可承载100到200个Calico节点的连接请求，最多不要超过200个。另外，整个集群中的calico-typha的Pod资源总数尽量不要超过20个。</p><h4 id="9-3-BGP路由模型"><a href="#9-3-BGP路由模型" class="headerlink" title="9.3 BGP路由模型"></a>9.3 BGP路由模型</h4><p>默认情况下，Calico的BGP网络工作于点对点的网格（node-to-node mesh）模型，它仅适用于较小规模的集群环境。中级集群环境应该使用全局对等BGP模型（Global BGP peers），以在同一二层网络中使用一个或一组BGP反射器构建BGP网络环境。而大型集群环境需要使用每节点对等BGP模型（Per-node BGP peers），即分布式BGP反射器模型，一个典型的用法是将每个节点都配置为自带BGP反射器接入机架顶部交换机上的路由反射器。</p><h4 id="9-4-使用BGP而非IPIP"><a href="#9-4-使用BGP而非IPIP" class="headerlink" title="9.4 使用BGP而非IPIP"></a>9.4 使用BGP而非IPIP</h4><p>事实上，仅在那些不支持用户自定义BGP配置的网络中才需要使用IPIP的隧道通信类型。如果有一个自主可控的网络环境且部署规模较大时，可以考虑启用BGP的通信类型降低网络开销以提升传输性能，并且应该部署BGP反射器来提高路由学习效率。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>Calico官网: <a href="http://www.projectcalico.org" target="_blank" rel="noopener">www.projectcalico.org</a></p><p>k8s网络之Calico网络: <a href="https://www.cnblogs.com/goldsunshine/p/10701242.html#mxAMjXzT" target="_blank" rel="noopener">https://www.cnblogs.com/goldsunshine/p/10701242.html#mxAMjXzT</a></p><p>kubernetes容器网络: <a href="https://tech.ipalfish.com/blog/2020/03/06/kubernetes_container_network/" target="_blank" rel="noopener">https://tech.ipalfish.com/blog/2020/03/06/kubernetes_container_network/</a> (伴鱼团队)</p><p>&lt;kubernetes进阶实战&gt; 11.4 马永亮</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kubernetes-Calico&quot;&gt;&lt;a href=&quot;#Kubernetes-Calico&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes Calico&quot;&gt;&lt;/a&gt;Kubernetes Calico&lt;/h2&gt;&lt;h3 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1.简介&quot;&gt;&lt;/a&gt;1.简介&lt;/h3&gt;&lt;p&gt;​    Calico是一个非常流行的Kubernetes网络插件和解决方案.Calico是一个开源虚拟化网络方案，用于为云原生应用实现互联及策略控制。与Flannel相比，Calico的一个显著优势是对网络策略（network policy）的支持，它允许用户动态定义ACL规则控制进出容器的数据报文，实现为Pod间的通信按需施加安全策略。事实上，Calico可以整合进大多数主流的编排系统，如Kubernetes、Apache Mesos、Docker和OpenStack等。&lt;/p&gt;
&lt;p&gt;​    Calico本身是一个三层的虚拟网络方案，它将每个节点都当作路由器（router），将每个节点的容器都当作是“节点路由器”的一个终端并为其分配一个IP地址，各节点路由器通过BGP（Border Gateway Protocol）学习生成路由规则，从而将不同节点上的容器连接起来。因此，Calico方案其实是一个纯三层的解决方案，通过每个节点协议栈的三层（网络层）确保容器之间的连通性，这摆脱了flannel host-gw类型的所有节点必须位于同一二层网络的限制，从而极大地扩展了网络规模和网络边界。&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
      <category term="network" scheme="https://jesse.top/categories/kubernetes/network/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes DNS</title>
    <link href="https://jesse.top/2021/02/06/kubernetes/service/DNS/"/>
    <id>https://jesse.top/2021/02/06/kubernetes/service/DNS/</id>
    <published>2021-02-06T15:39:58.000Z</published>
    <updated>2021-02-06T15:40:39.583Z</updated>
    
    <content type="html"><![CDATA[<h2 id="DNS介绍"><a href="#DNS介绍" class="headerlink" title="DNS介绍"></a>DNS介绍</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>kubernets的所有资源.包括Service,Pod都有生命周期,会频繁的销毁和创建.这些资源的IP地址也会随之动态变化.所以Kubernetes使用DNS实现通过资源名解析IP地址.</p><h3 id="DNS服务器"><a href="#DNS服务器" class="headerlink" title="DNS服务器"></a>DNS服务器</h3><p>Kubernetes集群安装了默认的Core-dns组件(通过Pod方式运行).以及kube-dns的service.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl get pods -n kube-system | grep dns</span><br><span class="line">coredns-7f9c544f75-9sh28                   1/1     Running   2          324d</span><br><span class="line">coredns-7f9c544f75-jgmqq                   1/1     Running   2          324d</span><br><span class="line"></span><br><span class="line">#下方这个10.96.0.10就是kubernetes集群的内部DNS服务器</span><br><span class="line">[root@k8s-master ~]$kubectl get svc -n kube-system | grep dns</span><br><span class="line">kube-dns         ClusterIP   10.96.0.10     &lt;none&gt;        53/UDP,53/TCP,9153/TCP   324d</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="pod容器内部dns解析"><a href="#pod容器内部dns解析" class="headerlink" title="pod容器内部dns解析"></a>pod容器内部dns解析</h3><p>创建一个临时的pod容器,测试DNS解析效果.下面的命令临时运行了一个busybox的镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl run -it dns-test --rm --image=busybox:1.28.4 -- sh</span><br></pre></td></tr></table></figure><blockquote><p>不要使用latest版本的镜像,其dns解析有问题.最好使用1.28.4版本的</p></blockquote><p>下方是Pod容器的内部dns服务器信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/ # cat /etc/resolv.conf</span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local localdomain</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure><p><code>resolv.conf</code> 配置文件说明</p><p><strong>nameserver</strong>：指明DNS服务器地址.也就是上文提到的<code>kube-dns</code> 的service</p><p><strong>search</strong>：当原始域名不能被DNS解析时，resolver会将该域名加上search指定的参数，重新请求DNS，直到被正确解析或试完search指定的列表为止 options：dns配置 </p><p><strong>ndots:5</strong>：所有DNS查询中，如果“.”的个数少于5个，则会根据search中配置的列表依次在对应域中先进行搜索，如果没有返回，则最后再直接查询域名本身</p><p>为了了解<code>search</code>和<code>ndots</code> 的概念,我们先要了解FQDN的概念.<code>FQDN(Fully qualified domain name)</code>即完整域名。一般来说如果一个域名以<code>.</code>结束，就表示一个完整域名。比如<code>www.abc.xyz.</code>就是一个<code>FQDN</code>，而<code>www.abc.xyz</code>则不是<code>FQDN</code>。了解了这个概念之后我们就来看<code>search</code>和<code>options ndots</code>。</p><p>如果一个域名是<code>FQDN</code>，那么这个域名会被转发给DNS服务器进行解析。如果域名不是<code>FQDN</code>，那么这个域名会到<code>search</code>搜索解析，还是通过一个例子说明，比如访问<code>abc.xyz</code>这个域名，因为它并不是一个<code>FQDN</code>，所以它会和<code>search</code>域中的值进行组合而变成一个<code>FQDN</code>，以上文的<code>resolv.conf</code>为例，这域名会这样组合：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">abx.xyz.default.svc.cluster.local.</span><br><span class="line">abc.xyz.svc.cluster.local.</span><br><span class="line">abc.xyz.cluster.local.</span><br></pre></td></tr></table></figure><p>然后这些域名先被<code>kube-DNS</code>解析，如果没有解析成功再由宿主机的<code>DNS</code>服务器进行解析。</p><p>而<code>ndots</code>是用来表示一个域名中<code>.</code>的个数在不小于该值的情况下会被认为是一个<code>FQDN</code>。简单说这个属性用来判断一个不是以<code>.</code>结束的域名在什么条件下会被认定为是一个<code>FQDN</code>.上面的示例中ndots为5,也就是说如果一个域名中<code>.</code>的数量大于等于5，即使域名不是以<code>.</code>结尾，也会被认定为是一个<code>FQDN</code>。比如：域名是<code>abc.xyz.xxx.yyy.zzz.aaa</code>这个域名就是<code>FQDN</code>.</p><p>之所以会有<code>search</code>域主要还是为了方便k8s内部服务之间的访问。比如：k8s在同一个<code>namespace</code>下是可以直接通过服务名称进行访问的，其原理就是会在<code>search</code>域查找，比如上面的<code>resolv.conf</code>中<code>jplat、oms-dev</code>着两个其实都是这两个pod所在的<code>namespace</code>的名称。所以通过服务名称访问的时候，会和<code>search</code>域进行组合，这样最终域名会组合成<code>servicename.namespace.svc.cluster.local</code>。而如果是跨<code>namespace</code>访问，则可以通过<code>servicename.namespace</code>这样的形式，在通过和<code>search</code>域组合，依然可以得到<code>servicename.namespace.svc.cluster.local</code>。</p><h3 id="DNS解析"><a href="#DNS解析" class="headerlink" title="DNS解析"></a>DNS解析</h3><h4 id="解析对象"><a href="#解析对象" class="headerlink" title="解析对象"></a>解析对象</h4><p>Kubernetes集群中的每个Service资源都会被指派一个DNS名称.客户端Pod的DNS搜索列表默认是搜索自己的<code>namespace</code> 名称空间内的资源.</p><p>例如上文的<code>resolv.conf</code> 文件内的search搜索列表为<code>search default.svc.cluster.local svc.cluster.local cluster.local localdomain</code> .此时Pod可以直接搜索<code>default</code> 名称空间下的所有Service:</p><p>例如.使用上面的临时busybox容器解析<code>my-svc</code>的Service</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/ # nslookup my-svc</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      my-svc</span><br><span class="line">Address 1: 10.96.51.58 my-svc.default.svc.cluster.local</span><br></pre></td></tr></table></figure><p>上面的IP地址<code>10.96.51.58</code> 表示成功解析到该Service的IP.<code>my-svc.default.svc.cluster.local</code>这个就是该Service的FQDN完全限定域名.</p><p>其中:</p><p><strong><code>default</code></strong> —表示名称空间,我们的名称空间名字就是默认的default</p><p><strong><code>svc</code></strong> —————-表示资源类型,这里是Service</p><p><strong><code>cluster.local</code></strong> –k8s集群域名</p><p>也可以解析其他名称空间内的资源,比如解析<code>kube-system</code> 名称空间下的DNS服务器的Service.(DNS服务器本身也会被指定一个DNS名称).就可以通过<code>&lt;svc-name&gt;.&lt;namespace-name&gt;</code> 实现.比如下面解析<code>kube-system</code>名称空间下的<code>kube-dns</code>的Service</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/ # nslookup kube-dns.kube-system</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      kube-dns.kube-system</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br></pre></td></tr></table></figure><p>实际上DNS解析的是完全FQDN域名,只不过后面一部分内容<code>default.svc.cluster.local</code> 可以省略罢了.默认就是解析当前名称空间下的资源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">/ # nslookup my-svc</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      my-svc</span><br><span class="line">Address 1: 10.96.51.58 my-svc.default.svc.cluster.local</span><br><span class="line"></span><br><span class="line">#等同于:</span><br><span class="line">/ # nslookup my-svc.default.svc.cluster.local</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      my-svc.default.svc.cluster.local</span><br><span class="line">Address 1: 10.96.51.58 my-svc.default.svc.cluster.local</span><br></pre></td></tr></table></figure><p>在kubernetes官网中也提到:</p><p>假设在 Kubernetes 集群的名字空间 <code>bar</code> 中，定义了一个服务 <code>foo</code>。 运行在名字空间 <code>bar</code> 中的 Pod 可以简单地通过 DNS 查询 <code>foo</code> 来找到该服务。 运行在名字空间 <code>quux</code> 中的 Pod 可以通过 DNS 查询 <code>foo.bar</code> 找到该服务。</p><h4 id="Service-A记录"><a href="#Service-A记录" class="headerlink" title="Service A记录"></a>Service A记录</h4><p>对于普通的Service资源.会以<code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local</code>这种形式被分配一个DNS A记录.也就是上文中的<code>my-svc</code>的<code>10.96.51.58</code>这个IP地址.</p><p>如果是对于无头服务(headless service).这种service没有IP.但是也会以上面的形式被指派一个DNS的A记录.只不过这种记录和普通Service不同,而是被解析成对应服务的POD集合的Pod的IP.客户端使用标准的负载均衡策略从这组Pod中进行选择.</p><p>例如下面创建一个headless的svc.和普通svc的区别在于<code>clusterIP的值为None</code>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$cat deployment-kubia-v1.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: hsq1</span><br><span class="line">  labels:</span><br><span class="line">     app: hsq-openapi</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx</span><br><span class="line">      image: nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: hsq2</span><br><span class="line">  labels:</span><br><span class="line">     app: hsq-openapi</span><br><span class="line">spec:</span><br><span class="line">    containers:</span><br><span class="line">       - name: nginx</span><br><span class="line">         image: nginx</span><br><span class="line">---</span><br><span class="line">#一个yaml文件可以定义多种资源,中间用---隔开</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: hsq-openapi</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: hsq-openapi</span><br><span class="line">  clusterIP: None</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br></pre></td></tr></table></figure><blockquote><p>headless服务一般用于statefulset资源.不能用于deployment控制器</p></blockquote><p>创建该文件后查看<code>hsq-openapi</code>service:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl describe svc hsq-openapi</span><br><span class="line">Name:              hsq-openapi</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       kubectl.kubernetes.io/last-applied-configuration:</span><br><span class="line">                     &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;hsq-openapi&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;spec&quot;:&#123;&quot;clusterIP&quot;:&quot;None&quot;,&quot;p...</span><br><span class="line">Selector:          app=hsq-openapi</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                None</span><br><span class="line">Port:              &lt;unset&gt;  80/TCP</span><br><span class="line">TargetPort:        80/TCP</span><br><span class="line">Endpoints:         10.100.36.66:80,10.100.36.69:80  #这里是后端pod列表</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br></pre></td></tr></table></figure><p><strong>headless类型服务的DNS解析</strong></p><p>仍然使用上文中的busybox测试容器.解析<code>hsq-openapi</code> service 的A记录.可以看到解析的结果返回了2个pod的IP地址列表.对于这种类型的service.和普通的service不同.他解析出来的是POD的ip地址列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/ # nslookup hsq-openapi</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      hsq-openapi</span><br><span class="line">Address 1: 10.100.36.69 10-100-36-69.hsq-openapi.default.svc.cluster.local</span><br><span class="line">Address 2: 10.100.36.66 10-100-36-66.hsq-openapi.default.svc.cluster.local</span><br><span class="line">/ # ???</span><br></pre></td></tr></table></figure><hr><h4 id="Pod的A记录"><a href="#Pod的A记录" class="headerlink" title="Pod的A记录"></a>Pod的A记录</h4><p>一般而言,Pod会对应如下DNS名字解析: <code>pod-ip-address.&lt;namespace-name&gt;.pod.cluster.local</code> 例如对于上面例子中的iP为<code>10.100.36.69</code> 的Pod.对应的DNS名称为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/ # nslookup 10-100-36-69.default.pod.cluster.local  #DNS名称</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      10-100-36-69.default.pod.cluster.local</span><br><span class="line">Address 1: 10.100.36.69 10-100-36-69.hsq-openapi.default.svc.cluster.local</span><br></pre></td></tr></table></figure><h3 id="k8s默认的DNS策略"><a href="#k8s默认的DNS策略" class="headerlink" title="k8s默认的DNS策略"></a>k8s默认的DNS策略</h3><p>k8s提供了5种DNS策略，如下：</p><ul><li><code>Default</code>: Pod 从运行所在的节点继承名称解析配置。</li><li><code>ClusterFirst</code>: 与配置的集群域后缀不匹配的任何 DNS 查询（例如 “<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fwww.kubernetes.io" target="_blank" rel="noopener">www.kubernetes.io</a>”） 都将转发到从节点继承的上游名称服务器。集群管理员可能配置了额外的存根域和上游 DNS 服务器。</li><li><code>ClusterFirstWithHostNet</code>：对于以 hostNetwork 方式运行的 Pod，应显式设置其 DNS 策略 <code>ClusterFirstWithHostNet</code>。</li><li><code>None</code>: 此设置允许 Pod 忽略 Kubernetes 环境中的 DNS 设置。Pod 会使用其 <code>dnsConfig</code> 字段 所提供的 DNS 设置。</li></ul><p>k8s默认使用的DNS策略是<code>ClusterFirst</code>，这点需要注意，也就是说域名解析会优先使用集群的DNS（<code>kube-DNS</code>）进行查询，如果k8s的DNS解析失败，会转发到宿主机的DNS进行解析。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;DNS介绍&quot;&gt;&lt;a href=&quot;#DNS介绍&quot; class=&quot;headerlink&quot; title=&quot;DNS介绍&quot;&gt;&lt;/a&gt;DNS介绍&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;kubernets的所有资源.包括Service,Pod都有生命周期,会频繁的销毁和创建.这些资源的IP地址也会随之动态变化.所以Kubernetes使用DNS实现通过资源名解析IP地址.&lt;/p&gt;
&lt;h3 id=&quot;DNS服务器&quot;&gt;&lt;a href=&quot;#DNS服务器&quot; class=&quot;headerlink&quot; title=&quot;DNS服务器&quot;&gt;&lt;/a&gt;DNS服务器&lt;/h3&gt;&lt;p&gt;Kubernetes集群安装了默认的Core-dns组件(通过Pod方式运行).以及kube-dns的service.&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[root@k8s-master ~]$kubectl get pods -n kube-system | grep dns&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;coredns-7f9c544f75-9sh28                   1/1     Running   2          324d&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;coredns-7f9c544f75-jgmqq                   1/1     Running   2          324d&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#下方这个10.96.0.10就是kubernetes集群的内部DNS服务器&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[root@k8s-master ~]$kubectl get svc -n kube-system | grep dns&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;kube-dns         ClusterIP   10.96.0.10     &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP   324d&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
      <category term="Service" scheme="https://jesse.top/categories/kubernetes/Service/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes DaemonSet</title>
    <link href="https://jesse.top/2021/02/06/kubernetes/controller/daemonset/"/>
    <id>https://jesse.top/2021/02/06/kubernetes/controller/daemonset/</id>
    <published>2021-02-06T15:39:58.000Z</published>
    <updated>2021-04-01T13:41:29.050Z</updated>
    
    <content type="html"><![CDATA[<h2 id="DaemonSet"><a href="#DaemonSet" class="headerlink" title="DaemonSet"></a>DaemonSet</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>顾名思义,DaemonSet的主要作用是让你在kubernetes集群里运行一个Daemon Pod.所以,这个Pod有如下三个特征:</p><p>1.这个Pod运行在kubernetes集群的每一个节点(Node)上</p><p>2.每个节点只有一个这样的Pod实例</p><p>3.当有新的节点加入 Kubernetes 集群后，该 Pod 会自动地在新节点上被创建出来；而当旧节点被删除后，它上面的 Pod 也相应地会被回收掉.</p><p>这个机制听起来很简单，但 Daemon Pod 的意义确实是非常重要的。我随便给你列举几个例子：</p><ol><li>各种网络插件的 Agent 组件，都必须运行在每一个节点上，用来处理这个节点上的容器网络；</li><li>各种存储插件的 Agent 组件，也必须运行在每一个节点上，用来在这个节点上挂载远程存储目录，操作容器的 Volume 目录；</li><li>各种监控组件和日志组件，也必须运行在每一个节点上，负责这个节点上的监控信息和日志搜集。</li></ol><a id="more"></a><p>更重要的是，跟其他编排对象不一样，DaemonSet 开始运行的时机，很多时候比整个 Kubernetes 集群出现的时机都要早。</p><p>这个乍一听起来可能有点儿奇怪。但其实你来想一下：如果这个 DaemonSet 正是一个网络插件的 Agent 组件呢？</p><p>这个时候，整个 Kubernetes 集群里还没有可用的容器网络，所有 Worker 节点的状态都是 NotReady（NetworkReady=false）。这种情况下，普通的 Pod 肯定不能运行在这个集群上。所以，这也就意味着 DaemonSet 的设计，必须要有某种“过人之处”才行。</p><h3 id="DaemonSet工作原理"><a href="#DaemonSet工作原理" class="headerlink" title="DaemonSet工作原理"></a>DaemonSet工作原理</h3><p>为了弄清楚 DaemonSet 的工作原理，我们还是按照老规矩，先从它的 API 对象的定义说起。下面是一个Nginx的daemonset资源清单</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      name: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: nginx</span><br><span class="line">    spec:</span><br><span class="line">      tolerations:</span><br><span class="line">        - key: node-role.kubernetes.io/master</span><br><span class="line">          effect: NoSchedule</span><br><span class="line">      containers:</span><br><span class="line">        - name: nginx</span><br><span class="line">          image: nginx:1.19.6</span><br><span class="line">          resources:</span><br><span class="line">            limits:</span><br><span class="line">              memory: 200Mi</span><br><span class="line">            requests:</span><br><span class="line">              cpu: 100m</span><br><span class="line">              memory: 200Mi</span><br></pre></td></tr></table></figure><p>这个DaemonSet非常简单,管理一个Nginx镜像的Pod.可以看到DaemonSet和Deployment非常相似,只不过没有replicas字段.他也使用selector选择管理所有携带了name=Nginx标签的POD</p><h4 id="Daemonset创建Pod原理"><a href="#Daemonset创建Pod原理" class="headerlink" title="Daemonset创建Pod原理"></a>Daemonset创建Pod原理</h4><p>那么，<strong>DaemonSet 又是如何保证每个 Node 上有且只有一个被管理的 Pod 呢？</strong></p><p>显然，这是一个典型的“控制器模型”能够处理的问题。</p><p>DaemonSet Controller，首先从 Etcd 里获取所有的 Node 列表，然后遍历所有的 Node。这时，它就可以很容易地去检查，当前这个 Node 上是不是有一个携带了 name=fluentd-elasticsearch 标签的 Pod 在运行。</p><p>而检查的结果，可能有这么三种情况：</p><ol><li>没有这种 Pod，那么就意味着要在这个 Node 上创建这样一个 Pod；</li><li>有这种 Pod，但是数量大于 1，那就说明要把多余的 Pod 从这个 Node 上删除掉；</li><li>正好只有一个这种 Pod，那说明这个节点是正常的。</li></ol><p>其中，删除节点（Node）上多余的 Pod 非常简单，直接调用 Kubernetes API 就可以了。</p><p>但是，<strong>如何在指定的 Node 上创建新 Pod 呢？</strong></p><h5 id="1-利用nodeAffinity"><a href="#1-利用nodeAffinity" class="headerlink" title="1.利用nodeAffinity"></a>1.<strong>利用nodeAffinity</strong></h5><p>如果你已经熟悉了 Pod API 对象的话，那一定可以立刻说出答案：用 nodeSelector，选择 Node 的名字即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nodeSelector:</span><br><span class="line">    name: &lt;Node 名字 &gt;</span><br></pre></td></tr></table></figure><p>没错。不过，在 Kubernetes 项目里，nodeSelector 其实已经是一个将要被废弃的字段了。因为，现在有了一个新的、功能更完善的字段可以代替它，即：nodeAffinity。我来举个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: with-node-affinity</span><br><span class="line">spec:</span><br><span class="line">  affinity:</span><br><span class="line">    nodeAffinity:</span><br><span class="line">      requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">        nodeSelectorTerms:</span><br><span class="line">        - matchExpressions:</span><br><span class="line">          - key: metadata.name</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">            - node-geektime</span><br></pre></td></tr></table></figure><p>在这个 Pod 里，我声明了一个 spec.affinity 字段，然后定义了一个 nodeAffinity。</p><p>而在这里，我定义的 nodeAffinity 的含义是：</p><ol><li>requiredDuringSchedulingIgnoredDuringExecution：它的意思是说，这个 nodeAffinity 必须在每次调度的时候予以考虑。同时，这也意味着你可以设置在某些情况下不考虑这个 nodeAffinity；</li><li>这个 Pod，将来只允许运行在“<code>metadata.name</code>”是“node-geektime”的节点上。</li></ol><p>在这里，你应该注意到 nodeAffinity 的定义，可以支持更加丰富的语法，比如 operator: In（即：部分匹配；如果你定义 operator: Equal，就是完全匹配），这也正是 nodeAffinity 会取代 nodeSelector 的原因之一。</p><blockquote><p>其实在大多数时候，这些 Operator 语义没啥用处。所以说，在学习开源项目的时候，一定要学会抓住“主线”。不要顾此失彼。</p></blockquote><p>所以，<strong>我们的 DaemonSet Controller 会在创建 Pod 的时候，自动在这个 Pod 的 API 对象里，加上这样一个 nodeAffinity 定义</strong>。其中，需要绑定的节点名字，正是当前正在遍历的这个 Node。</p><p>当然，DaemonSet 并不需要修改用户提交的 YAML 文件里的 Pod 模板，而是在向 Kubernetes 发起请求之前，直接修改根据模板生成的 Pod 对象。这个思路，也正是我在前面讲解 Pod 对象时介绍过的。</p><p>创建刚才的Nginx的yaml清单:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl get pods -n kube-system -l name=nginx</span><br><span class="line">NAME          READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-b44l8   1/1     Running   0          66m</span><br><span class="line">nginx-cvkgz   1/1     Running   0          66m</span><br><span class="line">nginx-hldhl   1/1     Running   0          66m</span><br></pre></td></tr></table></figure><p>查看其中任意一个pod的yaml文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl get pods -n kube-system nginx-b44l8 -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">...略....</span><br><span class="line">spec:</span><br><span class="line">  affinity:</span><br><span class="line">    nodeAffinity:</span><br><span class="line">      requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">        nodeSelectorTerms:</span><br><span class="line">        - matchFields:</span><br><span class="line">          - key: metadata.name</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">            - k8s-node2</span><br><span class="line">...略....</span><br></pre></td></tr></table></figure><p>这个是DaemonSet自动为Pod打上的nodeaffinity节点亲和性属性,表示将该Pod调度到<code>k8s-node2</code>这个hostname的节点上.</p><blockquote><p>如果查看其它2个Nginx的Pod,他的nodeaffinity调度的节点名称自然也会不一样</p></blockquote><p><strong>通过nodeaffinity,DaemonSet就可以确保每个Pod都调度到不同的k8s节点.而不会将多个pod调度到同一个节点.但是如果需要确保Pod可以被调度到节点上,还需要利用另外一个和调度相关的字段:tolerations</strong></p><h5 id="2-利用tolerations"><a href="#2-利用tolerations" class="headerlink" title="2.利用tolerations"></a>2.利用tolerations</h5><p>tolerations(容忍度)这个字段意味着这个 Pod，会“容忍”（Toleration）某些 Node 的“污点”（Taint）。</p><p>而tolerations字段也是daemonset自动加上去的.还是执行上面的那条命令查看Pod的yaml清单文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tolerations:</span><br><span class="line">  - effect: NoSchedule</span><br><span class="line">    key: node-role.kubernetes.io/master</span><br><span class="line">  - effect: NoExecute</span><br><span class="line">    key: node.kubernetes.io/not-ready</span><br><span class="line">    operator: Exists</span><br><span class="line">  - effect: NoExecute</span><br><span class="line">    key: node.kubernetes.io/unreachable</span><br><span class="line">    operator: Exists</span><br><span class="line">  - effect: NoSchedule</span><br><span class="line">    key: node.kubernetes.io/disk-pressure</span><br><span class="line">    operator: Exists</span><br><span class="line">  - effect: NoSchedule</span><br><span class="line">    key: node.kubernetes.io/memory-pressure</span><br><span class="line">    operator: Exists</span><br><span class="line">  - effect: NoSchedule</span><br><span class="line">    key: node.kubernetes.io/pid-pressure</span><br><span class="line">    operator: Exists</span><br><span class="line">  - effect: NoSchedule</span><br><span class="line">    key: node.kubernetes.io/unschedulable</span><br><span class="line">    operator: Exists</span><br></pre></td></tr></table></figure><p>可以看到DaemonSet自动为这个Pod打上了很多容忍度,包括节点not-ready,unreachable,节点的磁盘,内存,pid的压力,以及哪怕节点被标记为<code>unschedulable</code> .就使得这些 Pod 可以忽略所有这些节点限制，继而保证每个节点上都会被调度一个 Pod。当然，如果这个节点有故障的话，这个 Pod 可能会启动失败，而 DaemonSet 则会始终尝试下去，直到 Pod 启动成功。</p><blockquote><p>而在正常情况下，被标记了 unschedulable“污点”的 Node，是不会有任何 Pod 被调度上去的（effect: NoSchedule）</p></blockquote><p>当然,你也可以在daemonset的资源清单里手动加上各种toleration污点容忍度.就像上面的例子:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tolerations:</span><br><span class="line">        - key: node-role.kubernetes.io/master</span><br><span class="line">          effect: NoSchedule</span><br></pre></td></tr></table></figure><p>因为在默认情况下，Kubernetes 集群不允许用户在 Master 节点部署 Pod。因为，Master 节点默认携带了一个叫作<code>node-role.kubernetes.io/master</code>的“污点”。所以，为了能在 Master 节点上部署 DaemonSet 的 Pod，我就必须让这个 Pod“容忍”这个“污点”。</p><p>这时，你应该可以猜到，我在前面介绍到的<strong>DaemonSet 的“过人之处”，其实就是依靠 Toleration 实现的。</strong></p><p>假如当前 DaemonSet 管理的，是一个网络插件的 Agent Pod，那么你就必须在这个 DaemonSet 的 YAML 文件里，给它的 Pod 模板加上一个能够“容忍”<code>node.kubernetes.io/network-unavailable</code>“污点”的 Toleration。正如下面这个例子所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: network-plugin-agent</span><br><span class="line">    spec:</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: node.kubernetes.io/network-unavailable</span><br><span class="line">        operator: Exists</span><br><span class="line">        effect: NoSchedule</span><br></pre></td></tr></table></figure><p>在 Kubernetes 项目中，当一个节点的网络插件尚未安装时，这个节点就会被自动加上名为<code>node.kubernetes.io/network-unavailable</code>的“污点”。</p><p><strong>而通过这样一个 Toleration，调度器在调度这个 Pod 的时候，就会忽略当前节点上的“污点”，从而成功地将网络插件的 Agent 组件调度到这台机器上启动起来。</strong></p><h3 id="DaemonSet的滚动更新"><a href="#DaemonSet的滚动更新" class="headerlink" title="DaemonSet的滚动更新"></a>DaemonSet的滚动更新</h3><p>通过命令查看daemonset的对象</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl get ds nginx -n kube-system</span><br><span class="line">NAME    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">nginx   3         3         3       3            3           &lt;none&gt;          3h14m</span><br></pre></td></tr></table></figure><p>就会发现 DaemonSet 和 Deployment 一样，也有 DESIRED、CURRENT 等多个状态字段。这也就意味着，DaemonSet 可以像 Deployment 那样，进行版本管理。这个版本，可以使用 kubectl rollout history 看到该daemonset的发布版本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl rollout history daemonset nginx -n kube-system</span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">1         &lt;none&gt;</span><br></pre></td></tr></table></figure><p>接下来将nginx的镜像版本升级到1.19.0.顺便加上–record参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl set image ds nginx nginx=nginx:1.19.0 -n kube-system --record</span><br></pre></td></tr></table></figure><p>观察升级过程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl rollout status ds nginx -n kube-system</span><br><span class="line">Waiting for daemon set &quot;nginx&quot; rollout to finish: 1 out of 3 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;nginx&quot; rollout to finish: 1 out of 3 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;nginx&quot; rollout to finish: 1 out of 3 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;nginx&quot; rollout to finish: 2 out of 3 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;nginx&quot; rollout to finish: 2 out of 3 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;nginx&quot; rollout to finish: 2 out of 3 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;nginx&quot; rollout to finish: 2 of 3 updated pods are available...</span><br><span class="line">daemon set &quot;nginx&quot; successfully rolled out</span><br></pre></td></tr></table></figure><p>在rollout history里就能看到滚动更新的记录:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl rollout history daemonset nginx -n kube-system</span><br><span class="line">daemonset.apps/nginx</span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">3         kubectl set image ds/nginx nginx=nginx:latest --record=true --namespace=kube-system</span><br><span class="line">4         &lt;none&gt;</span><br><span class="line">5         kubectl set image ds/nginx nginx=nginx:latest --record=true --namespace=kube-system</span><br><span class="line">6         kubectl set image ds nginx nginx=nginx:1.19.0 --namespace=kube-system --record=true</span><br></pre></td></tr></table></figure><p>通过后面的具体命令可以看到,这里我们是发布到了第6版.有了版本号，你也就可以像 Deployment 一样，将 DaemonSet 回滚到某个指定的历史版本了。</p><p>而我在前面的文章中讲解 Deployment 对象的时候，曾经提到过，Deployment 管理这些版本，靠的是“一个版本对应一个 ReplicaSet 对象”。可是，DaemonSet 控制器操作的直接就是 Pod，不可能有 ReplicaSet 这样的对象参与其中。<strong>那么，它的这些版本又是如何维护的呢？</strong></p><p>所谓，一切皆对象！</p><p>在 Kubernetes 项目中，任何你觉得需要记录下来的状态，都可以被用 API 对象的方式实现。当然，“版本”也不例外。</p><p>Kubernetes v1.7 之后添加了一个 API 对象，名叫<strong>ControllerRevision</strong>，专门用来记录某种 Controller 对象的版本。比如，你可以通过如下命令查看 fluentd-elasticsearch 对应的 ControllerRevision：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl get controllerrevision -n kube-system -l name=nginx</span><br><span class="line">NAME               CONTROLLER             REVISION   AGE</span><br><span class="line">nginx-66bcf54bbc   daemonset.apps/nginx   5          3h17m</span><br><span class="line">nginx-6fdb467d8c   daemonset.apps/nginx   4          3h35m</span><br><span class="line">nginx-757b664477   daemonset.apps/nginx   3          3h10m</span><br><span class="line">nginx-7ccc97dc9f   daemonset.apps/nginx   6          4m30s</span><br><span class="line">[root@k8s-master daemonset]$</span><br></pre></td></tr></table></figure><p>可以看到每个版本号(REVISION)对应一个controller.而如果你使用 kubectl describe 查看这个 ControllerRevision 对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl describe controllerrevision -n kube-system nginx-7ccc97dc9f</span><br><span class="line">Name:         nginx-7ccc97dc9f</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Labels:       controller-revision-hash=7ccc97dc9f</span><br><span class="line">              name=nginx</span><br><span class="line">Annotations:  deprecated.daemonset.template.generation: 6</span><br><span class="line">              kubernetes.io/change-cause: kubectl set image ds nginx nginx=nginx:1.19.0 --namespace=kube-system --record=true</span><br><span class="line">API Version:  apps/v1</span><br><span class="line">Data:</span><br><span class="line">  Spec:</span><br><span class="line">    Template:</span><br><span class="line">      $patch:  replace</span><br><span class="line">      Metadata:</span><br><span class="line">        Creation Timestamp:  &lt;nil&gt;</span><br><span class="line">        Labels:</span><br><span class="line">          Name:  nginx</span><br><span class="line">      Spec:</span><br><span class="line">        Containers:</span><br><span class="line">          Image:              nginx:1.19.0</span><br><span class="line">          Image Pull Policy:  IfNotPresent</span><br><span class="line">          Name:               nginx</span><br><span class="line">          Resources:</span><br><span class="line">            Limits:</span><br><span class="line">              Memory:  200Mi</span><br><span class="line">            Requests:</span><br><span class="line">              Cpu:                     100m</span><br><span class="line">              Memory:                  200Mi</span><br><span class="line">          Termination Message Path:    /dev/termination-log</span><br><span class="line">          Termination Message Policy:  File</span><br><span class="line">        Dns Policy:                    ClusterFirst</span><br><span class="line">        Restart Policy:                Always</span><br><span class="line">        Scheduler Name:                default-scheduler</span><br><span class="line">        Security Context:</span><br><span class="line">        Termination Grace Period Seconds:  30</span><br><span class="line">        Tolerations:</span><br><span class="line">          Effect:  NoSchedule</span><br><span class="line">          Key:     node-role.kubernetes.io/master</span><br><span class="line">Kind:              ControllerRevision</span><br><span class="line">Metadata:</span><br><span class="line">  Creation Timestamp:  2021-03-30T06:00:05Z</span><br><span class="line">  Owner References:</span><br><span class="line">    API Version:           apps/v1</span><br><span class="line">    Block Owner Deletion:  true</span><br><span class="line">    Controller:            true</span><br><span class="line">    Kind:                  DaemonSet</span><br><span class="line">    Name:                  nginx</span><br><span class="line">    UID:                   21e0bf42-536c-43c6-b961-f6cc51317eba</span><br><span class="line">  Resource Version:        142550</span><br><span class="line">  Self Link:               /apis/apps/v1/namespaces/kube-system/controllerrevisions/nginx-7ccc97dc9f</span><br><span class="line">  UID:                     27f9800a-bafa-40d6-9a02-ca11fc2824ce</span><br><span class="line">Revision:                  6</span><br><span class="line">Events:                    &lt;none&gt;</span><br></pre></td></tr></table></figure><p>就会看到，这个 ControllerRevision 对象，实际上是在 Data 字段保存了该版本对应的完整的 DaemonSet 的 API 对象。并且，在 Annotation 字段保存了创建这个对象所使用的 kubectl 命令。</p><p>接下来，我们可以尝试将这个 DaemonSet 回滚到 Revision=4 时的状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl rollout undo daemonset nginx --to-revision=4 -n kube-system</span><br><span class="line">daemonset.apps/nginx rolled back</span><br></pre></td></tr></table></figure><p>这个 kubectl rollout undo 操作，实际上相当于读取到了 Revision=4 的 ControllerRevision 对象保存的 Data 字段。而这个 Data 字段里保存的信息，就是 Revision=1 时这个 DaemonSet 的完整 API 对象。</p><p>所以，现在 DaemonSet Controller 就可以使用这个历史 API 对象，对现有的 DaemonSet 做一次 PATCH 操作（等价于执行一次 kubectl apply -f “旧的 DaemonSet 对象”），从而把这个 DaemonSet“更新”到一个旧版本。</p><p>这也是为什么，在执行完这次回滚完成后，你会发现，DaemonSet 的 Revision 并不会从 Revision=6 退回到 4，而是会增加成 Revision=7。这是因为，一个新的 ControllerRevision 被创建了出来。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl rollout history daemonset nginx -n kube-system</span><br><span class="line">daemonset.apps/nginx</span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">3         kubectl set image ds/nginx nginx=nginx:latest --record=true --namespace=kube-system</span><br><span class="line">5         kubectl set image ds/nginx nginx=nginx:latest --record=true --namespace=kube-system</span><br><span class="line">6         kubectl set image ds nginx nginx=nginx:1.19.0 --namespace=kube-system --record=true</span><br><span class="line">7         &lt;none&gt;</span><br></pre></td></tr></table></figure><h5 id="商榷之处"><a href="#商榷之处" class="headerlink" title="商榷之处:"></a>商榷之处:</h5><p>原文文档里说是<code>一个新的 ControllerRevision 被创建了出来</code> .但是经过实践发现,并没有创建一个新的revision=7的controllerrevision.而是仍然使用revision=4的controllerrevision,只不过将他的版本从4替代成了7..仔细对比下面回滚后的controllerrevision信息和回滚之前的信息可以发现这点:</p><h5 id="回滚到revision-4后-revision7出现了原本revision4的位置"><a href="#回滚到revision-4后-revision7出现了原本revision4的位置" class="headerlink" title="回滚到revision=4后,revision7出现了原本revision4的位置"></a>回滚到revision=4后,revision7出现了原本revision4的位置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl get controllerrevision -n kube-system -l name=nginx</span><br><span class="line">NAME               CONTROLLER             REVISION   AGE</span><br><span class="line">nginx-66bcf54bbc   daemonset.apps/nginx   5          3h22m</span><br><span class="line">nginx-6fdb467d8c   daemonset.apps/nginx   7          3h40m</span><br><span class="line">nginx-757b664477   daemonset.apps/nginx   3          3h14m</span><br><span class="line">nginx-7ccc97dc9f   daemonset.apps/nginx   6          9m26s</span><br></pre></td></tr></table></figure><h5 id="下面这个是回滚到revision-4之前的controllervision信息-可以看到没有创建一个新的controllerrevision-而是原本revision-4的nginx-6fdb467d8c版本变更成了7"><a href="#下面这个是回滚到revision-4之前的controllervision信息-可以看到没有创建一个新的controllerrevision-而是原本revision-4的nginx-6fdb467d8c版本变更成了7" class="headerlink" title="下面这个是回滚到revision=4之前的controllervision信息.可以看到没有创建一个新的controllerrevision,而是原本revision=4的nginx-6fdb467d8c版本变更成了7"></a>下面这个是回滚到revision=4之前的controllervision信息.可以看到没有创建一个新的controllerrevision,而是原本revision=4的<code>nginx-6fdb467d8c</code>版本变更成了7</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl get controllerrevision -n kube-system -l name=nginx</span><br><span class="line">NAME               CONTROLLER             REVISION   AGE</span><br><span class="line">nginx-66bcf54bbc   daemonset.apps/nginx   5          3h17m</span><br><span class="line">nginx-6fdb467d8c   daemonset.apps/nginx   4          3h35m</span><br><span class="line">nginx-757b664477   daemonset.apps/nginx   3          3h10m</span><br><span class="line">nginx-7ccc97dc9f   daemonset.apps/nginx   6          4m30s</span><br><span class="line">[root@k8s-master daemonset]$</span><br></pre></td></tr></table></figure><p>这可能是作者使用的k8s集群版本(1.11)和我实践的版本(1.17.3)不同</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>相比于 Deployment，DaemonSet 只管理 Pod 对象，然后通过 nodeAffinity 和 Toleration 这两个调度器的小功能，保证了每个节点上有且只有一个 Pod。</p><p>与此同时，DaemonSet 使用 ControllerRevision，来保存和管理自己对应的“版本”。这种“面向 API 对象”的设计思路，大大简化了控制器本身的逻辑，也正是 Kubernetes 项目“声明式 API”的优势所在。</p><p>而且，相信聪明的你此时已经想到了，StatefulSet 也是直接控制 Pod 对象的，那么它是不是也在使用 ControllerRevision 进行版本管理呢？</p><p>没错。在 Kubernetes 项目里，ControllerRevision 其实是一个通用的版本管理对象。这样，Kubernetes 项目就巧妙地避免了每种控制器都要维护一套冗余的代码和逻辑的问题。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料:"></a>参考资料:</h3><p>张磊—&lt;深入剖析Kubernetes&gt;: 容器化守护进程的意义：DaemonSet</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;DaemonSet&quot;&gt;&lt;a href=&quot;#DaemonSet&quot; class=&quot;headerlink&quot; title=&quot;DaemonSet&quot;&gt;&lt;/a&gt;DaemonSet&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;顾名思义,DaemonSet的主要作用是让你在kubernetes集群里运行一个Daemon Pod.所以,这个Pod有如下三个特征:&lt;/p&gt;
&lt;p&gt;1.这个Pod运行在kubernetes集群的每一个节点(Node)上&lt;/p&gt;
&lt;p&gt;2.每个节点只有一个这样的Pod实例&lt;/p&gt;
&lt;p&gt;3.当有新的节点加入 Kubernetes 集群后，该 Pod 会自动地在新节点上被创建出来；而当旧节点被删除后，它上面的 Pod 也相应地会被回收掉.&lt;/p&gt;
&lt;p&gt;这个机制听起来很简单，但 Daemon Pod 的意义确实是非常重要的。我随便给你列举几个例子：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;各种网络插件的 Agent 组件，都必须运行在每一个节点上，用来处理这个节点上的容器网络；&lt;/li&gt;
&lt;li&gt;各种存储插件的 Agent 组件，也必须运行在每一个节点上，用来在这个节点上挂载远程存储目录，操作容器的 Volume 目录；&lt;/li&gt;
&lt;li&gt;各种监控组件和日志组件，也必须运行在每一个节点上，负责这个节点上的监控信息和日志搜集。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
      <category term="controller" scheme="https://jesse.top/categories/kubernetes/controller/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Kong自定义配置Nginx</title>
    <link href="https://jesse.top/2021/01/19/Linux-Web/Kong%E8%87%AA%E5%AE%9A%E4%B9%89%E9%85%8D%E7%BD%AENginx/"/>
    <id>https://jesse.top/2021/01/19/Linux-Web/Kong自定义配置Nginx/</id>
    <published>2021-01-19T03:59:58.000Z</published>
    <updated>2021-02-06T16:04:56.932Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kong自定义配置Nginx"><a href="#Kong自定义配置Nginx" class="headerlink" title="Kong自定义配置Nginx"></a>Kong自定义配置Nginx</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>Kong是基于Nginx实现代理转发.官方的 <code>nginx.conf</code> 配置文件过于简单.如果需要优化nginx的性能,就需要修改默认的nginx配置文件,或者重新自定义一个nginx配置文件.</p><p>具体方法可以参考官方文档: <a href="https://docs.konghq.com/2.2.x/configuration/#environment-variables" target="_blank" rel="noopener">https://docs.konghq.com/2.2.x/configuration/#environment-variables</a></p><p>下面介绍2种方式自定义nginx的配置</p><h3 id="通过环境变量注入"><a href="#通过环境变量注入" class="headerlink" title="通过环境变量注入"></a>通过环境变量注入</h3><p>Kong服务启动时会每次都新建一个新的nginx配置文件.可以通过将nginx指令注入到 <code>kong.conf</code> 配置文件中从而配置到这个新的nginx配置文件</p><a id="more"></a> <h4 id="注入Nginx单个指令"><a href="#注入Nginx单个指令" class="headerlink" title="注入Nginx单个指令"></a>注入Nginx单个指令</h4><p>注入到Kong的环境变量一般包含下面2种前缀.前缀名不同代表注入的nginx指令作用在不同的作用域下.Kong会将环境变量的前缀去掉,然后将环境变量的后面部分注入到nginx.</p><ul><li><code>nginx_http_</code> 该前缀环境变量会被注入到Nginx的http代码块</li><li><code>nginx_proxy_</code> 该前缀会被注入到nginx的server代码块</li></ul><p>例如.如果注入以下环境变量到 <code>kong.conf</code> 配置文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nginx_proxy_large_client_header_buffers=16 128k</span><br></pre></td></tr></table></figure><p>Kong会将以下环境变量注入到Nginx配置文件的代理 <code>server</code> 块中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">large_client_header_buffers 16 128k;</span><br></pre></td></tr></table></figure><p>下面的环境变量,会被注入到nginx的http块中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export KONG_NGINX_HTTP_OUTPUT_BUFFERS=&quot;4 64k&quot;</span><br><span class="line"></span><br><span class="line">#注入以下Nginx指令</span><br><span class="line">output_buffers 4 64k;</span><br></pre></td></tr></table></figure><blockquote><p>还有一种前缀 <code>Nginx_admin_</code> 这个作用在kong的admin api,所以用的较少</p></blockquote><h4 id="注入Nginx代码块"><a href="#注入Nginx代码块" class="headerlink" title="注入Nginx代码块"></a>注入Nginx代码块</h4><p>对于一些复杂的配置场景,比如需要将整个server代码块添加到Nginx配置文件.可以使用上面的环境变量注入的方式,注入一个 <code>include</code> 指令到Nginx配置文件.</p><p>例如下面这个nginx的server代码块文件.假如该文件名为 <code>my-server.conf</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># custom server</span><br><span class="line">server &#123;</span><br><span class="line">  listen 2112;</span><br><span class="line">  location / &#123;</span><br><span class="line">    # ...more settings...</span><br><span class="line">    return 200;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以通过下面的方式添加到 <code>kong.conf</code> 配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nginx_http_include = /path/to/your/my-server.conf</span><br></pre></td></tr></table></figure><p>或者通过环境变量方式注入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export KONG_NGINX_HTTP_INCLUDE=&quot;/path/to/your/my-server.conf&quot;</span><br></pre></td></tr></table></figure><p>这样当Kong启动后,server代码块会被添加到Nginx的配置文件.</p><blockquote><p>这里也可以使用相对路径来注入一个server代码块的配置文件,但是配置文件需要在 <code>kong.conf</code> 配置文件的prefix路径之下.或者kong启动时候通过 <code>-p</code> 参数自定义的prefix路径之下</p></blockquote><h3 id="自定义Nginx模板"><a href="#自定义Nginx模板" class="headerlink" title="自定义Nginx模板"></a>自定义Nginx模板</h3><p>kong在启动的时候会根据 <code>/usr/local/share/lua/5.1/kong/templates/nginx.lua</code>和 <code>/usr/local/share/lua/5.1/kong/templates/nginx_kong.lua</code> 这2个lua模板来自动生成nginx的配置文件.当Kong启动后会自动在prefix路径下生成 <code>nginx.conf</code> 和 <code>nginx-kong.conf</code> .前者是Nginx的主配置文件,然后通过include方式引入了 <code>nginx_kong.conf</code> </p><p>当kong启动后,会产生下面2个文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/kong</span><br><span class="line">    - nginx.conf</span><br><span class="line">    - nginx-kong.conf</span><br></pre></td></tr></table></figure><blockquote><p>在 <a href="https://github.com/kong/kong/tree/master/kong/templates下也存放了kong的默认模板文件" target="_blank" rel="noopener">https://github.com/kong/kong/tree/master/kong/templates下也存放了kong的默认模板文件</a>.</p></blockquote><p>所以在 <code>usr/local/kong</code> 目录下直接修改 <code>Nginx.conf</code> 配置文件无法永久生效.当kong重启时,配置文件会被默认的Lua目标所覆盖和替代</p><p>如果一定要自定义nginx配置文件.可以自定义nginx的模板文件来替代 <code>Nginx.lua</code> .然后在该模板文件里引入 <code>nginx-kong.conf</code> </p><h4 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h4><ol><li>拷贝 <code>nginx.conf</code> 配置文件为  <code>nginx.conf.template</code> </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/local/kong/nginx.conf nginx.conf.template</span><br></pre></td></tr></table></figure><ol><li>自定义配置 <code>nginx.conf.template</code> .例如下面是我的配置文件内容</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">pid pids/nginx.pid;</span><br><span class="line">error_log logs/error.log $&#123;&#123;LOG_LEVEL&#125;&#125;; </span><br><span class="line"></span><br><span class="line"># injected nginx_main_* directives</span><br><span class="line">daemon off;</span><br><span class="line">worker_processes auto;</span><br><span class="line">worker_rlimit_nofile 204800;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    # injected nginx_events_* directives</span><br><span class="line">    multi_accept on;</span><br><span class="line">    use epoll;</span><br><span class="line">    worker_connections  204800;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line">    sendfile            on;</span><br><span class="line">    tcp_nopush          on;</span><br><span class="line">    tcp_nodelay         on;</span><br><span class="line">    include &apos;nginx-kong.conf&apos;;</span><br><span class="line"></span><br><span class="line">    keepalive_timeout  60;</span><br><span class="line">    keepalive_requests 1024;</span><br><span class="line">    client_header_buffer_size 4k;</span><br><span class="line">    large_client_header_buffers 4 32k;</span><br><span class="line"></span><br><span class="line">    types_hash_max_size 2048;</span><br><span class="line">    client_body_timeout 180;</span><br><span class="line">    client_header_timeout 10;</span><br><span class="line">    send_timeout 240;</span><br><span class="line"></span><br><span class="line">    proxy_connect_timeout   1000ms;</span><br><span class="line">    proxy_send_timeout      5000ms;</span><br><span class="line">    proxy_read_timeout      5000ms;</span><br><span class="line">    proxy_buffers           64 8k;</span><br><span class="line">    proxy_busy_buffers_size    128k;</span><br><span class="line">    proxy_temp_file_write_size 64k;</span><br><span class="line">    proxy_redirect off;</span><br><span class="line">    proxy_next_upstream off;</span><br><span class="line">    </span><br><span class="line">    gzip on;</span><br><span class="line">    gzip_min_length 1k;</span><br><span class="line">    gzip_buffers 4 16k;</span><br><span class="line">    gzip_http_version 1.0;</span><br><span class="line">    gzip_comp_level 2;</span><br><span class="line">    gzip_types text/plain application/x-javascript text/css application/xml;</span><br><span class="line">    gzip_vary on;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>注意该配置文件内的指令不能和 <code>nginx-kong.conf</code> 配置文件有同名或者冲突.否则kong无法启动</p></blockquote><ol><li>重新启动Kong.使用下面的参数指定自定义的Nginx模板文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kong start -c /etc/kong/kong.conf --nginx-conf nginx.conf.template</span><br></pre></td></tr></table></figure><h4 id=""><a href="#" class="headerlink" title=" "></a> </h4><h4 id="docker运行kong"><a href="#docker运行kong" class="headerlink" title="docker运行kong"></a>docker运行kong</h4><p>如果是docker方式运行.可以使用 <code>Dockerfile</code> 自定义kong镜像</p><p>以下是Dockerfile文件内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FROM kong:2.2.0</span><br><span class="line">COPY nginx.conf.template /usr/local/kong/nginx.conf.template</span><br><span class="line">CMD kong start --nginx-conf /usr/local/kong/nginx.conf.template</span><br></pre></td></tr></table></figure><p>编译docker镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t dwd-kong:2.2.0 .</span><br></pre></td></tr></table></figure><p>重启运行docker.但是要先在kong容器运行 <code>kong migrations up</code> 和 <code>kong migrations finish</code> 命令.所以 <code>docker-compose.yml</code> 配置文件内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">kong:</span><br><span class="line">    image: hub.doweidu.com/beta/dwd-kong:2.2.0</span><br><span class="line">    container_name: kong</span><br><span class="line">    hostname: kong</span><br><span class="line">    environment:</span><br><span class="line">      - KONG_DATABASE=postgres</span><br><span class="line">      - KONG_PG_HOST=kong-database</span><br><span class="line">      - KONG_PROXY_ACCESS_LOG=/var/log/kong/access.log</span><br><span class="line">      - KONG_ADMIN_ACCESS_LOG=/var/log/kong/admin_access.log</span><br><span class="line">      - KONG_PROXY_ERROR_LOG=/var/log/kong/error.log</span><br><span class="line">      - KONG_ADMIN_ERROR_LOG=/var/log/kong/admin_error.log</span><br><span class="line">      - KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl</span><br><span class="line">      - KONG_TRUSTED_IPS=0.0.0.0/0,::/0</span><br><span class="line">      - KONG_REAL_IP_HEADER=X-Forwarded-For</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/logs/kong:/var/log/kong</span><br><span class="line">      - /etc/localtime:/etc/localtime</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8000:8000&quot;</span><br><span class="line">      - &quot;8443:8443&quot;</span><br><span class="line">      - &quot;8001:8001&quot;</span><br><span class="line">      - &quot;8444:8444&quot;</span><br><span class="line">    expose:</span><br><span class="line">      - &quot;8000&quot;</span><br><span class="line">      - &quot;8443&quot;</span><br><span class="line">      - &quot;8001&quot;</span><br><span class="line">      - &quot;8444&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - dev-net</span><br><span class="line">    restart: always</span><br><span class="line">    depends_on:</span><br><span class="line">        - kong-database</span><br><span class="line">        - kong-migration</span><br><span class="line">        - kong-migration-finish</span><br><span class="line">        </span><br><span class="line">kong-migration:</span><br><span class="line">    image: hub.doweidu.com/beta/dwd-kong:2.2.0</span><br><span class="line">   # command: &quot;kong migrations bootstrap&quot;</span><br><span class="line">    command: &quot;kong migrations up&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - dev-net</span><br><span class="line">    restart: on-failure</span><br><span class="line">    environment:</span><br><span class="line">      KONG_PG_HOST: kong-database</span><br><span class="line">    depends_on:</span><br><span class="line">      - kong-database</span><br><span class="line"></span><br><span class="line">  kong-migration-finish:</span><br><span class="line">    image: hub.doweidu.com/beta/dwd-kong:2.2.0</span><br><span class="line">   # command: &quot;kong migrations bootstrap&quot;</span><br><span class="line">    command: &quot;kong migrations finish&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - dev-net</span><br><span class="line">    restart: on-failure</span><br><span class="line">    environment:</span><br><span class="line">      KONG_PG_HOST: kong-database</span><br><span class="line">    depends_on:</span><br><span class="line">      - kong-database</span><br><span class="line">      - kong-migration</span><br></pre></td></tr></table></figure><p>启动容器后.可以查看配置文件是否生效:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[work@docker docker-compose]$docker exec kong cat /usr/local/kong/nginx.conf</span><br><span class="line">pid pids/nginx.pid;</span><br><span class="line">error_log logs/error.log notice;</span><br><span class="line"></span><br><span class="line"># injected nginx_main_* directives</span><br><span class="line">daemon off;</span><br><span class="line">worker_processes auto;</span><br><span class="line">worker_rlimit_nofile 204800;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    # injected nginx_events_* directives</span><br><span class="line">    multi_accept on;</span><br><span class="line">    use epoll;</span><br><span class="line">    worker_connections  204800;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line">    sendfile            on;</span><br><span class="line">    tcp_nopush          on;</span><br><span class="line">    tcp_nodelay         on;</span><br><span class="line">    include &apos;nginx-kong.conf&apos;;</span><br><span class="line"> .......略...........</span><br></pre></td></tr></table></figure><p>如此,便实现了自定义kong的nginx配置文件,这在大并发场景中可能需要优化nginx的转发性能.如果是小规模场景中,可以使用Kong的默认的Nginx配置文件即可.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kong自定义配置Nginx&quot;&gt;&lt;a href=&quot;#Kong自定义配置Nginx&quot; class=&quot;headerlink&quot; title=&quot;Kong自定义配置Nginx&quot;&gt;&lt;/a&gt;Kong自定义配置Nginx&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;Kong是基于Nginx实现代理转发.官方的 &lt;code&gt;nginx.conf&lt;/code&gt; 配置文件过于简单.如果需要优化nginx的性能,就需要修改默认的nginx配置文件,或者重新自定义一个nginx配置文件.&lt;/p&gt;
&lt;p&gt;具体方法可以参考官方文档: &lt;a href=&quot;https://docs.konghq.com/2.2.x/configuration/#environment-variables&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://docs.konghq.com/2.2.x/configuration/#environment-variables&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;下面介绍2种方式自定义nginx的配置&lt;/p&gt;
&lt;h3 id=&quot;通过环境变量注入&quot;&gt;&lt;a href=&quot;#通过环境变量注入&quot; class=&quot;headerlink&quot; title=&quot;通过环境变量注入&quot;&gt;&lt;/a&gt;通过环境变量注入&lt;/h3&gt;&lt;p&gt;Kong服务启动时会每次都新建一个新的nginx配置文件.可以通过将nginx指令注入到 &lt;code&gt;kong.conf&lt;/code&gt; 配置文件中从而配置到这个新的nginx配置文件&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>Kong实现限流</title>
    <link href="https://jesse.top/2021/01/19/Linux-Web/Kong%20Rate%20Limiting%E9%99%90%E6%B5%81%E6%8F%92%E4%BB%B6/"/>
    <id>https://jesse.top/2021/01/19/Linux-Web/Kong Rate Limiting限流插件/</id>
    <published>2021-01-19T03:59:58.000Z</published>
    <updated>2021-01-19T14:39:47.543Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kong实现限流"><a href="#Kong实现限流" class="headerlink" title="Kong实现限流"></a>Kong实现限流</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>近期发现公司某个业务对外的openapi接口的/merchantapi路径异常调用非常频繁.公司的第三方商户需要通过这个路径来调用ERP接口,但是经常发生被恶意刷接口的情况,导致公司的业务服务器资源使用率飙升,面临很大的宕机风险和隐患.</p><p>目前外部客户端访问公司业务仍然是阿里云SLB—–Nginx—php-fpm的架构.由于Nginx的限流能力并不出色,特别是针对具体path路径的限流.所以,引入了Kong api网关</p><h3 id="Rate-Limiting限流插件介绍"><a href="#Rate-Limiting限流插件介绍" class="headerlink" title="Rate Limiting限流插件介绍"></a>Rate Limiting限流插件介绍</h3><p>Rate Limiting是Kong社区版就已经自带的官方流量控制插件.详细信息可以参考Kong官网介绍. <a href="https://docs.konghq.com/hub/kong-inc/rate-limiting/" target="_blank" rel="noopener">https://docs.konghq.com/hub/kong-inc/rate-limiting/</a></p><p>它可以针对<code>consumer</code> ,<code>credential</code> ,<code>ip</code> ,<code>service</code>,<code>path</code>,<code>header</code> 等多种维度来进行限流.流量控制的精准度也有多种方式可以参考,比如可以做到秒级,分钟级,小时级等限流控制.</p><h4 id="响应客户端头部信息"><a href="#响应客户端头部信息" class="headerlink" title="响应客户端头部信息"></a>响应客户端头部信息</h4><p>当启用这个插件后.Kong会响应客户端一些额外的头部信息,告诉客户端限流信息.例如下面是Kong响应给客户端的header信息,告诉客户端当前的限流策略是10r/s</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RateLimit-Limit: 10</span><br><span class="line">RateLimit-Remaining: 0</span><br><span class="line">RateLimit-Reset: 1</span><br><span class="line"></span><br><span class="line">X-Kong-Response-Latency: 1</span><br><span class="line">X-RateLimit-Limit-Second: 10</span><br><span class="line">X-RateLimit-Remaining-Second: 0</span><br></pre></td></tr></table></figure><p>如果客户端的访问请求超过限流的阈值,Kong会返回status<code>429</code>的状态码以及下面的错误信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; &quot;message&quot;: &quot;API rate limit exceeded&quot; &#125;</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="限流策略对Kong性能影响"><a href="#限流策略对Kong性能影响" class="headerlink" title="限流策略对Kong性能影响"></a>限流策略对Kong性能影响</h4><p>Rate limiting插件支持3种限流策略.</p><p><code>cluster</code> 集群策略.Kong的数据库会维护一个计数器,并且在所有的Kong集群内每个节点共享这个计数器.如果计数器触发限流上线,所有的Kong节点都拒绝客户端的转发.这就意味着每个节点接收到客户端的请求,都会对数据库进行读写操作.</p><p><code>redis</code> redis策略和<code>cluster</code> 相似,唯一不同的是,计数器是存储在redis数据中.并且在集群内所有节点共享.</p><p><code>local</code> 本地策略.计数器保存在Kong节点服务器本地内存缓冲区.并且计数器只对该节点有效.这意味着<code>local</code>策略有最好的性能表现.但是由于计数器存储在本地.所以限流的精度没有<code>redis</code>和<code>cluster</code> 准确.并且会影响Kong节点服务器弹性扩容(比如限流设置30r/s,Kong集群从2个节点扩容到4个节点.限流就从60r/s变成了120r/s.此时需要手动将限流设置从30r/s降低到15r/s)</p><blockquote><p>或者,可以在Kong前面配置一个hash转发策略的负载均衡,将同一个外部客户端的请求代理到同一个节点.这样local策略的精确度可以提升,并且kong节点的弹性扩容不会影响限流效果</p></blockquote><p>下面是3种限流策略的对比表</p><table><thead><tr><th>policy</th><th>describe</th><th>pros</th><th>cons</th></tr></thead><tbody><tr><td>cluster</td><td>集群策略</td><td>限流精准度高,不需要第三方组件支持</td><td>对Kong性能影响比较大</td></tr><tr><td>redis</td><td>redis策略</td><td>限流精准度高,对Kong性能影响较低</td><td>需要额外的redis服务</td></tr><tr><td>local</td><td>本地策略</td><td>对Kong性能影响最低</td><td>精准度比较差,Kong节点扩容和缩容需要手动调整限流速率</td></tr></tbody></table><p>下面是以上集群策略的使用场景:</p><ul><li>如果对流量精确度要求非常高.比如金融,交易等.那么适合redis或者cluster的限流策略</li><li>如果是为了保护后端服务,避免大流量带来的服务器过载.那么适合local限流策略,这种场景对限流的精度要求不高</li></ul><h3 id="针对客户端IP限流"><a href="#针对客户端IP限流" class="headerlink" title="针对客户端IP限流"></a>针对客户端IP限流</h3><p>我们场景中针对客户端IP进行限流.但是由于Kong是在SLB或者Nginx的负载均衡后面,所以默认情况下,Kong采用的IP是上一级负载均衡器的IP.此时就需要将客户端的真实IP传递到Kong,并且使用该IP作为<code>remote_ip</code> .实现方法如下:</p><ul><li>虚拟机运行Kong</li></ul><p>针对rpm包或者其他方式安装的Kong服务,可以修改默认的<code>/etc/kong/kong.conf</code> 配置文件.加入下面2行配置信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trusted_ips = 0.0.0.0/0,::/0</span><br><span class="line">real_ip_header = X-Forwarded-For</span><br></pre></td></tr></table></figure><p>重载kong配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kong reload</span><br></pre></td></tr></table></figure><ul><li>docker容器方式运行kong</li></ul><p>针对docker容器方式运行的Kong,修改配置文件不方便,此时可以通过变量注入的方式自定义配置<code>kong.conf</code> 配置文件.还可以通过这种方式注入nginx自定义配置,具体可以参考官方的文档介绍:<a href="https://docs.konghq.com/2.2.x/configuration/#environment-variables" target="_blank" rel="noopener">environment-variables</a></p><p>例如,上面的2行配置内容可以通过在配置参数前面加<code>KONG_</code>以及大写的参数名的方式注入环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">KONG_TRUSTED_IPS=0.0.0.0/0,::/0</span><br><span class="line">KONG_REAL_IP_HEADER=X-Forwarded-For</span><br></pre></td></tr></table></figure><p>修改Kong的<code>docker-compose</code>文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">kong:</span><br><span class="line">    image: dwd-kong:2.2.0  #自定义kong镜像.也可以使用官方的docker镜像</span><br><span class="line">    container_name: kong</span><br><span class="line">    hostname: kong</span><br><span class="line">    environment:</span><br><span class="line">      - KONG_DATABASE=postgres</span><br><span class="line">      - KONG_PG_HOST=kong-database</span><br><span class="line">      - KONG_PROXY_ACCESS_LOG=/var/log/kong/access.log</span><br><span class="line">      - KONG_ADMIN_ACCESS_LOG=/var/log/kong/admin_access.log</span><br><span class="line">      - KONG_PROXY_ERROR_LOG=/var/log/kong/error.log</span><br><span class="line">      - KONG_ADMIN_ERROR_LOG=/var/log/kong/admin_error.log</span><br><span class="line">      - KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl</span><br><span class="line">      - KONG_TRUSTED_IPS=0.0.0.0/0,::/0       #增加这两行</span><br><span class="line">      - KONG_REAL_IP_HEADER=X-Forwarded-For   #增加这两行</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/logs/kong:/var/log/kong</span><br><span class="line">      - /etc/localtime:/etc/localtime</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8000:8000&quot;</span><br><span class="line">      - &quot;8443:8443&quot;</span><br><span class="line">      - &quot;8001:8001&quot;</span><br><span class="line">      - &quot;8444:8444&quot;</span><br><span class="line">    expose:</span><br><span class="line">      - &quot;8000&quot;</span><br><span class="line">      - &quot;8443&quot;</span><br><span class="line">      - &quot;8001&quot;</span><br><span class="line">      - &quot;8444&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - dev-net</span><br><span class="line">    restart: always</span><br><span class="line">    depends_on:</span><br><span class="line">        - kong-database</span><br><span class="line">        - kong-migration</span><br><span class="line">        - kong-migration-finish</span><br></pre></td></tr></table></figure><h3 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h3><p>Rate Limiting插件由Kong默认提供,所以无需自行安装.由于是针对<code>/merchantapi</code> 这个借口进行限流,所以只需配置该route,并且将插件应用到这个route下即可.由于我日常使用的python进行Kong的配置,所以这里只列出我的python配置文件中相关配置.不演示具体配置了.</p><blockquote><p>使用kong的dashboard也可以很方便的实现配置</p></blockquote><ul><li>配置service</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hsq_openapi_dev = &#123; &quot;name&quot;: &quot;hsq_openapi_dev&quot;,</span><br><span class="line">            &quot;host&quot;: &quot;kong.devapi.hsq.net&quot;,</span><br><span class="line">            &quot;port&quot;: 80,</span><br><span class="line">            &quot;protocol&quot;: &quot;http&quot;,</span><br><span class="line">            &quot;path&quot;: &quot;/&quot;</span><br><span class="line">          &#125;</span><br></pre></td></tr></table></figure><ul><li>配置route</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#默认转发路由</span><br><span class="line">hsq_openapi_dev =   &#123;  &quot;service_name&quot;:&quot;hsq_openapi_dev&quot;,</span><br><span class="line">                &quot;data&quot;:&#123;</span><br><span class="line">                &quot;name&quot;: &quot;hsq-openapi-dev&quot;,</span><br><span class="line">                &quot;hosts&quot;: &quot;m.devapi.hsq.net&quot;,</span><br><span class="line">                &quot;strip_path&quot;: &quot;false&quot;,</span><br><span class="line">                &quot;protocols&quot;: [&quot;http&quot;, &quot;https&quot;],</span><br><span class="line">                &quot;paths&quot;: &quot;/&quot;,</span><br><span class="line">                &quot;methods&quot;: [&quot;GET&quot;, &quot;POST&quot;,&quot;PUT&quot;,&quot;OPTIONS&quot;,&quot;DELETE&quot;]&#125;</span><br><span class="line">             &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#限流接口转发路由</span><br><span class="line">hsq_merchant_api_limit = &#123;  &quot;service_name&quot;:&quot;hsq_openapi_dev&quot;,</span><br><span class="line">                    &quot;data&quot;:&#123;</span><br><span class="line">                    &quot;name&quot;: &quot;hsq_merchant_api_limit&quot;,</span><br><span class="line">                    &quot;hosts&quot;: &quot;m.devapi.hsq.net&quot;,</span><br><span class="line">                    &quot;strip_path&quot;: &quot;false&quot;,</span><br><span class="line">                    &quot;protocols&quot;: [&quot;http&quot;, &quot;https&quot;],</span><br><span class="line">                    &quot;paths&quot;: &quot;/merchantapi&quot;,</span><br><span class="line">                    &quot;methods&quot;: [&quot;GET&quot;, &quot;POST&quot;,&quot;PUT&quot;,&quot;OPTIONS&quot;,&quot;DELETE&quot;]&#125;</span><br><span class="line">                 &#125;</span><br></pre></td></tr></table></figure><ul><li>配置rate-limiting插件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hsq_merchantapi_limit = &#123; &quot;route_name&quot;: &quot;hsq_merchant_api_limit&quot;,  #关联到上面的route.表示该插件作用在route级别</span><br><span class="line">         &quot;data&quot;: &#123;</span><br><span class="line">         &quot;name&quot;: &quot;rate-limiting&quot;, #插件名称</span><br><span class="line">         &quot;config.second&quot;: 10, # 限流.每秒10个请求</span><br><span class="line">         &quot;config.policy&quot;: &quot;local&quot;, #限流策略</span><br><span class="line">         &quot;config.limit_by&quot;: &quot;ip&quot;    #针对客户端IP限流</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>运行python脚本,配置Kong</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~/Desktop/kong-python   master ●✚  python3 kong.py</span><br><span class="line">请输入你要配置的Kong的环境名称,例如:dev,beta,prod:&gt;&gt;&gt;dev</span><br><span class="line">正在创建service:hsq_openapi_dev</span><br><span class="line">service:hsq_openapi_dev创建成功</span><br><span class="line">开始创建routes:hsq-openapi-dev</span><br><span class="line">routes路由hsq-openapi-dev创建成功</span><br><span class="line">开始创建routes:hsq_merchant_api_limit</span><br><span class="line">routes路由hsq_merchant_api_limit创建成功</span><br><span class="line">plugins:rate-limiting创建成功.绑定在route路由:hsq_merchant_api_limit中</span><br></pre></td></tr></table></figure><h3 id="压测效果"><a href="#压测效果" class="headerlink" title="压测效果"></a>压测效果</h3><p>为了验证插件效果,这里使用<code>ab</code> 这个简单的压测工具进行测试.</p><p>1.开启一个终端,执行下面的命令.压测命令运行了1.18秒,只有20个请求成功响应,其余80个请求失败.这恰好符合了rate-limiting插件每秒10个请求的限流策略</p><blockquote><p>由于是在dev环境,所有只有一个Kong节点.如果外部流量负载均衡分发到Kong集群的所有节点,那么总体的限流应该是:Kong节点数量x限流数量</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">ab -n 100 -c 10 https://m.devapi.hsq.net/merchantapi</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">Document Path:          /merchantapi</span><br><span class="line">Document Length:        122 bytes</span><br><span class="line"></span><br><span class="line">Concurrency Level:      10</span><br><span class="line">Time taken for tests:   1.180 seconds</span><br><span class="line">Complete requests:      100         #总共100个请求</span><br><span class="line">Failed requests:        80          #失败了80个</span><br><span class="line">   (Connect: 0, Receive: 0, Length: 80, Exceptions: 0)</span><br><span class="line">Non-2xx responses:      80</span><br><span class="line">Total transferred:      41888 bytes</span><br><span class="line">HTML transferred:       5720 bytes</span><br><span class="line">Requests per second:    84.71 [#/sec] (mean)</span><br><span class="line">Time per request:       118.044 [ms] (mean)</span><br><span class="line">Time per request:       11.804 [ms] (mean, across all concurrent requests)</span><br><span class="line">Transfer rate:          34.65 [Kbytes/sec] received</span><br><span class="line">......</span><br></pre></td></tr></table></figure><ol start="2"><li>将请求继续增大,同时使用curl和浏览器访问该域名.发现请求被拒绝</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl https://m.devapi.hsq.net/merchantapi</span><br><span class="line">&#123;</span><br><span class="line">  &quot;message&quot;:&quot;API rate limit exceeded&quot;</span><br><span class="line">&#125;%</span><br></pre></td></tr></table></figure><p><img src="https://img2.jesse.top/20210119164349.png" alt=""></p><ol start="3"><li>在压测的同时,使用另外一个客户端来同时访问该接口,可以正常访问</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.99.1 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 200 122 &quot;-&quot; &quot;curl/7.29.0&quot;   #其他客户端仍然可以正常访问</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kong实现限流&quot;&gt;&lt;a href=&quot;#Kong实现限流&quot; class=&quot;headerlink&quot; title=&quot;Kong实现限流&quot;&gt;&lt;/a&gt;Kong实现限流&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;近期发现公司某个业务对外的openapi接口的/merchantapi路径异常调用非常频繁.公司的第三方商户需要通过这个路径来调用ERP接口,但是经常发生被恶意刷接口的情况,导致公司的业务服务器资源使用率飙升,面临很大的宕机风险和隐患.&lt;/p&gt;
&lt;p&gt;目前外部客户端访问公司业务仍然是阿里云SLB—–Nginx—php-fpm的架构.由于Nginx的限流能力并不出色,特别是针对具体path路径的限流.所以,引入了Kong api网关&lt;/p&gt;
&lt;h3 id=&quot;Rate-Limiting限流插件介绍&quot;&gt;&lt;a href=&quot;#Rate-Limiting限流插件介绍&quot; class=&quot;headerlink&quot; title=&quot;Rate Limiting限流插件介绍&quot;&gt;&lt;/a&gt;Rate Limiting限流插件介绍&lt;/h3&gt;&lt;p&gt;Rate Limiting是Kong社区版就已经自带的官方流量控制插件.详细信息可以参考Kong官网介绍. &lt;a href=&quot;https://docs.konghq.com/hub/kong-inc/rate-limiting/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://docs.konghq.com/hub/kong-inc/rate-limiting/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;它可以针对&lt;code&gt;consumer&lt;/code&gt; ,&lt;code&gt;credential&lt;/code&gt; ,&lt;code&gt;ip&lt;/code&gt; ,&lt;code&gt;service&lt;/code&gt;,&lt;code&gt;path&lt;/code&gt;,&lt;code&gt;header&lt;/code&gt; 等多种维度来进行限流.流量控制的精准度也有多种方式可以参考,比如可以做到秒级,分钟级,小时级等限流控制.&lt;/p&gt;
&lt;h4 id=&quot;响应客户端头部信息&quot;&gt;&lt;a href=&quot;#响应客户端头部信息&quot; class=&quot;headerlink&quot; title=&quot;响应客户端头部信息&quot;&gt;&lt;/a&gt;响应客户端头部信息&lt;/h4&gt;&lt;p&gt;当启用这个插件后.Kong会响应客户端一些额外的头部信息,告诉客户端限流信息.例如下面是Kong响应给客户端的header信息,告诉客户端当前的限流策略是10r/s&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;RateLimit-Limit: 10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;RateLimit-Remaining: 0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;RateLimit-Reset: 1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X-Kong-Response-Latency: 1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X-RateLimit-Limit-Second: 10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X-RateLimit-Remaining-Second: 0&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果客户端的访问请求超过限流的阈值,Kong会返回status&lt;code&gt;429&lt;/code&gt;的状态码以及下面的错误信息&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123; &amp;quot;message&amp;quot;: &amp;quot;API rate limit exceeded&amp;quot; &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>kafka-3.1消费者与消费组</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/3-%E6%B6%88%E8%B4%B9%E8%80%85/3.1%20%E6%B6%88%E8%B4%B9%E8%80%85%E4%B8%8E%E6%B6%88%E8%B4%B9%E7%BB%84/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/3-消费者/3.1 消费者与消费组/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:49:55.160Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-1-消费者与消费组"><a href="#3-1-消费者与消费组" class="headerlink" title="3.1 消费者与消费组"></a>3.1 消费者与消费组</h2><h3 id="1-消费者和消费组介绍"><a href="#1-消费者和消费组介绍" class="headerlink" title="1.消费者和消费组介绍"></a>1.消费者和消费组介绍</h3><p>消费者( Consumer)负责订阅Kafka中的主题( Topic)，并且从订阅的主题上拉取消息.与其他一些消息中间件不同的是:在 Kafka的消费理念中还有一层消费组( Consumer Group)的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者 。</p><p>以下图为例,某个主题中共有 4 个分区( Partition) : PO、 Pl、 P2、 P3。 有两个消费组 A和 B 都订阅了这个主题，消费组 A 中有 4 个消费者 (CO、 Cl、 C2 和 C3)，消费组 B 中有 2个消费者 CC4 和 CS) 。按照 Kafka默认的规则，最后的分配结果是消费组 A 中的每一个消费 者分配到1个分区，消费组 B 中的每一个消费者分配到 2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。换言之 每一个分区只能被一个消费组中的一个消费者所消费.</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608547700645-c525d96a-ac02-471f-9276-ee885e471c86.png" alt="image.png"></p><a id="more"></a> <p>假设目前某消费组内只有一个消费者 co，订阅了一个主题，这个主题包含 7 个分区: PO、 Pl、 P2、 P3、 P4、PS、 P6o 也就是说，这个消费者co订阅了7个分区，具体分配情形参考图3-2。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608547782387-e3cd0cab-f862-465d-bfd7-96c7fa613b21.png" alt="image.png"></p><p>​                                        </p><p>此时消费组内又加入了一个新的消费者 Cl，按照既定的逻辑，需要将原来消费者 co 的部分分区分配给消费者 Cl 消费 ， 如图 3-3 所示 。 消费者 co 和 Cl 各自负责消费所分配到的分区 ，彼此之间并无逻辑上的干扰 </p><p>消费者与消费组这种模型可以让整体的消费能力具备横向伸缩性，我们 可以增加(或减少) 消费者的个数来提高 (或降低〕整体的消费能力 。 对于分区数固定的情况， 一昧地增加消费者并不会让消费能力 一直得到提升，<strong>如果消费者过多，出现了消费者的个数大于分区个数的情况，**</strong>就会有消费者分配不到任何分区**。</p><h3 id="2-两种消息投递模式"><a href="#2-两种消息投递模式" class="headerlink" title="2.两种消息投递模式"></a>2.两种消息投递模式</h3><p>对于消息中间件而言,一般有两种消息投递模式:<strong>点对点</strong>(P2P, Point-to-Point)模式和<strong>发**</strong>布/订阅**( Pub/Sub)模式.</p><p><strong>点对点模式</strong>是基于队列的，消息生产者发送消息到队列，消息消费者从队列中接收消息。</p><p><strong>发布订阅模式</strong>定义了如何向一个内容节点发布和订阅消息,这个内容节点称为主题(Topic),主题可以认为是消息传递的中介,消息发布者将消息发布到某个主题,而消息订阅者从主题中订阅消息.主题使得消息的订阅者和发布者互相保持独立,不需要进行接触即可保证消息的传递,发布/订阅模式在消息的一对多广播时采用.Kafka同时支持两种消息投递模式，而这正是得益于消费者与消费组模型的契合:</p><ul><li>如果所有的消费者都隶属于同一个消费组,那么所有的消息都会被均衡地投递给每一个消费者,即每条消息只会被一个消费者处理,这就相当于点对点模式的应用 。</li><li>如果所有的消费者都隶属于不同的消费组,那么所有的消息都会被广播给所有的消费者,即每条消息会被所有的消费者处理,这就相当于发布/订阅模式的应用.</li></ul><p>消费组是一个逻辑上的概念，它将旗下的消费者归为一类 ，每一个消费者只隶属于一个消费组。每一个消费组都会有一个固定的名称，消费者在进行消费前需要指定其所属消费组的名称，这个可以通过消费者客户端参数 group.id来配置，默认值为空宇符串。</p><p>消费者并非逻辑上的概念它是实际的应用实例它可以是一个线程，也可以是一个进程。同一个消费组内的消费者既可以部署在同一台机器上，也可以部署在不同的机器上。</p><p>​                                        </p><p>​                                        </p><p>​                                                                                </p><p>​                                        </p><p>​                                        </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-1-消费者与消费组&quot;&gt;&lt;a href=&quot;#3-1-消费者与消费组&quot; class=&quot;headerlink&quot; title=&quot;3.1 消费者与消费组&quot;&gt;&lt;/a&gt;3.1 消费者与消费组&lt;/h2&gt;&lt;h3 id=&quot;1-消费者和消费组介绍&quot;&gt;&lt;a href=&quot;#1-消费者和消费组介绍&quot; class=&quot;headerlink&quot; title=&quot;1.消费者和消费组介绍&quot;&gt;&lt;/a&gt;1.消费者和消费组介绍&lt;/h3&gt;&lt;p&gt;消费者( Consumer)负责订阅Kafka中的主题( Topic)，并且从订阅的主题上拉取消息.与其他一些消息中间件不同的是:在 Kafka的消费理念中还有一层消费组( Consumer Group)的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者 。&lt;/p&gt;
&lt;p&gt;以下图为例,某个主题中共有 4 个分区( Partition) : PO、 Pl、 P2、 P3。 有两个消费组 A和 B 都订阅了这个主题，消费组 A 中有 4 个消费者 (CO、 Cl、 C2 和 C3)，消费组 B 中有 2个消费者 CC4 和 CS) 。按照 Kafka默认的规则，最后的分配结果是消费组 A 中的每一个消费 者分配到1个分区，消费组 B 中的每一个消费者分配到 2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。换言之 每一个分区只能被一个消费组中的一个消费者所消费.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.nlark.com/yuque/0/2020/png/2992889/1608547700645-c525d96a-ac02-471f-9276-ee885e471c86.png&quot; alt=&quot;image.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="3-消费者" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/3-%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-4.1主题管理</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/4.1%E4%B8%BB%E9%A2%98%E7%AE%A1%E7%90%86/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/4-主题和分区/4.1主题管理/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:55:25.320Z</updated>
    
    <content type="html"><![CDATA[<h3 id="4-1-1-创建主题"><a href="#4-1-1-创建主题" class="headerlink" title="4.1.1 创建主题"></a>4.1.1 创建主题</h3><h4 id="4-1-1-1-自动创建主题以及分区副本"><a href="#4-1-1-1-自动创建主题以及分区副本" class="headerlink" title="4.1.1.1 自动创建主题以及分区副本"></a>4.1.1.1 自动创建主题以及分区副本</h4><p>在之前的笔记中提到了创建主题的一个简单示例.kafka提供 <code>kafka-topics.sh</code> 脚本来创建主题.下面这个示例创建了一个 <code>topic-test</code> 的主题,包含4个分区和2个副本.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-test --replication-factor 2 --partitions 4</span><br><span class="line"></span><br><span class="line">Created topic &quot;topic-test&quot;.</span><br></pre></td></tr></table></figure><p>分区创建完成后,会在kafka的 <code>log.dirs</code> 或者 <code>log.dir</code> 的目录下创建相应的主题分区.下面是在其中一台Broker节点的信息展示:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ ls /opt/logs/kafka/ | grep &quot;topic-test&quot;</span><br><span class="line">topic-test-0</span><br><span class="line">topic-test-2</span><br></pre></td></tr></table></figure><p>可以看到152节点中创建了2个文件夹 topic-test-0 和 topic-test-2,对应主题 topic-test的2个分区编号为0和2的分区，命名方式可以概括为 <code>&lt;topic&gt;-&lt;partition&gt;</code> .严谨地说,其实这类文件夹对应的不是分区,分区同主题一样是一个逻辑的概念而没有物理上的存在.并且这里我们也只是看到了2个分区,而我们创建的是4个分区,其余2个分区被分配到了153和154节点中，参考如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#153节点</span><br><span class="line">[hadoop@bi-dev153 ~]$ ls /opt/logs/kafka/ | grep &quot;topic-test&quot;</span><br><span class="line">topic-test-0</span><br><span class="line">topic-test-1</span><br><span class="line">topic-test-3</span><br><span class="line"></span><br><span class="line">#154节点</span><br><span class="line">[hadoop@bi-dev154 ~]$ ls /opt/logs/kafka/ | grep &quot;topic-test&quot;</span><br><span class="line">topic-test-1</span><br><span class="line">topic-test-2</span><br><span class="line">topic-test-3</span><br></pre></td></tr></table></figure><p>三个broker节点一共创建了8个文件夹,这个数字8实质上是分区数4与副本因子2的乘积.每个副本(或者更确切地说应该是日志,副本与日志一一对应)才真正对应 了一个命名形式.</p><a id="more"></a> <p><strong>主题</strong>,<strong>分区,副本和日志</strong>的关系如下图所示.<strong>主题</strong>和<strong>分区</strong>是提供给上层用户的抽象,而在副本层面(或者更确切的说是Log日志层面)才会实际物理存在.</p><p>同一个分区中的多个副本必须分布在不同broker中,并且一个分区副本同时存在多个broker中,这样才能提供有效的数据冗余.上面的示例中,每个副本都分布在至少2台不同的broker中.</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608631181435-40a87763-f28c-45c4-a25e-d85651c26d2e.png" alt="image.png"></p><h4 id="4-1-1-2-手动创建主题以及分区副本"><a href="#4-1-1-2-手动创建主题以及分区副本" class="headerlink" title="4.1.1.2 手动创建主题以及分区副本"></a>4.1.1.2 手动创建主题以及分区副本</h4><p>通过 <code>kafka-topics.sh</code> 脚本创建的主题会按照内部既定逻辑来分配分区和副本到Broker节点上.其实该脚本还提供一个 <code>replica-assignment</code> 参数来手动指定分区副本的分配方案.用法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">格式为: 分区1broker节点1:分区1broker节点2,分区2broker节点1:分区2broker节点2.副本集合用冒号隔开,分区之间用逗号隔开</span><br><span class="line">--replica-assignment broker_id_for_partition1_replica1:broker_id_for_partition1_replica2,broker_id_for_partition2_replica1:broker_id_for_partition2_replica2.......</span><br></pre></td></tr></table></figure><p>例如下面这个实例通过手动方式创建了一个和 <code>topic-test</code> 一样分区副本分配的 <code>topic-test-same</code> 主题.</p><p>下面是刚创建的自动分配的topic-test主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic &quot;topic-test&quot;</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br></pre></td></tr></table></figure><p>通过 <code>--replica-assignment</code> 手动指定分区副本分配情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-test-same --replica-assignment 153:152,154:153,152:154,153:154</span><br></pre></td></tr></table></figure><blockquote><p>–replica-assignment参数其实就是逗号隔开的所有分区的Replicas副本集合.副本集合内部用:冒号隔开</p></blockquote><p>查看 <code>topic-test-same</code> 分区信息.和 <code>topic-test</code> 主题分区副本分配一致</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic &quot;topic-test-same&quot;</span><br><span class="line">Topic:topic-test-same   PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test-same  Partition: 0    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">    Topic: topic-test-same  Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test-same  Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test-same  Partition: 3    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br></pre></td></tr></table></figure><p>手动分配分区副本需要遵循以下原则,否则会报错:</p><ul><li>同一个分区内的副本不能有重复.比如153:153</li><li>分区之间所指定的副本数量要相同.比如153:154,152,154:152</li><li>不能跳过一个分区.比如153:154,,154:152</li></ul><h4 id="4-1-1-3-自定义相关参数"><a href="#4-1-1-3-自定义相关参数" class="headerlink" title="4.1.1.3 自定义相关参数"></a>4.1.1.3 自定义相关参数</h4><p>在创建主题时,还可以通过 <code>config</code> 参数设置要创建主题的相关参数.可以覆盖原本的默认配置参数. <code>config</code> 可以指定多个参数.用法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--config 参数名=值 --config 参数名=值 ......</span><br></pre></td></tr></table></figure><p>下面示例使用 <code>config</code> 参数创建主题 <code>topic-config</code>.并且携带2个参数 :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-config --replication-factor 1 --partitions 1 --config cleanup.policy=compact --config max.message.bytes=10000</span><br><span class="line">Created topic &quot;topic-config&quot;.</span><br></pre></td></tr></table></figure><p>查看主题信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:1    ReplicationFactor:1 Configs:cleanup.policy=compact,max.message.bytes=10000</span><br><span class="line">    Topic: topic-config Partition: 0    Leader: 154 Replicas: 154   Isr: 154</span><br></pre></td></tr></table></figure><p>通过zk也能查看到config信息,config信息保存在 <code>/config/topics/TOPIC_NAME</code> 目录下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] get /config/topics/topic-config</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;config&quot;:&#123;&quot;max.message.bytes&quot;:&quot;10000&quot;,&quot;cleanup.policy&quot;:&quot;compact&quot;&#125;&#125;</span><br></pre></td></tr></table></figure><h4 id="4-1-1-4-总结"><a href="#4-1-1-4-总结" class="headerlink" title="4.1.1.4 总结"></a>4.1.1.4 总结</h4><p>创建主题时需要遵循几个原则</p><ul><li>主题名不能重复,否则会报错.(使用 <code>if-not-exists</code> 参数可以避免出现报错信息,但是不会成功创建一个同名主题)</li><li>主题名不推荐使用__双下划线开头的命名,双下划线开头主题一般看做Kafka的内部主题</li><li>主题名由大小写祖母,数字,点号,下划线,连接线等组成.不能只有特殊符号</li></ul><p><code>kafka-topics.sh</code> 创建主题信息支持以下参数:</p><ul><li><p><code>--create</code> 创建主题</p></li><li><ul><li><code>--replica-assignment</code> 手动创建主题的分区副本分配</li><li><code>--config</code> 手动指定参数</li></ul></li></ul><h3 id="4-1-2-查看主题的分区和副本信息"><a href="#4-1-2-查看主题的分区和副本信息" class="headerlink" title="4.1.2 查看主题的分区和副本信息"></a>4.1.2 查看主题的分区和副本信息</h3><h4 id="4-1-2-1-查看具体某个topic的信息"><a href="#4-1-2-1-查看具体某个topic的信息" class="headerlink" title="4.1.2.1.查看具体某个topic的信息"></a>4.1.2.1.查看具体某个topic的信息</h4><p><code>kafka-topics.sh</code> 脚本提供了 <code>--describe</code> 参数来查看一个topic的信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic &quot;topic-test&quot;</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br></pre></td></tr></table></figure><p>在上面的示例中,命令行提供了以下几个信息:</p><p>一共有3个broker节点:152,153,154</p><p><code>PartitionCount</code> 表示一共有3个分区</p><p><code>ReplicationFactor</code> 副本因子为2</p><p><code>Leader</code> 表示某个分区对应的leader副本在具体的Broker节点</p><p><code>Replicas</code> 表示分区内所有AR副本的集合</p><p><code>Isr</code> 表示ISR副本集合</p><h4 id="4-1-2-2-查看所有topic的信息"><a href="#4-1-2-2-查看所有topic的信息" class="headerlink" title="4.1.2.2 查看所有topic的信息"></a>4.1.2.2 查看所有topic的信息</h4><p>如果 <code>kafka-topics.sh</code> 脚本没有指定具体的 <code>--topic</code> 字段.则会展示所有的topic主题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe | head</span><br><span class="line">Topic:__consumer_offsets    PartitionCount:50   ReplicationFactor:1 Configs:segment.bytes=104857600,cleanup.policy=compact,compression.type=producer</span><br><span class="line">    Topic: __consumer_offsets   Partition: 0    Leader: 153 Replicas: 153   Isr: 153</span><br><span class="line">    Topic: __consumer_offsets   Partition: 1    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: __consumer_offsets   Partition: 2    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">    Topic: __consumer_offsets   Partition: 3    Leader: 153 Replicas: 153   Isr: 153</span><br><span class="line">    Topic: __consumer_offsets   Partition: 4    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: __consumer_offsets   Partition: 5    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">    Topic: __consumer_offsets   Partition: 6    Leader: 153 Replicas: 153   Isr: 153</span><br><span class="line">    Topic: __consumer_offsets   Partition: 7    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: __consumer_offsets   Partition: 8    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">  .....略.......</span><br></pre></td></tr></table></figure><h4 id="4-1-2-3-zookeeper查看topic信息"><a href="#4-1-2-3-zookeeper查看topic信息" class="headerlink" title="4.1.2.3 zookeeper查看topic信息"></a>4.1.2.3 zookeeper查看topic信息</h4><p>zookeeper提供了 <code>zkCli.sh</code> 客户端.使用客户端连接zookeeper:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/zookeeper-3.4.10/bin/zkCli.sh  -server localhost:2181</span><br><span class="line">[zk: localhost:2181(CONNECTED) 0]</span><br></pre></td></tr></table></figure><p>zookeeer的 <code>/brokers/topics</code> 目录下保存了主题的分区副本分片方案.通过查看这个目录即可查看主题的分区和副本信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] get /brokers/topics/topic-test</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:&#123;&quot;2&quot;:[152,154],&quot;1&quot;:[154,153],&quot;3&quot;:[153,154],&quot;0&quot;:[153,152]&#125;&#125;</span><br></pre></td></tr></table></figure><p>如上示例所示, <code>&quot;2&quot;:[152,154]</code> 表示分区2分配了2个副本,分别在152和153这2个broker节点上.</p><h4 id="4-1-2-4-查看kafka集群当前所有主题"><a href="#4-1-2-4-查看kafka集群当前所有主题" class="headerlink" title="4.1.2.4 查看kafka集群当前所有主题"></a>4.1.2.4 查看kafka集群当前所有主题</h4><p><code>--list</code> 参数可以列出当前的所有topic</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --list</span><br><span class="line">__consumer_offsets</span><br><span class="line">delivery_message</span><br><span class="line">example</span><br><span class="line">example1</span><br><span class="line">goods_center</span><br><span class="line">hsq-aliapp</span><br><span class="line">hsq-wxapp</span><br><span class="line">hsq_online_test</span><br><span class="line">monitor_report_app_log</span><br><span class="line">sample_consumer_dlq</span><br><span class="line">tidb_test</span><br><span class="line">topic-config</span><br><span class="line">topic-demo</span><br><span class="line">topic-demo1</span><br><span class="line">topic-test</span><br><span class="line">topic-test-same</span><br></pre></td></tr></table></figure><h4 id="4-1-2-5-查看主题其他详细信息"><a href="#4-1-2-5-查看主题其他详细信息" class="headerlink" title="4.1.2.5 查看主题其他详细信息"></a>4.1.2.5 查看主题其他详细信息</h4><p><code>kafka-topics.sh</code> 脚本的 <code>describe</code> 参数还支持很多额外的指令,用于查看更详细的信息.</p><p>1.<strong><code>--topics-with-overrides</code></strong> 参数表示查看覆盖配置的主题,列出包含了与集群不一样配置的主题.下面列出了 <code>topic-config</code> 这个主题,这个主题使用了 <code>--config</code> 参数创建</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topics-with-overrides</span><br><span class="line"></span><br><span class="line">Topic:topic-config  PartitionCount:1    ReplicationFactor:1 Configs:cleanup.policy=compact,max.message.bytes=10000</span><br></pre></td></tr></table></figure><p>2.<strong><code>--under-replicated-paritions</code></strong> 参数列出包含失效副本的分区.失效副本的分区可能正在进行同步操作,也有可能同步发生异常.此时分区的ISR集合小于AR集合.失效副本的分区是重点监控对象,因为这可能意味着集群中的某个broker已经失效或者同步效率降低等.</p><p>正常情况下此命令不会出现任何信息.例如查看主题 <code>topic-demo</code> 的失效副本信息,但是没有任何输出信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo --under-replicated-partitions</span><br></pre></td></tr></table></figure><p>此时将153这个节点下线.再次查看:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo --under-replicated-partitions</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 152,154</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 154 Replicas: 153,154,152   Isr: 154,152</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 154,152</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 152,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>可以看到Leader和ISR集合中都没有了153这个节点.将153节点上线.此时再次查询,恢复正常.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo --under-replicated-partitions</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>\3. <strong><code>unavailable-partitions</code></strong> 参数可以查看主题中没有leader副本的分区.这些分区已经处于离线状态,对于生产者或者消费者来说不可用.</p><p>同样正常情况下,该命令没有展示任何信息.</p><p>例如,下面的 <code>topic-test</code> 主题有4个分区,每个分区有2个副本.其中分区1和分区3的副本ISR是153和154这2个节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 153 Replicas: 153,152   Isr: 152,153</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: 153 Replicas: 153,154   Isr: 154,153</span><br></pre></td></tr></table></figure><p>现在停掉153和154这2个节点的kafka进程.使用 <code>unavailable-partitions</code> 参数查看分区信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test --unavailable-partitions</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: -1  Replicas: 154,153   Isr: 154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: -1  Replicas: 153,154   Isr: 154</span><br><span class="line">  </span><br><span class="line">  [hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 152 Replicas: 153,152   Isr: 152</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: -1  Replicas: 154,153   Isr: 154</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: -1  Replicas: 153,154   Isr: 154</span><br></pre></td></tr></table></figure><p>leader显示为-1,表示没有可用leader</p><p>节点恢复后,再次执行该命令,没有任何显示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test --unavailable-partitions</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><h4 id="4-1-2-6-总结"><a href="#4-1-2-6-总结" class="headerlink" title="4.1.2.6 总结"></a>4.1.2.6 总结</h4><p><code>kafka-topics.sh</code> 查看主题信息支持以下参数:</p><ul><li><p><code>--describe</code> </p></li><li><ul><li>默认展示所有topic的分区副本信息</li><li><code>--topic TOPIC_NAME</code> 展示具体某个topic主题的分区副本信息</li><li><code>--topics-with-overrides</code> 列出覆盖配置参数的主题</li><li><code>--under-replicated-partitions</code> 列出失效副本的主题分区信息</li><li><code>--unavailable-partitions</code> 列出没有副本的主题分区</li></ul></li><li><p><code>--list</code> 列出kafka集群下的所有topic主题名称</p></li></ul><h3 id="4-1-3-修改主题"><a href="#4-1-3-修改主题" class="headerlink" title="4.1.3 修改主题"></a>4.1.3 修改主题</h3><h4 id="4-1-3-1-修改主题分区数量"><a href="#4-1-3-1-修改主题分区数量" class="headerlink" title="4.1.3.1 修改主题分区数量"></a>4.1.3.1 修改主题分区数量</h4><p>当一个主题被修改后,依然允许我们对其做一定的修改,比如修改分区个数,修改配置等.这个功能就是 <code>kafka-topic.sh</code> 脚本中的 <code>alter</code> 指令提供的.</p><p>以 <code>topic-config</code> 主题为例,该主题下只有一个分区.将分区修改为3:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --partitions 3</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Adding partitions succeeded!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:3    ReplicationFactor:1 Configs:cleanup.policy=compact,max.message.bytes=10000</span><br><span class="line">    Topic: topic-config Partition: 0    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: topic-config Partition: 1    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">    Topic: topic-config Partition: 2    Leader: 153 Replicas: 153   Isr: 153</span><br></pre></td></tr></table></figure><p><code>--partition</code> 参数表示扩展后的分区个数.</p><blockquote><p>注意告警信息.如果主题中的消息包含key(key不为Null)时,根据key计算分区的行为就会受到影响.当分区数为1时,所以key的消息都会发送到这个分区.当分区扩展到3,会根据消息的key来计算区号.原本发往分区0的消息可能会发送到分区1或者2.此外,还会影响既定消息的顺序.</p></blockquote><p>对于基于key计算的主题,不建议修改分区数量.在一开始就设置好分区数量.另外需要注意的是,Kafka不支持减少分区.只能增加不能减少.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --partitions 1</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line"></span><br><span class="line">Error while executing topic command : The number of partitions for a topic can only be increased</span><br></pre></td></tr></table></figure><blockquote><p>不支持减少分区主要是考虑到保障kafka的消息可靠性和顺序性,事务性问题.</p></blockquote><p>如果修改一个不存在的主题分区,则会报错.添加 <code>--if-exists</code> 参数会忽略一些异常</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-none-exist  --partitions 3</span><br><span class="line">Error while executing topic command : Topic topic-none-exist does not exist on ZK path localhost:2181</span><br><span class="line"></span><br><span class="line">#使用--if-exists参数,没有报错,但是不会产生任何效果</span><br><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-none-exist  --if-exists --partitions 3</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><h4 id=""><a href="#" class="headerlink" title=" "></a> </h4><h4 id="4-1-3-2-修改主题配置"><a href="#4-1-3-2-修改主题配置" class="headerlink" title="4.1.3.2 修改主题配置"></a>4.1.3.2 修改主题配置</h4><p>还可以使用 <code>kafka-topics.sh</code> 脚本的 <code>alter</code> 指令修改主题的配置.在创建主题的时候通过 <code>config</code> 参数来设置要创建的主题相关参数.在创建完主题之后,还可以通过 <code>alter</code> 和 <code>config</code> 配合增加或者修改一些配置文件覆盖原有的值</p><p>下面例子演示修改主题 <code>topic-config</code> 的 <code>max.message.bytes</code> 配置.从10000修改到20000</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --config max.message.bytes=20000</span><br><span class="line">WARNING: Altering topic configuration from this script has been deprecated and may be removed in future releases.</span><br><span class="line">         Going forward, please use kafka-configs.sh for this functionality</span><br><span class="line">Updated config for topic &quot;topic-config&quot;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:3    ReplicationFactor:1 Configs:max.message.bytes=20000,cleanup.policy=compact</span><br></pre></td></tr></table></figure><p>通过 <code>alter</code> 也可以删除创建主题时候的自定义配置.使用 <code>--delete-config</code> 参数.下面这个例子中删除了 <code>max.message.bytes</code> 配置.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --delete-config max.message.bytes</span><br><span class="line">WARNING: Altering topic configuration from this script has been deprecated and may be removed in future releases.</span><br><span class="line">         Going forward, please use kafka-configs.sh for this functionality</span><br><span class="line">Updated config for topic &quot;topic-config&quot;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:3    ReplicationFactor:1 Configs:cleanup.policy=compact</span><br></pre></td></tr></table></figure><blockquote><p>注意.在对config配置进行增删改查时候,都会提示建议使用kafka-configs.sh这个脚本来实现.该脚本的使用方式下面马上讲到</p></blockquote><h3 id="4-1-4-配置管理"><a href="#4-1-4-配置管理" class="headerlink" title="4.1.4 配置管理"></a>4.1.4 配置管理</h3><p><code>kafka-configs.sh</code> 脚本专门用来对配置进行操作.可以在运行状态下动态更改配置.也可以查询主题的相关配置.而且该脚本不仅可以支持主题相关配置修改,还可以修改broker,用户和客户端这3个类型的配置</p><p><code>kafka-configs.sh</code> 脚本使用 <code>entity-type</code> 参数指定操作配置的类型, <code>entity-name</code> 参数指定操作配置的名称.</p><h4 id="4-1-4-1-查询配置"><a href="#4-1-4-1-查询配置" class="headerlink" title="4.1.4.1 查询配置"></a>4.1.4.1 查询配置</h4><p>下面这个例子查看主题 <code>topic-config</code> 的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics --entity-name topic-config</span><br><span class="line">Configs for topic &apos;topic-config&apos; are cleanup.policy=compact</span><br></pre></td></tr></table></figure><p><code>--entity-type</code> 指定查看的实体类型.支持以下几种类型:</p><ul><li>topics</li><li>clients</li><li>users</li><li>brokers</li></ul><p><code>--entity-name</code> 配置的实体名称:</p><ul><li>topic name (主题名称)</li><li>client id (客户端ID)</li><li>user principal name (用户名)</li><li>broker id (kafka节点ID)</li></ul><p>如果不指定 <code>--entity-name</code> 参数则会查询所有的 <code>entity-type</code> 对应的所有配置信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics</span><br><span class="line">Configs for topic &apos;topic-config&apos; are</span><br><span class="line">Configs for topic &apos;__consumer_offsets&apos; are segment.bytes=104857600,cleanup.policy=compact,compression.type=producer</span><br><span class="line">......</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>通过zookeeper也可以查询主题的配置信息.路径为 <code>/config/topics/TOPIC_NAME</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] get /config/topics/topic-config</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;config&quot;:&#123;&quot;cleanup.policy&quot;:&quot;compact&quot;,&quot;max.message.bytes&quot;:&quot;20000&quot;&#125;&#125;</span><br></pre></td></tr></table></figure><h4 id="4-1-4-2-修改配置"><a href="#4-1-4-2-修改配置" class="headerlink" title="4.1.4.2 修改配置"></a>4.1.4.2 修改配置</h4><p>使用 <code>alter</code> 对配置进行变更.需要配合 <code>add-config</code> 或者 <code>delete-config</code> 这2个参数一起使用.</p><p><code>add-config</code> 参数实现配置的增,改</p><p>下面的例子中,为主题 <code>topic-config</code> 添加 <code>max.message.bytes</code> 参数配置和 <code>cleanup.policy</code> 参数配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name topic-config --add-config cleanup.policy=compact,max.message.bytes=20000</span><br><span class="line">Completed Updating config for entity: topic &apos;topic-config&apos;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics --entity-name topic-config</span><br><span class="line">Configs for topic &apos;topic-config&apos; are cleanup.policy=compact,max.message.bytes=20000</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p><code>delete-config</code> 参数可以实现配置删除.</p><p>下面的例子中,删除上面的2个配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name topic-config --delete-config cleanup.policy,max.message.bytes</span><br><span class="line">Completed Updating config for entity: topic &apos;topic-config&apos;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics --entity-name topic-config</span><br><span class="line">Configs for topic &apos;topic-config&apos; are</span><br></pre></td></tr></table></figure><h3 id="4-1-5-删除主题"><a href="#4-1-5-删除主题" class="headerlink" title="4.1.5 删除主题"></a>4.1.5 删除主题</h3><p>如果确定不再使用一个主题,那么最好的方式是将其删除.这样可以释放一些资源,比如磁盘,文件句柄等. <code>kafka-topics.sh</code> 脚本中的 <code>delete</code> 命令可以用来删除主题.比如下面删除主题 <code>topic-demo1</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --delete --topic topic-demo1</span><br><span class="line">Topic topic-demo1 is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br></pre></td></tr></table></figure><blockquote><p>注意.必须将kafka服务器配置文件的delete.topic.enable选项设置为true才能删除.这个参数的默认值是false.删除主题的操作会被忽略.主题并没有被删除</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --list | grep topic-demo1</span><br><span class="line">topic-demo1</span><br></pre></td></tr></table></figure><p>编辑配置文件 <code>/opt/kafka/config/server.properties</code> 修改下面的参数为true</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Switch to enable topic deletion or not, default value is false</span><br><span class="line">delete.topic.enable=true</span><br></pre></td></tr></table></figure><p>如果删除一个kafka的内部主题,那么会报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --delete --topic __consumer_offsets</span><br><span class="line">Error while executing topic command : Topic __consumer_offsets is a kafka internal topic and is not allowed to be marked for deletion.</span><br></pre></td></tr></table></figure><p>删除一个不存在的主题也会报错,此时可以通过 <code>if-exists</code> 参数来忽略异常.</p><h3 id="4-1-5-总结"><a href="#4-1-5-总结" class="headerlink" title="4.1.5 总结"></a>4.1.5 总结</h3><p>下面这张图是 <code>kafka-topics.sh</code> 脚本的常用参数</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608805092662-042718a3-9a6a-489e-a987-db4e38217171.png" alt="image.png"></p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608805117734-2bd94456-e966-41b1-9465-dde20a8a9129.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;4-1-1-创建主题&quot;&gt;&lt;a href=&quot;#4-1-1-创建主题&quot; class=&quot;headerlink&quot; title=&quot;4.1.1 创建主题&quot;&gt;&lt;/a&gt;4.1.1 创建主题&lt;/h3&gt;&lt;h4 id=&quot;4-1-1-1-自动创建主题以及分区副本&quot;&gt;&lt;a href=&quot;#4-1-1-1-自动创建主题以及分区副本&quot; class=&quot;headerlink&quot; title=&quot;4.1.1.1 自动创建主题以及分区副本&quot;&gt;&lt;/a&gt;4.1.1.1 自动创建主题以及分区副本&lt;/h4&gt;&lt;p&gt;在之前的笔记中提到了创建主题的一个简单示例.kafka提供 &lt;code&gt;kafka-topics.sh&lt;/code&gt; 脚本来创建主题.下面这个示例创建了一个 &lt;code&gt;topic-test&lt;/code&gt; 的主题,包含4个分区和2个副本.&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;/opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-test --replication-factor 2 --partitions 4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Created topic &amp;quot;topic-test&amp;quot;.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;分区创建完成后,会在kafka的 &lt;code&gt;log.dirs&lt;/code&gt; 或者 &lt;code&gt;log.dir&lt;/code&gt; 的目录下创建相应的主题分区.下面是在其中一台Broker节点的信息展示:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev152 ~]$ ls /opt/logs/kafka/ | grep &amp;quot;topic-test&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;可以看到152节点中创建了2个文件夹 topic-test-0 和 topic-test-2,对应主题 topic-test的2个分区编号为0和2的分区，命名方式可以概括为 &lt;code&gt;&amp;lt;topic&amp;gt;-&amp;lt;partition&amp;gt;&lt;/code&gt; .严谨地说,其实这类文件夹对应的不是分区,分区同主题一样是一个逻辑的概念而没有物理上的存在.并且这里我们也只是看到了2个分区,而我们创建的是4个分区,其余2个分区被分配到了153和154节点中，参考如下:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;#153节点&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev153 ~]$ ls /opt/logs/kafka/ | grep &amp;quot;topic-test&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#154节点&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev154 ~]$ ls /opt/logs/kafka/ | grep &amp;quot;topic-test&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;三个broker节点一共创建了8个文件夹,这个数字8实质上是分区数4与副本因子2的乘积.每个副本(或者更确切地说应该是日志,副本与日志一一对应)才真正对应 了一个命名形式.&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="4-主题和分区" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-2.1Kafka副本</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/2-%E5%89%AF%E6%9C%AC%E4%BB%8B%E7%BB%8D/2.1%20%20Kafka%E5%89%AF%E6%9C%AC/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/2-副本介绍/2.1  Kafka副本/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T15:14:57.936Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2-1-Kafka副本"><a href="#2-1-Kafka副本" class="headerlink" title="2.1  Kafka副本"></a>2.1  Kafka副本</h1><h3 id="副本介绍"><a href="#副本介绍" class="headerlink" title="副本介绍"></a>副本介绍</h3><p>Kafka为分区引入了副本(Replica)机制.通过增加副本数量提升容灾能力.一个Topic主题可以有多个分区,一个分区又可以有多个副本.这多个副本中，只有一个是leader，而其他的都是follower副本。仅有leader副本可以对外提供服务。所以副本之间是一主多从的关系,而且每个副本中保存的相同的消息.(严格来说,同一时刻副本之间的消息并非能一定完全同步)</p><p>多个follower副本通常存放在和leader副本不同的broker中。通过这样的机制实现了高可用，当某台机器挂掉后，其他follower副本也能迅速”转正“，开始对外提供服务。</p><a id="more"></a> <p>在kafka中，实现副本的目的就是冗余备份，且仅仅是冗余备份，所有的读写请求都是由leader副本进行处理的。follower副本仅有一个功能，那就是从leader副本拉取消息，尽量让自己跟leader副本的内容一致。</p><blockquote><p>follower副本之所以不能对外提供服务,主要是为了保障数据一致性</p></blockquote><p>下图是一个多副本架构图.</p><p>Kafka集群中有4个broker，某个主题中有3个分区，且副本因子（即副本个数）也为3，如此每个分区便有1个leader副本和2个follower副本。生产者和消费者只与leader副本进行交互，而follower副本只负责消息的同步，很多时候follower副本中的消息相对leader副本而言会有一定的滞后。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608450120303-2d66cd2e-611a-4e3e-a507-53b4f81adfa0.png" alt="image.png"></p><h3 id="副本同步"><a href="#副本同步" class="headerlink" title="副本同步"></a>副本同步</h3><p><strong>AR</strong>: 分区内的所有副本统称.</p><p><strong>ISR</strong>: In-Sync Replicas.所有与Leader副本保持一定程度同步的副本(包括Leader副本).一起组成ISR</p><p><strong>OSR</strong>: Out-of-Sync Replicas: 与leader副本同步滞后过多的副本(不包括leader副本),一起注册呢个OSR</p><p><strong>AR = ISR + OSR.</strong></p><blockquote><p>正常情况下,所有的follower副本都应该与leader副本保持一定程度的同步,即AR = ISR,OSR集合为空</p></blockquote><p>Leader副本负责维护和跟踪ISR集合中所有follower副本的滞后状态,当follower副本落后太多或者失效时,leader副本会把它从ISR集合中剔除,如果OSR的follower副本追上了leader副本,那会从OSR转移到ISR.</p><blockquote><p>默认情况下,只有ISR集合中的follower副本才有资格被选举为新的Leader</p></blockquote><h3 id="HW和LEO"><a href="#HW和LEO" class="headerlink" title="HW和LEO"></a>HW和LEO</h3><p><strong>HW(High Watermark)</strong>: 俗称高水位.它标识了一个特点的消息偏移量(offset).消费者只能拉取这个offset之前的信息.</p><p><strong>LEO(Log End Offset)</strong>: 标识当前日志文件中下一条代写入消息的offset. </p><p><strong></strong></p><p>下面一张图能说明这两个概念</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608450932841-d4f33228-91b2-484f-b28e-24d0762ef670.png" alt="image.png"></p><p>上面的图代表一个日志文件.这个日志文件中有 9 条消息，第一条消息的 offset（LogStartOffset）为0，最后一条消息的offset为8，offset为9的消息用虚线框表示，代表下一条待写入的消息。日志文件的HW为6，表示消费者只能拉取到offset在0至5之间的消息，而offset为6的消息对消费者而言是不可见的。</p><p>offset为9的位置即为当前日志文件的LEO，LEO的大小相当于当前日志分区中最后一条消息的offset值加1。分区ISR集合中的每个副本都会维护自身的LEO，<strong>而ISR集合中最小的LEO即为分区的HW，对消费者而言只能消费HW之前的消息。</strong></p><h3 id="ISR和HW-LEO的关系"><a href="#ISR和HW-LEO的关系" class="headerlink" title="ISR和HW,LEO的关系"></a>ISR和HW,LEO的关系</h3><p>为了让读者更好地理解ISR集合，以及HW和LEO之间的关系，下面通过一个简单的示例来进行相关的说明。如图1-5所示，假设某个分区的ISR集合中有3个副本，即一个leader副本和2个follower副本，此时分区的LEO和HW都为3。消息3和消息4从生产者发出之后会被先存入leader副本</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451481234-2309dd97-7662-49d0-a43a-42b822f7a7d4.png" alt="image.png"></p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451501313-69357f66-f23b-4d07-940f-ef9f0afb9260.png" alt="image.png"></p><p>在同步过程中，不同的 follower 副本的同步效率也不尽相同。如图 所示，在某一时刻follower1完全跟上了leader副本而follower2只同步了消息3，如此leader副本的LEO为5，follower1的LEO为5，follower2的LEO为4，那么当前分区的HW取最小值4，此时消费者可以消费到offset为0至3之间的消息。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451550216-2bc62531-a164-40a1-b039-e9e911401d02.png" alt="image.png"></p><p>如果所有的副本都成功写入了消息3和消息4，整个分区的HW和LEO都变为5，因此消费者可以消费到offset为4的消息了。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451625357-266f1693-1d4b-4524-9f64-89256893555e.png" alt="image.png"></p><p>Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的 follower 副本都复制完，这条消息才会被确认为已成功提交，这种复制方式极大地影响了性能。而在异步复制方式下，follower副本异步地从leader副本中复制数据，数据只要被leader副本写入就被认为已经成功提交。在这种情况下，如果follower副本都还没有复制完而落后于leader副本，突然leader副本宕机，则会造成数据丢失。Kafka使用的这种ISR的方式则有效地权衡了数据可靠性和性能之间的关系。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;2-1-Kafka副本&quot;&gt;&lt;a href=&quot;#2-1-Kafka副本&quot; class=&quot;headerlink&quot; title=&quot;2.1  Kafka副本&quot;&gt;&lt;/a&gt;2.1  Kafka副本&lt;/h1&gt;&lt;h3 id=&quot;副本介绍&quot;&gt;&lt;a href=&quot;#副本介绍&quot; class=&quot;headerlink&quot; title=&quot;副本介绍&quot;&gt;&lt;/a&gt;副本介绍&lt;/h3&gt;&lt;p&gt;Kafka为分区引入了副本(Replica)机制.通过增加副本数量提升容灾能力.一个Topic主题可以有多个分区,一个分区又可以有多个副本.这多个副本中，只有一个是leader，而其他的都是follower副本。仅有leader副本可以对外提供服务。所以副本之间是一主多从的关系,而且每个副本中保存的相同的消息.(严格来说,同一时刻副本之间的消息并非能一定完全同步)&lt;/p&gt;
&lt;p&gt;多个follower副本通常存放在和leader副本不同的broker中。通过这样的机制实现了高可用，当某台机器挂掉后，其他follower副本也能迅速”转正“，开始对外提供服务。&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="2-副本介绍" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/2-%E5%89%AF%E6%9C%AC%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-1.1基本概念介绍</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/1.1%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/1-概念介绍/1.1 基本概念介绍/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:48:01.269Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-1-基本概念介绍"><a href="#1-1-基本概念介绍" class="headerlink" title="1.1 基本概念介绍"></a>1.1 基本概念介绍</h1><h3 id="kafka特性"><a href="#kafka特性" class="headerlink" title="kafka特性"></a>kafka特性</h3><ul><li><strong>消息系统</strong>: Kafka和传统的消息中间件都具备流量削峰,缓冲,异步通信,扩展性等.另外,Kafka还提供了大多数消息中间件难以实现的消息顺序保障及回溯消费的功能</li><li><strong>存储系统</strong>: 消息持久化到存盘,可以实现永久存储</li><li><strong>流式处理平台</strong>: Kafka提供了流式处理类库</li></ul><a id="more"></a><h3 id="Kafka架构"><a href="#Kafka架构" class="headerlink" title="Kafka架构"></a>Kafka架构</h3><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608447536390-cba7d090-f67a-435c-8d7d-704fb446e573.png" alt="image.png"></p><p>一个Kafka体系主要包括:</p><ul><li>producer: 生产者</li><li>broker: kafka节点服务器</li><li>consumer: 消费者</li><li>zookeeper: 负责管理kafka集群元数据,集群选举等</li></ul><p>producer将消息发送到Broker,Broker负责将受到的消息存储到磁盘中,Consumer负责从Broker订阅并消费消息.</p><h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><ul><li>Kafka 通过 <em>topic</em> 对存储的流数据进行分类。</li><li>每条记录中包含一个key，一个value和一个timestamp（时间戳).</li><li>kafka保留所有的发布记录(无论是否已经被消费过).通过一个可配置的参数—保留期限来控制记录存在时间.</li></ul><blockquote><p>举个例子， 如果保留策略设置为2天，一条记录发布后两天内，可以随时被消费，两天过后这条记录会被抛弃并释放磁盘空间。</p></blockquote><h3 id="kafka核心API"><a href="#kafka核心API" class="headerlink" title="kafka核心API"></a>kafka核心API</h3><ul><li><a href="https://kafka.apachecn.org/documentation.html#producerapi" target="_blank" rel="noopener">Producer API</a> : 允许一个应用程序发布一串流式的数据到一个或者多个Kafka topic。</li><li><a href="https://kafka.apachecn.org/documentation.html#consumerapi" target="_blank" rel="noopener">Consumer API</a>: 允许一个应用程序订阅一个或多个 topic ，并且对发布给他们的流式数据进行处理。</li><li><a href="https://kafka.apachecn.org/documentation/streams" target="_blank" rel="noopener">Streams API</a>: 允许一个应用程序作为一个<em>流处理器</em>，消费一个或者多个topic产生的输入流，然后生产一个输出流到一个或多个topic中去，在输入输出流中进行有效的转换。</li><li><a href="https://kafka.apachecn.org/documentation.html#connect" target="_blank" rel="noopener">Connector API</a>: 允许构建并运行可重用的生产者或者消费者，将Kafka topics连接到已存在的应用程序或者数据系统。比如，连接到一个关系型数据库，捕捉表（table）的所有变更内容。</li></ul><h3 id="理解topics和Partition和offset"><a href="#理解topics和Partition和offset" class="headerlink" title="理解topics和Partition和offset"></a>理解topics和Partition和offset</h3><p><strong>Topic</strong>: 就是数据主题，生产者将消息发送到特点的主题.消费者负责订阅主题并进行消费.</p><p><strong>Partition</strong>: 一个Topic可以划分成多个partition(分区).但是一个分区只属于单个主题.很多时候也会将partition称为主题分区(Topic-Partition).同一个主题下的不同分区包含的消息也不同.分区在存储层面可以看做一个追加的日志(Log)文件.</p><p>一个主题的分区可以在不同的节点服务器上,所有的消息会均匀的分配到不同的分区中(也就是不同的节点服务器),这样可以提高磁盘IO和性能.在创建主题的时候可以设置分区数量,当然也可以在主题创建完成后去修改分区数量.通过增加分区的数量实现水平扩展.</p><p>好比是为公路运输，不同的起始点和目的地需要修不同高速公路（主题），高速公路上可以提供多条车道（分区），流量大的公路多修几条车道保证畅通，流量小的公路少修几条车道避免浪费。收费站好比消费者，车多的时候多开几个一起收费避免堵在路上，车少的时候开几个让汽车并道就好了</p><p>Kafka中的Topics总是多订阅者模式，一个topic可以拥有一个或者多个消费者来订阅它的数据。对于每一个topic， Kafka集群都会维持一个分区日志，如下所示：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608119181003-5ca1cc98-c0ec-4c00-b7b0-d0b916204ab0.png" alt="image.png"></p><p>每个partition分区都是有序切不可变的记录集.并且不断的追加到结构化的commit log文件.</p><p><strong>Offset</strong>: 消息被存储到分区的日志文件时会分片一个偏移量(offset).offset是消息在分区中的唯一表示.kafka通过它来保障消息在分区内的顺序.</p><p>不过Offset并不跨越分区,也就是说Kafka保证的是分区有序,而不是主题有序.</p><p>在每一个消费者中唯一保存的元数据是offset（偏移量）即消费在log中的位置.偏移量由消费者所控制:通常在读取记录后，消费者会以线性的方式增加偏移量，但是实际上，由于这个位置由消费者控制，所以消费者可以采用任何顺序来消费记录。例如，一个消费者可以重置到一个旧的偏移量，从而重新处理过去的数据；也可以跳过最近的记录，从”现在”开始消费。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608120423518-7cddb09d-e7fa-4f35-809f-908a36b5a4d1.png?x-oss-process=image%2Fresize%2Cw_1500" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-1-基本概念介绍&quot;&gt;&lt;a href=&quot;#1-1-基本概念介绍&quot; class=&quot;headerlink&quot; title=&quot;1.1 基本概念介绍&quot;&gt;&lt;/a&gt;1.1 基本概念介绍&lt;/h1&gt;&lt;h3 id=&quot;kafka特性&quot;&gt;&lt;a href=&quot;#kafka特性&quot; class=&quot;headerlink&quot; title=&quot;kafka特性&quot;&gt;&lt;/a&gt;kafka特性&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;消息系统&lt;/strong&gt;: Kafka和传统的消息中间件都具备流量削峰,缓冲,异步通信,扩展性等.另外,Kafka还提供了大多数消息中间件难以实现的消息顺序保障及回溯消费的功能&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;存储系统&lt;/strong&gt;: 消息持久化到存盘,可以实现永久存储&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;流式处理平台&lt;/strong&gt;: Kafka提供了流式处理类库&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="1-概念介绍" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-1.2 生产与消费简单实例</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/1.2%20%E7%94%9F%E4%BA%A7%E4%B8%8E%E6%B6%88%E8%B4%B9%E7%AE%80%E5%8D%95%E5%AE%9E%E4%BE%8B/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/1-概念介绍/1.2 生产与消费简单实例/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:48:34.104Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-2-生产与消费简单实例"><a href="#1-2-生产与消费简单实例" class="headerlink" title="1.2 生产与消费简单实例"></a>1.2 生产与消费简单实例</h1><h3 id="创建topic"><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h3><p>kafka提供了许多实用的脚本工具,存放在$KAFKA_HOME的bin目录下.其中与主题相关的就是kafka-topic.sh脚本.例如.下面创建一个分区数为4,副本为3的主题topic-demon<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-demo --replication-factor 3 --partitions 4</span><br><span class="line"></span><br><span class="line">Created topic <span class="string">"topic-demo"</span>.</span><br></pre></td></tr></table></figure></p><p><code>--zoopkeer</code> 指定kafka连接的zookeeper服务地址<br><code>--topic</code> 指定一个topic主题<br><code>--replication-factor</code>  指定副本因子数量<br><code>--partition</code> 指定分区数量<br><code>--create</code> 表示创建</p><a id="more"></a> <p>下面命令展示了刚创建的主题信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 bin]$ ./kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demoPartitionCount:4ReplicationFactor:3Configs:</span><br><span class="line">Topic: topic-demoPartition: 0Leader: 152Replicas: 152,153,154Isr: 152,153,154</span><br><span class="line">Topic: topic-demoPartition: 1Leader: 153Replicas: 153,154,152Isr: 153,154,152</span><br><span class="line">Topic: topic-demoPartition: 2Leader: 154Replicas: 154,152,153Isr: 154,152,153</span><br><span class="line">Topic: topic-demoPartition: 3Leader: 152Replicas: 152,154,153Isr: 152,154,153</span><br></pre></td></tr></table></figure></p><p>上面的命令结果表示 <code>topic-demon</code> 这个主题一共有4个分区,存放在3台Kafka broker服务器节点.3个broker均是ISR集合,没有OSR集合</p><blockquote><p>在任意一台kafka集群内的节点服务器上执行上述命令,会得到完全相同的结果</p></blockquote><h3 id="创建consumer"><a href="#创建consumer" class="headerlink" title="创建consumer"></a>创建consumer</h3><p><code>kafka-console-consumer.sh</code> 在任意一台kafka集群内的节点服务器上可以通过控制台创建一个 <code>consumer</code> 消费者.示例如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev154 bin]$ ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-demo</span><br></pre></td></tr></table></figure></p><p><code>--bootstrap-server</code> 指定连接的kafka集群地址<br><code>--topic</code> 指定消费者订阅的主题</p><h3 id="创建producer"><a href="#创建producer" class="headerlink" title="创建producer"></a>创建producer</h3><p><code>kafka-console-producer.sh</code> 在任意一台kafka集群内的节点服务器上可以通过控制台创建一个 <code>producer</code> 消费者.示例如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev153 bin]$ ./kafka-console-producer.sh --broker-list localhost:9092 --topic topic-demo</span><br></pre></td></tr></table></figure></p><p><code>--broker-list</code> 指定连接的kafka集群地址<br><code>--topic</code> 指定发小时时的主题<br>在弹出的shell终端中,输入 <code>hello world!</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;hello,world!</span><br></pre></td></tr></table></figure></p><p>回到 <code>consumer</code> 的shell终端界面,发现消费到了刚生产的消息:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev154 bin]$ ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-demo</span><br><span class="line">hello,world!</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-2-生产与消费简单实例&quot;&gt;&lt;a href=&quot;#1-2-生产与消费简单实例&quot; class=&quot;headerlink&quot; title=&quot;1.2 生产与消费简单实例&quot;&gt;&lt;/a&gt;1.2 生产与消费简单实例&lt;/h1&gt;&lt;h3 id=&quot;创建topic&quot;&gt;&lt;a href=&quot;#创建topic&quot; class=&quot;headerlink&quot; title=&quot;创建topic&quot;&gt;&lt;/a&gt;创建topic&lt;/h3&gt;&lt;p&gt;kafka提供了许多实用的脚本工具,存放在$KAFKA_HOME的bin目录下.其中与主题相关的就是kafka-topic.sh脚本.例如.下面创建一个分区数为4,副本为3的主题topic-demon&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;./kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-demo --replication-factor 3 --partitions 4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Created topic &lt;span class=&quot;string&quot;&gt;&quot;topic-demo&quot;&lt;/span&gt;.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;--zoopkeer&lt;/code&gt; 指定kafka连接的zookeeper服务地址&lt;br&gt;&lt;code&gt;--topic&lt;/code&gt; 指定一个topic主题&lt;br&gt;&lt;code&gt;--replication-factor&lt;/code&gt;  指定副本因子数量&lt;br&gt;&lt;code&gt;--partition&lt;/code&gt; 指定分区数量&lt;br&gt;&lt;code&gt;--create&lt;/code&gt; 表示创建&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="1-概念介绍" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-4.2分区管理</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/4.2%E5%88%86%E5%8C%BA%E7%AE%A1%E7%90%86/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/4-主题和分区/4.2分区管理/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:54:58.225Z</updated>
    
    <content type="html"><![CDATA[<h3 id="4-2-1-优选副本的选举"><a href="#4-2-1-优选副本的选举" class="headerlink" title="4.2.1 优选副本的选举"></a>4.2.1 优选副本的选举</h3><h4 id="4-2-1-1-什么是优先副本"><a href="#4-2-1-1-什么是优先副本" class="headerlink" title="4.2.1.1 什么是优先副本"></a>4.2.1.1 什么是优先副本</h4><p>分区使用多副本机制来提升可靠性,但是只有leader副本对外提供读写服务.而follower副本只负责在内部进行消息的同步.如果一个分区的leader副本不可用,那么就意味着整个分区变得不可用.此时就需要从剩余的follower副本中挑选一个新的leader副本继续对外提供服务.</p><blockquote><p>broker节点中的Leader副本个数决定了这个节点负载的高低</p></blockquote><p>在创建主题的时候,主题的分区和副本会尽可能的均匀分布在kafka集群的各个broker节点.对应的Leader副本的分配也比较均匀.例如下面的 <code>topic-demo</code> 主题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 152,153,154</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 152,153,154</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 152,153,154</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 152,153,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><a id="more"></a> <p>可以看到,leader副本均匀分布在所有的broker节点.另外,同一个分区,在同一台broker节点只能存在一个副本.所以leader副本所在的broker节点叫做分区的leader节点.而follower副本所在的broker节点叫做分区的follower节点.</p><p>可以想象的是,随着时间的推移,kafka集群中不可避免的出现节点宕机或者崩溃的情况.当分区的Leader节点发生故障时,其中一个follower节点就会成为新的Leader节点.这样导致集群中的节点之间负载不均衡,从而影响kafka整个集群的稳定性和健壮性.</p><p>即使原来的Leader节点恢复后,加入到集群时,也只能成为一个新的follower节点,而不会自动”抢班夺权”变成leader.</p><p>例如刚才的 <code>topic-demo</code> 分区重启152节点后,leader分布如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 153 Replicas: 152,153,154   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 154 Replicas: 152,154,153   Isr: 153,154,152</span><br></pre></td></tr></table></figure><p>尽管kafka非常均匀的将leader副本分布在其他另外2个几点.但是此时152节点的负载几乎为零.</p><p>为了有效的治理负载失衡的情况,kafka引入了<strong>优先副本(preferred replica)</strong>的概念.所谓的优先副本就是在AR集合列表中的第一个副本为优先副本,理想情况下优先副本就是该分区的leader副本.所以也可以称之为 <code>preferred leader</code> .</p><p>比如上面的例子中,分区0的AR集合(Replicas)是[152,153,154].那么分区0的优先副本就是152.<strong>Kafka会确保所有主题的优先副本均匀分布.这样就保证了所有分区的leader均衡分布.</strong></p><p><strong></strong></p><h4 id="4-2-1-2-优先副本选举"><a href="#4-2-1-2-优先副本选举" class="headerlink" title="4.2.1.2 优先副本选举"></a>4.2.1.2 优先副本选举</h4><p>所谓的优先副本选举是指通过一定的方式促使优先副本选举为Leader副本,促进集群的负载均衡.这一行为也称之为”分区平衡”.</p><p>kafka broker端(server.properties配置文件)有个 <code>auto.leader.rebalance.enble</code> 参数.默认为true.也就是分区自动平衡功能.Kafka会启动一个定时任务,轮询所有的broker节点,自动执行优先副本选举动作.</p><p>不过在生产环境中建议将该配置设置为 <code>false</code> .因为kafka自动平衡分区可能在某些关键高分期时刻引起负面性能问题.也有可能引起客户端的阻塞.为了防止出现此类情况,建议针对副本不均衡的问题进行相应监控和告警,然后在合适的时间通过手动来执行分区平衡.</p><p>Kafka中的 <code>kafka-preferred-replica-election.sh</code> 脚本提供了对分区leader副本进行重新平衡的功能.优先副本选举过程是一个安全的过程,kafka客户端会自动感知leader副本的变更.</p><p>命令用法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181</span><br></pre></td></tr></table></figure><p>但是这样一来会对kafka集群的所有主题和分区都执行一遍优先副本的选举操作.如果集群中包含大量的分区,那么可能选举会失败,并且会对性能造成一定的应用.比较建议的是使用 <code>path-to-json-file</code> 参数来小批量的对部分指定的主题分区进行优先副本的选举操作.该参数指定一个JSON文件,这个JSON文件保存需要执行优先副本选举的分区清单.</p><p>举个例子,对上面的 <code>topic-demo</code> 分区进行优先副本选举操作.先创建一个JSON文件,文件名可以任意:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">  &quot;partitions&quot;: [</span><br><span class="line">    &#123; </span><br><span class="line">        &quot;partition&quot;:0,</span><br><span class="line">        &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;partition&quot;:1,</span><br><span class="line">        &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;   &quot;partition&quot;:2,</span><br><span class="line">            &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;   &quot;partition&quot;:3,</span><br><span class="line">            &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;</span><br><span class="line">   ]</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>将上述内容保存为 <code>election.json</code> 文件.然后执行下列命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-preferred-replica-election.sh --zookeeper localhost:2181 --path-to-json-file ~/election.json</span><br><span class="line"> </span><br><span class="line">Created preferred replica election path with &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:0&#125;,&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:1&#125;,&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:2&#125;,&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:3&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic-demo,0], [topic-demo,1], [topic-demo,2], [topic-demo,3])</span><br></pre></td></tr></table></figure><p>提示优先副本选举成功.下列结果显示leader副本已经均衡分配到所有Broker节点了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 153,154,152</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>在实际生产环境中,建议使用这种方式来分批的执行优先副本选举操作.杜绝直接粗暴的进行所有分区的优先副本选举.另外,这类操作也应该需要避开业务高峰期,以免对性能造成负面影响,或者出现意外故障.</p><h3 id="4-2-2-分区重分配"><a href="#4-2-2-分区重分配" class="headerlink" title="4.2.2 分区重分配"></a>4.2.2 分区重分配</h3><p>当集群中一个Broker节点宕机,该节点的所有副本都处于丢失状态.kafka并不会自动将这些失效的分区副本自动迁移到集群其他broker节点.另外当集群中新增一台Broker节点时,只有新创建的主题分区才能被分配到这个节点上,而之前的主题分区并不会自动的加入到新节点(因为在创建时,并没有这个节点).这就导致新节点负载和原有节点负载之间严重不均衡.</p><p>为了解决这些问题,需要让分区副本再次进行合理的分配.也就是所谓的分区重分配.kafka提供了 <code>kafka-reassign-paritions.sh</code> 脚本执行分区重分配的工作.可以在集群节点失效或者扩容时使用.使用需要3个步骤:</p><ul><li>创建一个包含主题清单的JSON文件</li><li>根据主题清单和Broker节点清单生成一份重分配方案</li><li>执行具体重分配工作</li></ul><blockquote><p>要执行分区重分配,前提是broker节点清单数量要大于或者等于副本因子数量,否则会报错</p><p>Partitions reassignment failed due to replication factor: 3 larger than available brokers: 2</p></blockquote><p>下面创建一个4分区,2个副本因子的主题 <code>topic-reassign</code> 举例.假定要将152这个broker节点下线.下线之前需要将该节点上的分区副本迁移出去.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-reassign --replication-factor 2 --partitions 4</span><br><span class="line">Created topic &quot;topic-reassign&quot;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$  kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,153   Isr: 152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>第一步,创建一个JSON文件(文件名假定为reassign.json).文件内容是主题清单:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">    &quot;topics&quot;:[</span><br><span class="line">      &#123; </span><br><span class="line">                &quot;topic&quot;:&quot;topic-reassign&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;version&quot;:1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第二步,根据这个JSON文件和指定要分配的broker节点列表生成一份候选重分配方案:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --generate --topics-to-move-json-file ~/reassign.json --broker-list 153,154</span><br><span class="line"> </span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[153,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,153]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file project.json</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[153,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,153]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>在上面的例子中有以下几个参数:</p><p><code>--zookeeper</code> 这个参数已经非常熟悉了</p><p><code>--generate</code> 指令类型参数,类似于kafka-topics.sh脚本中的 <code>--create</code> , <code>--list</code> . <code>--describe</code> 等</p><p><code>--topics-to-move-json-file</code> 指定主题清单文件路径</p><p><code>--broker-list</code> 指定要分配的broker节点列表</p><p>上面的例子中打印了2个JSON格式内容:</p><p><code>Current partition replica assignment</code> 表示目前的分区副本分配情况,在执行分区重分配前最好备份这个内容,以便后续回滚操作</p><p><code>Proposed partition reassignment configuration</code> 表示候选重分配方案.这里只是一个方案,并没有真正执行.</p><p>将第二个Json内容格式化输出后,我们发现这个方案正如我们计划的那样,将该主题的所有分区下的AR副本集合分配到153和154节点,所有副本已经从即将要下线的152节点迁移走.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;:1,</span><br><span class="line">    &quot;partitions&quot;:[</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:1,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                153</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:0,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                153,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:3,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                153</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:2,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                153,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第三步,将 <code>Proposed partition reassignment configuration</code> JSON文件内容保存在一个文件中(假定为project.json).然后执行具体的重分配的动作,命令如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file project.json</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[153,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,153]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><blockquote><p>这里仍然打印了之前的副本分配方案,并且提示保存到JSON文件,以便回滚</p></blockquote><p>这里使用了2个不同的命令参数:</p><ul><li><code>--execute</code> 指令类型参数,执行重分配动作</li><li><code>--reassignment-json-file</code> 指定重分配方案文件路径</li></ul><p>再次查看 <code>topic-reassign</code> 主题分区副本分配情况,所有的副本都从152迁移出去,此时该节点可以顺利下线</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 153 Replicas: 154,153   Isr: 153,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>当然,我们也可以直接编写第二个JSON文件来自定义重分配方案,这样就不需要执行上面的第一步和第二步操作了.</p><p>分区重分配的基本原理是为每个分区添加新副本(增加副本数量),新副本会从leader副本复制所有的数据.复制完成后,控制器将旧副本从副本清单里移除.(恢复成原来的副本数量).</p><blockquote><p>所以,分区重分配需要确保有足够的空间,并且避免在业务高峰期操作</p></blockquote><p>从刚才的主题分区结果可以看到,大部分的分区leader副本都集中在153这个broker节点.这样负载非常不均衡,我们可以继续借助 <code>kafka-preferred-replica-election.sh</code> 脚本执行一次优先副本选举.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-preferred-replica-election.sh --zookeeper localhost:2181 --path-to-json-file election.json</span><br><span class="line"></span><br><span class="line">Created preferred replica election path with &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic-reassign,0], [topic-reassign,1], [topic-reassign,2], [topic-reassign,3])</span><br><span class="line"></span><br><span class="line">#选举完成后,副本分配均衡</span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 154 Replicas: 154,153   Isr: 153,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><blockquote><p>和优先副本选举一样,分区重分配对集群的性能有很大的影响.需要占用额外的磁盘,网络IO等资源.在生产环境执行操作时应该分批次执行.</p></blockquote><h3 id="4-2-3-复制限流"><a href="#4-2-3-复制限流" class="headerlink" title="4.2.3 复制限流"></a>4.2.3 复制限流</h3><p>我们了解分区重分配的本质在于数据复制,先增加新副本,进行数据同步,然后删除旧副本.如果副本数据量太大必然会占用很多额外的资源,从而影响集群整体性能.kafka有限流机制,可以对副本之间的复制流量进行限制.</p><p>副本复制限流有2种实现方式:</p><ul><li><code>kafka-config.sh</code> </li><li><code>kafka-reassign-partitions.sh</code> </li></ul><p>前者的实现方式有点繁琐,这里介绍后者的使用方式.</p><p><code>kafka-reassign-partitions.sh</code> 的实现方式非常简单,只需要一个throttle参数即可.例如上面的例子中副本都在153和154节点,现在继续使用分区重分配,让副本从153节点迁移到152节点.但是这次使用限流工具</p><p>首先,修改 <code>project.json</code> 文件,将153替换成152</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &apos;s/153/152/g&apos; project.json</span><br></pre></td></tr></table></figure><p>然后,执行分区重分配,这里使用 <code>--throttle</code> 参数,指定一个限流速度(单位是B/s)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file project.json --throttle 10</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,153]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[154,153]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[153,154]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value.</span><br><span class="line">The throttle limit was set to 10 B/s</span><br><span class="line">Successfully started reassignment of partitions.</span><br></pre></td></tr></table></figure><p>上面的示例输出中提示以下3点信息:</p><ol><li>需要周期性的使用 <code>--verify</code> 参数来周期性的查看副本复制进度,直到分区重分配完成,也就是说需要显示的使用这种方式确保分区重分配完成后解除限流的设置</li><li>限流的速度为10B/s</li><li>如果想要修改限流速度,重复此条执行命令,修改throttle的值即可</li></ol><p>接下来使用 <code>--verify</code> 参数查看复制进度.下面的示例显示复制已经完成,并且限流已被解除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --verify --reassignment-json-file project.json</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition [topic-reassign,1] completed successfully</span><br><span class="line">Reassignment of partition [topic-reassign,0] completed successfully</span><br><span class="line">Reassignment of partition [topic-reassign,3] completed successfully</span><br><span class="line">Reassignment of partition [topic-reassign,2] completed successfully</span><br><span class="line">Throttle was removed.</span><br></pre></td></tr></table></figure><p>此时153的副本已经被移除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 152 Replicas: 152,154   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 154 Replicas: 154,152   Isr: 154,152</span><br></pre></td></tr></table></figure><h3 id="4-2-4-修改副本因子"><a href="#4-2-4-修改副本因子" class="headerlink" title="4.2.4 修改副本因子"></a>4.2.4 修改副本因子</h3><p>上面的例子中分区重分配,将副本从一个broker节点中移除,此时kafka集群的broker节点数量只剩下2个.副本因子也只有2个.这里有个问题,此时153节点重启,或者新增broker节点后,如何将新增的broker节点加入进群,扩展副本数量呢?或者还有一种情况,当创建主题和分区后,想要修改副本因子呢?</p><p><code>kafka-reassign-parition.sh</code> 脚本同样实现了修改副本因子的功能..仔细观察一下分区重分配案例中的 <code>project.json</code> 文件内容:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ cat project.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;:1,</span><br><span class="line">    &quot;partitions&quot;:[</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:1,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                152</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:0,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                152,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:3,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                152</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:2,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                152,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>json文件中的副本集合(replicas)都是2个副本,我们可以很简单的添加一个副本.比如对于分区0而言,可以将153节点添加进去.(其他分区也是如此)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:1,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                152,</span><br><span class="line">                153</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br></pre></td></tr></table></figure><p>执行 <code>kafka-reassign-partition.sh</code> 脚本,执行命令的方法和参数和分区重分片几乎一致:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file add.json</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[152,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,154]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>查看副本分配情况.副本数量已经增加到了3个</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,154,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 154 Replicas: 153,154,152   Isr: 154,152,153</span><br></pre></td></tr></table></figure><blockquote><p>虽然副本因子增加到3个,但是Leader还是没有分配到新的153这个broker节点.此时可以通过优先副本选举重新分配</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-preferred-replica-election.sh --zookeeper localhost:2181 --path-to-json-file election.json</span><br><span class="line">Created preferred replica election path with &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic-reassign,0], [topic-reassign,1], [topic-reassign,2], [topic-reassign,3])</span><br><span class="line"></span><br><span class="line">#再次查看topic-reassign主题,分区副本分配均衡</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,154,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 153 Replicas: 153,154,152   Isr: 154,152,153</span><br></pre></td></tr></table></figure><p>重点: <strong>与修改分区数量不同,副本数还可以减少</strong>,修改方法和命令几乎一样,只需要编辑JSON配置文件即可.这里就不再演示</p><h3 id="4-2-5-如何选择合适的分区数量"><a href="#4-2-5-如何选择合适的分区数量" class="headerlink" title="4.2.5 如何选择合适的分区数量"></a>4.2.5 如何选择合适的分区数量</h3><p>如何选择合适的分区数量是需要经常面对的问题,但是这个问题似乎并没有权威的标准答案,需要根据实际的业务场景,硬件资源,应用软件,负载等情况做具体考量.这一章节主要介绍与本问题相关的一些决策因素,以供参考</p><h4 id="4-2-5-1-性能测试工具"><a href="#4-2-5-1-性能测试工具" class="headerlink" title="4.2.5.1 性能测试工具"></a>4.2.5.1 性能测试工具</h4><p>在生产环境中设定分区数量需要考虑性能因素.所以性能测试工具必不可少,kafka本身提供了用于生产者性能测试的 <code>kafka-producer-pref-test.sh</code> 脚本和用于消费者性能测试的 <code>kafka-consumer-perf-test.sh</code>脚本</p><p>首先创建一个用于测试的分区为1,副本为1的 <code>topic-1</code> 的主题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Topic:topic-1   PartitionCount:1    ReplicationFactor:1 Configs:</span><br><span class="line">    Topic: topic-1  Partition: 0    Leader: 153 Replicas: 153   Isr: 153</span><br></pre></td></tr></table></figure><p>其次.我们往这个主题发送100万条消息,并且每条消息大小为1024B,生产者对应的acks参数为1:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-producer-perf-test.sh --topic topic-1 --num-records 1000000 --record-size 1024 --throughput -1 --producer-props bootstrap.servers=localhost:9092 acks=1</span><br><span class="line">76666 records sent, 15333.2 records/sec (14.97 MB/sec), 1517.5 ms avg latency, 2061.0 max latency.</span><br><span class="line">119400 records sent, 23880.0 records/sec (23.32 MB/sec), 1353.6 ms avg latency, 1631.0 max latency.</span><br><span class="line">124560 records sent, 24912.0 records/sec (24.33 MB/sec), 1231.2 ms avg latency, 1375.0 max latency.</span><br><span class="line">146520 records sent, 29304.0 records/sec (28.62 MB/sec), 1066.6 ms avg latency, 1146.0 max latency.</span><br><span class="line">156795 records sent, 31359.0 records/sec (30.62 MB/sec), 972.3 ms avg latency, 1051.0 max latency.</span><br><span class="line">133365 records sent, 26673.0 records/sec (26.05 MB/sec), 1141.1 ms avg latency, 1322.0 max latency.</span><br><span class="line">159945 records sent, 31989.0 records/sec (31.24 MB/sec), 964.3 ms avg latency, 1178.0 max latency.</span><br><span class="line">1000000 records sent, 26148.576210 records/sec (25.54 MB/sec), 1143.54 ms avg latency, 2061.00 ms max latency, 1114 ms 50th, 1654 ms 95th, 1869 ms 99th, 2036 ms 99.9th.</span><br></pre></td></tr></table></figure><p>示例中使用了多个参数:</p><p><code>num-records</code>:  指定发送消息的总条数</p><p><code>record-size</code>: 设置每条消息的字节数</p><p><code>throughtput</code> : 限流控制,-1表示不限流,大于0表示限流值</p><p><code>producer-props</code> : 指定生产者的配置,可以同时指定多组配置</p><p>除此之外还有其他参数,比如 <code>print-metrics</code> 在测试完成之后,打印很多指标信息.有兴趣可以执行 <code>--help</code> 查看更多参数信息.</p><p>回过头再看看上面示例中的压测结果信息,以第一条和最后一条为例:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">76666 records sent, 15333.2 records/sec (14.97 MB/sec), 1517.5 ms avg latency, 2061.0 max latency.</span><br><span class="line">1114 ms 50th, 1654 ms 95th, 1869 ms 99th, 2036 ms 99.9th</span><br></pre></td></tr></table></figure><p><strong>records sent: 表示发送的消息综述</strong></p><p><strong>records/sec: 吞吐量,表示每秒发送的消息数量</strong></p><p><strong>MB/sec: 吞吐量,表示每秒发送的消息大小</strong></p><p><strong>avg latency: 表示消息处理的平均耗时</strong></p><p><strong>max latency: 表示消息处理的最大耗时</strong></p><p><strong>50th,95th,99th,99.th</strong> 表示50%,95%,99%,99.9%时消息处理耗时</p><p><strong></strong></p><p>消费者压测工具的脚本使用也比较简单,下面的简单实例演示了消费主题 <code>topic-1</code> 中的100万条消息.命令使用方法:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-consumer-perf-test.sh --topic topic-1 --messages 1000000 --broker-list localhost:9092</span><br><span class="line">start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec</span><br><span class="line">2020-12-26 14:07:29:612, 2020-12-26 14:07:35:272, 977.0264, 172.6195, 1000475, 176762.3675</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p><strong>data.consumed.in.MB: 消费的消息总量,单位为MB</strong></p><p><strong>MB.sec</strong>: 按字节大小计算的消费吞吐量(单位:MB/s)</p><p><strong>data.consumed.in.nMsg</strong>: 消费的消息消息总数</p><p><strong>nMsg.sec</strong>: 按消息个数计算的吞独量(单位n/s)</p><blockquote><p>可以创建多个分区,比如10,100,200,500等(副本数量都为1)来测试生产和消费的性能表现,</p></blockquote><h4 id="4-2-5-2-分区数量越多不代表吞独量越高"><a href="#4-2-5-2-分区数量越多不代表吞独量越高" class="headerlink" title="4.2.5.2 分区数量越多不代表吞独量越高"></a>4.2.5.2 分区数量越多不代表吞独量越高</h4><p>消息中间件的性能一般是指吞吐量(还包括延迟),吞吐量会受到硬件资源,消息大小,消息压缩,消息发送方式(同步,异步),副本因子等参数影响.分区数量越多不一定吞吐量越高,超过一定的临界值后,kafka的吞吐量会不升反降.</p><h4 id="4-2-5-3-分区数量的上限"><a href="#4-2-5-3-分区数量的上限" class="headerlink" title="4.2.5.3 分区数量的上限"></a>4.2.5.3 分区数量的上限</h4><p>一味的增加分区数量并不能使吞吐量得到提升,并且分区的数量也不能一直增加,如果超过一定的临界值还会引起kafka进程的崩溃.</p><p><strong>每次创建一个分区,都会消耗一个Linux系统的文件描述符.</strong></p><p>通过kafka的pid编号,可以查看当前kafka进程占用的文件描述符数量:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ ls /proc/22858/fd/ | wc -l</span><br><span class="line">173</span><br></pre></td></tr></table></figure><p>此时创建一个分区数量为400个的topic-demo4的主题.由于分区会平均创建在集群内的3个broker节点,所以需要统计一下152这个本地节点的分区数量.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics --describe --topic topic-demo4 | grep -Eo &quot;Leader:\s[0-9]+&quot; | sort | uniq -c</span><br><span class="line">    134 Leader: 152</span><br><span class="line">    133 Leader: 153</span><br><span class="line">    133 Leader: 154</span><br></pre></td></tr></table></figure><p>可以看到152这个节点创建了134个分区.接下来看看系统文件描述符的数量.正好增加了134个文件描述符</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ ls /proc/22858/fd/ | wc -l</span><br><span class="line">307</span><br></pre></td></tr></table></figure><p>可以想见的是,一旦分区数量超过了操作系统规定的文件描述符上限,kafka进程就会崩溃</p><h4 id="4-2-5-4-分区数量的考量"><a href="#4-2-5-4-分区数量的考量" class="headerlink" title="4.2.5.4 分区数量的考量"></a>4.2.5.4 分区数量的考量</h4><p>如何选择合适的分区数量,一个恰当的答案就是视具体情况而定.</p><p>从吞吐量方面考虑,增加合适的分区数量可以在一定程度上提升整体吞吐量,但是超过临界值之后吞吐量不升反降.在投入生产环境之前,应该对吞吐量进行相关的测试,以找到合适的分区数量</p><p>分区数量太多会影响系统可用性,当broker发生故障时,broker节点上的所有分区的leader副本不可用,此时如果有大量的分区要进行leader角色切换,这个切换的过程会耗费相当的时间,并且这个时间段内分区会变的不可用.并且分区数量太多不仅为增加日志清理的耗时,而且在被删除时也会消费更多时间.</p><p>一个好的建议是,创建主题之前对分区数量性能进行充分压测,在创建主题之后,还需要对其进行追踪,监控,调优.如果分区数量较少,还能通过增加分区数量,或者增加broker进行分区重分配等改进.</p><p>最后,一个通用的准则是,建议分区数量设定为集群中broker的倍数,例如集群中有3个broker节点,可以设定分区数为3,6,9等.</p><h3 id="4-2-6-总结"><a href="#4-2-6-总结" class="headerlink" title="4.2.6 总结"></a>4.2.6 总结</h3><p><code>kafka-topics.sh</code> 查看,创建主题分区,副本</p><p><code>kafka-configs.sh</code> 修改主题配置文件</p><p><code>kafka_perferred-replica-elections.sh</code> 优先副本选举</p><p><code>kafka-reassign-partitions.sh</code> 分区重分配,副本复制限流,修改副本因子数量</p><p><code>kafka-producer-perf-test.sh</code> 生产者分区数和吞吐量性能压测</p><p><code>kafka-consumer-perf-test.sh</code> 消费者性能压测</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;4-2-1-优选副本的选举&quot;&gt;&lt;a href=&quot;#4-2-1-优选副本的选举&quot; class=&quot;headerlink&quot; title=&quot;4.2.1 优选副本的选举&quot;&gt;&lt;/a&gt;4.2.1 优选副本的选举&lt;/h3&gt;&lt;h4 id=&quot;4-2-1-1-什么是优先副本&quot;&gt;&lt;a href=&quot;#4-2-1-1-什么是优先副本&quot; class=&quot;headerlink&quot; title=&quot;4.2.1.1 什么是优先副本&quot;&gt;&lt;/a&gt;4.2.1.1 什么是优先副本&lt;/h4&gt;&lt;p&gt;分区使用多副本机制来提升可靠性,但是只有leader副本对外提供读写服务.而follower副本只负责在内部进行消息的同步.如果一个分区的leader副本不可用,那么就意味着整个分区变得不可用.此时就需要从剩余的follower副本中挑选一个新的leader副本继续对外提供服务.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;broker节点中的Leader副本个数决定了这个节点负载的高低&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在创建主题的时候,主题的分区和副本会尽可能的均匀分布在kafka集群的各个broker节点.对应的Leader副本的分配也比较均匀.例如下面的 &lt;code&gt;topic-demo&lt;/code&gt; 主题:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev152 ~]$&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="4-主题和分区" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>prometheus监控mysql</title>
    <link href="https://jesse.top/2020/11/10/prometheus/prometheus%E7%9B%91%E6%8E%A7mysql/"/>
    <id>https://jesse.top/2020/11/10/prometheus/prometheus监控mysql/</id>
    <published>2020-11-09T23:59:58.000Z</published>
    <updated>2021-04-01T13:39:11.691Z</updated>
    
    <content type="html"><![CDATA[<h3 id="prometheus监控mysql"><a href="#prometheus监控mysql" class="headerlink" title="prometheus监控mysql"></a>prometheus监控mysql</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>本文使用的是官方的mysqld_exporter.github地址:<a href="https://github.com/prometheus/mysqld_exporter" target="_blank" rel="noopener">mysqld_exporter</a></p><blockquote><p>mysql版本需要在5.6版本或以上</p></blockquote><p>mysqld_exporter提供很多监控项(具体参考github项目介绍).如果需要开启一个监控项,在启动mysqld_exporter时,携带以下命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--collect.key</span><br></pre></td></tr></table></figure><p>如果需要关闭某个监控项.携带以下命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--no-collect.key</span><br></pre></td></tr></table></figure><blockquote><p>如果mysqld_exporter的版本小于0.10.0,命令有些变化,双横杠变成单横杠,使用-collect.key 或者-collect.key=True|false</p></blockquote><hr><h3 id="mysqld-exporter安装"><a href="#mysqld-exporter安装" class="headerlink" title="mysqld_exporter安装"></a>mysqld_exporter安装</h3><p>安装方式很简单,.下面是一个Ansible脚本以供参考</p><a id="more"></a><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">- hosts:</span> <span class="string">mysql-prod</span></span><br><span class="line"><span class="attr">  remote_user:</span> <span class="string">work</span></span><br><span class="line"><span class="attr">  become:</span> <span class="literal">yes</span></span><br><span class="line"><span class="attr">  gather_facts:</span> <span class="literal">no</span></span><br><span class="line"><span class="attr">  tasks:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">检查mysqld_exporter是否已经安装</span></span><br><span class="line"><span class="attr">      systemd:</span> </span><br><span class="line"><span class="attr">        name:</span> <span class="string">mysqld_exporter</span></span><br><span class="line"><span class="attr">        state:</span> <span class="string">started</span></span><br><span class="line"><span class="attr">      ignore_errors:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      register:</span> <span class="string">result</span></span><br><span class="line">    </span><br><span class="line"><span class="attr">    - name:</span> <span class="string">安装mysqld_exporter</span></span><br><span class="line"><span class="attr">      unarchive:</span></span><br><span class="line"><span class="attr">        src:</span> <span class="attr">http://repo.doweidu.com/prometheus/mysqld_exporter-0.12.1.linux-amd64.tar.gz</span></span><br><span class="line"><span class="attr">        dest:</span> <span class="string">/home/work/</span></span><br><span class="line"><span class="attr">        remote_src:</span> <span class="literal">yes</span></span><br><span class="line"><span class="attr">      when:</span> <span class="string">result</span> <span class="string">is</span> <span class="string">failed</span> </span><br><span class="line">    </span><br><span class="line"><span class="attr">    - name:</span> <span class="string">重命名</span></span><br><span class="line"><span class="attr">      command:</span> <span class="string">mv</span> <span class="string">/home/work/mysqld_exporter-0.12.1.linux-amd64</span> <span class="string">/home/work/mysqld_exporter</span></span><br><span class="line"><span class="attr">      when:</span> <span class="string">result</span> <span class="string">is</span> <span class="string">failed</span> </span><br><span class="line">    </span><br><span class="line"><span class="attr">    - name:</span> <span class="string">拷贝到/usr/local下</span></span><br><span class="line"><span class="attr">      shell:</span> <span class="string">cp</span> <span class="bullet">-r</span> <span class="string">/home/work/mysqld_exporter</span> <span class="string">/usr/local/</span></span><br><span class="line"><span class="attr">      when:</span> <span class="string">result</span> <span class="string">is</span> <span class="string">failed</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">拷贝配置文件到/usr/local/mysqld_exporter下</span></span><br><span class="line"><span class="attr">      copy:</span> <span class="string">src=files/localhost_db.cnf</span> <span class="string">dest=/usr/local/mysqld_exporter/</span></span><br><span class="line"><span class="attr">      when:</span> <span class="string">result</span> <span class="string">is</span> <span class="string">failed</span></span><br><span class="line">    </span><br><span class="line"><span class="attr">    - name:</span> <span class="string">拷贝systemd文件</span></span><br><span class="line"><span class="attr">      copy:</span> <span class="string">src=files/mysqld_exporter.service</span> <span class="string">dest=/usr/lib/systemd/system/mysqld_exporter.service</span></span><br><span class="line"><span class="attr">      when:</span> <span class="string">result</span> <span class="string">is</span> <span class="string">failed</span> </span><br><span class="line"></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">启动mysqld_exporter</span></span><br><span class="line"><span class="attr">      systemd:</span> <span class="string">name=mysqld_exporter</span> <span class="string">state=started</span> <span class="string">enabled=yes</span></span><br><span class="line"><span class="attr">      when:</span> <span class="string">result</span> <span class="string">is</span> <span class="string">failed</span></span><br></pre></td></tr></table></figure><p>这里需要准备一个<code>localhost_db.cnf</code>配置文件.内容如下</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[client]</span></span><br><span class="line"><span class="string">host=127.0.0.1</span>  <span class="comment">#mysql服务器地址</span></span><br><span class="line"><span class="comment">#port=          #mysql端口,默认为3306,如果端口是默认3306,可以删掉这行</span></span><br><span class="line"><span class="string">user=zabbix</span>     <span class="comment">#mysql用于监控的账号</span></span><br><span class="line"><span class="string">password=xxxxxx</span>  <span class="comment">#密码</span></span><br></pre></td></tr></table></figure><p>mysqld_exporter启动脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=mysqld_exporter</span><br><span class="line">Documentation=https://prometheus.io/</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">User=root</span><br><span class="line">ExecStart=/usr/local/mysqld_exporter/mysqld_exporter --config.my-cnf=/usr/local/mysqld_exporter/localhost_db.cnf</span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>Ansible脚本执行完毕后,就可以通过<code>IP:9104</code>来访问mysql的Metrics</p><p>为了监控到主机的情况.此时还需要部署node_exporter客户端.关于Node_exporter我在另一篇笔记中再详细介绍</p><hr><h4 id="prometheus配置"><a href="#prometheus配置" class="headerlink" title="prometheus配置"></a>prometheus配置</h4><p>由于mysqld_exporter没有任何监控项能获取到mysql服务器的主机名.所以将数据展示到grafana时,只能通过IP去查看监控图表.这非常的不方便.比如下面截图中,当mysql服务器数量较多时,很难知道IP地址对应的是具体哪台服务器:</p><p><img src="https://img2.jesse.top/image-20201126103328877.png" alt="image-20201126103328877"></p><p>此时,就需要在prometheus配置文件中,将每个mysql服务器添加labels,给服务器打上主机名和组名的标签</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">- job_name: &apos;aliyun-mysql-exporter&apos;</span><br><span class="line">    static_configs:</span><br><span class="line">     - targets:</span><br><span class="line">         - &apos;10.111.200.176:9104&apos;</span><br><span class="line">       labels:</span><br><span class="line">         hostname: &apos;mg-goodscenter-mysql-master&apos;</span><br><span class="line">         group: &apos;mg-mysql&apos;</span><br><span class="line"></span><br><span class="line">     - targets: [&apos;10.111.50.39:9104&apos;]</span><br><span class="line">       labels:</span><br><span class="line">         hostname: &apos;mg-msg-mysql-master&apos;</span><br><span class="line">         group: &apos;mg-mysql&apos;</span><br><span class="line"></span><br><span class="line">......以下省略......</span><br></pre></td></tr></table></figure><hr><h4 id="grafana-dashboard"><a href="#grafana-dashboard" class="headerlink" title="grafana dashboard"></a>grafana dashboard</h4><p>下载dashboard: <a href="https://grafana.com/grafana/dashboards/7362" target="_blank" rel="noopener">https://grafana.com/grafana/dashboards/7362</a> </p><p>或者直接在grafana中添加7362的dashboard</p><p>原生的dashboard只有一个instance的变量.为了添加主机名,更好的区分和展示监控效果,需要做一些修改.</p><p><strong>1.定义变量:</strong></p><ul><li><p>job变量:</p><ul><li>name:job</li><li><p>label: job</p></li><li><p>query: label_values(mysql_up,job)</p></li></ul></li><li><p>group变量:</p><ul><li>name: group</li><li>label:主机组</li><li>Query:label_values(mysql_up{job=~”$job”}, group)</li></ul></li></ul><p><strong>2.修改变量</strong></p><p>将host变量修改为:</p><p>name: host</p><p>label: 主机名</p><p>query:label_values(mysql_up{job=~”$job”,group=~”$group”}, hostname)</p><p><strong>3.变量页面最终设置如下:</strong></p><p><img src="https://img2.jesse.top/image-20201126105202201.png" alt="image-20201126105202201"></p><p>接着,将当前的dashboard的json文件导出.复制下面的json文件</p><p><img src="https://img2.jesse.top/image-20201126105345163.png" alt="image-20201126105345163"></p><p>将文件中的<code>instance=~</code>全部替换为<code>hostname=~</code>.然后复制回去,点击<code>Save Changes</code></p><p>此时,可以通过主机组和主机名筛选具体的mysql服务器.</p><p><img src="https://img2.jesse.top/image-20210329171207919.png" alt="image-20210329171207919"></p><p><img src="https://img2.jesse.top/image-20210329171842731.png" alt="image-20210329171842731"></p><blockquote><p>别忘记保存dashboard</p></blockquote><hr><h4 id="rules告警规则"><a href="#rules告警规则" class="headerlink" title="rules告警规则"></a>rules告警规则</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br></pre></td><td class="code"><pre><span class="line">groups:</span><br><span class="line">- name: MySQLStatsAlert</span><br><span class="line">  rules:</span><br><span class="line">  - alert: MySQL is down</span><br><span class="line">    expr: mysql_up == 0</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; MySQL is down&quot;</span><br><span class="line">      description: &quot;MySQL database is down. This requires immediate action!&quot;</span><br><span class="line">  - alert: open files high</span><br><span class="line">    expr: mysql_global_status_innodb_num_open_files &gt; (mysql_global_variables_open_files_limit) * 0.75</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; open files high&quot;</span><br><span class="line">      description: &quot;Open files is high. Please consider increasing open_files_limit.&quot;</span><br><span class="line">  - alert: Read buffer size is bigger than max. allowed packet size</span><br><span class="line">    expr: mysql_global_variables_read_buffer_size &gt; mysql_global_variables_slave_max_allowed_packet</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Read buffer size is bigger than max. allowed packet size&quot;</span><br><span class="line">      description: &quot;Read buffer size (read_buffer_size) is bigger than max. allowed packet size (max_allowed_packet).This can break your replication.&quot;</span><br><span class="line">  - alert: Sort buffer possibly missconfigured</span><br><span class="line">    expr: mysql_global_variables_innodb_sort_buffer_size &lt;256*1024 or mysql_global_variables_read_buffer_size &gt; 4*1024*1024</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Sort buffer possibly missconfigured&quot;</span><br><span class="line">      description: &quot;Sort buffer size is either too big or too small. A good value for sort_buffer_size is between 256k and 4M.&quot;</span><br><span class="line">  - alert: Thread stack size is too small</span><br><span class="line">    expr: mysql_global_variables_thread_stack &lt;196608</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Thread stack size is too small&quot;</span><br><span class="line">      description: &quot;Thread stack size is too small. This can cause problems when you use Stored Language constructs for example. A typical is 256k for thread_stack_size.&quot;</span><br><span class="line">  - alert: Used more than 80% of max connections limited</span><br><span class="line">    expr: mysql_global_status_max_used_connections &gt; mysql_global_variables_max_connections * 0.8</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Used more than 80% of max connections limited&quot;</span><br><span class="line">      description: &quot;Used more than 80% of max connections limited&quot;</span><br><span class="line">  - alert: InnoDB Force Recovery is enabled</span><br><span class="line">    expr: mysql_global_variables_innodb_force_recovery != 0</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Force Recovery is enabled&quot;</span><br><span class="line">      description: &quot;InnoDB Force Recovery is enabled. This mode should be used for data recovery purposes only. It prohibits writing to the data.&quot;</span><br><span class="line">  - alert: InnoDB Log File size is too small</span><br><span class="line">    expr: mysql_global_variables_innodb_log_file_size &lt; 16777216</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Log File size is too small&quot;</span><br><span class="line">      description: &quot;The InnoDB Log File size is possibly too small. Choosing a small InnoDB Log File size can have significant performance impacts.&quot;</span><br><span class="line">  - alert: InnoDB Flush Log at Transaction Commit</span><br><span class="line">    expr: mysql_global_variables_innodb_flush_log_at_trx_commit != 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Flush Log at Transaction Commit&quot;</span><br><span class="line">      description: &quot;InnoDB Flush Log at Transaction Commit is set to a values != 1. This can lead to a loss of commited transactions in case of a power failure.&quot;</span><br><span class="line">  - alert: Table definition cache too small</span><br><span class="line">    expr: mysql_global_status_open_table_definitions &gt; mysql_global_variables_table_definition_cache</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Table definition cache too small&quot;</span><br><span class="line">      description: &quot;Your Table Definition Cache is possibly too small. If it is much too small this can have significant performance impacts!&quot;</span><br><span class="line">  - alert: Table open cache too small</span><br><span class="line">    expr: mysql_global_status_open_tables &gt;mysql_global_variables_table_open_cache * 99/100</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Table open cache too small&quot;</span><br><span class="line">      description: &quot;Your Table Open Cache is possibly too small (old name Table Cache). If it is much too small this can have significant performance impacts!&quot;</span><br><span class="line">  - alert: Thread stack size is possibly too small</span><br><span class="line">    expr: mysql_global_variables_thread_stack &lt; 262144</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Thread stack size is possibly too small&quot;</span><br><span class="line">      description: &quot;Thread stack size is possibly too small. This can cause problems when you use Stored Language constructs for example. A typical is 256k for thread_stack_size.&quot;</span><br><span class="line">  - alert: InnoDB Buffer Pool Instances is too small</span><br><span class="line">    expr: mysql_global_variables_innodb_buffer_pool_instances == 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Buffer Pool Instances is too small&quot;</span><br><span class="line">      description: &quot;If you are using MySQL 5.5 and higher you should use several InnoDB Buffer Pool Instances for performance reasons. Some rules are: InnoDB Buffer Pool Instance should be at least 1 Gbyte in size. InnoDB Buffer Pool Instances you can set equal to the number of cores of your machine.&quot;</span><br><span class="line">  - alert: InnoDB Plugin is enabled</span><br><span class="line">    expr: mysql_global_variables_ignore_builtin_innodb == 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Plugin is enabled&quot;</span><br><span class="line">      description: &quot;InnoDB Plugin is enabled&quot;</span><br><span class="line">  - alert: Binary Log is disabled</span><br><span class="line">    expr: mysql_global_variables_log_bin != 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Binary Log is disabled&quot;</span><br><span class="line">      description: &quot;Binary Log is disabled. This prohibits you to do Point in Time Recovery (PiTR).&quot;</span><br><span class="line">  - alert: Binlog Cache size too small</span><br><span class="line">    expr: mysql_global_variables_binlog_cache_size &lt; 1048576</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Binlog Cache size too small&quot;</span><br><span class="line">      description: &quot;Binlog Cache size is possibly to small. A value of 1 Mbyte or higher is OK.&quot;</span><br><span class="line">  - alert: Binlog Statement Cache size too small</span><br><span class="line">    expr: mysql_global_variables_binlog_stmt_cache_size &lt;1048576 and mysql_global_variables_binlog_stmt_cache_size &gt; 0</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Binlog Statement Cache size too small&quot;</span><br><span class="line">      description: &quot;Binlog Statement Cache size is possibly to small. A value of 1 Mbyte or higher is typically OK.&quot;</span><br><span class="line">  - alert: Binlog Transaction Cache size too small</span><br><span class="line">    expr: mysql_global_variables_binlog_cache_size  &lt;1048576</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Binlog Transaction Cache size too small&quot;</span><br><span class="line">      description: &quot;Binlog Transaction Cache size is possibly to small. A value of 1 Mbyte or higher is typically OK.&quot;</span><br><span class="line">  - alert: Sync Binlog is enabled</span><br><span class="line">    expr: mysql_global_variables_sync_binlog == 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Sync Binlog is enabled&quot;</span><br><span class="line">      description: &quot;Sync Binlog is enabled. This leads to higher data security but on the cost of write performance.&quot;</span><br><span class="line">  - alert: IO thread stopped</span><br><span class="line">    expr: mysql_slave_status_slave_io_running != 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; IO thread stopped&quot;</span><br><span class="line">      description: &quot;IO thread has stopped. This is usually because it cannot connect to the Master any more.&quot;</span><br><span class="line">  - alert: SQL thread stopped</span><br><span class="line">    expr: mysql_slave_status_slave_sql_running == 0</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; SQL thread stopped&quot;</span><br><span class="line">      description: &quot;SQL thread has stopped. This is usually because it cannot apply a SQL statement received from the master.&quot;</span><br><span class="line">  - alert: SQL thread stopped</span><br><span class="line">    expr: mysql_slave_status_slave_sql_running != 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Sync Binlog is enabled&quot;</span><br><span class="line">      description: &quot;SQL thread has stopped. This is usually because it cannot apply a SQL statement received from the master.&quot;</span><br><span class="line">  - alert: Slave lagging behind Master</span><br><span class="line">    expr: rate(mysql_slave_status_seconds_behind_master[1m]) &gt;30</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Slave lagging behind Master&quot;</span><br><span class="line">      description: &quot;Slave is lagging behind Master. Please check if Slave threads are running and if there are some performance issues!&quot;</span><br><span class="line">  - alert: Slave is NOT read only(Please ignore this warning indicator.)</span><br><span class="line">    expr: mysql_global_variables_read_only != 0</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Slave is NOT read only&quot;</span><br><span class="line">      description: &quot;Slave is NOT set to read only. You can accidentally manipulate data on the slave and get inconsistencies...&quot;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;prometheus监控mysql&quot;&gt;&lt;a href=&quot;#prometheus监控mysql&quot; class=&quot;headerlink&quot; title=&quot;prometheus监控mysql&quot;&gt;&lt;/a&gt;prometheus监控mysql&lt;/h3&gt;&lt;h4 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h4&gt;&lt;p&gt;本文使用的是官方的mysqld_exporter.github地址:&lt;a href=&quot;https://github.com/prometheus/mysqld_exporter&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;mysqld_exporter&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;mysql版本需要在5.6版本或以上&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;mysqld_exporter提供很多监控项(具体参考github项目介绍).如果需要开启一个监控项,在启动mysqld_exporter时,携带以下命令:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;--collect.key&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果需要关闭某个监控项.携带以下命令:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;--no-collect.key&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;如果mysqld_exporter的版本小于0.10.0,命令有些变化,双横杠变成单横杠,使用-collect.key 或者-collect.key=True|false&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&quot;mysqld-exporter安装&quot;&gt;&lt;a href=&quot;#mysqld-exporter安装&quot; class=&quot;headerlink&quot; title=&quot;mysqld_exporter安装&quot;&gt;&lt;/a&gt;mysqld_exporter安装&lt;/h3&gt;&lt;p&gt;安装方式很简单,.下面是一个Ansible脚本以供参考&lt;/p&gt;
    
    </summary>
    
      <category term="prometheus" scheme="https://jesse.top/categories/prometheus/"/>
    
    
      <category term="prometheus" scheme="https://jesse.top/tags/prometheus/"/>
    
  </entry>
  
  <entry>
    <title>Prometheus Alertmanager 告警路由</title>
    <link href="https://jesse.top/2020/11/10/prometheus/Prometheus%20alertmanager%20%E5%91%8A%E8%AD%A6%E8%B7%AF%E7%94%B1/"/>
    <id>https://jesse.top/2020/11/10/prometheus/Prometheus alertmanager 告警路由/</id>
    <published>2020-11-09T23:59:58.000Z</published>
    <updated>2021-04-01T13:39:46.941Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Prometheus-Alertmanager-告警路由"><a href="#Prometheus-Alertmanager-告警路由" class="headerlink" title="Prometheus Alertmanager 告警路由"></a>Prometheus Alertmanager 告警路由</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>公司目前Prometheus监控了IDC数据中心的主机,中间,站点等,同时也监控了阿里云线上的rabbitmq,mysql,kong(所有资源都是ECS自己搭建的,非阿里云的saas服务)</p><p>prometheus使用了第三方的钉钉监控插件(prometheus-webhook-dingtalk),github地址: <a href="https://github.com/timonwong/prometheus-webhook-dingtalk" target="_blank" rel="noopener">https://github.com/timonwong/prometheus-webhook-dingtalk</a> </p><p>Prometheus通过alertmanager将告警信息发送到钉钉机器人</p><hr><h3 id="告警路由"><a href="#告警路由" class="headerlink" title="告警路由"></a>告警路由</h3><p>我们需要将阿里云的线上中间件监控告警发送到阿里云的钉钉群.IDC资源的监控告警发送到IDC的钉钉群,不同的钉钉群面对的人群也不同.方便监控告警信息的分类和管理.</p><p>幸好,alertmanager天生支持告警路由的功能,将不同的告警信息发送给不同的receiver接收人</p><hr><h3 id="alertmanager概念介绍"><a href="#alertmanager概念介绍" class="headerlink" title="alertmanager概念介绍"></a>alertmanager概念介绍</h3><p>Prometheus本身并不提供告警功能,所有告警信息都是发送给Alertmanager处理.Alertmanager接收到告警信息后负责将它们分组,抑制,静默,然后路由到相关接收者.</p><h5 id="Grouping分组"><a href="#Grouping分组" class="headerlink" title="Grouping分组"></a>Grouping分组</h5><p>分组功能将多个同一类型的告警合并一起后发送,这在某个服务发生故障从而影响其他几十,上百个相关依赖性的服务时非常有用,可以有效避免告警信息轰炸.例如当网络出现问题时,可能该网络下的数百个服务都出现访问故障,结果数以百计的告警被发送给Alertmanager.此时Alertmanager将同类型的服务合并到一起仅仅使用单条告警通知发送给接收者</p><a id="more"></a><h5 id="Inhibition抑制"><a href="#Inhibition抑制" class="headerlink" title="Inhibition抑制"></a>Inhibition抑制</h5><p>抑止是指如果某个告警已经触发,那么抑止其他有关该服务的告警消息.</p><p>例如如果某个集群A不可达,已经触发了告警.那么其他B,C,D等集群和服务发出的A集群不可达的告警通知将被Alertmanager抑止.告警抑制机制可以防止数百上千的重复故障告警</p><h5 id="Silences静默"><a href="#Silences静默" class="headerlink" title="Silences静默"></a>Silences静默</h5><p>Silence静默配置的作用类似于Zabbix中的Maintenance维护功能，可以配置一个时间区间和相关规则，符合该配置的事件将不会进行告警。比如明确凌晨会暂停服务，这个时候就可以提前设置好静默规则，减少不必要的告警骚扰。Prometheus的Silence规则只需要通过AlertManager的Web界面就可以完成，不需要配置文件</p><hr><h3 id="Alertmanager主要处理流程"><a href="#Alertmanager主要处理流程" class="headerlink" title="Alertmanager主要处理流程"></a>Alertmanager主要处理流程</h3><p>处理流程:<br><strong>1.</strong> 接收到Alert，根据labels判断属于哪些Route（可存在多个Route，一个Route有多个Group，一个Group有多个Alert）。<br><strong>2.</strong> 将Alert分配到Group中，没有则新建Group。<br><strong>3.</strong> 新的Group等待group_wait指定的时间（等待时可能收到同一Group的Alert），根据resolve_timeout判断Alert是否解决，然后发送通知。<br><strong>4.</strong> 已有的Group等待group_interval指定的时间，判断Alert是否解决，当上次发送通知到现在的间隔大于repeat_interval或者Group有更新时会发送通知。</p><hr><h3 id="Alertmanager配置文件"><a href="#Alertmanager配置文件" class="headerlink" title="Alertmanager配置文件"></a>Alertmanager配置文件</h3><p>Alertmanager可以通过命令行配置和yaml配置文件配置.<code>./alertmanager -h</code> 可以打印出所有的命令行配置选项.这里主要介绍<code>alertmanager.yaml</code>这个配置文件的相关配置</p><p><code>alertmanager.yaml</code> 配置文件主要字段有如下几个:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">   #顶级路由,在route下可以定义routes子路由树</span><br><span class="line">   route:</span><br><span class="line">   </span><br><span class="line">   #告警接收者,如果有多个routes,那么需要在receivers下定义多个接收者</span><br><span class="line">   receivers:</span><br><span class="line">   </span><br><span class="line">   #告警抑制配置</span><br><span class="line">   inhibit_rules:</span><br></pre></td></tr></table></figure><h5 id="route"><a href="#route" class="headerlink" title="route"></a>route</h5><p><code>route</code> 字段定义路由树的节点,以及子节点的相关配置.子节点可以从父节点继承所有配置参数.</p><p>每个告警进入到顶级配置的route.该route必须是一个默认路由,匹配所有Prometheus的告警规则.然后去遍历所有子路由.如果<code>continue</code> 设置为<code>false</code> ,在匹配到第一个子路由(routes)后就停止继续匹配,并且交给子路由的receiver发出告警.如果<code>continue</code> 设置为<code>true</code> 则继续与后续的其他子路由(routes)匹配.如果某条告警信息不匹配任何子路由,或者当前没有配置任何子路由,则交给默认的顶级route处理.</p><p>下面是一个route路由配置的案例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">route:</span><br><span class="line">  resolve_timeout: 5m #一条告警消息发出后,如果在多长时间内没有再次告警,则认为该故障已经解除,发送告警恢复消息</span><br><span class="line">  group_by: [&apos;alertname&apos;]  #根据告警规则名称分组 也就是rule文件的顶部定义的名称.还可以根据标签label或者job来分组</span><br><span class="line">  group_wait: 30s #一组警报消息到达后等待多少秒发送,这允许在这期间收集更多同组警报消息</span><br><span class="line">  group_interval: 5m #发送某组的第一个警告信息后,等待多久继续发送改组新的警告消息</span><br><span class="line">  repeat_interval: 4h #如果告警信息已经成功发送,等待多久重新发送</span><br><span class="line">  receiver: &apos;default&apos; #顶级route的默认接收者,所有未匹配到子路由的消息都发送到该receiver</span><br><span class="line">  routes: #子路由配置.在子路由下默认继承上面的group_by,group_wait等配置,也可以重写</span><br><span class="line">    - receiver: &quot;aliyun&quot; #定义子路由的接收者</span><br><span class="line">      match_re: #匹配告警信息,可以通过Match固定匹配,也可以通过match_re正则匹配</span><br><span class="line">        service: rabbitmq|mysql #只要是service这个lable标签,值为rabbitmq或者mysql的告警信息都使用该子路由处理</span><br><span class="line">    - receiver: &apos;frontend-pager&apos;</span><br><span class="line">    group_by: [product, environment] #对标签名为product和environment的告警信息作为一组</span><br><span class="line">    match:</span><br><span class="line">      team: frontend</span><br></pre></td></tr></table></figure><h5 id="receiver"><a href="#receiver" class="headerlink" title="receiver"></a>receiver</h5><p>该配置定义了一个或者多个告警消息接收器.Alertmanager并不会主动联系receiver,而是需要第三方webhook插件实现告警接收.我们这里使用的是钉钉告警插件.关于钉钉告警插件的配置在后文会有详细介绍.</p><p>有多少个子route,就对应多少个receiver.(当然也可以多个子route对应同一个receiver).receiver定义了告警消息接受地址</p><p>下面是receiver的配置,定义了2个reciever,对应上面的route.<code>aliyun</code>的receiver用来接收rabbitmq,mysql的告警通知,<code>defualt</code> 用来接收其他所有的告警消息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">      </span><br><span class="line"># 定义接收者信息 </span><br><span class="line">receivers:</span><br><span class="line">- name: &apos;default&apos;</span><br><span class="line">  webhook_configs: #第三方插件的webhook地址</span><br><span class="line">  - url: &apos;http://localhost:8060/dingtalk/default/send&apos; #这里使用的是我本地运行的钉钉插件的发送告警消息</span><br><span class="line">- name: &apos;aliyun&apos;</span><br><span class="line">  webhook_configs:</span><br><span class="line">  - url: &apos;http://localhost:8060/dingtalk/aliyun/send&apos;</span><br></pre></td></tr></table></figure><h5 id="inhibit-rules"><a href="#inhibit-rules" class="headerlink" title="inhibit_rules"></a>inhibit_rules</h5><p>在Alertmanager配置文件中,使用<code>inhibit_rules</code> 定义一组告警的抑制规则.当已经发送的告警通知匹配到target_match和target_match_re规则，当有新的告警规则如果满足source_match或者定义的匹配规则，并且以发送的告警与新产生的告警中equal定义的标签完全相同，则启动抑制机制，新的告警不会发送。</p><p>上面这段概念理解起来比较拗口,使用下面的配置作为一个案例解读:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- source_match:</span><br><span class="line">    alertname: NodeDown</span><br><span class="line">  target_match_reg:</span><br><span class="line">    severity: ~&quot;middle|low&quot;</span><br><span class="line">  equal:</span><br><span class="line">    - node</span><br></pre></td></tr></table></figure><p>当接收到一个lable名称为<code>alertname</code> ,值为<code>NodeDown</code> 的告警.并且为该告警发送了一个通知:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;alertname=&quot;NodeDown&quot;,node=&quot;x.x.x.x&quot;,...&#125; time annotation</span><br></pre></td></tr></table></figure><p>那么Alertmanager就会创建一条抑制规则:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;node=&quot;x.x.x.x&quot;,serverity=~&quot;middle|low&quot;&#125;</span><br></pre></td></tr></table></figure><p>如果新的告警满足severity=~”middle|low”,并且node标签相等(也就是equal的作用).那么该告警就会被抑制..例如该主机上的mysqldown的告警消息就不会被发送</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;alertname=&quot;MysqlDown&quot;,node=&quot;x.x.x.x&quot;,serverity=&quot;middle&quot;,...&#125; time annotation</span><br></pre></td></tr></table></figure><p>这也是我们期望看到的,因为当我们收到了某个主机节点Down的告警通知,那么该主机上的所有服务不可用的告警消息不应该再次发送.</p><hr><h3 id="钉钉插件"><a href="#钉钉插件" class="headerlink" title="钉钉插件"></a>钉钉插件</h3><p>目前使用的是prometheus-webhook-dingtalk钉钉告警插件.在github上可以直接下载二进制文件运行.默认监听在8060端口.使用<code>./prometheus-webhook-dingtalk -h</code>可以指定自定义配置文件和监听端口</p><p>该插件提供了一下路由供Alertmanager的webhook_configs使用</p><p><code>/dingtalk/&lt;profile&gt;/send</code> </p><p>这里的profile需要在插件启动时<code>-ding.profile</code> 中指定相应的名称.为了支持多个receiver,同时往多个钉钉自定义机器人发送告警消息,该插件可以指定多个<code>-ding.profile</code> 参数,从而指定多个钉钉机器人的地址.例如下面的prometheus-webhook-dingtalk启动配置文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[work@172 prometheus-webhook-dingtalk]$ systemctl cat prometheus-webhook-dingtalk</span><br><span class="line"># /usr/lib/systemd/system/prometheus-webhook-dingtalk.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=prometheus-webhook-dingtalk</span><br><span class="line">After=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Restart=on-failure</span><br><span class="line">user=root</span><br><span class="line">#定义default和aliyun2个钉钉机器人地址,用来实现告警路由,将不同的告警消息发送到不同的钉钉群</span><br><span class="line">ExecStart=/data/prometheus-webhook-dingtalk/prometheus-webhook-dingtalk \</span><br><span class="line">          --ding.profile=default=https://oapi.dingtalk.com/robot/send?access_token=c35575a65532bc15b0ff50cfad1xxxxx \</span><br><span class="line">          --ding.profile=aliyun=https://oapi.dingtalk.com/robot/send?access_token=f064adad17bc66ad30d94c7c9cxxxxxxx</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>上面定义了2个profile:<code>aliyun</code>和<code>default</code> 对应了<code>alertmanager.yaml</code>配置文件中的不同的receiver配置.需要注意的是不同的profile,它供alertmanager调用的地址也是不同的.比如:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">default的profile的地址为: http://localhost:8060/dingtalk/default/send</span><br></pre></td></tr></table></figure><p>经过测试下来,不同的监控对象告警信息发送到不同的钉钉群组,方便相关的团队和人员第一时间接收和处理</p><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://prometheus.io/docs/alerting/latest/configuration/" target="_blank" rel="noopener">https://prometheus.io/docs/alerting/latest/configuration/</a>   #alertmanager 官方介绍</p><p><a href="https://www.kancloud.cn/pshizhsysu/prometheus/1803807" target="_blank" rel="noopener">https://www.kancloud.cn/pshizhsysu/prometheus/1803807</a> #Alertmanager介绍</p><p><a href="https://www.pianshen.com/article/410280293/" target="_blank" rel="noopener">https://www.pianshen.com/article/410280293/</a> #钉钉插件介绍</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Prometheus-Alertmanager-告警路由&quot;&gt;&lt;a href=&quot;#Prometheus-Alertmanager-告警路由&quot; class=&quot;headerlink&quot; title=&quot;Prometheus Alertmanager 告警路由&quot;&gt;&lt;/a&gt;Prometheus Alertmanager 告警路由&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;公司目前Prometheus监控了IDC数据中心的主机,中间,站点等,同时也监控了阿里云线上的rabbitmq,mysql,kong(所有资源都是ECS自己搭建的,非阿里云的saas服务)&lt;/p&gt;
&lt;p&gt;prometheus使用了第三方的钉钉监控插件(prometheus-webhook-dingtalk),github地址: &lt;a href=&quot;https://github.com/timonwong/prometheus-webhook-dingtalk&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/timonwong/prometheus-webhook-dingtalk&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;Prometheus通过alertmanager将告警信息发送到钉钉机器人&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;告警路由&quot;&gt;&lt;a href=&quot;#告警路由&quot; class=&quot;headerlink&quot; title=&quot;告警路由&quot;&gt;&lt;/a&gt;告警路由&lt;/h3&gt;&lt;p&gt;我们需要将阿里云的线上中间件监控告警发送到阿里云的钉钉群.IDC资源的监控告警发送到IDC的钉钉群,不同的钉钉群面对的人群也不同.方便监控告警信息的分类和管理.&lt;/p&gt;
&lt;p&gt;幸好,alertmanager天生支持告警路由的功能,将不同的告警信息发送给不同的receiver接收人&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;alertmanager概念介绍&quot;&gt;&lt;a href=&quot;#alertmanager概念介绍&quot; class=&quot;headerlink&quot; title=&quot;alertmanager概念介绍&quot;&gt;&lt;/a&gt;alertmanager概念介绍&lt;/h3&gt;&lt;p&gt;Prometheus本身并不提供告警功能,所有告警信息都是发送给Alertmanager处理.Alertmanager接收到告警信息后负责将它们分组,抑制,静默,然后路由到相关接收者.&lt;/p&gt;
&lt;h5 id=&quot;Grouping分组&quot;&gt;&lt;a href=&quot;#Grouping分组&quot; class=&quot;headerlink&quot; title=&quot;Grouping分组&quot;&gt;&lt;/a&gt;Grouping分组&lt;/h5&gt;&lt;p&gt;分组功能将多个同一类型的告警合并一起后发送,这在某个服务发生故障从而影响其他几十,上百个相关依赖性的服务时非常有用,可以有效避免告警信息轰炸.例如当网络出现问题时,可能该网络下的数百个服务都出现访问故障,结果数以百计的告警被发送给Alertmanager.此时Alertmanager将同类型的服务合并到一起仅仅使用单条告警通知发送给接收者&lt;/p&gt;
    
    </summary>
    
      <category term="prometheus" scheme="https://jesse.top/categories/prometheus/"/>
    
    
      <category term="prometheus" scheme="https://jesse.top/tags/prometheus/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch索引迁移</title>
    <link href="https://jesse.top/2020/09/30/elk/Elasticsearch%E7%B4%A2%E5%BC%95%E8%BF%81%E7%A7%BB/"/>
    <id>https://jesse.top/2020/09/30/elk/Elasticsearch索引迁移/</id>
    <published>2020-09-30T14:59:58.000Z</published>
    <updated>2021-01-19T14:39:47.543Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Elasticsearch索引迁移"><a href="#Elasticsearch索引迁移" class="headerlink" title="Elasticsearch索引迁移"></a>Elasticsearch索引迁移</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>旧Elasticsearch版本:2.4.4</p><p>新Elasticsearch版本:2.4.4</p><p>近期dev环境服务器迁移到一台新的物理机,所以需要迁移部分Elasticsearch索引数据.</p><p>Elasticsearch索引迁移有许多方法.测试过elasticsearch-exporter.但是没有成功.报错如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[work@docker elasticsearch-exporter]$ node exporter.js -a 10.0.0.250 -b 10.0.0.101 -p 9200 -q 9200 -i mid_mg_gc_datasource_items -j mid_mg_gc_datasource_items</span><br><span class="line">Elasticsearch Exporter - Version 1.4.0</span><br><span class="line">Reading source statistics from ElasticSearch</span><br><span class="line">The source driver has not reported any documents that can be exported. Not exporting.</span><br><span class="line">Number of calls:0</span><br><span class="line">Fetched Entries:0 documents</span><br><span class="line">Processed Entries:0 documents</span><br><span class="line">Source DB Size:0 documents</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="Elasticsearch-dump"><a href="#Elasticsearch-dump" class="headerlink" title="Elasticsearch-dump"></a>Elasticsearch-dump</h3><p>本次使用elasticsearch-dump进行索引迁移.在github上可以找到具体使用方法:<a href="https://github.com/elasticsearch-dump/elasticsearch-dump" target="_blank" rel="noopener">elasticsearch-dump</a></p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install elasticdump</span><br></pre></td></tr></table></figure><p>这里稍微踩了个坑,如果报错:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pm WARN deprecated nomnom@1.8.1: Package no longer supported. Contact support@npmjs.com for more info.</span><br><span class="line">npm WARN saveError ENOENT: no such file or directory, open &apos;/home/work/package.json&apos;</span><br><span class="line">npm WARN enoent ENOENT: no such file or directory, open &apos;/home/work/package.json&apos;</span><br><span class="line">npm WARN work No description</span><br><span class="line">npm WARN work No repository field.</span><br><span class="line">npm WARN work No README data</span><br><span class="line">npm WARN work No license field.</span><br></pre></td></tr></table></figure><p>则需要初始化一下npm</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">work@docker ~]$ npm init</span><br></pre></td></tr></table></figure><p>安装完成后,进入到<code>elasticsearch dump</code>的<code>bin</code>目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[work@docker ~]$ cd node_modules/elasticdump/bin/</span><br></pre></td></tr></table></figure><h3 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h3><p>查看elasticsearchdump的具体用法</p><p>[work@docker bin]$ ./elasticdump –help</p><p><code>elaticsearchdump</code>支持两个ES跨版本迁移索引,还支持索引备份到文件,以及从文件恢复到Elasticsearch</p><h5 id="迁移mid-gm-gc-brand这个索引数据"><a href="#迁移mid-gm-gc-brand这个索引数据" class="headerlink" title="迁移mid_gm_gc_brand这个索引数据"></a>迁移mid_gm_gc_brand这个索引数据</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[work@docker bin]$ ./elasticdump --input=http://10.0.0.250:9200/mid_mg_gc_brand --output=http://10.0.0.101:9200/mid_mg_gc_brand --type=analyzer</span><br><span class="line"></span><br><span class="line">[work@docker bin]$ ./elasticdump --input=http://10.0.0.250:9200/mid_mg_gc_brand --</span><br><span class="line"></span><br><span class="line">[work@docker bin]$ ./elasticdump --input=http://10.0.0.250:9200/mid_mg_gc_brand --output=http://10.0.0.101:9200/mid_mg_gc_brand --type=data</span><br></pre></td></tr></table></figure><blockquote><p>文档中的type类型有settings, analyzer, data, mapping, alias, template</p></blockquote><h5 id="查看新服务器上的索引-迁移成功"><a href="#查看新服务器上的索引-迁移成功" class="headerlink" title="查看新服务器上的索引.迁移成功"></a>查看新服务器上的索引.迁移成功</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl -Ssl &apos;http://10.0.0.101:9200/_cat/indices?v&apos; | grep &apos;mid_mg*&apos;</span><br><span class="line">yellow open   mid_mg_gc_datasource_items             5   1         96            1     87.3kb         87.3kb</span><br><span class="line">yellow open   mid_mg_gc_brand                        5   1        966            0    312.1kb        312.1kb</span><br><span class="line">yellow open   mid_mg_gc_synonyms                     5   1        116            0     82.1kb         82.1kb</span><br><span class="line"></span><br><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl -Ssl &apos;http://10.0.0.250:9200/_cat/indices?v&apos; | grep &apos;mid_mg*&apos;</span><br><span class="line">yellow open   mid_mg_gc_datasource_items             3   1         92            9     99.1kb         99.1kb</span><br><span class="line">yellow open   mid_mg_gc_brand                        3   1        966            0    345.4kb        345.4kb</span><br><span class="line">yellow open   mid_mg_gc_synonyms                     3   1        116           16    106.1kb        106.1kb</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Elasticsearch索引迁移&quot;&gt;&lt;a href=&quot;#Elasticsearch索引迁移&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearch索引迁移&quot;&gt;&lt;/a&gt;Elasticsearch索引迁移&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;旧Elasticsearch版本:2.4.4&lt;/p&gt;
&lt;p&gt;新Elasticsearch版本:2.4.4&lt;/p&gt;
&lt;p&gt;近期dev环境服务器迁移到一台新的物理机,所以需要迁移部分Elasticsearch索引数据.&lt;/p&gt;
&lt;p&gt;Elasticsearch索引迁移有许多方法.测试过elasticsearch-exporter.但是没有成功.报错如下:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[work@docker elasticsearch-exporter]$ node exporter.js -a 10.0.0.250 -b 10.0.0.101 -p 9200 -q 9200 -i mid_mg_gc_datasource_items -j mid_mg_gc_datasource_items&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Elasticsearch Exporter - Version 1.4.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Reading source statistics from ElasticSearch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;The source driver has not reported any documents that can be exported. Not exporting.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Number of calls:	0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Fetched Entries:	0 documents&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Processed Entries:	0 documents&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Source DB Size:		0 documents&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>Openvpn客户端无法连接OpenSSL</title>
    <link href="https://jesse.top/2020/09/22/Linux-Service/Openvpn%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5%20OpenSSL-%20error/"/>
    <id>https://jesse.top/2020/09/22/Linux-Service/Openvpn客户端无法连接 OpenSSL- error/</id>
    <published>2020-09-22T14:59:58.000Z</published>
    <updated>2021-01-19T14:42:39.686Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Openvpn客户端无法连接OpenSSL-error"><a href="#Openvpn客户端无法连接OpenSSL-error" class="headerlink" title="Openvpn客户端无法连接OpenSSL: error"></a>Openvpn客户端无法连接OpenSSL: error</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>今天阿里云的Openvpn服务器部署了docker服务后,需要升级内存配置.服务器升级重启后,发现客户端无法连接Openvpn.</p><hr><h3 id="故障表现"><a href="#故障表现" class="headerlink" title="故障表现"></a>故障表现</h3><p>在openvpn客户端日志中发现下面报错信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2020-09-22 16:00:49 WARNING: No server certificate verification method has been enabled.  See http://openvpn.net/howto.html#mitm for more info.</span><br></pre></td></tr></table></figure><p>openvpn服务端日志报错信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS: Initial packet from [AF_INET]27.115.51.166:26184, sid=c0cb2b12 4b3187b2</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 OpenSSL: error:14089086:SSL routines:ssl3_get_client_certificate:certificate verify failed</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS_ERROR: BIO read tls_read_plaintext error</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS object -&gt; incoming plaintext read error</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS handshake failed</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 SIGUSR1[soft,tls-error] received, client-instance restarting</span><br><span class="line">Tue Sep 22 15:48:06 2020 27.115.51.166:33480 TLS: Initial packet from [AF_INET]27.115.51.166:33480, sid=11b9760e 97f6d068</span><br><span class="line">Tue Sep 22 15:48:07 2020 27.115.51.166:33480 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx</span><br></pre></td></tr></table></figure><p><strong>日志关键字</strong></p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OpenSSL: error:14089086:SSL routines:ssl3_get_client_certificate:certificate verify failed</span><br></pre></td></tr></table></figure><hr><h3 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h3><p>服务器重启后需要注意的几个问题:</p><p>1.检查以下几个服务是否启动:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl status openvpn@server</span><br><span class="line">systemctl status iptables</span><br></pre></td></tr></table></figure><p>2.由于docker服务会初始化iptables,所以docker启动后需要手动添加一条iptables规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A POSTROUTING -s 10.111.255.0/24 -o eth0 -j MASQUERADE</span><br></pre></td></tr></table></figure><p>3.检查ip转发功能</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysctl.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br></pre></td></tr></table></figure><p>4.检查阿里云的安全组规则,是否开通了1094的udp协议</p><hr><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>最终解决方案还是要靠google解决,在openvpn的wiki上找到了解决方案,</p><p>具体网站链接:<a href="https://community.openvpn.net/openvpn/wiki/CertificateRevocationListExpired?__cf_chl_jschl_tk__=40e85ba16653b7db84d828db819eafbc2b5a9faf-1600761449-0-AUAdZiIXwLUqBJDJAcDe9htVtUTZlGJm8m_RYLUsxLu2he8Myk5WXwzQn-CZdZyBDJRHn9clM-6y0ITsnKk0Pru3AwB7EOc0LhjyrV9unNnBv0V9_skxNC2n3per9e1TQJjcmmtwnaNl23Sp5D8p9FZYyX5PO-vtkdp1i7dyh_x1KSFwZqibI8Zt4saVoABGkfMJ3nKeUJpIIlnEhRGhfJrQwQZqvvG6EAS1CJUHRR8uqKNyqmEz90RmqcLGc8ytoOTIBYhJMs8OsPk_dA2nKA47QObzI5-4SEUZkC5ZaxhNCfkRBGx9kwimWfBJFtxJkPzI5_vkXONWXhoB0wi5cfE" target="_blank" rel="noopener">openvpn wiki</a></p><p>解决问题很简单:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#需要进入到下面这个目录下</span><br><span class="line">cd /etc/openvpn/easy-rsa/3</span><br><span class="line"></span><br><span class="line">#更新一下crl文件.</span><br><span class="line">[root@dwd-Dnsmasq 3]# ./easyrsa gen-crl</span><br><span class="line"></span><br><span class="line">Using configuration from ./openssl-1.0.cnf</span><br><span class="line"></span><br><span class="line">An updated CRL has been created.</span><br><span class="line">CRL file: /etc/openvpn/easy-rsa/3/pki/crl.pem</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Openvpn客户端无法连接OpenSSL-error&quot;&gt;&lt;a href=&quot;#Openvpn客户端无法连接OpenSSL-error&quot; class=&quot;headerlink&quot; title=&quot;Openvpn客户端无法连接OpenSSL: error&quot;&gt;&lt;/a&gt;Openvpn客户端无法连接OpenSSL: error&lt;/h2&gt;&lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;今天阿里云的Openvpn服务器部署了docker服务后,需要升级内存配置.服务器升级重启后,发现客户端无法连接Openvpn.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;故障表现&quot;&gt;&lt;a href=&quot;#故障表现&quot; class=&quot;headerlink&quot; title=&quot;故障表现&quot;&gt;&lt;/a&gt;故障表现&lt;/h3&gt;&lt;p&gt;在openvpn客户端日志中发现下面报错信息:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;2020-09-22 16:00:49 WARNING: No server certificate verification method has been enabled.  See http://openvpn.net/howto.html#mitm for more info.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;openvpn服务端日志报错信息:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;ue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS: Initial packet from [AF_INET]27.115.51.166:26184, sid=c0cb2b12 4b3187b2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 OpenSSL: error:14089086:SSL routines:ssl3_get_client_certificate:certificate verify failed&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS_ERROR: BIO read tls_read_plaintext error&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS object -&amp;gt; incoming plaintext read error&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS handshake failed&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 SIGUSR1[soft,tls-error] received, client-instance restarting&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:48:06 2020 27.115.51.166:33480 TLS: Initial packet from [AF_INET]27.115.51.166:33480, sid=11b9760e 97f6d068&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:48:07 2020 27.115.51.166:33480 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;日志关键字&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-Service" scheme="https://jesse.top/categories/Linux-Service/"/>
    
    
      <category term="Linux" scheme="https://jesse.top/tags/Linux/"/>
    
      <category term="openvpn" scheme="https://jesse.top/tags/openvpn/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://jesse.top/2020/09/16/SRE%E8%BF%90%E7%BB%B4/SRE%E8%BF%90%E7%BB%B4%E7%AC%94%E8%AE%B0/"/>
    <id>https://jesse.top/2020/09/16/SRE运维/SRE运维笔记/</id>
    <published>2020-09-16T14:30:54.835Z</published>
    <updated>2020-09-16T14:30:54.836Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SRE运维笔记-chapter-One-概念介绍"><a href="#SRE运维笔记-chapter-One-概念介绍" class="headerlink" title="SRE运维笔记-chapter One-概念介绍"></a>SRE运维笔记-chapter One-概念介绍</h2><h3 id="SRE概念"><a href="#SRE概念" class="headerlink" title="SRE概念"></a>SRE概念</h3><p>SRE(sitereliabilityengineering).中文翻译为站点可靠性工程师.SRE概念中比较重要的特性在于:</p><p>1.engineer表示SRE是工程师,使用软件工程手段设计,研发和维护业务软件系统.</p><p>2.SRE的关注焦点在于<strong>可靠性</strong>,专注于软件系统架构设计,运维流程优化,让业务软件系统运行更可靠.</p><p>3.SRE主要工作是运维业务服务,</p><h4 id="SRE团队职责"><a href="#SRE团队职责" class="headerlink" title="SRE团队职责:"></a>SRE团队职责:</h4><ul><li>可用性改进</li><li>延迟优化</li><li>性能优化</li><li>效率优化</li><li>变更管理</li><li>监控</li><li>紧急事务处理</li><li>容量规划与管理</li></ul><h3 id="SRE方法论"><a href="#SRE方法论" class="headerlink" title="SRE方法论"></a>SRE方法论</h3><ul><li><p><strong>确保长期关注研发工作</strong></p><p>运维工作限制在50%以内,剩余的时间花在研发项目上.</p></li><li><p><strong>在保障服务SLO的前提下最大化迭代速度</strong></p><ul><li>错误预算: 1-可靠性目标.<ul><li>如果一个服务的可靠性目标是99.99%,那么错误预算就是0.01%.</li></ul></li></ul></li><li><p><strong>监控系统</strong></p><p>监控系统不应该依赖人来分析警报信息.而是应该由系统自动分析.仅当需要用户执行某种操作时,才需要通知用户</p><p>监控系统需要具备三类输出:</p><ul><li>紧急警报(alert)</li><li>工单:接受工单的用户应该执行某种操作,但是并非立即执行</li><li>日志</li></ul></li><li><p><strong>应急事件处理</strong></p><ul><li>可靠性:MTTF(平均失败时间),MTTR(平均恢复时间),MTBF(平均故障间隔时间).</li><li>任何需要人工操作的事情都只会延长恢复时间,一个可以自动恢复的系统即使有更多故障发生,也比事事都要人工干预的系统可用性更高.当不可避免需要人工介入时,最佳方法是事先预案,并且记录在<strong>运维手册(playbook)</strong>中,这能降低<strong>MTTR(平均恢复)</strong>时间.</li></ul></li><li><p><strong>变更管理</strong></p><p> 大概70%的生产事故是由某种变更触发,变更管理的最佳实践是使用自动化完成以下几个项目:</p><ul><li>采用渐进式发布机制</li><li>迅速而准确的检测到问题的发生</li><li>安全迅速的回滚</li></ul></li><li><p><strong>需求预测和容量规划</strong></p><ul><li>必须有一个准确的自然增城需求预测模型,需求预测的时间应该超过资源获取时间</li><li>规划中必须有准确的非自然增长的需求来源统计</li><li>必须有周期性压力测试,以便准确的将系统原始资源与业务容量对应起来</li></ul></li><li><p><strong>资源部署</strong></p></li><li><p><strong>效率与性能</strong></p><p>一个业务总体资源使用情况是由以下几个因素驱动的:</p><ul><li>用户需求(流量)</li><li>可用容量</li><li>软件资源使用效率</li></ul><p>SRE需要通过模型预测用户需求,合理部署和配置可用容量,改进软件以提高资源使用效率,这3个因素能够推动一个服务器的效率提升.</p><blockquote><p>软件系统在负载上升的时候,会导致延迟升高.SRE的目标是根据一个预设的延迟目标部署和维护足够的容量,SRE和研发团队应该共同监控和优化整个系统的性能.</p></blockquote></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;SRE运维笔记-chapter-One-概念介绍&quot;&gt;&lt;a href=&quot;#SRE运维笔记-chapter-One-概念介绍&quot; class=&quot;headerlink&quot; title=&quot;SRE运维笔记-chapter One-概念介绍&quot;&gt;&lt;/a&gt;SRE运维笔记-chapte
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>kong利用request-transformer插件重写URL</title>
    <link href="https://jesse.top/2020/09/10/Linux-Web/kong%E5%88%A9%E7%94%A8request-transformer%E6%8F%92%E4%BB%B6%E9%87%8D%E5%86%99URL/"/>
    <id>https://jesse.top/2020/09/10/Linux-Web/kong利用request-transformer插件重写URL/</id>
    <published>2020-09-10T04:59:58.000Z</published>
    <updated>2020-09-10T14:58:43.601Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kong利用request-transformer插件重写URL"><a href="#kong利用request-transformer插件重写URL" class="headerlink" title="kong利用request-transformer插件重写URL"></a>kong利用request-transformer插件重写URL</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>最近业务整合后有一个需求,将URL:<a href="http://www.abc.com/api/item/111" target="_blank" rel="noopener">www.abc.com/api/item/111</a> 想重写成<a href="http://www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转" target="_blank" rel="noopener">www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转</a>.</p><p>使用Nginx的rewrite redirect指令可以实现URL重写需求,但是redirect会跳转到新域名,不符合需求.</p><p>刚好该业务的的前端是用Kong网关处理,所以研究kong的插件实现这个需求</p><hr><h3 id="request-transformer介绍"><a href="#request-transformer介绍" class="headerlink" title="request-transformer介绍"></a>request-transformer介绍</h3><p><strong>request-transformer</strong>是Kong官方的插件,允许修改重写用户的请求,还可以使用正则表达式匹配URL,并将匹配到的字符串保存在变量中,然后使用模板将变量转换成用户的请求</p><p>简而言之:<strong>就是重写用户的请求</strong>,包括URL,args,headers,methods等等</p><p>官方地址: <a href="https://docs.konghq.com/hub/kong-inc/request-transformer/" target="_blank" rel="noopener">reuqest-transformer官方地址</a></p><p>github项目地址: <a href="https://github.com/Kong/kong-plugin-request-transformer" target="_blank" rel="noopener">request-transformer github</a></p><a id="more"></a><hr><h3 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h3><blockquote><p>kong使用的是2.1.3最新版本,试过使用Kong1.0版本插件无法生效</p></blockquote><p>这里举2个例子说明</p><ul><li><p>将URL:/v4/jkf/branch/qrcode&amp;code=100006 重写为 /v4/jkf/branch/qrcode?code=100006.也就是将<code>&amp;</code>转换为<code>?</code></p><ul><li>首先配置Service和Route.具体配置方法略过,这里主要关心一下Route中的Path设置:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/v4/jkf/branch/qrcode\&amp;code=(?&lt;code&gt;\d+)</span><br></pre></td></tr></table></figure><p>该PATH表示:</p><p>1.匹配/v4/jkf/branch/qrcode&amp;code=任意长度数字.</p><p>2.正则<code>\d</code>表示匹配数字,并且将匹配到的数字保存为<code>code</code>变量</p><p>3.<code>\&amp;</code>表示转义URL中的<code>$</code>符号</p><blockquote><p>关闭route中的script path可选项</p></blockquote><ul><li><p>其次在该route下添加<code>request-transformer</code>插件,表示该插件只应用到此条route下.并且配置插件参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://localhost:8001/routes/21292565-e7ae-40ff-a465-f449c16b7819/plugins \</span><br><span class="line">      --data &quot;name=request-transformer&quot; \</span><br><span class="line">      --data &quot;config.replace.uri=/jkf/branch/qrcode&quot; \</span><br><span class="line">      --data &quot;config.add.querystring=code:\$(uri_captures.code)&quot;</span><br></pre></td></tr></table></figure><p>对于上面的命令行解释如下</p><ol><li><code>21292565-e7ae-40ff-a465-f449c16b7819</code>就是刚才创建的路由ID</li><li><code>config.replace.uri</code>表示将route匹配到的PATH重写为<code>/jkf/branch/qrcode</code></li><li><code>onfig.add.querystring</code>表示给URL添加args参数</li><li><code>code:\$(uri_captures.code)</code>.参数名是<code>code</code>,<code>uri_captures.code</code>表示获取route的PATH中code变量的值,由于命令行shell环境关系,所以要在变量符号<code>$</code>前进行转义.</li></ol></li></ul></li></ul><pre><code>或者也可以使用konga的UI管理平台添加和编辑插件</code></pre><p><img src="https://img2.jesse.top/image-20200910160826719.png" alt=""></p><p>  <img src="https://img2.jesse.top/image-20200910160949836.png" alt="image-20200910160949836"></p><ul><li><p>最后,<code>reload</code>Kong进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker ~]# docker exec kong kong reload</span><br></pre></td></tr></table></figure><p><strong>验证</strong></p><p>本地访问网站:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">✘ huangyong@huangyong-Macbook-Pro  ~  curl -XGET https://m.devapi.xxx.com/v4/jkf/branch/qrcode\&amp;code\=100006</span><br></pre></td></tr></table></figure><p>Kong和后端nginx日志如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">172.18.0.1 - - [10/Sep/2020:16:52:11 +0800] &quot;GET /jkf/branch/qrcode?code=100006 HTTP/1.1&quot; 200 163 &quot;-&quot; &quot;curl/7.54.0&quot; &quot;10.0.4.9, 172.18.0.2&quot; 10.0.4.9, 172.18.0.2, 172.18.0.1 &quot;2b7dcdc621d1928f456d561f31e95b25&quot;0.065 0.065</span><br></pre></td></tr></table></figure><p>可以看到已经成功重写了URL</p></li></ul><hr><ul><li>第二个例子,将/api/item/111 重写为/open/item/itemdetail?id=111</li></ul><p>将URL后面的数字拼接到id的值,作为参数拼接成URL后,传递给后端</p><p>1.添加Service和Routes,Routes的PATH部分如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#该PATH只匹配/api/item/数字格式的URL.并且将\d+正则匹配到的数字保存为变量id</span><br><span class="line">/api/item/(?&lt;id&gt;\d+)$</span><br></pre></td></tr></table></figure><p>2.添加和配置插件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://localhost:8001/routes/38fce1b7-2a36-42cb-9619-f30539889137/plugins \</span><br><span class="line">      --data &quot;name=request-transformer&quot; \</span><br><span class="line">      --data &quot;config.replace.uri=/open/item/itemdetail&quot; \</span><br><span class="line">      --data &quot;config.add.querystring=id:\$(uri_captures.id)&quot;</span><br></pre></td></tr></table></figure><p>3.重载kong</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker ~]# docker exec kong kong reload</span><br></pre></td></tr></table></figure><p>4.验证</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huangyong@huangyong-Macbook-Pro  ~  curl -XGET https://m.devapi.xxx.com/api/item/111</span><br></pre></td></tr></table></figure><p>后端日志如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">172.18.0.1 - - [10/Sep/2020:17:11:43 +0800] &quot;GET /open/item/itemdetail?id=111 HTTP/1.1&quot; 200 160 &quot;-&quot; &quot;curl/7.54.0&quot; &quot;10.0.4.9, 172.18.0.2&quot; 10.0.4.9, 172.18.0.2, 172.18.0.1 &quot;54ac589d36f90c1ef99ba6a43c4d488e&quot;0.105 0.105</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kong利用request-transformer插件重写URL&quot;&gt;&lt;a href=&quot;#kong利用request-transformer插件重写URL&quot; class=&quot;headerlink&quot; title=&quot;kong利用request-transformer插件重写URL&quot;&gt;&lt;/a&gt;kong利用request-transformer插件重写URL&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;最近业务整合后有一个需求,将URL:&lt;a href=&quot;http://www.abc.com/api/item/111&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;www.abc.com/api/item/111&lt;/a&gt; 想重写成&lt;a href=&quot;http://www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;使用Nginx的rewrite redirect指令可以实现URL重写需求,但是redirect会跳转到新域名,不符合需求.&lt;/p&gt;
&lt;p&gt;刚好该业务的的前端是用Kong网关处理,所以研究kong的插件实现这个需求&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;request-transformer介绍&quot;&gt;&lt;a href=&quot;#request-transformer介绍&quot; class=&quot;headerlink&quot; title=&quot;request-transformer介绍&quot;&gt;&lt;/a&gt;request-transformer介绍&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;request-transformer&lt;/strong&gt;是Kong官方的插件,允许修改重写用户的请求,还可以使用正则表达式匹配URL,并将匹配到的字符串保存在变量中,然后使用模板将变量转换成用户的请求&lt;/p&gt;
&lt;p&gt;简而言之:&lt;strong&gt;就是重写用户的请求&lt;/strong&gt;,包括URL,args,headers,methods等等&lt;/p&gt;
&lt;p&gt;官方地址: &lt;a href=&quot;https://docs.konghq.com/hub/kong-inc/request-transformer/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;reuqest-transformer官方地址&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;github项目地址: &lt;a href=&quot;https://github.com/Kong/kong-plugin-request-transformer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;request-transformer github&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>zabbix监控vmware主机以及GuestOS</title>
    <link href="https://jesse.top/2020/08/26/%E7%9B%91%E6%8E%A7/zabbix%E7%9B%91%E6%8E%A7vmware%E4%B8%BB%E6%9C%BA%E4%BB%A5%E5%8F%8AGuestOS/"/>
    <id>https://jesse.top/2020/08/26/监控/zabbix监控vmware主机以及GuestOS/</id>
    <published>2020-08-26T01:20:58.000Z</published>
    <updated>2020-08-26T23:55:20.314Z</updated>
    
    <content type="html"><![CDATA[<h3 id="zabbix监控vmware主机以及GuestOS"><a href="#zabbix监控vmware主机以及GuestOS" class="headerlink" title="zabbix监控vmware主机以及GuestOS"></a>zabbix监控vmware主机以及GuestOS</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>ESXI主机无法安装zabbix agent,所以不能使用传统的agent客户端监控vmware主机,但是Zabbix有自导的vmware hypervisors监控模板.Zabbix 通过 vmware collector 进程来监控虚拟机,使用SOAP协议从vmware web服务器获取必要的监控信息.</p><hr><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p>1.在zabbix服务器修改<code>zabbix_server.conf</code>配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">StartVMwareCollectors=6</span><br><span class="line">VMwareCacheSize=50M</span><br><span class="line">VMwareFrequency=10</span><br><span class="line">VMwarePerfFrequency=60</span><br><span class="line">VMwareTimeout=300</span><br></pre></td></tr></table></figure><a id="more"></a><p><strong>说明</strong>: </p><p><strong>StartVMwareCollectors</strong>：vmware 收集器实例的数量。<br>此值取决于要监控的 VMware 服务的数量。在大多数情况下，这应该是：<code>servicenum &lt; StartVMwareCollectors &lt; (servicenum * 2)</code>其中 servicenum 是 VMware 服务的数量。</p><p>例如：如果您有 1 个 VMware 服务要将 StartVMwareCollectors 设置为 2，那么如果您有 3 个 VMware 服务，请将其设置为 5。请注意，在大多数情况下，此值不应小于 2，不应大于 VMware 数量的 2 倍服务。</p><p>2.重启zabbix服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart zabbix_server</span><br></pre></td></tr></table></figure><hr><h4 id="Esxi物理主机配置"><a href="#Esxi物理主机配置" class="headerlink" title="Esxi物理主机配置"></a>Esxi物理主机配置</h4><p>1.登陆Esxi web界面: <a href="https://172.16.0.55" target="_blank" rel="noopener">https://172.16.0.55</a><br>2.在<code>manage</code>—<code>system</code>—-<code>advanced settings</code>.修改<code>Config.HostAgent.plugins.solo.enableMob</code>的值为True</p><p><img src="https://img2.jesse.top/image-20200818112727513.png" alt="image-20200818112727513"></p><p>3.访问:<a href="https://172.16.0.55/mob/?moid=ha-host&amp;doPath=hardware.systemInfo" target="_blank" rel="noopener">https://172.16.0.55/mob/?moid=ha-host&amp;doPath=hardware.systemInfo</a><br>记录UUID<br><img src="https://img2.jesse.top/image-20200818112949235.png" alt="image-20200818112949235"></p><p>4.在zabbix添加主机</p><p><img src="https://img2.jesse.top/image-20200818113114835.png" alt="image-20200818113114835"></p><ul><li><strong>主机名称</strong>:上面查到的UUID</li><li><strong>IP地址</strong>:Esxi的IP地址</li><li><strong>端口</strong>:80</li></ul><p><strong>模板</strong>:</p><p><img src="https://img2.jesse.top/image-20200818113315388.png" alt="image-20200818113315388"></p><p><strong>宏</strong></p><p><img src="https://img2.jesse.top/image-20200818113428859.png" alt="image-20200818113428859"></p><ul><li><strong>password</strong>: Esxi主机密码</li><li><p><strong>URL</strong>: <a href="https://Esxi_IP/sdk" target="_blank" rel="noopener">https://Esxi_IP/sdk</a> </p></li><li><p><strong>username</strong>: ESXI主机用户名</p></li><li><strong>UUID</strong>: 上文记录的UUID</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;zabbix监控vmware主机以及GuestOS&quot;&gt;&lt;a href=&quot;#zabbix监控vmware主机以及GuestOS&quot; class=&quot;headerlink&quot; title=&quot;zabbix监控vmware主机以及GuestOS&quot;&gt;&lt;/a&gt;zabbix监控vmware主机以及GuestOS&lt;/h3&gt;&lt;h4 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h4&gt;&lt;p&gt;ESXI主机无法安装zabbix agent,所以不能使用传统的agent客户端监控vmware主机,但是Zabbix有自导的vmware hypervisors监控模板.Zabbix 通过 vmware collector 进程来监控虚拟机,使用SOAP协议从vmware web服务器获取必要的监控信息.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h4&gt;&lt;p&gt;1.在zabbix服务器修改&lt;code&gt;zabbix_server.conf&lt;/code&gt;配置文件&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;StartVMwareCollectors=6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwareCacheSize=50M&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwareFrequency=10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwarePerfFrequency=60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwareTimeout=300&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="监控" scheme="https://jesse.top/categories/%E7%9B%91%E6%8E%A7/"/>
    
    
      <category term="zabbix监控" scheme="https://jesse.top/tags/zabbix%E7%9B%91%E6%8E%A7/"/>
    
  </entry>
  
</feed>

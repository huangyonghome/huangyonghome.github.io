<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jesse&#39;s home</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jesse.top/"/>
  <updated>2021-02-06T15:48:07.531Z</updated>
  <id>https://jesse.top/</id>
  
  <author>
    <name>Jesse</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kubernetes 网络</title>
    <link href="https://jesse.top/2021/02/06/kubernetes/network/kubernetes%20network/"/>
    <id>https://jesse.top/2021/02/06/kubernetes/network/kubernetes network/</id>
    <published>2021-02-06T15:39:58.000Z</published>
    <updated>2021-02-06T15:48:07.531Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kubernetes-网络"><a href="#kubernetes-网络" class="headerlink" title="kubernetes 网络"></a>kubernetes 网络</h2><h3 id="1-开篇"><a href="#1-开篇" class="headerlink" title="1.开篇"></a>1.开篇</h3><p>​    Docker容器诞生以来,,如何确定合适的网络方案是亟待解决的难题之一.在日趋复杂的业务场景下,网络的复杂性也呈几何级数上升.本篇首先回顾了Docker容器的网络通信,然后介绍Kubernertes的网络模型.在Kubernetes集群中,IP地址的分配对象是以Pod为单位,而非容器.同一个Pod内的所有容器共享同一个网络名称空间</p><a id="more"></a><h3 id="2-容器网络基础"><a href="#2-容器网络基础" class="headerlink" title="2 容器网络基础"></a>2 容器网络基础</h3><p>​    一个Linux容器的网络栈是被隔离在它自己的Network Namespace中，Network Namespace包括了：网卡（Network Interface），回环设备（Lookback Device），路由表（Routing Table）和iptables规则，对于服务进程来讲这些就构建了它发起请求和相应的基本环境。而要实现一个容器网络，离不开以下Linux网络功能：</p><ul><li>网络命名空间：将独立的网络协议栈隔离到不同的命令空间中，彼此间无法通信</li><li>Veth Pair：Veth设备对的引入是为了实现在不同网络命名空间的通信，总是以两张虚拟网卡（veth peer）的形式成对出现的。并且，从其中一端发出的数据，总是能在另外一端收到</li><li>Iptables/Netfilter：Netfilter负责在内核中执行各种挂接的规则（过滤、修改、丢弃等），运行在内核中；Iptables模式是在用户模式下运行的进程，负责协助维护内核中Netfilter的各种规则表；通过二者的配合来实现整个Linux网络协议栈中灵活的数据包处理机制</li><li>网桥：网桥是一个二层网络虚拟设备，类似交换机，主要功能是通过学习而来的Mac地址将数据帧转发到网桥的不同端口上</li><li>路由: Linux系统包含一个完整的路由功能，当IP层在处理数据发送或转发的时候，会使用路由表来决定发往哪里</li></ul><h3 id="2-Docker容器网络模型"><a href="#2-Docker容器网络模型" class="headerlink" title="2.Docker容器网络模型"></a>2.Docker容器网络模型</h3><h4 id="2-1-同节点容器通信"><a href="#2-1-同节点容器通信" class="headerlink" title="2.1 同节点容器通信"></a>2.1 同节点容器通信</h4><p>​    Docker容器网络的原始模型主要用到的就是Bridge桥接网络.Docker守护进程首次启动时,会在当前宿主机节点创建一个名为<code>docker0</code> 的虚拟网桥设备.并默认配置其使用172.17.0.0/16的网络.</p><blockquote><p>Host和Container网络模型使用场景非常少.不再费篇幅介绍 </p></blockquote><p>​    并且为该主机节点上的每一个容器分配一个虚拟的以<code>vethxxx</code> 开头的虚拟网卡.从而使得同一节点下的所有容器都可以在二层网络模式下.利用<code>docker0</code> 虚拟网桥实现容器和容器之间,容器和宿主机节点之间的网络通信.</p><p>​    同一宿主机节点下的容器网络通信方式如下</p><p><img src="https://img2.jesse.top/image-20210206153900657.png" alt="image-20210206153900657"></p><h4 id="2-2-不同节点容器通信"><a href="#2-2-不同节点容器通信" class="headerlink" title="2.2 不同节点容器通信"></a>2.2 不同节点容器通信</h4><p>​    以上是同节点上的容器通信方式.对于不同节点的容器之间进行通信,Docker则无能为力.因为每个节点的docker0网桥分配的虚拟IP都是同一网段,所以不同宿主机节点上的容器可能使用的是同一个IP地址,双方并不清楚对方容器具体在哪台节点.</p><p>​    解决此问题的方式是NAT.所有容器均会被NAT隐藏在节点网络之内.他们发往Docker主机外部的所有流量都会SNAT后出去,容器若要接入Docker主机外部的流量,则需要事先将网络端口暴露到宿主机的端口..然后对方容器的流量达到宿主机后再执行DNAT转发给目的容器.</p><p>不同宿主机节点下的容器网络通信方式如下</p><p><img src="https://img2.jesse.top/image-20210206155556803.png" alt="image-20210206155556803"></p><p>这种解决方式在网络规模庞大的时候兼职就是个灾难.转发效率非常低下,宿主机上端口变成一种稀缺资源.</p><h3 id="3-Kubernetes网络模型"><a href="#3-Kubernetes网络模型" class="headerlink" title="3. Kubernetes网络模型"></a>3. Kubernetes网络模型</h3><p>Kubernetes的网络模型主要用于解决四类通信需求:</p><h4 id="1-容器间通信"><a href="#1-容器间通信" class="headerlink" title="(1) 容器间通信"></a>(1) 容器间通信</h4><p>​    Pod对象内的各容器共享同一个网络名称空间.所有运行于同一个Pod内部的容器与同一主机上的多个进程类似.彼此之间可以通过<code>localhost</code> 或者<code>lo</code> 回环接口进行通信.</p><p>例如下图3-1所示,每个节点上的Container1和container2容器在一个Pod内部,共享同一个IP地址和网络接口</p><p><img src="https://img2.jesse.top/image-20210206160446993.png" alt="image-20210206160446993"></p><p>​                                                                                                                图 3-1 Pod网络</p><h4 id="2-Pod间通信"><a href="#2-Pod间通信" class="headerlink" title="(2) Pod间通信"></a>(2) Pod间通信</h4><p>​    Kubernertes要求每个Pod对象需要运行于同一个平面网络中,并且都拥有一个集群内全局唯一的IP地址,可以直接于其他Pod通信.例如上图3-1中的Pod P和Pod Q之间通信.另外，运行Pod的各节点也会通过桥接设备等持有此平面网络中的一个IP地址，如图3-1中的cbr0接口，这就意味着Node到Pod间的通信也可在此网络上直接进行。因此，Pod间的通信或Pod到Node间的通信比较类似于同一IP网络中主机间进行的通信。</p><h4 id="3-Service与Pod间通信"><a href="#3-Service与Pod间通信" class="headerlink" title="(3) Service与Pod间通信"></a>(3) Service与Pod间通信</h4><p>​    Service资源的专用网络也称为集群网络（Cluster Network），需要在启动kube-apiserver时经由“–service-cluster-ip-range”选项进行指定，如10.96.0.0/12，而每个Service对象在此网络中均拥一个称为Cluster-IP的固定地址。管理员或用户对Service对象的创建或更改操作由API Server存储完成后触发各节点上的kube-proxy，并根据代理模式的不同将其定义为相应节点上的iptables规则或ipvs规则，借此完成从Service的Cluster-IP与Pod-IP之间的报文转发，如图3-2所示。</p><p><img src="https://img2.jesse.top/image-20210206161001841.png" alt="image-20210206161001841"></p><p>​                                                                                                         图 3-2 Service和Pod</p><h4 id="4-集群外部到Pod对象之间的通信"><a href="#4-集群外部到Pod对象之间的通信" class="headerlink" title="(4) 集群外部到Pod对象之间的通信"></a>(4) 集群外部到Pod对象之间的通信</h4><p>​    将集群外部的流量引入到Pod对象的方式有受限于Pod所在的工作节点范围的节点端口（nodePort）和主机网络（hostNetwork）两种，以及工作于集群级别的NodePort或LoadBalancer类型的Service对象。不过，即便是四层代理的模式也要经由两级转发才能到达目标Pod资源：请求流量首先到达外部负载均衡器，由其调度至某个工作节点之上，而后再由工作节点的netfilter（kube-proxy）组件上的规则（iptables或ipvs）调度至某个目标Pod对象。</p><h3 id="4-Kubernetes-CNI插件"><a href="#4-Kubernetes-CNI插件" class="headerlink" title="4. Kubernetes CNI插件"></a>4. Kubernetes CNI插件</h3><p>​    Kubernetes设计了以上四种网络模型.但是Kubernetes自己并不负责网络具体工作,而是交给的了第三方网络插件.为了规范以及兼容各种解决方案.CoreOS和Google联合制定了CNI（Container Network Interface）标准，旨在定义容器网络模型规范。它连接了两个组件：容器管理系统和网络插件。它们之间通过JSON格式的文件进行通信，以实现容器的网络功能.具体的网络工作均由插件来实现，包括创建容器netns、关联网络接口到对应的netns以及为网络接口分配IP等。</p><p>CNI的基本思想是:容器运行时环境在创建容器时，先创建好网络名称空间（netns），然后调用CNI插件为这个netns配置网络，而后再启动容器内的进程。</p><p>Kubernetes要求网络插件需要满足以下基本原则:</p><ul><li>Pod无论运行在任何节点都可以互相直接通信，而不需要借助NAT地址转换实现。</li><li>Node与Pod可以互相通信，在不限制的前提下，Pod可以访问任意网络。</li><li>Pod拥有独立的网络栈，Pod看到自己的地址和外部看见的地址应该是一样的，并且同个Pod内所有的容器共享同个网络栈。 </li></ul><p>CNI本身只是规范，付诸生产还需要有特定的实现。目前，CNI提供的插件分为三类：main、meta和ipam。main一类的插件主要在于实现某种特定的网络功能，例如loopback、bridge、macvlan和ipvlan等；meta一类的插件自身并不提供任何网络实现，而是用于调用其他插件，例如调用flannel；ipam仅用于分配IP地址，而不提供网络实现。</p><p>CNI具有很强的扩展性和灵活性，例如，如果用户对某个插件具有额外的需求，则可以通过输入中的args和环境变量CNI_ARGS进行传递，然后在插件中实现自定义的功能，这大大增加了它的扩展性。另外，CNI插件将main和ipam分开，赋予了用户自由组合它们的机制，甚至一个CNI插件也可以直接调用另外一个CNI插件。CNI目前已经是Kubernetes当前推荐的网络方案。常见的CNI网络插件包含如下这些主流的项目.</p><ul><li><p><strong>Flannel</strong>.</p><p>一个为Kubernetes提供叠加网络的网络插件，它基于Linux TUN/TAP，使用UDP封装IP报文来创建叠加网络，并借助etcd维护网络的分配情况。</p></li><li><p><strong>Calico</strong></p><p>一个基于BGP的三层网络插件，并且也支持网络策略来实现网络的访问控制；它在每台机器上运行一个vRouter，利用Linux内核来转发网络数据包，并借助iptables实现防火墙等功能。</p></li><li><p><strong>Weave Net</strong>：</p><p>Weave Net是一个多主机容器的网络方案，支持去中心化的控制平面，在各个host上的wRouter间建立Full Mesh的TCP连接，并通过Gossip来同步控制信息。</p></li></ul><h3 id="5-Kubernetes-CNI网络通信"><a href="#5-Kubernetes-CNI网络通信" class="headerlink" title="5. Kubernetes CNI网络通信"></a>5. Kubernetes CNI网络通信</h3><p>实际上CNI的容器网络通信流程跟前面的基础网络一样，只是CNI维护了一个单独的网桥来代替 docker0。这个网桥的名字就叫作：CNI 网桥，它在宿主机上的设备名称默认是：cni0。cni的设计思想，就是：Kubernetes在启动Infra容器之后，就可以直接调用CNI网络插件，为这个Infra容器的Network Namespace，配置符合预期的网络栈。</p><p>CNI插件三种网络实现模式：</p><p><img src="https://img2.jesse.top/cni-network.png" alt="img"></p><ul><li>overlay 模式是基于隧道技术实现的，整个容器网络和主机网络独立，容器之间跨主机通信时将整个容器网络封装到底层网络中，然后到达目标机器后再解封装传递到目标容器。不依赖与底层网络的实现。实现的插件有flannel(UDP、vxlan)、calico(IPIP)等等</li><li>三层路由模式中容器和主机也属于不通的网段，他们容器互通主要是基于路由表打通，无需在主机之间建立隧道封包。但是限制条件必须依赖大二层同个局域网内。实现的插件有flannel(host-gw)、calico(BGP)等等</li><li>underlay网络是底层网络，负责互联互通。 容器网络和主机网络依然分属不同的网段，但是彼此处于同一层网络，处于相同的地位。整个网络三层互通，没有大二层的限制，但是需要强依赖底层网络的实现支持.实现的插件有calico(BGP)等等</li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>kubernetes容器网络: <a href="https://tech.ipalfish.com/blog/2020/03/06/kubernetes_container_network/" target="_blank" rel="noopener">https://tech.ipalfish.com/blog/2020/03/06/kubernetes_container_network/</a> (伴鱼团队)</p><p>&lt;kubernetes进阶实战&gt; 11.1 马永亮</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kubernetes-网络&quot;&gt;&lt;a href=&quot;#kubernetes-网络&quot; class=&quot;headerlink&quot; title=&quot;kubernetes 网络&quot;&gt;&lt;/a&gt;kubernetes 网络&lt;/h2&gt;&lt;h3 id=&quot;1-开篇&quot;&gt;&lt;a href=&quot;#1-开篇&quot; class=&quot;headerlink&quot; title=&quot;1.开篇&quot;&gt;&lt;/a&gt;1.开篇&lt;/h3&gt;&lt;p&gt;​    Docker容器诞生以来,,如何确定合适的网络方案是亟待解决的难题之一.在日趋复杂的业务场景下,网络的复杂性也呈几何级数上升.本篇首先回顾了Docker容器的网络通信,然后介绍Kubernertes的网络模型.在Kubernetes集群中,IP地址的分配对象是以Pod为单位,而非容器.同一个Pod内的所有容器共享同一个网络名称空间&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
      <category term="network" scheme="https://jesse.top/categories/kubernetes/network/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes DNS</title>
    <link href="https://jesse.top/2021/02/06/kubernetes/service/DNS/"/>
    <id>https://jesse.top/2021/02/06/kubernetes/service/DNS/</id>
    <published>2021-02-06T15:39:58.000Z</published>
    <updated>2021-02-06T15:40:39.583Z</updated>
    
    <content type="html"><![CDATA[<h2 id="DNS介绍"><a href="#DNS介绍" class="headerlink" title="DNS介绍"></a>DNS介绍</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>kubernets的所有资源.包括Service,Pod都有生命周期,会频繁的销毁和创建.这些资源的IP地址也会随之动态变化.所以Kubernetes使用DNS实现通过资源名解析IP地址.</p><h3 id="DNS服务器"><a href="#DNS服务器" class="headerlink" title="DNS服务器"></a>DNS服务器</h3><p>Kubernetes集群安装了默认的Core-dns组件(通过Pod方式运行).以及kube-dns的service.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl get pods -n kube-system | grep dns</span><br><span class="line">coredns-7f9c544f75-9sh28                   1/1     Running   2          324d</span><br><span class="line">coredns-7f9c544f75-jgmqq                   1/1     Running   2          324d</span><br><span class="line"></span><br><span class="line">#下方这个10.96.0.10就是kubernetes集群的内部DNS服务器</span><br><span class="line">[root@k8s-master ~]$kubectl get svc -n kube-system | grep dns</span><br><span class="line">kube-dns         ClusterIP   10.96.0.10     &lt;none&gt;        53/UDP,53/TCP,9153/TCP   324d</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="pod容器内部dns解析"><a href="#pod容器内部dns解析" class="headerlink" title="pod容器内部dns解析"></a>pod容器内部dns解析</h3><p>创建一个临时的pod容器,测试DNS解析效果.下面的命令临时运行了一个busybox的镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl run -it dns-test --rm --image=busybox:1.28.4 -- sh</span><br></pre></td></tr></table></figure><blockquote><p>不要使用latest版本的镜像,其dns解析有问题.最好使用1.28.4版本的</p></blockquote><p>下方是Pod容器的内部dns服务器信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/ # cat /etc/resolv.conf</span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local localdomain</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure><p><code>resolv.conf</code> 配置文件说明</p><p><strong>nameserver</strong>：指明DNS服务器地址.也就是上文提到的<code>kube-dns</code> 的service</p><p><strong>search</strong>：当原始域名不能被DNS解析时，resolver会将该域名加上search指定的参数，重新请求DNS，直到被正确解析或试完search指定的列表为止 options：dns配置 </p><p><strong>ndots:5</strong>：所有DNS查询中，如果“.”的个数少于5个，则会根据search中配置的列表依次在对应域中先进行搜索，如果没有返回，则最后再直接查询域名本身</p><p>为了了解<code>search</code>和<code>ndots</code> 的概念,我们先要了解FQDN的概念.<code>FQDN(Fully qualified domain name)</code>即完整域名。一般来说如果一个域名以<code>.</code>结束，就表示一个完整域名。比如<code>www.abc.xyz.</code>就是一个<code>FQDN</code>，而<code>www.abc.xyz</code>则不是<code>FQDN</code>。了解了这个概念之后我们就来看<code>search</code>和<code>options ndots</code>。</p><p>如果一个域名是<code>FQDN</code>，那么这个域名会被转发给DNS服务器进行解析。如果域名不是<code>FQDN</code>，那么这个域名会到<code>search</code>搜索解析，还是通过一个例子说明，比如访问<code>abc.xyz</code>这个域名，因为它并不是一个<code>FQDN</code>，所以它会和<code>search</code>域中的值进行组合而变成一个<code>FQDN</code>，以上文的<code>resolv.conf</code>为例，这域名会这样组合：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">abx.xyz.default.svc.cluster.local.</span><br><span class="line">abc.xyz.svc.cluster.local.</span><br><span class="line">abc.xyz.cluster.local.</span><br></pre></td></tr></table></figure><p>然后这些域名先被<code>kube-DNS</code>解析，如果没有解析成功再由宿主机的<code>DNS</code>服务器进行解析。</p><p>而<code>ndots</code>是用来表示一个域名中<code>.</code>的个数在不小于该值的情况下会被认为是一个<code>FQDN</code>。简单说这个属性用来判断一个不是以<code>.</code>结束的域名在什么条件下会被认定为是一个<code>FQDN</code>.上面的示例中ndots为5,也就是说如果一个域名中<code>.</code>的数量大于等于5，即使域名不是以<code>.</code>结尾，也会被认定为是一个<code>FQDN</code>。比如：域名是<code>abc.xyz.xxx.yyy.zzz.aaa</code>这个域名就是<code>FQDN</code>.</p><p>之所以会有<code>search</code>域主要还是为了方便k8s内部服务之间的访问。比如：k8s在同一个<code>namespace</code>下是可以直接通过服务名称进行访问的，其原理就是会在<code>search</code>域查找，比如上面的<code>resolv.conf</code>中<code>jplat、oms-dev</code>着两个其实都是这两个pod所在的<code>namespace</code>的名称。所以通过服务名称访问的时候，会和<code>search</code>域进行组合，这样最终域名会组合成<code>servicename.namespace.svc.cluster.local</code>。而如果是跨<code>namespace</code>访问，则可以通过<code>servicename.namespace</code>这样的形式，在通过和<code>search</code>域组合，依然可以得到<code>servicename.namespace.svc.cluster.local</code>。</p><h3 id="DNS解析"><a href="#DNS解析" class="headerlink" title="DNS解析"></a>DNS解析</h3><h4 id="解析对象"><a href="#解析对象" class="headerlink" title="解析对象"></a>解析对象</h4><p>Kubernetes集群中的每个Service资源都会被指派一个DNS名称.客户端Pod的DNS搜索列表默认是搜索自己的<code>namespace</code> 名称空间内的资源.</p><p>例如上文的<code>resolv.conf</code> 文件内的search搜索列表为<code>search default.svc.cluster.local svc.cluster.local cluster.local localdomain</code> .此时Pod可以直接搜索<code>default</code> 名称空间下的所有Service:</p><p>例如.使用上面的临时busybox容器解析<code>my-svc</code>的Service</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/ # nslookup my-svc</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      my-svc</span><br><span class="line">Address 1: 10.96.51.58 my-svc.default.svc.cluster.local</span><br></pre></td></tr></table></figure><p>上面的IP地址<code>10.96.51.58</code> 表示成功解析到该Service的IP.<code>my-svc.default.svc.cluster.local</code>这个就是该Service的FQDN完全限定域名.</p><p>其中:</p><p><strong><code>default</code></strong> —表示名称空间,我们的名称空间名字就是默认的default</p><p><strong><code>svc</code></strong> —————-表示资源类型,这里是Service</p><p><strong><code>cluster.local</code></strong> –k8s集群域名</p><p>也可以解析其他名称空间内的资源,比如解析<code>kube-system</code> 名称空间下的DNS服务器的Service.(DNS服务器本身也会被指定一个DNS名称).就可以通过<code>&lt;svc-name&gt;.&lt;namespace-name&gt;</code> 实现.比如下面解析<code>kube-system</code>名称空间下的<code>kube-dns</code>的Service</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/ # nslookup kube-dns.kube-system</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      kube-dns.kube-system</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br></pre></td></tr></table></figure><p>实际上DNS解析的是完全FQDN域名,只不过后面一部分内容<code>default.svc.cluster.local</code> 可以省略罢了.默认就是解析当前名称空间下的资源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">/ # nslookup my-svc</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      my-svc</span><br><span class="line">Address 1: 10.96.51.58 my-svc.default.svc.cluster.local</span><br><span class="line"></span><br><span class="line">#等同于:</span><br><span class="line">/ # nslookup my-svc.default.svc.cluster.local</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      my-svc.default.svc.cluster.local</span><br><span class="line">Address 1: 10.96.51.58 my-svc.default.svc.cluster.local</span><br></pre></td></tr></table></figure><p>在kubernetes官网中也提到:</p><p>假设在 Kubernetes 集群的名字空间 <code>bar</code> 中，定义了一个服务 <code>foo</code>。 运行在名字空间 <code>bar</code> 中的 Pod 可以简单地通过 DNS 查询 <code>foo</code> 来找到该服务。 运行在名字空间 <code>quux</code> 中的 Pod 可以通过 DNS 查询 <code>foo.bar</code> 找到该服务。</p><h4 id="Service-A记录"><a href="#Service-A记录" class="headerlink" title="Service A记录"></a>Service A记录</h4><p>对于普通的Service资源.会以<code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local</code>这种形式被分配一个DNS A记录.也就是上文中的<code>my-svc</code>的<code>10.96.51.58</code>这个IP地址.</p><p>如果是对于无头服务(headless service).这种service没有IP.但是也会以上面的形式被指派一个DNS的A记录.只不过这种记录和普通Service不同,而是被解析成对应服务的POD集合的Pod的IP.客户端使用标准的负载均衡策略从这组Pod中进行选择.</p><p>例如下面创建一个headless的svc.和普通svc的区别在于<code>clusterIP的值为None</code>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$cat deployment-kubia-v1.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: hsq1</span><br><span class="line">  labels:</span><br><span class="line">     app: hsq-openapi</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx</span><br><span class="line">      image: nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: hsq2</span><br><span class="line">  labels:</span><br><span class="line">     app: hsq-openapi</span><br><span class="line">spec:</span><br><span class="line">    containers:</span><br><span class="line">       - name: nginx</span><br><span class="line">         image: nginx</span><br><span class="line">---</span><br><span class="line">#一个yaml文件可以定义多种资源,中间用---隔开</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: hsq-openapi</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: hsq-openapi</span><br><span class="line">  clusterIP: None</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br></pre></td></tr></table></figure><blockquote><p>headless服务一般用于statefulset资源.不能用于deployment控制器</p></blockquote><p>创建该文件后查看<code>hsq-openapi</code>service:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl describe svc hsq-openapi</span><br><span class="line">Name:              hsq-openapi</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       kubectl.kubernetes.io/last-applied-configuration:</span><br><span class="line">                     &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;hsq-openapi&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;spec&quot;:&#123;&quot;clusterIP&quot;:&quot;None&quot;,&quot;p...</span><br><span class="line">Selector:          app=hsq-openapi</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                None</span><br><span class="line">Port:              &lt;unset&gt;  80/TCP</span><br><span class="line">TargetPort:        80/TCP</span><br><span class="line">Endpoints:         10.100.36.66:80,10.100.36.69:80  #这里是后端pod列表</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br></pre></td></tr></table></figure><p><strong>headless类型服务的DNS解析</strong></p><p>仍然使用上文中的busybox测试容器.解析<code>hsq-openapi</code> service 的A记录.可以看到解析的结果返回了2个pod的IP地址列表.对于这种类型的service.和普通的service不同.他解析出来的是POD的ip地址列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/ # nslookup hsq-openapi</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      hsq-openapi</span><br><span class="line">Address 1: 10.100.36.69 10-100-36-69.hsq-openapi.default.svc.cluster.local</span><br><span class="line">Address 2: 10.100.36.66 10-100-36-66.hsq-openapi.default.svc.cluster.local</span><br><span class="line">/ # ???</span><br></pre></td></tr></table></figure><hr><h4 id="Pod的A记录"><a href="#Pod的A记录" class="headerlink" title="Pod的A记录"></a>Pod的A记录</h4><p>一般而言,Pod会对应如下DNS名字解析: <code>pod-ip-address.&lt;namespace-name&gt;.pod.cluster.local</code> 例如对于上面例子中的iP为<code>10.100.36.69</code> 的Pod.对应的DNS名称为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/ # nslookup 10-100-36-69.default.pod.cluster.local  #DNS名称</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      10-100-36-69.default.pod.cluster.local</span><br><span class="line">Address 1: 10.100.36.69 10-100-36-69.hsq-openapi.default.svc.cluster.local</span><br></pre></td></tr></table></figure><h3 id="k8s默认的DNS策略"><a href="#k8s默认的DNS策略" class="headerlink" title="k8s默认的DNS策略"></a>k8s默认的DNS策略</h3><p>k8s提供了5种DNS策略，如下：</p><ul><li><code>Default</code>: Pod 从运行所在的节点继承名称解析配置。</li><li><code>ClusterFirst</code>: 与配置的集群域后缀不匹配的任何 DNS 查询（例如 “<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fwww.kubernetes.io" target="_blank" rel="noopener">www.kubernetes.io</a>”） 都将转发到从节点继承的上游名称服务器。集群管理员可能配置了额外的存根域和上游 DNS 服务器。</li><li><code>ClusterFirstWithHostNet</code>：对于以 hostNetwork 方式运行的 Pod，应显式设置其 DNS 策略 <code>ClusterFirstWithHostNet</code>。</li><li><code>None</code>: 此设置允许 Pod 忽略 Kubernetes 环境中的 DNS 设置。Pod 会使用其 <code>dnsConfig</code> 字段 所提供的 DNS 设置。</li></ul><p>k8s默认使用的DNS策略是<code>ClusterFirst</code>，这点需要注意，也就是说域名解析会优先使用集群的DNS（<code>kube-DNS</code>）进行查询，如果k8s的DNS解析失败，会转发到宿主机的DNS进行解析。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;DNS介绍&quot;&gt;&lt;a href=&quot;#DNS介绍&quot; class=&quot;headerlink&quot; title=&quot;DNS介绍&quot;&gt;&lt;/a&gt;DNS介绍&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;kubernets的所有资源.包括Service,Pod都有生命周期,会频繁的销毁和创建.这些资源的IP地址也会随之动态变化.所以Kubernetes使用DNS实现通过资源名解析IP地址.&lt;/p&gt;
&lt;h3 id=&quot;DNS服务器&quot;&gt;&lt;a href=&quot;#DNS服务器&quot; class=&quot;headerlink&quot; title=&quot;DNS服务器&quot;&gt;&lt;/a&gt;DNS服务器&lt;/h3&gt;&lt;p&gt;Kubernetes集群安装了默认的Core-dns组件(通过Pod方式运行).以及kube-dns的service.&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[root@k8s-master ~]$kubectl get pods -n kube-system | grep dns&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;coredns-7f9c544f75-9sh28                   1/1     Running   2          324d&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;coredns-7f9c544f75-jgmqq                   1/1     Running   2          324d&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#下方这个10.96.0.10就是kubernetes集群的内部DNS服务器&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[root@k8s-master ~]$kubectl get svc -n kube-system | grep dns&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;kube-dns         ClusterIP   10.96.0.10     &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP   324d&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
      <category term="Service" scheme="https://jesse.top/categories/kubernetes/Service/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Calico</title>
    <link href="https://jesse.top/2021/02/06/kubernetes/network/kubernetes%20Calico/"/>
    <id>https://jesse.top/2021/02/06/kubernetes/network/kubernetes Calico/</id>
    <published>2021-02-06T15:39:58.000Z</published>
    <updated>2021-02-06T15:56:37.275Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kubernetes-Calico"><a href="#Kubernetes-Calico" class="headerlink" title="Kubernetes Calico"></a>Kubernetes Calico</h2><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h3><p>​    Calico是一个非常流行的Kubernetes网络插件和解决方案.Calico是一个开源虚拟化网络方案，用于为云原生应用实现互联及策略控制。与Flannel相比，Calico的一个显著优势是对网络策略（network policy）的支持，它允许用户动态定义ACL规则控制进出容器的数据报文，实现为Pod间的通信按需施加安全策略。事实上，Calico可以整合进大多数主流的编排系统，如Kubernetes、Apache Mesos、Docker和OpenStack等。</p><p>​    Calico本身是一个三层的虚拟网络方案，它将每个节点都当作路由器（router），将每个节点的容器都当作是“节点路由器”的一个终端并为其分配一个IP地址，各节点路由器通过BGP（Border Gateway Protocol）学习生成路由规则，从而将不同节点上的容器连接起来。因此，Calico方案其实是一个纯三层的解决方案，通过每个节点协议栈的三层（网络层）确保容器之间的连通性，这摆脱了flannel host-gw类型的所有节点必须位于同一二层网络的限制，从而极大地扩展了网络规模和网络边界。</p><a id="more"></a><p>​    Calico利用Linux内核在每一个计算节点上实现了一个高效的vRouter（虚拟路由器）进行报文转发，而每个vRouter都通过BGP负责把自身所属的节点上运行的Pod资源的IP地址信息基于节点的agent程序（Felix）直接由vRouter生成路由规则向整个Calico网络内进行传播.</p><p>​    Calico承载的各Pod资源直接通过vRouter经由基础网络进行互联，它非叠加、无隧道、不使用VRF表，也不依赖于NAT，因此每个工作负载都可以直接配置使用公网IP接入互联网，当然，也可以按需使用网络策略控制它的网络连通性。</p><p>​    Calico官网介绍: projectcaclico.org</p><h3 id="2-重要特性"><a href="#2-重要特性" class="headerlink" title="2.重要特性"></a>2.重要特性</h3><h4 id="2-1-经IP路由直连"><a href="#2-1-经IP路由直连" class="headerlink" title="2.1 经IP路由直连"></a>2.1 经IP路由直连</h4><p>Calico中，Pod收发的IP报文由所在节点的Linux内核路由表负责转发，并通过iptables规则实现其安全功能。某Pod对象发送报文时，Calico应确保节点总是作为下一跳MAC地址返回，不管工作负载本身可能配置什么路由，而发往某Pod对象的报文，其最后一个IP跃点就是Pod所在的节点，也就是说，报文的最后一程即由节点送往目标Pod对象，如下图所示。</p><p><img src="https://img2.jesse.top/image-20210206163936241.png" alt="image-20210206163936241"></p><p>需为某Pod对象提供连接时，系统上的专用插件（如Kubernetes的CNI）负责将需求通知给Calico Agent。收到消息后，Calico Agent会为每个工作负载添加直接路径信息到工作负载的TAP设备（如veth）。而运行于当前节点的BGP客户端监控到此类消息后会调用路由reflector向工作于其他节点的BGP客户端进行通告。</p><h4 id="2-2-简单、高效、易扩展"><a href="#2-2-简单、高效、易扩展" class="headerlink" title="2.2 简单、高效、易扩展"></a>2.2 简单、高效、易扩展</h4><p>Calico未使用额外的报文封装和解封装，从而简化了网络拓扑，这也是Calico高性能、易扩展的关键因素。毕竟，小的报文减少了报文分片的可能性，而且较少的封装和解封装操作也降低了对CPU的占用。此外，较少的封装也易于实现报文分析，易于进行故障排查。</p><p>创建、移动或删除Pod对象时，相关路由信息的通告速度也是影响其扩展性的一个重要因素。Calico出色的扩展性缘于与互联网架构设计原则别无二致的方式，它们都使用了BGP作为控制平面。BGP以高效管理百万级的路由设备而闻名于世，Calico自然可以游刃有余地适配大型IDC网络规模。另外，由于Calico各工作负载使用基IP直接进行互联，因此它还支持多个跨地域的IDC之间进行协同。</p><h3 id="3-Calico系统架构"><a href="#3-Calico系统架构" class="headerlink" title="3.Calico系统架构"></a>3.Calico系统架构</h3><p><img src="https://img2.jesse.top/1060878-20190413152300545-538840176.png" alt="img"></p><p>各组件介绍如下:</p><ul><li><p><strong>Felix</strong>：Calico Agent，运行于每个节点。主要负责网络接口管理和监听、路由、ARP 管理、ACL 管理和同步、状态上报等。</p></li><li><p><strong>etcd</strong>：分布式键值存储，主要负责网络元数据一致性，确保Calico网络状态的准确性，可以与kubernetes共用；</p></li><li><p><strong>BGP Client（BIRD）</strong>：Calico 为每一台 Host 部署一个 BGP Client，使用 BIRD 实现，BIRD 是一个单独的持续发展的项目，实现了众多动态路由协议比如 BGP、OSPF、RIP 等。在 Calico 的角色是监听 Host 上由 Felix 注入的路由信息，然后通过 BGP 协议广播告诉剩余 Host 节点，从而实现网络互通。</p></li><li><strong>BGP Route Reflector</strong>：在大型网络规模中，如果仅仅使用 BGP client 形成 mesh 全网互联的方案就会导致规模限制，因为所有节点之间俩俩互联，需要 N^2 个连接，为了解决这个规模问题，可以采用 BGP 的 Router Reflector 的方法，使所有 BGP Client 仅与特定 RR 节点互联并做路由同步，从而大大减少连接数。</li></ul><h4 id="3-1-Felix"><a href="#3-1-Felix" class="headerlink" title="3.1 Felix"></a>3.1 Felix</h4><p>​    Felix运行于各节点的用于支持端点（VM或Container）构建的守护进程，它负责生成路由和ACL，以及其他任何由节点用到的信息，从而为各端点构建连接机制。Felix在各编排系统中主要负责以下任务。</p><p>​    首先是接口管理（Interface Management）功能，负责为接口生成必要的信息并送往内核，以确保内核能够正确处理各端点的流量，尤其是要确保各节点能够响应目标MAC为当前节点上各工作负载的MAC地址的ARP请求，以及为其管理的接口打开转发功能。另外，它还要监控各接口的变动以确保规则能够得到正确的应用。</p><p>​    其次是路由规划（Route Programming）功能，其负责为当前节点运行的各端点在内核FIB（Forwarding Information Base）中生成路由信息，以保证到达当前节点的报文可正确转发给端点。</p><p>​    再次是ACL规划（ACL Programming）功能，负责在Linux内核中生成ACL，用于实现仅放行端点间的合法流量，并确保流量不能绕过Calico的安全措施。</p><p>​    最后是状态报告（State Reporting）功能，负责提供网络健康状态的相关数据，尤其是报告由其管理的节点上的错误和问题。这些报告数据会存储于etcd，供其他组件或网络管理员使用。</p><h4 id="3-2-编排系统插件"><a href="#3-2-编排系统插件" class="headerlink" title="3.2 编排系统插件"></a>3.2 编排系统插件</h4><p>​    编排系统插件（Orchestrator Plugin）依赖于编排系统自身的实现，故此并不存在一个固定的插件以代表此组件。编排系统插件的主要功能是将Calico整合进系统中，并让管理员和用户能够使用Calico的网络功能。它主要负责完成API的转换和反馈输出。</p><p>​    编排系统通常有其自身的网络管理API，网络插件需要负责将对这些API的调用转为Calico的数据模型并存储于Calico的存储系统中。如果有必要，网络插件还要将Calico系统的信息反馈给编排系统，如Felix的存活状态，网络发生错误时设定相应的端点为故障等。</p><h4 id="3-3-etcd存储系统"><a href="#3-3-etcd存储系统" class="headerlink" title="3.3 etcd存储系统"></a>3.3 etcd存储系统</h4><p>​    Calico使用etcd完成组件间的通信，并以之作为一个持久数据存储系统。根据编排系统的不同，etcd所扮演角色的重要性也因之而异，但它贯穿了整个Calico部署全程，并被分为两类主机：核心集群和代理（proxy）。在每个运行着Felix或编排系统插件的主机上都应该运行一个etcd代理以降低etcd集群和集群边缘节点的压力。此模式中，每个运行着插件的节点都会运行着etcd集群的一个成员节点。</p><p>​    etcd是一个分布式、强一致、具有容错功能的存储系统，这一点有助于将Calico网络实现为一个状态确切的系统：要么正常，要么发生故障。另外，分布式存储易于通过扩展应对访问压力的提升，而避免成为系统瓶颈。另外，etcd也是Calico各组件的通信总线，可用于确保让非etcd组件在键空间（keyspace）中监控某些特定的键，以确保它们能够看到所做的任何更改，从而使它们能够及时地响应这些更改。</p><h4 id="3-4-BGP客户端-BIRD"><a href="#3-4-BGP客户端-BIRD" class="headerlink" title="3.4 BGP客户端(BIRD)"></a>3.4 BGP客户端(BIRD)</h4><p>​    Calico要求在每个运行着Felix的节点上同时还要运行一个BGP客户端，负责将Felix生成的路由信息载入内核并通告到整个IDC。在Calico语境中，此组件是通用的BIRD，因此任何BGP客户端（如GoBGP等）都可以从内核中提取路由并对其分发对于它们来说都适合的角色。</p><p>​    BGP客户端的核心功能就是路由分发，在Felix插入路由信息至内核FIB中时，BGP客户端会捕获这些信息并将其分发至其他节点，从而确保了流量的高效路由。</p><h4 id="3-5-BGP路由反射器-BRID"><a href="#3-5-BGP路由反射器-BRID" class="headerlink" title="3.5 BGP路由反射器(BRID)"></a>3.5 BGP路由反射器(BRID)</h4><p>​    在大规模的部署场景中，简易版的BGP客户端易于成为性能瓶颈，因为它要求每个BGP客户端都必须连接至其同一网络中的其他所有BGP客户端以传递路由信息，一个有着N个节点的部署环境中，其存在网络连接的数量为N的二次方，随着N值的逐渐增大，其连接复杂度会急剧上升。因而在较大规模的部署场景中，Calico应该选择部署一个BGP路由反射器，它是由BGP客户端连接的中心点，BGP的点到点通信也就因此转化为与中心点的单路通信模型，如图11-18所示。出于冗余之需，生产实践中应该部署多个BGP路由反射器。对于Calico来说，BGP客户端程序除了作为客户端使用之外，还可以配置成路由反射器。</p><h3 id="4-Calico网络工作模式"><a href="#4-Calico网络工作模式" class="headerlink" title="4.Calico网络工作模式"></a>4.Calico网络工作模式</h3><h4 id="4-1-BGP模式"><a href="#4-1-BGP模式" class="headerlink" title="4.1 BGP模式"></a>4.1 BGP模式</h4><p>边界网关协议（Border Gateway Protocol, BGP）是互联网上一个核心的去中心化自治路由协议，它通过维护IP路由表或“前缀”表来实现自治系统（AS）之间的可达性，属于矢量路由协议。不过，考虑到并非所有的网络都能支持BGP，以及Calico控制平面的设计要求物理网络必须是二层网络，以确保vRouter间均直接可达，路由不能够将物理设备当作下一跳等原因，为了支持三层网络</p><h4 id="4-2-IPIP模式"><a href="#4-2-IPIP模式" class="headerlink" title="4.2 IPIP模式"></a>4.2 IPIP模式</h4><p>​    BGP模式要求Kubernetes的所有物理节点网络必须是二层网络.为了支持三层网络，Calico还推出了IP-in-IP叠加的模型，它也使用Overlay的方式来传输数据。IPIP的包头非常小，而且也是内置在内核中，因此理论上它的速度要比VxLAN快一点，但安全性更差。Calico 3.x的默认配置使用的是IPIP类型的传输方案而非BGP。</p><p>​    工作于IPIP模式的Calico会在每个节点上创建一个tunl0接口（TUN类型虚拟设备）用于封装三层隧道报文。节点上创建的每一个Pod资源，都会由Calico自动创建一对虚拟以太网接口（TAP类型的虚拟设备），其中一个附加于Pod的网络名称空间，另一个（名称以cali为前缀后跟随机字串）留置在节点的根网络名称空间，并经由tunl0封装或解封三层隧道报文。Calico IPIP模式如下图所示。</p><p><img src="https://img2.jesse.top/image-20210206165304293.png" alt="image-20210206165304293"></p><h3 id="5-Calico-网络通信方式"><a href="#5-Calico-网络通信方式" class="headerlink" title="5. Calico 网络通信方式"></a>5. Calico 网络通信方式</h3><h4 id="5-1-Calico网络环境介绍"><a href="#5-1-Calico网络环境介绍" class="headerlink" title="5.1 Calico网络环境介绍"></a>5.1 Calico网络环境介绍</h4><p>当前k8s集群使用的是v1.17.3的版本.有2个node节点.IP地址分别如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl get nodes -o wide | awk &apos;&#123;print $1,$6&#125;&apos; | sed 1,2d</span><br><span class="line">k8s-node1 172.16.20.252</span><br><span class="line">k8s-node2 172.16.20.253</span><br></pre></td></tr></table></figure><p>每个node节点都启动一个<code>tunl0</code> 的虚拟路由器.和许多<code>calixxx</code> 开头的虚拟网卡设备</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-node1 ~]# ifconfig</span><br><span class="line">cali42b086c8543: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1440</span><br><span class="line">        inet6 fe80::ecee:eeff:feee:eeee  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether ee:ee:ee:ee:ee:ee  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 13335563  bytes 928478769 (885.4 MiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 13335563  bytes 928478769 (885.4 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">tunl0: flags=193&lt;UP,RUNNING,NOARP&gt;  mtu 1440</span><br><span class="line">        inet 10.100.36.64  netmask 255.255.255.255</span><br><span class="line">        tunnel   txqueuelen 1000  (IPIP Tunnel)  #默认是IPIP模式</span><br><span class="line">        RX packets 3978810  bytes 345003038 (329.0 MiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 3674392  bytes 613045453 (584.6 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions</span><br></pre></td></tr></table></figure><p>Calico的CNI插件会为每个容器设置一个veth pair设备，然后把另一端接入到宿主机网络空间，由于没有网桥，CNI插件还需要在宿主机上为每个容器的veth pair设备配置一条路由规则，用于接收传入的IP包.</p><p>了这样的veth pair设备以后，容器发出的IP包就会通过veth pair设备到达宿主机，这些路由规则都是Felix维护配置的，而路由信息则是calico bird组件基于BGP分发而来。Calico实际上是将集群里所有的节点都当做边界路由器来处理，他们一起组成了一个全互联的网络，彼此之间通过BGP交换路由，这些节点我们叫做BGP Peer。</p><p>为了下面试验Calico的网络工作.当前集群使用daemonSet控制器运行了2个<code>busybox:1.28.4</code> 镜像的容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl get pods -o wide</span><br><span class="line">NAME            READY   STATUS    RESTARTS   AGE    IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">busybox-g5rkr   1/1     Running   0          130m   10.100.36.103    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">busybox-zdwsc   1/1     Running   0          130m   10.100.169.176   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>在<code>k8s-node1</code>节点上可以看到两条相关路由</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">10.100.36.103   0.0.0.0         255.255.255.255 UH    0      0        0 cali96df9f67b52</span><br><span class="line">10.100.169.128  172.16.20.253   255.255.255.192 UG    0      0        0 tunl0</span><br></pre></td></tr></table></figure><p>第一条路由是访问该节点下的Busybox容器.它的下一跳是<code>calixxxx</code>开头的虚拟网卡.这种通信方式和docker的Bridge网桥模式其实并没有任何区别.</p><p>第二条路由的目的网络是10.100.169.128,子网掩码是255.255.255.192.它代表了IP范围为10.100.169.128-190的地址.而运行于另外一个节点下的<code>busybox-zdwsc</code>Pod的IP地址就位于这个范围之内.所以这条路由可以使node1节点借助于tunl0可以直接和node2节点下的pod进行通信.</p><blockquote><p>在<code>k8s-node2</code> 服务器可以看到类似的这2条路由</p></blockquote><h4 id="5-2-Calico网络模型解密"><a href="#5-2-Calico网络模型解密" class="headerlink" title="5.2 Calico网络模型解密"></a>5.2 Calico网络模型解密</h4><p>登录<code>k8s-node1</code>节点下的Pod容器内部.查看Pod容器的IP地址,以及路由条目.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl exec -it busybox-g5rkr -- sh</span><br><span class="line">/ # ifconfig</span><br><span class="line">eth0      Link encap:Ethernet  HWaddr 4A:7C:E7:FA:4B:CC</span><br><span class="line">          inet addr:10.100.36.103  Bcast:0.0.0.0  Mask:255.255.255.255</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1440  Metric:1</span><br><span class="line">          RX packets:14 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0</span><br><span class="line">          RX bytes:1322 (1.2 KiB)  TX bytes:426 (426.0 B)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/ # route -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">0.0.0.0         169.254.1.1     0.0.0.0         UG    0      0        0 eth0</span><br><span class="line">169.254.1.1     0.0.0.0         255.255.255.255 UH    0      0        0 eth0</span><br></pre></td></tr></table></figure><p>通过<code>k8s-node</code>节点上的下面的路由条目,我们可以知道节点主机和Pod容器的IP地址<code>10.100.36.103</code>通信使用的是<code>cali96df9f67b52</code>这个虚拟网卡</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10.100.36.103   0.0.0.0         255.255.255.255 UH    0      0        0 cali96df9f67b52</span><br></pre></td></tr></table></figure><p>路由条目显示<code>169.254.1.1</code> 是Pod容器的默认网关.但是有网络常识的我们都知道这个IP是个保留的IP地址,不存在于互联网或者任何设备中.那Pod如何和网关通信呢?</p><p>回顾一下网络课程,我们知道任何网络设备和网关设备都是在一个二层局域网中,而二层数据链路层使用MAC地址进行通信,不需要双方的IP地址信息.通信方(这里是Pod容器)会通过ARP协议获取网关的MAC地址,然后通过MAC地址将数据包发送给网关..也就是说网络设备不关心对方的IP是否可达,只要能找到对应的MAC地址就可以.</p><p>通过<code>ip neigh</code>命令查看Pod容器的ARP缓存</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/ # ip neigh</span><br><span class="line">169.254.1.1 dev eth0 lladdr ee:ee:ee:ee:ee:ee ref 1 used 0/0/0 probes 4 REACHABLE</span><br></pre></td></tr></table></figure><blockquote><p>如果是新的Pod容器可能无法获得ARP缓存,此时只需要随便发生一个网络交互(例如ping百度)即可</p></blockquote><p>这个MAC地址(ee:ee:ee:ee:ee:ee)也是Calico的虚拟<code>cali96df9f67b52</code>网卡的虚拟MAC地址.下放是宿主机网卡信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cali96df9f67b52: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1440</span><br><span class="line">        inet6 fe80::ecee:eeff:feee:eeee  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether ee:ee:ee:ee:ee:ee  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure><p>所有虚拟网卡默认开启了ARP代理协议</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-node1 ~]# cat /proc/sys/net/ipv4/conf/cali96df9f67b52/proxy_arp</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p>所以Calico 通过一个巧妙的方法将 Pod 的所有流量引导到一个特殊的网关 169.254.1.1，从而引流到主机的 calixxx 网络设备上，最终将二三层流量全部转换成三层流量来转发。</p><h3 id="6-Calico-IPIP网络模式"><a href="#6-Calico-IPIP网络模式" class="headerlink" title="6.Calico IPIP网络模式"></a>6.Calico IPIP网络模式</h3><p>登录<code>busybox-g5rkr</code>Pod容器内部.ping位于另外一台<code>k8s-node2</code> 下的<code>busybox-zdwsc</code>Pod容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl get pods -o wide</span><br><span class="line">NAME            READY   STATUS    RESTARTS   AGE    IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">busybox-g5rkr   1/1     Running   0          130m   10.100.36.103    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">busybox-zdwsc   1/1     Running   0          130m   10.100.169.176   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>两个Pod之前可以直接访问对方的IP地址.而不需要像Docker容器那样暴露端口,然后利用对方宿主机的IP进行通信</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl exec -it busybox-g5rkr -- sh</span><br><span class="line">/ # ifconfig</span><br><span class="line">eth0      Link encap:Ethernet  HWaddr 4A:7C:E7:FA:4B:CC</span><br><span class="line">          inet addr:10.100.36.103  Bcast:0.0.0.0  Mask:255.255.255.255</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1440  Metric:1</span><br><span class="line">          RX packets:14 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0</span><br><span class="line">          RX bytes:1322 (1.2 KiB)  TX bytes:426 (426.0 B)</span><br><span class="line"></span><br><span class="line">/ # ping 10.100.169.176</span><br><span class="line">PING 10.100.169.176 (10.100.169.176): 56 data bytes</span><br><span class="line">64 bytes from 10.100.169.176: seq=0 ttl=62 time=0.622 ms</span><br><span class="line">64 bytes from 10.100.169.176: seq=1 ttl=62 time=0.552 ms</span><br><span class="line">64 bytes from 10.100.169.176: seq=2 ttl=62 time=0.597 ms</span><br></pre></td></tr></table></figure><p>在<code>k8s-node2</code> 节点抓包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-node2 ~]# tcpdump -i ens192 -nn  -w imcp.cap</span><br></pre></td></tr></table></figure><p>用wireshark软件打开抓包文件.发现如下ICMP的报文</p><p><img src="https://img2.jesse.top/image-20210206220720374.png" alt="image-20210206220720374"></p><p>可以看到每个数据报文共有两个IP网络层,内层是Pod容器之间的IP网络报文,外层是宿主机节点的网络报文(2个node节点).之所以要这样做是因为tunl0是一个隧道端点设备，在数据到达时要加上一层封装，便于发送到对端隧道设备中。 </p><p>Pod间的通信经由IPIP的三层隧道转发,相比较VxLAN的二层隧道来说，IPIP隧道的开销较小，但其安全性也更差一些。</p><p>IPIP的通信方式如下:</p><p><img src="https://img2.jesse.top/1060878-20190415165144848-1984358878.png" alt="img"></p><h4 id="6-1-Pod和Service网络通信"><a href="#6-1-Pod和Service网络通信" class="headerlink" title="6.1 Pod和Service网络通信"></a>6.1 Pod和Service网络通信</h4><p>经过测试.在k8s集群内部物理节点和pod容器内部访问Service的http服务.仍然使用的是Ipip通信模式.</p><p>下面是在容器内部通过Service访问busybox pod容器的http服务的抓包报文</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl exec -it busybox-6hnvc -- sh</span><br><span class="line">/ # curl http://10.96.166.242</span><br><span class="line">sh: curl: not found</span><br><span class="line">/ # wget -O - -q http://10.96.166.242</span><br><span class="line">wget: server returned error: HTTP/1.0 404 Not Found</span><br><span class="line">/ # wget -O - -q http://10.96.166.242</span><br><span class="line">wget: server returned error: HTTP/1.0 404 Not Found</span><br></pre></td></tr></table></figure><p><img src="https://img2.jesse.top/image-20210206222911793.png" alt="image-20210206222911793"></p><h3 id="7-BGP网络模式"><a href="#7-BGP网络模式" class="headerlink" title="7. BGP网络模式"></a>7. BGP网络模式</h3><p>Calico网络部署时,默认安装就是IPIP网络.通过修改calico.yaml部署文件中的<code>CALICO_IPV4POOL_IPIP</code> 值修改成<code>off</code> 就切换到BGP网络模式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Enable IPIP</span><br><span class="line">- name: CALICO_IPV4POOL_IPIP</span><br><span class="line">  value: &quot;Always&quot;  #改成Off</span><br></pre></td></tr></table></figure><p>重新部署calico</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl apply -f calico-3.10.2.yaml</span><br></pre></td></tr></table></figure><p>然后关闭ipipMode.把ipipMode从Always修改成为Never</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 target]# kubectl edit ippool</span><br><span class="line"></span><br><span class="line">  ipipMode: Never</span><br></pre></td></tr></table></figure><h4 id="7-1-和Ipip的区别"><a href="#7-1-和Ipip的区别" class="headerlink" title="7.1 和Ipip的区别"></a>7.1 和Ipip的区别</h4><p>BGP网络相比较IPIP网络，最大的不同之处就是没有了隧道设备 tunl0。 前面介绍过IPIP网络pod之间的流量发送tunl0，然后tunl0发送对端设备。BGP网络中，pod之间的流量直接从网卡发送目的地，减少了tunl0这个环节。</p><h4 id="7-2-通信方式"><a href="#7-2-通信方式" class="headerlink" title="7.2 通信方式"></a>7.2 通信方式</h4><p>删除原来的pod.重新启动新的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl create -f deployment-kubia-v1.yaml</span><br><span class="line">daemonset.apps/busybox created</span><br><span class="line">service/busybox created</span><br><span class="line">[root@k8s-master ~]$kubectl get pods -o wide</span><br><span class="line">NAME            READY   STATUS    RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">busybox-bd566   1/1     Running   0          16s   10.100.36.97     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">busybox-fntv9   1/1     Running   0          16s   10.100.169.129   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>再次查看路由表.发现节点和pod容器通信直接通过宿主机的物理网卡,而不是tunl0设备了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$route -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">0.0.0.0         172.16.20.254   0.0.0.0         UG    100    0        0 ens192</span><br><span class="line">10.100.36.64    172.16.20.252   255.255.255.192 UG    0      0        0 ens192</span><br><span class="line">10.100.169.128  172.16.20.253   255.255.255.192 UG    0      0        0 ens192</span><br></pre></td></tr></table></figure><p>此时,再次2个Pod容器互ping抓包分析.发现两个Pod像物理机一样直接通信,而不需要进行任何数据包封装和解封装.并且数据报文的MAC地址也是node1和node2物理网卡的MAC地址</p><p><img src="https://img2.jesse.top/image-20210206224938109.png" alt="image-20210206224938109"></p><p>BGP的网络连接方式:</p><p><img src="https://img2.jesse.top/1060878-20190415165320714-135136611.png" alt="img"></p><h3 id="8-BGP和ipip网络模式对比"><a href="#8-BGP和ipip网络模式对比" class="headerlink" title="8. BGP和ipip网络模式对比"></a>8. BGP和ipip网络模式对比</h3><ul><li><p><strong>IPIP</strong>:</p><p>特点: tunl0封装数据.形成隧道.所有Pod和pod.pod和节点之间进行三层网络传输</p><p>优点: 适用所有网络类型.能够解决跨网段的路由问题.</p></li><li><p><strong>BGP</strong>:</p><p>特点: 适用BGP路由导向流量</p><p>优点: Pod之间直接通信.省去了隧道,封装,解封装等任何中间环节,传输效率非常高.</p><p>缺点: 需要确保所有物理节点在同一个二层网络,否则Pod无法跨节点网段通信</p></li></ul><h3 id="9-Calico网络优化"><a href="#9-Calico网络优化" class="headerlink" title="9. Calico网络优化"></a>9. Calico网络优化</h3><h4 id="9-1-MTU"><a href="#9-1-MTU" class="headerlink" title="9.1 MTU"></a>9.1 MTU</h4><p>Calico 的IPIP网络模型下tunl0接口的MTU默认为1440，这种设置主要是为适配Google的GCE环境，在非GCE的物理环境中，其最佳值为1480。因此，对于非GCE环境的部署，建议将配置清单calico.yaml下载至本地修改后，再将其应用到集群中。要修改的内容是DaemonSet资源calico-node的Pod模板，将容器calico-node的环境变量“FELIX_INPUTMTU”的值修改为1480即可</p><blockquote><p>因为IPIP多了一层IP报文封装,而IP报文头部一般是20个字节.所以MUT的值应该是最大1500-20.</p></blockquote><h4 id="9-2-Calico-typha"><a href="#9-2-Calico-typha" class="headerlink" title="9.2 Calico-typha"></a>9.2 Calico-typha</h4><p>对于50个节点以上规模的集群来说，所有Calico节点均基于Kubernetes API存取数据会为API Server带来不小的通信压力，这就应该使用calico-typha进程将所有Calico的通信集中起来与API Server进行统一交互。calico-typha以Pod资源的形式托管运行于Kubernetes系统之上，启用的方法为下载前面步骤中用到的Calico的部署清单文件至本地，修改其calico-typha的Pod资源副本数量为所期望的值并重新应用配置清单即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-typha</span><br><span class="line">  ...</span><br><span class="line">spec:</span><br><span class="line">  ...</span><br><span class="line">  replicas: &lt;number of replicas&gt;</span><br></pre></td></tr></table></figure><p>每个calico-typha Pod资源可承载100到200个Calico节点的连接请求，最多不要超过200个。另外，整个集群中的calico-typha的Pod资源总数尽量不要超过20个。</p><h4 id="9-3-BGP路由模型"><a href="#9-3-BGP路由模型" class="headerlink" title="9.3 BGP路由模型"></a>9.3 BGP路由模型</h4><p>默认情况下，Calico的BGP网络工作于点对点的网格（node-to-node mesh）模型，它仅适用于较小规模的集群环境。中级集群环境应该使用全局对等BGP模型（Global BGP peers），以在同一二层网络中使用一个或一组BGP反射器构建BGP网络环境。而大型集群环境需要使用每节点对等BGP模型（Per-node BGP peers），即分布式BGP反射器模型，一个典型的用法是将每个节点都配置为自带BGP反射器接入机架顶部交换机上的路由反射器。</p><h4 id="9-4-使用BGP而非IPIP"><a href="#9-4-使用BGP而非IPIP" class="headerlink" title="9.4 使用BGP而非IPIP"></a>9.4 使用BGP而非IPIP</h4><p>事实上，仅在那些不支持用户自定义BGP配置的网络中才需要使用IPIP的隧道通信类型。如果有一个自主可控的网络环境且部署规模较大时，可以考虑启用BGP的通信类型降低网络开销以提升传输性能，并且应该部署BGP反射器来提高路由学习效率。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>Calico官网: <a href="http://www.projectcalico.org" target="_blank" rel="noopener">www.projectcalico.org</a></p><p>k8s网络之Calico网络: <a href="https://www.cnblogs.com/goldsunshine/p/10701242.html#mxAMjXzT" target="_blank" rel="noopener">https://www.cnblogs.com/goldsunshine/p/10701242.html#mxAMjXzT</a></p><p>kubernetes容器网络: <a href="https://tech.ipalfish.com/blog/2020/03/06/kubernetes_container_network/" target="_blank" rel="noopener">https://tech.ipalfish.com/blog/2020/03/06/kubernetes_container_network/</a> (伴鱼团队)</p><p>&lt;kubernetes进阶实战&gt; 11.4 马永亮</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kubernetes-Calico&quot;&gt;&lt;a href=&quot;#Kubernetes-Calico&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes Calico&quot;&gt;&lt;/a&gt;Kubernetes Calico&lt;/h2&gt;&lt;h3 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1.简介&quot;&gt;&lt;/a&gt;1.简介&lt;/h3&gt;&lt;p&gt;​    Calico是一个非常流行的Kubernetes网络插件和解决方案.Calico是一个开源虚拟化网络方案，用于为云原生应用实现互联及策略控制。与Flannel相比，Calico的一个显著优势是对网络策略（network policy）的支持，它允许用户动态定义ACL规则控制进出容器的数据报文，实现为Pod间的通信按需施加安全策略。事实上，Calico可以整合进大多数主流的编排系统，如Kubernetes、Apache Mesos、Docker和OpenStack等。&lt;/p&gt;
&lt;p&gt;​    Calico本身是一个三层的虚拟网络方案，它将每个节点都当作路由器（router），将每个节点的容器都当作是“节点路由器”的一个终端并为其分配一个IP地址，各节点路由器通过BGP（Border Gateway Protocol）学习生成路由规则，从而将不同节点上的容器连接起来。因此，Calico方案其实是一个纯三层的解决方案，通过每个节点协议栈的三层（网络层）确保容器之间的连通性，这摆脱了flannel host-gw类型的所有节点必须位于同一二层网络的限制，从而极大地扩展了网络规模和网络边界。&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
      <category term="network" scheme="https://jesse.top/categories/kubernetes/network/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Kong自定义配置Nginx</title>
    <link href="https://jesse.top/2021/01/19/Linux-Web/Kong%E8%87%AA%E5%AE%9A%E4%B9%89%E9%85%8D%E7%BD%AENginx/"/>
    <id>https://jesse.top/2021/01/19/Linux-Web/Kong自定义配置Nginx/</id>
    <published>2021-01-19T03:59:58.000Z</published>
    <updated>2021-02-06T16:04:56.932Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kong自定义配置Nginx"><a href="#Kong自定义配置Nginx" class="headerlink" title="Kong自定义配置Nginx"></a>Kong自定义配置Nginx</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>Kong是基于Nginx实现代理转发.官方的 <code>nginx.conf</code> 配置文件过于简单.如果需要优化nginx的性能,就需要修改默认的nginx配置文件,或者重新自定义一个nginx配置文件.</p><p>具体方法可以参考官方文档: <a href="https://docs.konghq.com/2.2.x/configuration/#environment-variables" target="_blank" rel="noopener">https://docs.konghq.com/2.2.x/configuration/#environment-variables</a></p><p>下面介绍2种方式自定义nginx的配置</p><h3 id="通过环境变量注入"><a href="#通过环境变量注入" class="headerlink" title="通过环境变量注入"></a>通过环境变量注入</h3><p>Kong服务启动时会每次都新建一个新的nginx配置文件.可以通过将nginx指令注入到 <code>kong.conf</code> 配置文件中从而配置到这个新的nginx配置文件</p><a id="more"></a> <h4 id="注入Nginx单个指令"><a href="#注入Nginx单个指令" class="headerlink" title="注入Nginx单个指令"></a>注入Nginx单个指令</h4><p>注入到Kong的环境变量一般包含下面2种前缀.前缀名不同代表注入的nginx指令作用在不同的作用域下.Kong会将环境变量的前缀去掉,然后将环境变量的后面部分注入到nginx.</p><ul><li><code>nginx_http_</code> 该前缀环境变量会被注入到Nginx的http代码块</li><li><code>nginx_proxy_</code> 该前缀会被注入到nginx的server代码块</li></ul><p>例如.如果注入以下环境变量到 <code>kong.conf</code> 配置文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nginx_proxy_large_client_header_buffers=16 128k</span><br></pre></td></tr></table></figure><p>Kong会将以下环境变量注入到Nginx配置文件的代理 <code>server</code> 块中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">large_client_header_buffers 16 128k;</span><br></pre></td></tr></table></figure><p>下面的环境变量,会被注入到nginx的http块中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export KONG_NGINX_HTTP_OUTPUT_BUFFERS=&quot;4 64k&quot;</span><br><span class="line"></span><br><span class="line">#注入以下Nginx指令</span><br><span class="line">output_buffers 4 64k;</span><br></pre></td></tr></table></figure><blockquote><p>还有一种前缀 <code>Nginx_admin_</code> 这个作用在kong的admin api,所以用的较少</p></blockquote><h4 id="注入Nginx代码块"><a href="#注入Nginx代码块" class="headerlink" title="注入Nginx代码块"></a>注入Nginx代码块</h4><p>对于一些复杂的配置场景,比如需要将整个server代码块添加到Nginx配置文件.可以使用上面的环境变量注入的方式,注入一个 <code>include</code> 指令到Nginx配置文件.</p><p>例如下面这个nginx的server代码块文件.假如该文件名为 <code>my-server.conf</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># custom server</span><br><span class="line">server &#123;</span><br><span class="line">  listen 2112;</span><br><span class="line">  location / &#123;</span><br><span class="line">    # ...more settings...</span><br><span class="line">    return 200;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以通过下面的方式添加到 <code>kong.conf</code> 配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nginx_http_include = /path/to/your/my-server.conf</span><br></pre></td></tr></table></figure><p>或者通过环境变量方式注入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export KONG_NGINX_HTTP_INCLUDE=&quot;/path/to/your/my-server.conf&quot;</span><br></pre></td></tr></table></figure><p>这样当Kong启动后,server代码块会被添加到Nginx的配置文件.</p><blockquote><p>这里也可以使用相对路径来注入一个server代码块的配置文件,但是配置文件需要在 <code>kong.conf</code> 配置文件的prefix路径之下.或者kong启动时候通过 <code>-p</code> 参数自定义的prefix路径之下</p></blockquote><h3 id="自定义Nginx模板"><a href="#自定义Nginx模板" class="headerlink" title="自定义Nginx模板"></a>自定义Nginx模板</h3><p>kong在启动的时候会根据 <code>/usr/local/share/lua/5.1/kong/templates/nginx.lua</code>和 <code>/usr/local/share/lua/5.1/kong/templates/nginx_kong.lua</code> 这2个lua模板来自动生成nginx的配置文件.当Kong启动后会自动在prefix路径下生成 <code>nginx.conf</code> 和 <code>nginx-kong.conf</code> .前者是Nginx的主配置文件,然后通过include方式引入了 <code>nginx_kong.conf</code> </p><p>当kong启动后,会产生下面2个文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/kong</span><br><span class="line">    - nginx.conf</span><br><span class="line">    - nginx-kong.conf</span><br></pre></td></tr></table></figure><blockquote><p>在 <a href="https://github.com/kong/kong/tree/master/kong/templates下也存放了kong的默认模板文件" target="_blank" rel="noopener">https://github.com/kong/kong/tree/master/kong/templates下也存放了kong的默认模板文件</a>.</p></blockquote><p>所以在 <code>usr/local/kong</code> 目录下直接修改 <code>Nginx.conf</code> 配置文件无法永久生效.当kong重启时,配置文件会被默认的Lua目标所覆盖和替代</p><p>如果一定要自定义nginx配置文件.可以自定义nginx的模板文件来替代 <code>Nginx.lua</code> .然后在该模板文件里引入 <code>nginx-kong.conf</code> </p><h4 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h4><ol><li>拷贝 <code>nginx.conf</code> 配置文件为  <code>nginx.conf.template</code> </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/local/kong/nginx.conf nginx.conf.template</span><br></pre></td></tr></table></figure><ol><li>自定义配置 <code>nginx.conf.template</code> .例如下面是我的配置文件内容</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">pid pids/nginx.pid;</span><br><span class="line">error_log logs/error.log $&#123;&#123;LOG_LEVEL&#125;&#125;; </span><br><span class="line"></span><br><span class="line"># injected nginx_main_* directives</span><br><span class="line">daemon off;</span><br><span class="line">worker_processes auto;</span><br><span class="line">worker_rlimit_nofile 204800;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    # injected nginx_events_* directives</span><br><span class="line">    multi_accept on;</span><br><span class="line">    use epoll;</span><br><span class="line">    worker_connections  204800;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line">    sendfile            on;</span><br><span class="line">    tcp_nopush          on;</span><br><span class="line">    tcp_nodelay         on;</span><br><span class="line">    include &apos;nginx-kong.conf&apos;;</span><br><span class="line"></span><br><span class="line">    keepalive_timeout  60;</span><br><span class="line">    keepalive_requests 1024;</span><br><span class="line">    client_header_buffer_size 4k;</span><br><span class="line">    large_client_header_buffers 4 32k;</span><br><span class="line"></span><br><span class="line">    types_hash_max_size 2048;</span><br><span class="line">    client_body_timeout 180;</span><br><span class="line">    client_header_timeout 10;</span><br><span class="line">    send_timeout 240;</span><br><span class="line"></span><br><span class="line">    proxy_connect_timeout   1000ms;</span><br><span class="line">    proxy_send_timeout      5000ms;</span><br><span class="line">    proxy_read_timeout      5000ms;</span><br><span class="line">    proxy_buffers           64 8k;</span><br><span class="line">    proxy_busy_buffers_size    128k;</span><br><span class="line">    proxy_temp_file_write_size 64k;</span><br><span class="line">    proxy_redirect off;</span><br><span class="line">    proxy_next_upstream off;</span><br><span class="line">    </span><br><span class="line">    gzip on;</span><br><span class="line">    gzip_min_length 1k;</span><br><span class="line">    gzip_buffers 4 16k;</span><br><span class="line">    gzip_http_version 1.0;</span><br><span class="line">    gzip_comp_level 2;</span><br><span class="line">    gzip_types text/plain application/x-javascript text/css application/xml;</span><br><span class="line">    gzip_vary on;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>注意该配置文件内的指令不能和 <code>nginx-kong.conf</code> 配置文件有同名或者冲突.否则kong无法启动</p></blockquote><ol><li>重新启动Kong.使用下面的参数指定自定义的Nginx模板文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kong start -c /etc/kong/kong.conf --nginx-conf nginx.conf.template</span><br></pre></td></tr></table></figure><h4 id=""><a href="#" class="headerlink" title=" "></a> </h4><h4 id="docker运行kong"><a href="#docker运行kong" class="headerlink" title="docker运行kong"></a>docker运行kong</h4><p>如果是docker方式运行.可以使用 <code>Dockerfile</code> 自定义kong镜像</p><p>以下是Dockerfile文件内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FROM kong:2.2.0</span><br><span class="line">COPY nginx.conf.template /usr/local/kong/nginx.conf.template</span><br><span class="line">CMD kong start --nginx-conf /usr/local/kong/nginx.conf.template</span><br></pre></td></tr></table></figure><p>编译docker镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t dwd-kong:2.2.0 .</span><br></pre></td></tr></table></figure><p>重启运行docker.但是要先在kong容器运行 <code>kong migrations up</code> 和 <code>kong migrations finish</code> 命令.所以 <code>docker-compose.yml</code> 配置文件内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">kong:</span><br><span class="line">    image: hub.doweidu.com/beta/dwd-kong:2.2.0</span><br><span class="line">    container_name: kong</span><br><span class="line">    hostname: kong</span><br><span class="line">    environment:</span><br><span class="line">      - KONG_DATABASE=postgres</span><br><span class="line">      - KONG_PG_HOST=kong-database</span><br><span class="line">      - KONG_PROXY_ACCESS_LOG=/var/log/kong/access.log</span><br><span class="line">      - KONG_ADMIN_ACCESS_LOG=/var/log/kong/admin_access.log</span><br><span class="line">      - KONG_PROXY_ERROR_LOG=/var/log/kong/error.log</span><br><span class="line">      - KONG_ADMIN_ERROR_LOG=/var/log/kong/admin_error.log</span><br><span class="line">      - KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl</span><br><span class="line">      - KONG_TRUSTED_IPS=0.0.0.0/0,::/0</span><br><span class="line">      - KONG_REAL_IP_HEADER=X-Forwarded-For</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/logs/kong:/var/log/kong</span><br><span class="line">      - /etc/localtime:/etc/localtime</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8000:8000&quot;</span><br><span class="line">      - &quot;8443:8443&quot;</span><br><span class="line">      - &quot;8001:8001&quot;</span><br><span class="line">      - &quot;8444:8444&quot;</span><br><span class="line">    expose:</span><br><span class="line">      - &quot;8000&quot;</span><br><span class="line">      - &quot;8443&quot;</span><br><span class="line">      - &quot;8001&quot;</span><br><span class="line">      - &quot;8444&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - dev-net</span><br><span class="line">    restart: always</span><br><span class="line">    depends_on:</span><br><span class="line">        - kong-database</span><br><span class="line">        - kong-migration</span><br><span class="line">        - kong-migration-finish</span><br><span class="line">        </span><br><span class="line">kong-migration:</span><br><span class="line">    image: hub.doweidu.com/beta/dwd-kong:2.2.0</span><br><span class="line">   # command: &quot;kong migrations bootstrap&quot;</span><br><span class="line">    command: &quot;kong migrations up&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - dev-net</span><br><span class="line">    restart: on-failure</span><br><span class="line">    environment:</span><br><span class="line">      KONG_PG_HOST: kong-database</span><br><span class="line">    depends_on:</span><br><span class="line">      - kong-database</span><br><span class="line"></span><br><span class="line">  kong-migration-finish:</span><br><span class="line">    image: hub.doweidu.com/beta/dwd-kong:2.2.0</span><br><span class="line">   # command: &quot;kong migrations bootstrap&quot;</span><br><span class="line">    command: &quot;kong migrations finish&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - dev-net</span><br><span class="line">    restart: on-failure</span><br><span class="line">    environment:</span><br><span class="line">      KONG_PG_HOST: kong-database</span><br><span class="line">    depends_on:</span><br><span class="line">      - kong-database</span><br><span class="line">      - kong-migration</span><br></pre></td></tr></table></figure><p>启动容器后.可以查看配置文件是否生效:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[work@docker docker-compose]$docker exec kong cat /usr/local/kong/nginx.conf</span><br><span class="line">pid pids/nginx.pid;</span><br><span class="line">error_log logs/error.log notice;</span><br><span class="line"></span><br><span class="line"># injected nginx_main_* directives</span><br><span class="line">daemon off;</span><br><span class="line">worker_processes auto;</span><br><span class="line">worker_rlimit_nofile 204800;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    # injected nginx_events_* directives</span><br><span class="line">    multi_accept on;</span><br><span class="line">    use epoll;</span><br><span class="line">    worker_connections  204800;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line">    sendfile            on;</span><br><span class="line">    tcp_nopush          on;</span><br><span class="line">    tcp_nodelay         on;</span><br><span class="line">    include &apos;nginx-kong.conf&apos;;</span><br><span class="line"> .......略...........</span><br></pre></td></tr></table></figure><p>如此,便实现了自定义kong的nginx配置文件,这在大并发场景中可能需要优化nginx的转发性能.如果是小规模场景中,可以使用Kong的默认的Nginx配置文件即可.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kong自定义配置Nginx&quot;&gt;&lt;a href=&quot;#Kong自定义配置Nginx&quot; class=&quot;headerlink&quot; title=&quot;Kong自定义配置Nginx&quot;&gt;&lt;/a&gt;Kong自定义配置Nginx&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;Kong是基于Nginx实现代理转发.官方的 &lt;code&gt;nginx.conf&lt;/code&gt; 配置文件过于简单.如果需要优化nginx的性能,就需要修改默认的nginx配置文件,或者重新自定义一个nginx配置文件.&lt;/p&gt;
&lt;p&gt;具体方法可以参考官方文档: &lt;a href=&quot;https://docs.konghq.com/2.2.x/configuration/#environment-variables&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://docs.konghq.com/2.2.x/configuration/#environment-variables&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;下面介绍2种方式自定义nginx的配置&lt;/p&gt;
&lt;h3 id=&quot;通过环境变量注入&quot;&gt;&lt;a href=&quot;#通过环境变量注入&quot; class=&quot;headerlink&quot; title=&quot;通过环境变量注入&quot;&gt;&lt;/a&gt;通过环境变量注入&lt;/h3&gt;&lt;p&gt;Kong服务启动时会每次都新建一个新的nginx配置文件.可以通过将nginx指令注入到 &lt;code&gt;kong.conf&lt;/code&gt; 配置文件中从而配置到这个新的nginx配置文件&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>Kong实现限流</title>
    <link href="https://jesse.top/2021/01/19/Linux-Web/Kong%20Rate%20Limiting%E9%99%90%E6%B5%81%E6%8F%92%E4%BB%B6/"/>
    <id>https://jesse.top/2021/01/19/Linux-Web/Kong Rate Limiting限流插件/</id>
    <published>2021-01-19T03:59:58.000Z</published>
    <updated>2021-01-19T14:39:47.543Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kong实现限流"><a href="#Kong实现限流" class="headerlink" title="Kong实现限流"></a>Kong实现限流</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>近期发现公司某个业务对外的openapi接口的/merchantapi路径异常调用非常频繁.公司的第三方商户需要通过这个路径来调用ERP接口,但是经常发生被恶意刷接口的情况,导致公司的业务服务器资源使用率飙升,面临很大的宕机风险和隐患.</p><p>目前外部客户端访问公司业务仍然是阿里云SLB—–Nginx—php-fpm的架构.由于Nginx的限流能力并不出色,特别是针对具体path路径的限流.所以,引入了Kong api网关</p><h3 id="Rate-Limiting限流插件介绍"><a href="#Rate-Limiting限流插件介绍" class="headerlink" title="Rate Limiting限流插件介绍"></a>Rate Limiting限流插件介绍</h3><p>Rate Limiting是Kong社区版就已经自带的官方流量控制插件.详细信息可以参考Kong官网介绍. <a href="https://docs.konghq.com/hub/kong-inc/rate-limiting/" target="_blank" rel="noopener">https://docs.konghq.com/hub/kong-inc/rate-limiting/</a></p><p>它可以针对<code>consumer</code> ,<code>credential</code> ,<code>ip</code> ,<code>service</code>,<code>path</code>,<code>header</code> 等多种维度来进行限流.流量控制的精准度也有多种方式可以参考,比如可以做到秒级,分钟级,小时级等限流控制.</p><h4 id="响应客户端头部信息"><a href="#响应客户端头部信息" class="headerlink" title="响应客户端头部信息"></a>响应客户端头部信息</h4><p>当启用这个插件后.Kong会响应客户端一些额外的头部信息,告诉客户端限流信息.例如下面是Kong响应给客户端的header信息,告诉客户端当前的限流策略是10r/s</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RateLimit-Limit: 10</span><br><span class="line">RateLimit-Remaining: 0</span><br><span class="line">RateLimit-Reset: 1</span><br><span class="line"></span><br><span class="line">X-Kong-Response-Latency: 1</span><br><span class="line">X-RateLimit-Limit-Second: 10</span><br><span class="line">X-RateLimit-Remaining-Second: 0</span><br></pre></td></tr></table></figure><p>如果客户端的访问请求超过限流的阈值,Kong会返回status<code>429</code>的状态码以及下面的错误信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; &quot;message&quot;: &quot;API rate limit exceeded&quot; &#125;</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="限流策略对Kong性能影响"><a href="#限流策略对Kong性能影响" class="headerlink" title="限流策略对Kong性能影响"></a>限流策略对Kong性能影响</h4><p>Rate limiting插件支持3种限流策略.</p><p><code>cluster</code> 集群策略.Kong的数据库会维护一个计数器,并且在所有的Kong集群内每个节点共享这个计数器.如果计数器触发限流上线,所有的Kong节点都拒绝客户端的转发.这就意味着每个节点接收到客户端的请求,都会对数据库进行读写操作.</p><p><code>redis</code> redis策略和<code>cluster</code> 相似,唯一不同的是,计数器是存储在redis数据中.并且在集群内所有节点共享.</p><p><code>local</code> 本地策略.计数器保存在Kong节点服务器本地内存缓冲区.并且计数器只对该节点有效.这意味着<code>local</code>策略有最好的性能表现.但是由于计数器存储在本地.所以限流的精度没有<code>redis</code>和<code>cluster</code> 准确.并且会影响Kong节点服务器弹性扩容(比如限流设置30r/s,Kong集群从2个节点扩容到4个节点.限流就从60r/s变成了120r/s.此时需要手动将限流设置从30r/s降低到15r/s)</p><blockquote><p>或者,可以在Kong前面配置一个hash转发策略的负载均衡,将同一个外部客户端的请求代理到同一个节点.这样local策略的精确度可以提升,并且kong节点的弹性扩容不会影响限流效果</p></blockquote><p>下面是3种限流策略的对比表</p><table><thead><tr><th>policy</th><th>describe</th><th>pros</th><th>cons</th></tr></thead><tbody><tr><td>cluster</td><td>集群策略</td><td>限流精准度高,不需要第三方组件支持</td><td>对Kong性能影响比较大</td></tr><tr><td>redis</td><td>redis策略</td><td>限流精准度高,对Kong性能影响较低</td><td>需要额外的redis服务</td></tr><tr><td>local</td><td>本地策略</td><td>对Kong性能影响最低</td><td>精准度比较差,Kong节点扩容和缩容需要手动调整限流速率</td></tr></tbody></table><p>下面是以上集群策略的使用场景:</p><ul><li>如果对流量精确度要求非常高.比如金融,交易等.那么适合redis或者cluster的限流策略</li><li>如果是为了保护后端服务,避免大流量带来的服务器过载.那么适合local限流策略,这种场景对限流的精度要求不高</li></ul><h3 id="针对客户端IP限流"><a href="#针对客户端IP限流" class="headerlink" title="针对客户端IP限流"></a>针对客户端IP限流</h3><p>我们场景中针对客户端IP进行限流.但是由于Kong是在SLB或者Nginx的负载均衡后面,所以默认情况下,Kong采用的IP是上一级负载均衡器的IP.此时就需要将客户端的真实IP传递到Kong,并且使用该IP作为<code>remote_ip</code> .实现方法如下:</p><ul><li>虚拟机运行Kong</li></ul><p>针对rpm包或者其他方式安装的Kong服务,可以修改默认的<code>/etc/kong/kong.conf</code> 配置文件.加入下面2行配置信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trusted_ips = 0.0.0.0/0,::/0</span><br><span class="line">real_ip_header = X-Forwarded-For</span><br></pre></td></tr></table></figure><p>重载kong配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kong reload</span><br></pre></td></tr></table></figure><ul><li>docker容器方式运行kong</li></ul><p>针对docker容器方式运行的Kong,修改配置文件不方便,此时可以通过变量注入的方式自定义配置<code>kong.conf</code> 配置文件.还可以通过这种方式注入nginx自定义配置,具体可以参考官方的文档介绍:<a href="https://docs.konghq.com/2.2.x/configuration/#environment-variables" target="_blank" rel="noopener">environment-variables</a></p><p>例如,上面的2行配置内容可以通过在配置参数前面加<code>KONG_</code>以及大写的参数名的方式注入环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">KONG_TRUSTED_IPS=0.0.0.0/0,::/0</span><br><span class="line">KONG_REAL_IP_HEADER=X-Forwarded-For</span><br></pre></td></tr></table></figure><p>修改Kong的<code>docker-compose</code>文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">kong:</span><br><span class="line">    image: dwd-kong:2.2.0  #自定义kong镜像.也可以使用官方的docker镜像</span><br><span class="line">    container_name: kong</span><br><span class="line">    hostname: kong</span><br><span class="line">    environment:</span><br><span class="line">      - KONG_DATABASE=postgres</span><br><span class="line">      - KONG_PG_HOST=kong-database</span><br><span class="line">      - KONG_PROXY_ACCESS_LOG=/var/log/kong/access.log</span><br><span class="line">      - KONG_ADMIN_ACCESS_LOG=/var/log/kong/admin_access.log</span><br><span class="line">      - KONG_PROXY_ERROR_LOG=/var/log/kong/error.log</span><br><span class="line">      - KONG_ADMIN_ERROR_LOG=/var/log/kong/admin_error.log</span><br><span class="line">      - KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl</span><br><span class="line">      - KONG_TRUSTED_IPS=0.0.0.0/0,::/0       #增加这两行</span><br><span class="line">      - KONG_REAL_IP_HEADER=X-Forwarded-For   #增加这两行</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/logs/kong:/var/log/kong</span><br><span class="line">      - /etc/localtime:/etc/localtime</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8000:8000&quot;</span><br><span class="line">      - &quot;8443:8443&quot;</span><br><span class="line">      - &quot;8001:8001&quot;</span><br><span class="line">      - &quot;8444:8444&quot;</span><br><span class="line">    expose:</span><br><span class="line">      - &quot;8000&quot;</span><br><span class="line">      - &quot;8443&quot;</span><br><span class="line">      - &quot;8001&quot;</span><br><span class="line">      - &quot;8444&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - dev-net</span><br><span class="line">    restart: always</span><br><span class="line">    depends_on:</span><br><span class="line">        - kong-database</span><br><span class="line">        - kong-migration</span><br><span class="line">        - kong-migration-finish</span><br></pre></td></tr></table></figure><h3 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h3><p>Rate Limiting插件由Kong默认提供,所以无需自行安装.由于是针对<code>/merchantapi</code> 这个借口进行限流,所以只需配置该route,并且将插件应用到这个route下即可.由于我日常使用的python进行Kong的配置,所以这里只列出我的python配置文件中相关配置.不演示具体配置了.</p><blockquote><p>使用kong的dashboard也可以很方便的实现配置</p></blockquote><ul><li>配置service</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hsq_openapi_dev = &#123; &quot;name&quot;: &quot;hsq_openapi_dev&quot;,</span><br><span class="line">            &quot;host&quot;: &quot;kong.devapi.hsq.net&quot;,</span><br><span class="line">            &quot;port&quot;: 80,</span><br><span class="line">            &quot;protocol&quot;: &quot;http&quot;,</span><br><span class="line">            &quot;path&quot;: &quot;/&quot;</span><br><span class="line">          &#125;</span><br></pre></td></tr></table></figure><ul><li>配置route</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#默认转发路由</span><br><span class="line">hsq_openapi_dev =   &#123;  &quot;service_name&quot;:&quot;hsq_openapi_dev&quot;,</span><br><span class="line">                &quot;data&quot;:&#123;</span><br><span class="line">                &quot;name&quot;: &quot;hsq-openapi-dev&quot;,</span><br><span class="line">                &quot;hosts&quot;: &quot;m.devapi.hsq.net&quot;,</span><br><span class="line">                &quot;strip_path&quot;: &quot;false&quot;,</span><br><span class="line">                &quot;protocols&quot;: [&quot;http&quot;, &quot;https&quot;],</span><br><span class="line">                &quot;paths&quot;: &quot;/&quot;,</span><br><span class="line">                &quot;methods&quot;: [&quot;GET&quot;, &quot;POST&quot;,&quot;PUT&quot;,&quot;OPTIONS&quot;,&quot;DELETE&quot;]&#125;</span><br><span class="line">             &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#限流接口转发路由</span><br><span class="line">hsq_merchant_api_limit = &#123;  &quot;service_name&quot;:&quot;hsq_openapi_dev&quot;,</span><br><span class="line">                    &quot;data&quot;:&#123;</span><br><span class="line">                    &quot;name&quot;: &quot;hsq_merchant_api_limit&quot;,</span><br><span class="line">                    &quot;hosts&quot;: &quot;m.devapi.hsq.net&quot;,</span><br><span class="line">                    &quot;strip_path&quot;: &quot;false&quot;,</span><br><span class="line">                    &quot;protocols&quot;: [&quot;http&quot;, &quot;https&quot;],</span><br><span class="line">                    &quot;paths&quot;: &quot;/merchantapi&quot;,</span><br><span class="line">                    &quot;methods&quot;: [&quot;GET&quot;, &quot;POST&quot;,&quot;PUT&quot;,&quot;OPTIONS&quot;,&quot;DELETE&quot;]&#125;</span><br><span class="line">                 &#125;</span><br></pre></td></tr></table></figure><ul><li>配置rate-limiting插件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hsq_merchantapi_limit = &#123; &quot;route_name&quot;: &quot;hsq_merchant_api_limit&quot;,  #关联到上面的route.表示该插件作用在route级别</span><br><span class="line">         &quot;data&quot;: &#123;</span><br><span class="line">         &quot;name&quot;: &quot;rate-limiting&quot;, #插件名称</span><br><span class="line">         &quot;config.second&quot;: 10, # 限流.每秒10个请求</span><br><span class="line">         &quot;config.policy&quot;: &quot;local&quot;, #限流策略</span><br><span class="line">         &quot;config.limit_by&quot;: &quot;ip&quot;    #针对客户端IP限流</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>运行python脚本,配置Kong</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~/Desktop/kong-python   master ●✚  python3 kong.py</span><br><span class="line">请输入你要配置的Kong的环境名称,例如:dev,beta,prod:&gt;&gt;&gt;dev</span><br><span class="line">正在创建service:hsq_openapi_dev</span><br><span class="line">service:hsq_openapi_dev创建成功</span><br><span class="line">开始创建routes:hsq-openapi-dev</span><br><span class="line">routes路由hsq-openapi-dev创建成功</span><br><span class="line">开始创建routes:hsq_merchant_api_limit</span><br><span class="line">routes路由hsq_merchant_api_limit创建成功</span><br><span class="line">plugins:rate-limiting创建成功.绑定在route路由:hsq_merchant_api_limit中</span><br></pre></td></tr></table></figure><h3 id="压测效果"><a href="#压测效果" class="headerlink" title="压测效果"></a>压测效果</h3><p>为了验证插件效果,这里使用<code>ab</code> 这个简单的压测工具进行测试.</p><p>1.开启一个终端,执行下面的命令.压测命令运行了1.18秒,只有20个请求成功响应,其余80个请求失败.这恰好符合了rate-limiting插件每秒10个请求的限流策略</p><blockquote><p>由于是在dev环境,所有只有一个Kong节点.如果外部流量负载均衡分发到Kong集群的所有节点,那么总体的限流应该是:Kong节点数量x限流数量</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">ab -n 100 -c 10 https://m.devapi.hsq.net/merchantapi</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">Document Path:          /merchantapi</span><br><span class="line">Document Length:        122 bytes</span><br><span class="line"></span><br><span class="line">Concurrency Level:      10</span><br><span class="line">Time taken for tests:   1.180 seconds</span><br><span class="line">Complete requests:      100         #总共100个请求</span><br><span class="line">Failed requests:        80          #失败了80个</span><br><span class="line">   (Connect: 0, Receive: 0, Length: 80, Exceptions: 0)</span><br><span class="line">Non-2xx responses:      80</span><br><span class="line">Total transferred:      41888 bytes</span><br><span class="line">HTML transferred:       5720 bytes</span><br><span class="line">Requests per second:    84.71 [#/sec] (mean)</span><br><span class="line">Time per request:       118.044 [ms] (mean)</span><br><span class="line">Time per request:       11.804 [ms] (mean, across all concurrent requests)</span><br><span class="line">Transfer rate:          34.65 [Kbytes/sec] received</span><br><span class="line">......</span><br></pre></td></tr></table></figure><ol start="2"><li>将请求继续增大,同时使用curl和浏览器访问该域名.发现请求被拒绝</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl https://m.devapi.hsq.net/merchantapi</span><br><span class="line">&#123;</span><br><span class="line">  &quot;message&quot;:&quot;API rate limit exceeded&quot;</span><br><span class="line">&#125;%</span><br></pre></td></tr></table></figure><p><img src="https://img2.jesse.top/20210119164349.png" alt=""></p><ol start="3"><li>在压测的同时,使用另外一个客户端来同时访问该接口,可以正常访问</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.99.1 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 200 122 &quot;-&quot; &quot;curl/7.29.0&quot;   #其他客户端仍然可以正常访问</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kong实现限流&quot;&gt;&lt;a href=&quot;#Kong实现限流&quot; class=&quot;headerlink&quot; title=&quot;Kong实现限流&quot;&gt;&lt;/a&gt;Kong实现限流&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;近期发现公司某个业务对外的openapi接口的/merchantapi路径异常调用非常频繁.公司的第三方商户需要通过这个路径来调用ERP接口,但是经常发生被恶意刷接口的情况,导致公司的业务服务器资源使用率飙升,面临很大的宕机风险和隐患.&lt;/p&gt;
&lt;p&gt;目前外部客户端访问公司业务仍然是阿里云SLB—–Nginx—php-fpm的架构.由于Nginx的限流能力并不出色,特别是针对具体path路径的限流.所以,引入了Kong api网关&lt;/p&gt;
&lt;h3 id=&quot;Rate-Limiting限流插件介绍&quot;&gt;&lt;a href=&quot;#Rate-Limiting限流插件介绍&quot; class=&quot;headerlink&quot; title=&quot;Rate Limiting限流插件介绍&quot;&gt;&lt;/a&gt;Rate Limiting限流插件介绍&lt;/h3&gt;&lt;p&gt;Rate Limiting是Kong社区版就已经自带的官方流量控制插件.详细信息可以参考Kong官网介绍. &lt;a href=&quot;https://docs.konghq.com/hub/kong-inc/rate-limiting/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://docs.konghq.com/hub/kong-inc/rate-limiting/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;它可以针对&lt;code&gt;consumer&lt;/code&gt; ,&lt;code&gt;credential&lt;/code&gt; ,&lt;code&gt;ip&lt;/code&gt; ,&lt;code&gt;service&lt;/code&gt;,&lt;code&gt;path&lt;/code&gt;,&lt;code&gt;header&lt;/code&gt; 等多种维度来进行限流.流量控制的精准度也有多种方式可以参考,比如可以做到秒级,分钟级,小时级等限流控制.&lt;/p&gt;
&lt;h4 id=&quot;响应客户端头部信息&quot;&gt;&lt;a href=&quot;#响应客户端头部信息&quot; class=&quot;headerlink&quot; title=&quot;响应客户端头部信息&quot;&gt;&lt;/a&gt;响应客户端头部信息&lt;/h4&gt;&lt;p&gt;当启用这个插件后.Kong会响应客户端一些额外的头部信息,告诉客户端限流信息.例如下面是Kong响应给客户端的header信息,告诉客户端当前的限流策略是10r/s&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;RateLimit-Limit: 10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;RateLimit-Remaining: 0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;RateLimit-Reset: 1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X-Kong-Response-Latency: 1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X-RateLimit-Limit-Second: 10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X-RateLimit-Remaining-Second: 0&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果客户端的访问请求超过限流的阈值,Kong会返回status&lt;code&gt;429&lt;/code&gt;的状态码以及下面的错误信息&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123; &amp;quot;message&amp;quot;: &amp;quot;API rate limit exceeded&amp;quot; &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>kafka-1.1基本概念介绍</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/1.1%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/1-概念介绍/1.1 基本概念介绍/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:48:01.269Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-1-基本概念介绍"><a href="#1-1-基本概念介绍" class="headerlink" title="1.1 基本概念介绍"></a>1.1 基本概念介绍</h1><h3 id="kafka特性"><a href="#kafka特性" class="headerlink" title="kafka特性"></a>kafka特性</h3><ul><li><strong>消息系统</strong>: Kafka和传统的消息中间件都具备流量削峰,缓冲,异步通信,扩展性等.另外,Kafka还提供了大多数消息中间件难以实现的消息顺序保障及回溯消费的功能</li><li><strong>存储系统</strong>: 消息持久化到存盘,可以实现永久存储</li><li><strong>流式处理平台</strong>: Kafka提供了流式处理类库</li></ul><a id="more"></a><h3 id="Kafka架构"><a href="#Kafka架构" class="headerlink" title="Kafka架构"></a>Kafka架构</h3><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608447536390-cba7d090-f67a-435c-8d7d-704fb446e573.png" alt="image.png"></p><p>一个Kafka体系主要包括:</p><ul><li>producer: 生产者</li><li>broker: kafka节点服务器</li><li>consumer: 消费者</li><li>zookeeper: 负责管理kafka集群元数据,集群选举等</li></ul><p>producer将消息发送到Broker,Broker负责将受到的消息存储到磁盘中,Consumer负责从Broker订阅并消费消息.</p><h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><ul><li>Kafka 通过 <em>topic</em> 对存储的流数据进行分类。</li><li>每条记录中包含一个key，一个value和一个timestamp（时间戳).</li><li>kafka保留所有的发布记录(无论是否已经被消费过).通过一个可配置的参数—保留期限来控制记录存在时间.</li></ul><blockquote><p>举个例子， 如果保留策略设置为2天，一条记录发布后两天内，可以随时被消费，两天过后这条记录会被抛弃并释放磁盘空间。</p></blockquote><h3 id="kafka核心API"><a href="#kafka核心API" class="headerlink" title="kafka核心API"></a>kafka核心API</h3><ul><li><a href="https://kafka.apachecn.org/documentation.html#producerapi" target="_blank" rel="noopener">Producer API</a> : 允许一个应用程序发布一串流式的数据到一个或者多个Kafka topic。</li><li><a href="https://kafka.apachecn.org/documentation.html#consumerapi" target="_blank" rel="noopener">Consumer API</a>: 允许一个应用程序订阅一个或多个 topic ，并且对发布给他们的流式数据进行处理。</li><li><a href="https://kafka.apachecn.org/documentation/streams" target="_blank" rel="noopener">Streams API</a>: 允许一个应用程序作为一个<em>流处理器</em>，消费一个或者多个topic产生的输入流，然后生产一个输出流到一个或多个topic中去，在输入输出流中进行有效的转换。</li><li><a href="https://kafka.apachecn.org/documentation.html#connect" target="_blank" rel="noopener">Connector API</a>: 允许构建并运行可重用的生产者或者消费者，将Kafka topics连接到已存在的应用程序或者数据系统。比如，连接到一个关系型数据库，捕捉表（table）的所有变更内容。</li></ul><h3 id="理解topics和Partition和offset"><a href="#理解topics和Partition和offset" class="headerlink" title="理解topics和Partition和offset"></a>理解topics和Partition和offset</h3><p><strong>Topic</strong>: 就是数据主题，生产者将消息发送到特点的主题.消费者负责订阅主题并进行消费.</p><p><strong>Partition</strong>: 一个Topic可以划分成多个partition(分区).但是一个分区只属于单个主题.很多时候也会将partition称为主题分区(Topic-Partition).同一个主题下的不同分区包含的消息也不同.分区在存储层面可以看做一个追加的日志(Log)文件.</p><p>一个主题的分区可以在不同的节点服务器上,所有的消息会均匀的分配到不同的分区中(也就是不同的节点服务器),这样可以提高磁盘IO和性能.在创建主题的时候可以设置分区数量,当然也可以在主题创建完成后去修改分区数量.通过增加分区的数量实现水平扩展.</p><p>好比是为公路运输，不同的起始点和目的地需要修不同高速公路（主题），高速公路上可以提供多条车道（分区），流量大的公路多修几条车道保证畅通，流量小的公路少修几条车道避免浪费。收费站好比消费者，车多的时候多开几个一起收费避免堵在路上，车少的时候开几个让汽车并道就好了</p><p>Kafka中的Topics总是多订阅者模式，一个topic可以拥有一个或者多个消费者来订阅它的数据。对于每一个topic， Kafka集群都会维持一个分区日志，如下所示：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608119181003-5ca1cc98-c0ec-4c00-b7b0-d0b916204ab0.png" alt="image.png"></p><p>每个partition分区都是有序切不可变的记录集.并且不断的追加到结构化的commit log文件.</p><p><strong>Offset</strong>: 消息被存储到分区的日志文件时会分片一个偏移量(offset).offset是消息在分区中的唯一表示.kafka通过它来保障消息在分区内的顺序.</p><p>不过Offset并不跨越分区,也就是说Kafka保证的是分区有序,而不是主题有序.</p><p>在每一个消费者中唯一保存的元数据是offset（偏移量）即消费在log中的位置.偏移量由消费者所控制:通常在读取记录后，消费者会以线性的方式增加偏移量，但是实际上，由于这个位置由消费者控制，所以消费者可以采用任何顺序来消费记录。例如，一个消费者可以重置到一个旧的偏移量，从而重新处理过去的数据；也可以跳过最近的记录，从”现在”开始消费。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608120423518-7cddb09d-e7fa-4f35-809f-908a36b5a4d1.png?x-oss-process=image%2Fresize%2Cw_1500" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-1-基本概念介绍&quot;&gt;&lt;a href=&quot;#1-1-基本概念介绍&quot; class=&quot;headerlink&quot; title=&quot;1.1 基本概念介绍&quot;&gt;&lt;/a&gt;1.1 基本概念介绍&lt;/h1&gt;&lt;h3 id=&quot;kafka特性&quot;&gt;&lt;a href=&quot;#kafka特性&quot; class=&quot;headerlink&quot; title=&quot;kafka特性&quot;&gt;&lt;/a&gt;kafka特性&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;消息系统&lt;/strong&gt;: Kafka和传统的消息中间件都具备流量削峰,缓冲,异步通信,扩展性等.另外,Kafka还提供了大多数消息中间件难以实现的消息顺序保障及回溯消费的功能&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;存储系统&lt;/strong&gt;: 消息持久化到存盘,可以实现永久存储&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;流式处理平台&lt;/strong&gt;: Kafka提供了流式处理类库&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="1-概念介绍" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-3.1消费者与消费组</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/3-%E6%B6%88%E8%B4%B9%E8%80%85/3.1%20%E6%B6%88%E8%B4%B9%E8%80%85%E4%B8%8E%E6%B6%88%E8%B4%B9%E7%BB%84/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/3-消费者/3.1 消费者与消费组/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:49:55.160Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-1-消费者与消费组"><a href="#3-1-消费者与消费组" class="headerlink" title="3.1 消费者与消费组"></a>3.1 消费者与消费组</h2><h3 id="1-消费者和消费组介绍"><a href="#1-消费者和消费组介绍" class="headerlink" title="1.消费者和消费组介绍"></a>1.消费者和消费组介绍</h3><p>消费者( Consumer)负责订阅Kafka中的主题( Topic)，并且从订阅的主题上拉取消息.与其他一些消息中间件不同的是:在 Kafka的消费理念中还有一层消费组( Consumer Group)的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者 。</p><p>以下图为例,某个主题中共有 4 个分区( Partition) : PO、 Pl、 P2、 P3。 有两个消费组 A和 B 都订阅了这个主题，消费组 A 中有 4 个消费者 (CO、 Cl、 C2 和 C3)，消费组 B 中有 2个消费者 CC4 和 CS) 。按照 Kafka默认的规则，最后的分配结果是消费组 A 中的每一个消费 者分配到1个分区，消费组 B 中的每一个消费者分配到 2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。换言之 每一个分区只能被一个消费组中的一个消费者所消费.</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608547700645-c525d96a-ac02-471f-9276-ee885e471c86.png" alt="image.png"></p><a id="more"></a> <p>假设目前某消费组内只有一个消费者 co，订阅了一个主题，这个主题包含 7 个分区: PO、 Pl、 P2、 P3、 P4、PS、 P6o 也就是说，这个消费者co订阅了7个分区，具体分配情形参考图3-2。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608547782387-e3cd0cab-f862-465d-bfd7-96c7fa613b21.png" alt="image.png"></p><p>​                                        </p><p>此时消费组内又加入了一个新的消费者 Cl，按照既定的逻辑，需要将原来消费者 co 的部分分区分配给消费者 Cl 消费 ， 如图 3-3 所示 。 消费者 co 和 Cl 各自负责消费所分配到的分区 ，彼此之间并无逻辑上的干扰 </p><p>消费者与消费组这种模型可以让整体的消费能力具备横向伸缩性，我们 可以增加(或减少) 消费者的个数来提高 (或降低〕整体的消费能力 。 对于分区数固定的情况， 一昧地增加消费者并不会让消费能力 一直得到提升，<strong>如果消费者过多，出现了消费者的个数大于分区个数的情况，**</strong>就会有消费者分配不到任何分区**。</p><h3 id="2-两种消息投递模式"><a href="#2-两种消息投递模式" class="headerlink" title="2.两种消息投递模式"></a>2.两种消息投递模式</h3><p>对于消息中间件而言,一般有两种消息投递模式:<strong>点对点</strong>(P2P, Point-to-Point)模式和<strong>发**</strong>布/订阅**( Pub/Sub)模式.</p><p><strong>点对点模式</strong>是基于队列的，消息生产者发送消息到队列，消息消费者从队列中接收消息。</p><p><strong>发布订阅模式</strong>定义了如何向一个内容节点发布和订阅消息,这个内容节点称为主题(Topic),主题可以认为是消息传递的中介,消息发布者将消息发布到某个主题,而消息订阅者从主题中订阅消息.主题使得消息的订阅者和发布者互相保持独立,不需要进行接触即可保证消息的传递,发布/订阅模式在消息的一对多广播时采用.Kafka同时支持两种消息投递模式，而这正是得益于消费者与消费组模型的契合:</p><ul><li>如果所有的消费者都隶属于同一个消费组,那么所有的消息都会被均衡地投递给每一个消费者,即每条消息只会被一个消费者处理,这就相当于点对点模式的应用 。</li><li>如果所有的消费者都隶属于不同的消费组,那么所有的消息都会被广播给所有的消费者,即每条消息会被所有的消费者处理,这就相当于发布/订阅模式的应用.</li></ul><p>消费组是一个逻辑上的概念，它将旗下的消费者归为一类 ，每一个消费者只隶属于一个消费组。每一个消费组都会有一个固定的名称，消费者在进行消费前需要指定其所属消费组的名称，这个可以通过消费者客户端参数 group.id来配置，默认值为空宇符串。</p><p>消费者并非逻辑上的概念它是实际的应用实例它可以是一个线程，也可以是一个进程。同一个消费组内的消费者既可以部署在同一台机器上，也可以部署在不同的机器上。</p><p>​                                        </p><p>​                                        </p><p>​                                                                                </p><p>​                                        </p><p>​                                        </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-1-消费者与消费组&quot;&gt;&lt;a href=&quot;#3-1-消费者与消费组&quot; class=&quot;headerlink&quot; title=&quot;3.1 消费者与消费组&quot;&gt;&lt;/a&gt;3.1 消费者与消费组&lt;/h2&gt;&lt;h3 id=&quot;1-消费者和消费组介绍&quot;&gt;&lt;a href=&quot;#1-消费者和消费组介绍&quot; class=&quot;headerlink&quot; title=&quot;1.消费者和消费组介绍&quot;&gt;&lt;/a&gt;1.消费者和消费组介绍&lt;/h3&gt;&lt;p&gt;消费者( Consumer)负责订阅Kafka中的主题( Topic)，并且从订阅的主题上拉取消息.与其他一些消息中间件不同的是:在 Kafka的消费理念中还有一层消费组( Consumer Group)的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者 。&lt;/p&gt;
&lt;p&gt;以下图为例,某个主题中共有 4 个分区( Partition) : PO、 Pl、 P2、 P3。 有两个消费组 A和 B 都订阅了这个主题，消费组 A 中有 4 个消费者 (CO、 Cl、 C2 和 C3)，消费组 B 中有 2个消费者 CC4 和 CS) 。按照 Kafka默认的规则，最后的分配结果是消费组 A 中的每一个消费 者分配到1个分区，消费组 B 中的每一个消费者分配到 2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。换言之 每一个分区只能被一个消费组中的一个消费者所消费.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.nlark.com/yuque/0/2020/png/2992889/1608547700645-c525d96a-ac02-471f-9276-ee885e471c86.png&quot; alt=&quot;image.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="3-消费者" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/3-%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-4.2分区管理</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/4.2%E5%88%86%E5%8C%BA%E7%AE%A1%E7%90%86/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/4-主题和分区/4.2分区管理/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:54:58.225Z</updated>
    
    <content type="html"><![CDATA[<h3 id="4-2-1-优选副本的选举"><a href="#4-2-1-优选副本的选举" class="headerlink" title="4.2.1 优选副本的选举"></a>4.2.1 优选副本的选举</h3><h4 id="4-2-1-1-什么是优先副本"><a href="#4-2-1-1-什么是优先副本" class="headerlink" title="4.2.1.1 什么是优先副本"></a>4.2.1.1 什么是优先副本</h4><p>分区使用多副本机制来提升可靠性,但是只有leader副本对外提供读写服务.而follower副本只负责在内部进行消息的同步.如果一个分区的leader副本不可用,那么就意味着整个分区变得不可用.此时就需要从剩余的follower副本中挑选一个新的leader副本继续对外提供服务.</p><blockquote><p>broker节点中的Leader副本个数决定了这个节点负载的高低</p></blockquote><p>在创建主题的时候,主题的分区和副本会尽可能的均匀分布在kafka集群的各个broker节点.对应的Leader副本的分配也比较均匀.例如下面的 <code>topic-demo</code> 主题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 152,153,154</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 152,153,154</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 152,153,154</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 152,153,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><a id="more"></a> <p>可以看到,leader副本均匀分布在所有的broker节点.另外,同一个分区,在同一台broker节点只能存在一个副本.所以leader副本所在的broker节点叫做分区的leader节点.而follower副本所在的broker节点叫做分区的follower节点.</p><p>可以想象的是,随着时间的推移,kafka集群中不可避免的出现节点宕机或者崩溃的情况.当分区的Leader节点发生故障时,其中一个follower节点就会成为新的Leader节点.这样导致集群中的节点之间负载不均衡,从而影响kafka整个集群的稳定性和健壮性.</p><p>即使原来的Leader节点恢复后,加入到集群时,也只能成为一个新的follower节点,而不会自动”抢班夺权”变成leader.</p><p>例如刚才的 <code>topic-demo</code> 分区重启152节点后,leader分布如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 153 Replicas: 152,153,154   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 154 Replicas: 152,154,153   Isr: 153,154,152</span><br></pre></td></tr></table></figure><p>尽管kafka非常均匀的将leader副本分布在其他另外2个几点.但是此时152节点的负载几乎为零.</p><p>为了有效的治理负载失衡的情况,kafka引入了<strong>优先副本(preferred replica)</strong>的概念.所谓的优先副本就是在AR集合列表中的第一个副本为优先副本,理想情况下优先副本就是该分区的leader副本.所以也可以称之为 <code>preferred leader</code> .</p><p>比如上面的例子中,分区0的AR集合(Replicas)是[152,153,154].那么分区0的优先副本就是152.<strong>Kafka会确保所有主题的优先副本均匀分布.这样就保证了所有分区的leader均衡分布.</strong></p><p><strong></strong></p><h4 id="4-2-1-2-优先副本选举"><a href="#4-2-1-2-优先副本选举" class="headerlink" title="4.2.1.2 优先副本选举"></a>4.2.1.2 优先副本选举</h4><p>所谓的优先副本选举是指通过一定的方式促使优先副本选举为Leader副本,促进集群的负载均衡.这一行为也称之为”分区平衡”.</p><p>kafka broker端(server.properties配置文件)有个 <code>auto.leader.rebalance.enble</code> 参数.默认为true.也就是分区自动平衡功能.Kafka会启动一个定时任务,轮询所有的broker节点,自动执行优先副本选举动作.</p><p>不过在生产环境中建议将该配置设置为 <code>false</code> .因为kafka自动平衡分区可能在某些关键高分期时刻引起负面性能问题.也有可能引起客户端的阻塞.为了防止出现此类情况,建议针对副本不均衡的问题进行相应监控和告警,然后在合适的时间通过手动来执行分区平衡.</p><p>Kafka中的 <code>kafka-preferred-replica-election.sh</code> 脚本提供了对分区leader副本进行重新平衡的功能.优先副本选举过程是一个安全的过程,kafka客户端会自动感知leader副本的变更.</p><p>命令用法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181</span><br></pre></td></tr></table></figure><p>但是这样一来会对kafka集群的所有主题和分区都执行一遍优先副本的选举操作.如果集群中包含大量的分区,那么可能选举会失败,并且会对性能造成一定的应用.比较建议的是使用 <code>path-to-json-file</code> 参数来小批量的对部分指定的主题分区进行优先副本的选举操作.该参数指定一个JSON文件,这个JSON文件保存需要执行优先副本选举的分区清单.</p><p>举个例子,对上面的 <code>topic-demo</code> 分区进行优先副本选举操作.先创建一个JSON文件,文件名可以任意:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">  &quot;partitions&quot;: [</span><br><span class="line">    &#123; </span><br><span class="line">        &quot;partition&quot;:0,</span><br><span class="line">        &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;partition&quot;:1,</span><br><span class="line">        &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;   &quot;partition&quot;:2,</span><br><span class="line">            &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;   &quot;partition&quot;:3,</span><br><span class="line">            &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;</span><br><span class="line">   ]</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>将上述内容保存为 <code>election.json</code> 文件.然后执行下列命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-preferred-replica-election.sh --zookeeper localhost:2181 --path-to-json-file ~/election.json</span><br><span class="line"> </span><br><span class="line">Created preferred replica election path with &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:0&#125;,&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:1&#125;,&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:2&#125;,&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:3&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic-demo,0], [topic-demo,1], [topic-demo,2], [topic-demo,3])</span><br></pre></td></tr></table></figure><p>提示优先副本选举成功.下列结果显示leader副本已经均衡分配到所有Broker节点了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 153,154,152</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>在实际生产环境中,建议使用这种方式来分批的执行优先副本选举操作.杜绝直接粗暴的进行所有分区的优先副本选举.另外,这类操作也应该需要避开业务高峰期,以免对性能造成负面影响,或者出现意外故障.</p><h3 id="4-2-2-分区重分配"><a href="#4-2-2-分区重分配" class="headerlink" title="4.2.2 分区重分配"></a>4.2.2 分区重分配</h3><p>当集群中一个Broker节点宕机,该节点的所有副本都处于丢失状态.kafka并不会自动将这些失效的分区副本自动迁移到集群其他broker节点.另外当集群中新增一台Broker节点时,只有新创建的主题分区才能被分配到这个节点上,而之前的主题分区并不会自动的加入到新节点(因为在创建时,并没有这个节点).这就导致新节点负载和原有节点负载之间严重不均衡.</p><p>为了解决这些问题,需要让分区副本再次进行合理的分配.也就是所谓的分区重分配.kafka提供了 <code>kafka-reassign-paritions.sh</code> 脚本执行分区重分配的工作.可以在集群节点失效或者扩容时使用.使用需要3个步骤:</p><ul><li>创建一个包含主题清单的JSON文件</li><li>根据主题清单和Broker节点清单生成一份重分配方案</li><li>执行具体重分配工作</li></ul><blockquote><p>要执行分区重分配,前提是broker节点清单数量要大于或者等于副本因子数量,否则会报错</p><p>Partitions reassignment failed due to replication factor: 3 larger than available brokers: 2</p></blockquote><p>下面创建一个4分区,2个副本因子的主题 <code>topic-reassign</code> 举例.假定要将152这个broker节点下线.下线之前需要将该节点上的分区副本迁移出去.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-reassign --replication-factor 2 --partitions 4</span><br><span class="line">Created topic &quot;topic-reassign&quot;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$  kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,153   Isr: 152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>第一步,创建一个JSON文件(文件名假定为reassign.json).文件内容是主题清单:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">    &quot;topics&quot;:[</span><br><span class="line">      &#123; </span><br><span class="line">                &quot;topic&quot;:&quot;topic-reassign&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;version&quot;:1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第二步,根据这个JSON文件和指定要分配的broker节点列表生成一份候选重分配方案:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --generate --topics-to-move-json-file ~/reassign.json --broker-list 153,154</span><br><span class="line"> </span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[153,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,153]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file project.json</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[153,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,153]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>在上面的例子中有以下几个参数:</p><p><code>--zookeeper</code> 这个参数已经非常熟悉了</p><p><code>--generate</code> 指令类型参数,类似于kafka-topics.sh脚本中的 <code>--create</code> , <code>--list</code> . <code>--describe</code> 等</p><p><code>--topics-to-move-json-file</code> 指定主题清单文件路径</p><p><code>--broker-list</code> 指定要分配的broker节点列表</p><p>上面的例子中打印了2个JSON格式内容:</p><p><code>Current partition replica assignment</code> 表示目前的分区副本分配情况,在执行分区重分配前最好备份这个内容,以便后续回滚操作</p><p><code>Proposed partition reassignment configuration</code> 表示候选重分配方案.这里只是一个方案,并没有真正执行.</p><p>将第二个Json内容格式化输出后,我们发现这个方案正如我们计划的那样,将该主题的所有分区下的AR副本集合分配到153和154节点,所有副本已经从即将要下线的152节点迁移走.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;:1,</span><br><span class="line">    &quot;partitions&quot;:[</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:1,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                153</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:0,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                153,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:3,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                153</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:2,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                153,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第三步,将 <code>Proposed partition reassignment configuration</code> JSON文件内容保存在一个文件中(假定为project.json).然后执行具体的重分配的动作,命令如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file project.json</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[153,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,153]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><blockquote><p>这里仍然打印了之前的副本分配方案,并且提示保存到JSON文件,以便回滚</p></blockquote><p>这里使用了2个不同的命令参数:</p><ul><li><code>--execute</code> 指令类型参数,执行重分配动作</li><li><code>--reassignment-json-file</code> 指定重分配方案文件路径</li></ul><p>再次查看 <code>topic-reassign</code> 主题分区副本分配情况,所有的副本都从152迁移出去,此时该节点可以顺利下线</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 153 Replicas: 154,153   Isr: 153,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>当然,我们也可以直接编写第二个JSON文件来自定义重分配方案,这样就不需要执行上面的第一步和第二步操作了.</p><p>分区重分配的基本原理是为每个分区添加新副本(增加副本数量),新副本会从leader副本复制所有的数据.复制完成后,控制器将旧副本从副本清单里移除.(恢复成原来的副本数量).</p><blockquote><p>所以,分区重分配需要确保有足够的空间,并且避免在业务高峰期操作</p></blockquote><p>从刚才的主题分区结果可以看到,大部分的分区leader副本都集中在153这个broker节点.这样负载非常不均衡,我们可以继续借助 <code>kafka-preferred-replica-election.sh</code> 脚本执行一次优先副本选举.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-preferred-replica-election.sh --zookeeper localhost:2181 --path-to-json-file election.json</span><br><span class="line"></span><br><span class="line">Created preferred replica election path with &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic-reassign,0], [topic-reassign,1], [topic-reassign,2], [topic-reassign,3])</span><br><span class="line"></span><br><span class="line">#选举完成后,副本分配均衡</span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 154 Replicas: 154,153   Isr: 153,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><blockquote><p>和优先副本选举一样,分区重分配对集群的性能有很大的影响.需要占用额外的磁盘,网络IO等资源.在生产环境执行操作时应该分批次执行.</p></blockquote><h3 id="4-2-3-复制限流"><a href="#4-2-3-复制限流" class="headerlink" title="4.2.3 复制限流"></a>4.2.3 复制限流</h3><p>我们了解分区重分配的本质在于数据复制,先增加新副本,进行数据同步,然后删除旧副本.如果副本数据量太大必然会占用很多额外的资源,从而影响集群整体性能.kafka有限流机制,可以对副本之间的复制流量进行限制.</p><p>副本复制限流有2种实现方式:</p><ul><li><code>kafka-config.sh</code> </li><li><code>kafka-reassign-partitions.sh</code> </li></ul><p>前者的实现方式有点繁琐,这里介绍后者的使用方式.</p><p><code>kafka-reassign-partitions.sh</code> 的实现方式非常简单,只需要一个throttle参数即可.例如上面的例子中副本都在153和154节点,现在继续使用分区重分配,让副本从153节点迁移到152节点.但是这次使用限流工具</p><p>首先,修改 <code>project.json</code> 文件,将153替换成152</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &apos;s/153/152/g&apos; project.json</span><br></pre></td></tr></table></figure><p>然后,执行分区重分配,这里使用 <code>--throttle</code> 参数,指定一个限流速度(单位是B/s)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file project.json --throttle 10</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,153]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[154,153]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[153,154]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value.</span><br><span class="line">The throttle limit was set to 10 B/s</span><br><span class="line">Successfully started reassignment of partitions.</span><br></pre></td></tr></table></figure><p>上面的示例输出中提示以下3点信息:</p><ol><li>需要周期性的使用 <code>--verify</code> 参数来周期性的查看副本复制进度,直到分区重分配完成,也就是说需要显示的使用这种方式确保分区重分配完成后解除限流的设置</li><li>限流的速度为10B/s</li><li>如果想要修改限流速度,重复此条执行命令,修改throttle的值即可</li></ol><p>接下来使用 <code>--verify</code> 参数查看复制进度.下面的示例显示复制已经完成,并且限流已被解除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --verify --reassignment-json-file project.json</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition [topic-reassign,1] completed successfully</span><br><span class="line">Reassignment of partition [topic-reassign,0] completed successfully</span><br><span class="line">Reassignment of partition [topic-reassign,3] completed successfully</span><br><span class="line">Reassignment of partition [topic-reassign,2] completed successfully</span><br><span class="line">Throttle was removed.</span><br></pre></td></tr></table></figure><p>此时153的副本已经被移除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 152 Replicas: 152,154   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 154 Replicas: 154,152   Isr: 154,152</span><br></pre></td></tr></table></figure><h3 id="4-2-4-修改副本因子"><a href="#4-2-4-修改副本因子" class="headerlink" title="4.2.4 修改副本因子"></a>4.2.4 修改副本因子</h3><p>上面的例子中分区重分配,将副本从一个broker节点中移除,此时kafka集群的broker节点数量只剩下2个.副本因子也只有2个.这里有个问题,此时153节点重启,或者新增broker节点后,如何将新增的broker节点加入进群,扩展副本数量呢?或者还有一种情况,当创建主题和分区后,想要修改副本因子呢?</p><p><code>kafka-reassign-parition.sh</code> 脚本同样实现了修改副本因子的功能..仔细观察一下分区重分配案例中的 <code>project.json</code> 文件内容:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ cat project.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;:1,</span><br><span class="line">    &quot;partitions&quot;:[</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:1,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                152</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:0,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                152,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:3,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                152</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:2,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                152,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>json文件中的副本集合(replicas)都是2个副本,我们可以很简单的添加一个副本.比如对于分区0而言,可以将153节点添加进去.(其他分区也是如此)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:1,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                152,</span><br><span class="line">                153</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br></pre></td></tr></table></figure><p>执行 <code>kafka-reassign-partition.sh</code> 脚本,执行命令的方法和参数和分区重分片几乎一致:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file add.json</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[152,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,154]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>查看副本分配情况.副本数量已经增加到了3个</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,154,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 154 Replicas: 153,154,152   Isr: 154,152,153</span><br></pre></td></tr></table></figure><blockquote><p>虽然副本因子增加到3个,但是Leader还是没有分配到新的153这个broker节点.此时可以通过优先副本选举重新分配</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-preferred-replica-election.sh --zookeeper localhost:2181 --path-to-json-file election.json</span><br><span class="line">Created preferred replica election path with &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic-reassign,0], [topic-reassign,1], [topic-reassign,2], [topic-reassign,3])</span><br><span class="line"></span><br><span class="line">#再次查看topic-reassign主题,分区副本分配均衡</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,154,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 153 Replicas: 153,154,152   Isr: 154,152,153</span><br></pre></td></tr></table></figure><p>重点: <strong>与修改分区数量不同,副本数还可以减少</strong>,修改方法和命令几乎一样,只需要编辑JSON配置文件即可.这里就不再演示</p><h3 id="4-2-5-如何选择合适的分区数量"><a href="#4-2-5-如何选择合适的分区数量" class="headerlink" title="4.2.5 如何选择合适的分区数量"></a>4.2.5 如何选择合适的分区数量</h3><p>如何选择合适的分区数量是需要经常面对的问题,但是这个问题似乎并没有权威的标准答案,需要根据实际的业务场景,硬件资源,应用软件,负载等情况做具体考量.这一章节主要介绍与本问题相关的一些决策因素,以供参考</p><h4 id="4-2-5-1-性能测试工具"><a href="#4-2-5-1-性能测试工具" class="headerlink" title="4.2.5.1 性能测试工具"></a>4.2.5.1 性能测试工具</h4><p>在生产环境中设定分区数量需要考虑性能因素.所以性能测试工具必不可少,kafka本身提供了用于生产者性能测试的 <code>kafka-producer-pref-test.sh</code> 脚本和用于消费者性能测试的 <code>kafka-consumer-perf-test.sh</code>脚本</p><p>首先创建一个用于测试的分区为1,副本为1的 <code>topic-1</code> 的主题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Topic:topic-1   PartitionCount:1    ReplicationFactor:1 Configs:</span><br><span class="line">    Topic: topic-1  Partition: 0    Leader: 153 Replicas: 153   Isr: 153</span><br></pre></td></tr></table></figure><p>其次.我们往这个主题发送100万条消息,并且每条消息大小为1024B,生产者对应的acks参数为1:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-producer-perf-test.sh --topic topic-1 --num-records 1000000 --record-size 1024 --throughput -1 --producer-props bootstrap.servers=localhost:9092 acks=1</span><br><span class="line">76666 records sent, 15333.2 records/sec (14.97 MB/sec), 1517.5 ms avg latency, 2061.0 max latency.</span><br><span class="line">119400 records sent, 23880.0 records/sec (23.32 MB/sec), 1353.6 ms avg latency, 1631.0 max latency.</span><br><span class="line">124560 records sent, 24912.0 records/sec (24.33 MB/sec), 1231.2 ms avg latency, 1375.0 max latency.</span><br><span class="line">146520 records sent, 29304.0 records/sec (28.62 MB/sec), 1066.6 ms avg latency, 1146.0 max latency.</span><br><span class="line">156795 records sent, 31359.0 records/sec (30.62 MB/sec), 972.3 ms avg latency, 1051.0 max latency.</span><br><span class="line">133365 records sent, 26673.0 records/sec (26.05 MB/sec), 1141.1 ms avg latency, 1322.0 max latency.</span><br><span class="line">159945 records sent, 31989.0 records/sec (31.24 MB/sec), 964.3 ms avg latency, 1178.0 max latency.</span><br><span class="line">1000000 records sent, 26148.576210 records/sec (25.54 MB/sec), 1143.54 ms avg latency, 2061.00 ms max latency, 1114 ms 50th, 1654 ms 95th, 1869 ms 99th, 2036 ms 99.9th.</span><br></pre></td></tr></table></figure><p>示例中使用了多个参数:</p><p><code>num-records</code>:  指定发送消息的总条数</p><p><code>record-size</code>: 设置每条消息的字节数</p><p><code>throughtput</code> : 限流控制,-1表示不限流,大于0表示限流值</p><p><code>producer-props</code> : 指定生产者的配置,可以同时指定多组配置</p><p>除此之外还有其他参数,比如 <code>print-metrics</code> 在测试完成之后,打印很多指标信息.有兴趣可以执行 <code>--help</code> 查看更多参数信息.</p><p>回过头再看看上面示例中的压测结果信息,以第一条和最后一条为例:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">76666 records sent, 15333.2 records/sec (14.97 MB/sec), 1517.5 ms avg latency, 2061.0 max latency.</span><br><span class="line">1114 ms 50th, 1654 ms 95th, 1869 ms 99th, 2036 ms 99.9th</span><br></pre></td></tr></table></figure><p><strong>records sent: 表示发送的消息综述</strong></p><p><strong>records/sec: 吞吐量,表示每秒发送的消息数量</strong></p><p><strong>MB/sec: 吞吐量,表示每秒发送的消息大小</strong></p><p><strong>avg latency: 表示消息处理的平均耗时</strong></p><p><strong>max latency: 表示消息处理的最大耗时</strong></p><p><strong>50th,95th,99th,99.th</strong> 表示50%,95%,99%,99.9%时消息处理耗时</p><p><strong></strong></p><p>消费者压测工具的脚本使用也比较简单,下面的简单实例演示了消费主题 <code>topic-1</code> 中的100万条消息.命令使用方法:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-consumer-perf-test.sh --topic topic-1 --messages 1000000 --broker-list localhost:9092</span><br><span class="line">start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec</span><br><span class="line">2020-12-26 14:07:29:612, 2020-12-26 14:07:35:272, 977.0264, 172.6195, 1000475, 176762.3675</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p><strong>data.consumed.in.MB: 消费的消息总量,单位为MB</strong></p><p><strong>MB.sec</strong>: 按字节大小计算的消费吞吐量(单位:MB/s)</p><p><strong>data.consumed.in.nMsg</strong>: 消费的消息消息总数</p><p><strong>nMsg.sec</strong>: 按消息个数计算的吞独量(单位n/s)</p><blockquote><p>可以创建多个分区,比如10,100,200,500等(副本数量都为1)来测试生产和消费的性能表现,</p></blockquote><h4 id="4-2-5-2-分区数量越多不代表吞独量越高"><a href="#4-2-5-2-分区数量越多不代表吞独量越高" class="headerlink" title="4.2.5.2 分区数量越多不代表吞独量越高"></a>4.2.5.2 分区数量越多不代表吞独量越高</h4><p>消息中间件的性能一般是指吞吐量(还包括延迟),吞吐量会受到硬件资源,消息大小,消息压缩,消息发送方式(同步,异步),副本因子等参数影响.分区数量越多不一定吞吐量越高,超过一定的临界值后,kafka的吞吐量会不升反降.</p><h4 id="4-2-5-3-分区数量的上限"><a href="#4-2-5-3-分区数量的上限" class="headerlink" title="4.2.5.3 分区数量的上限"></a>4.2.5.3 分区数量的上限</h4><p>一味的增加分区数量并不能使吞吐量得到提升,并且分区的数量也不能一直增加,如果超过一定的临界值还会引起kafka进程的崩溃.</p><p><strong>每次创建一个分区,都会消耗一个Linux系统的文件描述符.</strong></p><p>通过kafka的pid编号,可以查看当前kafka进程占用的文件描述符数量:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ ls /proc/22858/fd/ | wc -l</span><br><span class="line">173</span><br></pre></td></tr></table></figure><p>此时创建一个分区数量为400个的topic-demo4的主题.由于分区会平均创建在集群内的3个broker节点,所以需要统计一下152这个本地节点的分区数量.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics --describe --topic topic-demo4 | grep -Eo &quot;Leader:\s[0-9]+&quot; | sort | uniq -c</span><br><span class="line">    134 Leader: 152</span><br><span class="line">    133 Leader: 153</span><br><span class="line">    133 Leader: 154</span><br></pre></td></tr></table></figure><p>可以看到152这个节点创建了134个分区.接下来看看系统文件描述符的数量.正好增加了134个文件描述符</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ ls /proc/22858/fd/ | wc -l</span><br><span class="line">307</span><br></pre></td></tr></table></figure><p>可以想见的是,一旦分区数量超过了操作系统规定的文件描述符上限,kafka进程就会崩溃</p><h4 id="4-2-5-4-分区数量的考量"><a href="#4-2-5-4-分区数量的考量" class="headerlink" title="4.2.5.4 分区数量的考量"></a>4.2.5.4 分区数量的考量</h4><p>如何选择合适的分区数量,一个恰当的答案就是视具体情况而定.</p><p>从吞吐量方面考虑,增加合适的分区数量可以在一定程度上提升整体吞吐量,但是超过临界值之后吞吐量不升反降.在投入生产环境之前,应该对吞吐量进行相关的测试,以找到合适的分区数量</p><p>分区数量太多会影响系统可用性,当broker发生故障时,broker节点上的所有分区的leader副本不可用,此时如果有大量的分区要进行leader角色切换,这个切换的过程会耗费相当的时间,并且这个时间段内分区会变的不可用.并且分区数量太多不仅为增加日志清理的耗时,而且在被删除时也会消费更多时间.</p><p>一个好的建议是,创建主题之前对分区数量性能进行充分压测,在创建主题之后,还需要对其进行追踪,监控,调优.如果分区数量较少,还能通过增加分区数量,或者增加broker进行分区重分配等改进.</p><p>最后,一个通用的准则是,建议分区数量设定为集群中broker的倍数,例如集群中有3个broker节点,可以设定分区数为3,6,9等.</p><h3 id="4-2-6-总结"><a href="#4-2-6-总结" class="headerlink" title="4.2.6 总结"></a>4.2.6 总结</h3><p><code>kafka-topics.sh</code> 查看,创建主题分区,副本</p><p><code>kafka-configs.sh</code> 修改主题配置文件</p><p><code>kafka_perferred-replica-elections.sh</code> 优先副本选举</p><p><code>kafka-reassign-partitions.sh</code> 分区重分配,副本复制限流,修改副本因子数量</p><p><code>kafka-producer-perf-test.sh</code> 生产者分区数和吞吐量性能压测</p><p><code>kafka-consumer-perf-test.sh</code> 消费者性能压测</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;4-2-1-优选副本的选举&quot;&gt;&lt;a href=&quot;#4-2-1-优选副本的选举&quot; class=&quot;headerlink&quot; title=&quot;4.2.1 优选副本的选举&quot;&gt;&lt;/a&gt;4.2.1 优选副本的选举&lt;/h3&gt;&lt;h4 id=&quot;4-2-1-1-什么是优先副本&quot;&gt;&lt;a href=&quot;#4-2-1-1-什么是优先副本&quot; class=&quot;headerlink&quot; title=&quot;4.2.1.1 什么是优先副本&quot;&gt;&lt;/a&gt;4.2.1.1 什么是优先副本&lt;/h4&gt;&lt;p&gt;分区使用多副本机制来提升可靠性,但是只有leader副本对外提供读写服务.而follower副本只负责在内部进行消息的同步.如果一个分区的leader副本不可用,那么就意味着整个分区变得不可用.此时就需要从剩余的follower副本中挑选一个新的leader副本继续对外提供服务.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;broker节点中的Leader副本个数决定了这个节点负载的高低&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在创建主题的时候,主题的分区和副本会尽可能的均匀分布在kafka集群的各个broker节点.对应的Leader副本的分配也比较均匀.例如下面的 &lt;code&gt;topic-demo&lt;/code&gt; 主题:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev152 ~]$&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="4-主题和分区" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-1.2 生产与消费简单实例</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/1.2%20%E7%94%9F%E4%BA%A7%E4%B8%8E%E6%B6%88%E8%B4%B9%E7%AE%80%E5%8D%95%E5%AE%9E%E4%BE%8B/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/1-概念介绍/1.2 生产与消费简单实例/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:48:34.104Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-2-生产与消费简单实例"><a href="#1-2-生产与消费简单实例" class="headerlink" title="1.2 生产与消费简单实例"></a>1.2 生产与消费简单实例</h1><h3 id="创建topic"><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h3><p>kafka提供了许多实用的脚本工具,存放在$KAFKA_HOME的bin目录下.其中与主题相关的就是kafka-topic.sh脚本.例如.下面创建一个分区数为4,副本为3的主题topic-demon<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-demo --replication-factor 3 --partitions 4</span><br><span class="line"></span><br><span class="line">Created topic <span class="string">"topic-demo"</span>.</span><br></pre></td></tr></table></figure></p><p><code>--zoopkeer</code> 指定kafka连接的zookeeper服务地址<br><code>--topic</code> 指定一个topic主题<br><code>--replication-factor</code>  指定副本因子数量<br><code>--partition</code> 指定分区数量<br><code>--create</code> 表示创建</p><a id="more"></a> <p>下面命令展示了刚创建的主题信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 bin]$ ./kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demoPartitionCount:4ReplicationFactor:3Configs:</span><br><span class="line">Topic: topic-demoPartition: 0Leader: 152Replicas: 152,153,154Isr: 152,153,154</span><br><span class="line">Topic: topic-demoPartition: 1Leader: 153Replicas: 153,154,152Isr: 153,154,152</span><br><span class="line">Topic: topic-demoPartition: 2Leader: 154Replicas: 154,152,153Isr: 154,152,153</span><br><span class="line">Topic: topic-demoPartition: 3Leader: 152Replicas: 152,154,153Isr: 152,154,153</span><br></pre></td></tr></table></figure></p><p>上面的命令结果表示 <code>topic-demon</code> 这个主题一共有4个分区,存放在3台Kafka broker服务器节点.3个broker均是ISR集合,没有OSR集合</p><blockquote><p>在任意一台kafka集群内的节点服务器上执行上述命令,会得到完全相同的结果</p></blockquote><h3 id="创建consumer"><a href="#创建consumer" class="headerlink" title="创建consumer"></a>创建consumer</h3><p><code>kafka-console-consumer.sh</code> 在任意一台kafka集群内的节点服务器上可以通过控制台创建一个 <code>consumer</code> 消费者.示例如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev154 bin]$ ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-demo</span><br></pre></td></tr></table></figure></p><p><code>--bootstrap-server</code> 指定连接的kafka集群地址<br><code>--topic</code> 指定消费者订阅的主题</p><h3 id="创建producer"><a href="#创建producer" class="headerlink" title="创建producer"></a>创建producer</h3><p><code>kafka-console-producer.sh</code> 在任意一台kafka集群内的节点服务器上可以通过控制台创建一个 <code>producer</code> 消费者.示例如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev153 bin]$ ./kafka-console-producer.sh --broker-list localhost:9092 --topic topic-demo</span><br></pre></td></tr></table></figure></p><p><code>--broker-list</code> 指定连接的kafka集群地址<br><code>--topic</code> 指定发小时时的主题<br>在弹出的shell终端中,输入 <code>hello world!</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;hello,world!</span><br></pre></td></tr></table></figure></p><p>回到 <code>consumer</code> 的shell终端界面,发现消费到了刚生产的消息:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev154 bin]$ ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-demo</span><br><span class="line">hello,world!</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-2-生产与消费简单实例&quot;&gt;&lt;a href=&quot;#1-2-生产与消费简单实例&quot; class=&quot;headerlink&quot; title=&quot;1.2 生产与消费简单实例&quot;&gt;&lt;/a&gt;1.2 生产与消费简单实例&lt;/h1&gt;&lt;h3 id=&quot;创建topic&quot;&gt;&lt;a href=&quot;#创建topic&quot; class=&quot;headerlink&quot; title=&quot;创建topic&quot;&gt;&lt;/a&gt;创建topic&lt;/h3&gt;&lt;p&gt;kafka提供了许多实用的脚本工具,存放在$KAFKA_HOME的bin目录下.其中与主题相关的就是kafka-topic.sh脚本.例如.下面创建一个分区数为4,副本为3的主题topic-demon&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;./kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-demo --replication-factor 3 --partitions 4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Created topic &lt;span class=&quot;string&quot;&gt;&quot;topic-demo&quot;&lt;/span&gt;.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;--zoopkeer&lt;/code&gt; 指定kafka连接的zookeeper服务地址&lt;br&gt;&lt;code&gt;--topic&lt;/code&gt; 指定一个topic主题&lt;br&gt;&lt;code&gt;--replication-factor&lt;/code&gt;  指定副本因子数量&lt;br&gt;&lt;code&gt;--partition&lt;/code&gt; 指定分区数量&lt;br&gt;&lt;code&gt;--create&lt;/code&gt; 表示创建&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="1-概念介绍" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-2.1Kafka副本</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/2-%E5%89%AF%E6%9C%AC%E4%BB%8B%E7%BB%8D/2.1%20%20Kafka%E5%89%AF%E6%9C%AC/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/2-副本介绍/2.1  Kafka副本/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T15:14:57.936Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2-1-Kafka副本"><a href="#2-1-Kafka副本" class="headerlink" title="2.1  Kafka副本"></a>2.1  Kafka副本</h1><h3 id="副本介绍"><a href="#副本介绍" class="headerlink" title="副本介绍"></a>副本介绍</h3><p>Kafka为分区引入了副本(Replica)机制.通过增加副本数量提升容灾能力.一个Topic主题可以有多个分区,一个分区又可以有多个副本.这多个副本中，只有一个是leader，而其他的都是follower副本。仅有leader副本可以对外提供服务。所以副本之间是一主多从的关系,而且每个副本中保存的相同的消息.(严格来说,同一时刻副本之间的消息并非能一定完全同步)</p><p>多个follower副本通常存放在和leader副本不同的broker中。通过这样的机制实现了高可用，当某台机器挂掉后，其他follower副本也能迅速”转正“，开始对外提供服务。</p><a id="more"></a> <p>在kafka中，实现副本的目的就是冗余备份，且仅仅是冗余备份，所有的读写请求都是由leader副本进行处理的。follower副本仅有一个功能，那就是从leader副本拉取消息，尽量让自己跟leader副本的内容一致。</p><blockquote><p>follower副本之所以不能对外提供服务,主要是为了保障数据一致性</p></blockquote><p>下图是一个多副本架构图.</p><p>Kafka集群中有4个broker，某个主题中有3个分区，且副本因子（即副本个数）也为3，如此每个分区便有1个leader副本和2个follower副本。生产者和消费者只与leader副本进行交互，而follower副本只负责消息的同步，很多时候follower副本中的消息相对leader副本而言会有一定的滞后。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608450120303-2d66cd2e-611a-4e3e-a507-53b4f81adfa0.png" alt="image.png"></p><h3 id="副本同步"><a href="#副本同步" class="headerlink" title="副本同步"></a>副本同步</h3><p><strong>AR</strong>: 分区内的所有副本统称.</p><p><strong>ISR</strong>: In-Sync Replicas.所有与Leader副本保持一定程度同步的副本(包括Leader副本).一起组成ISR</p><p><strong>OSR</strong>: Out-of-Sync Replicas: 与leader副本同步滞后过多的副本(不包括leader副本),一起注册呢个OSR</p><p><strong>AR = ISR + OSR.</strong></p><blockquote><p>正常情况下,所有的follower副本都应该与leader副本保持一定程度的同步,即AR = ISR,OSR集合为空</p></blockquote><p>Leader副本负责维护和跟踪ISR集合中所有follower副本的滞后状态,当follower副本落后太多或者失效时,leader副本会把它从ISR集合中剔除,如果OSR的follower副本追上了leader副本,那会从OSR转移到ISR.</p><blockquote><p>默认情况下,只有ISR集合中的follower副本才有资格被选举为新的Leader</p></blockquote><h3 id="HW和LEO"><a href="#HW和LEO" class="headerlink" title="HW和LEO"></a>HW和LEO</h3><p><strong>HW(High Watermark)</strong>: 俗称高水位.它标识了一个特点的消息偏移量(offset).消费者只能拉取这个offset之前的信息.</p><p><strong>LEO(Log End Offset)</strong>: 标识当前日志文件中下一条代写入消息的offset. </p><p><strong></strong></p><p>下面一张图能说明这两个概念</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608450932841-d4f33228-91b2-484f-b28e-24d0762ef670.png" alt="image.png"></p><p>上面的图代表一个日志文件.这个日志文件中有 9 条消息，第一条消息的 offset（LogStartOffset）为0，最后一条消息的offset为8，offset为9的消息用虚线框表示，代表下一条待写入的消息。日志文件的HW为6，表示消费者只能拉取到offset在0至5之间的消息，而offset为6的消息对消费者而言是不可见的。</p><p>offset为9的位置即为当前日志文件的LEO，LEO的大小相当于当前日志分区中最后一条消息的offset值加1。分区ISR集合中的每个副本都会维护自身的LEO，<strong>而ISR集合中最小的LEO即为分区的HW，对消费者而言只能消费HW之前的消息。</strong></p><h3 id="ISR和HW-LEO的关系"><a href="#ISR和HW-LEO的关系" class="headerlink" title="ISR和HW,LEO的关系"></a>ISR和HW,LEO的关系</h3><p>为了让读者更好地理解ISR集合，以及HW和LEO之间的关系，下面通过一个简单的示例来进行相关的说明。如图1-5所示，假设某个分区的ISR集合中有3个副本，即一个leader副本和2个follower副本，此时分区的LEO和HW都为3。消息3和消息4从生产者发出之后会被先存入leader副本</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451481234-2309dd97-7662-49d0-a43a-42b822f7a7d4.png" alt="image.png"></p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451501313-69357f66-f23b-4d07-940f-ef9f0afb9260.png" alt="image.png"></p><p>在同步过程中，不同的 follower 副本的同步效率也不尽相同。如图 所示，在某一时刻follower1完全跟上了leader副本而follower2只同步了消息3，如此leader副本的LEO为5，follower1的LEO为5，follower2的LEO为4，那么当前分区的HW取最小值4，此时消费者可以消费到offset为0至3之间的消息。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451550216-2bc62531-a164-40a1-b039-e9e911401d02.png" alt="image.png"></p><p>如果所有的副本都成功写入了消息3和消息4，整个分区的HW和LEO都变为5，因此消费者可以消费到offset为4的消息了。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451625357-266f1693-1d4b-4524-9f64-89256893555e.png" alt="image.png"></p><p>Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的 follower 副本都复制完，这条消息才会被确认为已成功提交，这种复制方式极大地影响了性能。而在异步复制方式下，follower副本异步地从leader副本中复制数据，数据只要被leader副本写入就被认为已经成功提交。在这种情况下，如果follower副本都还没有复制完而落后于leader副本，突然leader副本宕机，则会造成数据丢失。Kafka使用的这种ISR的方式则有效地权衡了数据可靠性和性能之间的关系。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;2-1-Kafka副本&quot;&gt;&lt;a href=&quot;#2-1-Kafka副本&quot; class=&quot;headerlink&quot; title=&quot;2.1  Kafka副本&quot;&gt;&lt;/a&gt;2.1  Kafka副本&lt;/h1&gt;&lt;h3 id=&quot;副本介绍&quot;&gt;&lt;a href=&quot;#副本介绍&quot; class=&quot;headerlink&quot; title=&quot;副本介绍&quot;&gt;&lt;/a&gt;副本介绍&lt;/h3&gt;&lt;p&gt;Kafka为分区引入了副本(Replica)机制.通过增加副本数量提升容灾能力.一个Topic主题可以有多个分区,一个分区又可以有多个副本.这多个副本中，只有一个是leader，而其他的都是follower副本。仅有leader副本可以对外提供服务。所以副本之间是一主多从的关系,而且每个副本中保存的相同的消息.(严格来说,同一时刻副本之间的消息并非能一定完全同步)&lt;/p&gt;
&lt;p&gt;多个follower副本通常存放在和leader副本不同的broker中。通过这样的机制实现了高可用，当某台机器挂掉后，其他follower副本也能迅速”转正“，开始对外提供服务。&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="2-副本介绍" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/2-%E5%89%AF%E6%9C%AC%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-4.1主题管理</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/4.1%E4%B8%BB%E9%A2%98%E7%AE%A1%E7%90%86/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/4-主题和分区/4.1主题管理/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:55:25.320Z</updated>
    
    <content type="html"><![CDATA[<h3 id="4-1-1-创建主题"><a href="#4-1-1-创建主题" class="headerlink" title="4.1.1 创建主题"></a>4.1.1 创建主题</h3><h4 id="4-1-1-1-自动创建主题以及分区副本"><a href="#4-1-1-1-自动创建主题以及分区副本" class="headerlink" title="4.1.1.1 自动创建主题以及分区副本"></a>4.1.1.1 自动创建主题以及分区副本</h4><p>在之前的笔记中提到了创建主题的一个简单示例.kafka提供 <code>kafka-topics.sh</code> 脚本来创建主题.下面这个示例创建了一个 <code>topic-test</code> 的主题,包含4个分区和2个副本.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-test --replication-factor 2 --partitions 4</span><br><span class="line"></span><br><span class="line">Created topic &quot;topic-test&quot;.</span><br></pre></td></tr></table></figure><p>分区创建完成后,会在kafka的 <code>log.dirs</code> 或者 <code>log.dir</code> 的目录下创建相应的主题分区.下面是在其中一台Broker节点的信息展示:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ ls /opt/logs/kafka/ | grep &quot;topic-test&quot;</span><br><span class="line">topic-test-0</span><br><span class="line">topic-test-2</span><br></pre></td></tr></table></figure><p>可以看到152节点中创建了2个文件夹 topic-test-0 和 topic-test-2,对应主题 topic-test的2个分区编号为0和2的分区，命名方式可以概括为 <code>&lt;topic&gt;-&lt;partition&gt;</code> .严谨地说,其实这类文件夹对应的不是分区,分区同主题一样是一个逻辑的概念而没有物理上的存在.并且这里我们也只是看到了2个分区,而我们创建的是4个分区,其余2个分区被分配到了153和154节点中，参考如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#153节点</span><br><span class="line">[hadoop@bi-dev153 ~]$ ls /opt/logs/kafka/ | grep &quot;topic-test&quot;</span><br><span class="line">topic-test-0</span><br><span class="line">topic-test-1</span><br><span class="line">topic-test-3</span><br><span class="line"></span><br><span class="line">#154节点</span><br><span class="line">[hadoop@bi-dev154 ~]$ ls /opt/logs/kafka/ | grep &quot;topic-test&quot;</span><br><span class="line">topic-test-1</span><br><span class="line">topic-test-2</span><br><span class="line">topic-test-3</span><br></pre></td></tr></table></figure><p>三个broker节点一共创建了8个文件夹,这个数字8实质上是分区数4与副本因子2的乘积.每个副本(或者更确切地说应该是日志,副本与日志一一对应)才真正对应 了一个命名形式.</p><a id="more"></a> <p><strong>主题</strong>,<strong>分区,副本和日志</strong>的关系如下图所示.<strong>主题</strong>和<strong>分区</strong>是提供给上层用户的抽象,而在副本层面(或者更确切的说是Log日志层面)才会实际物理存在.</p><p>同一个分区中的多个副本必须分布在不同broker中,并且一个分区副本同时存在多个broker中,这样才能提供有效的数据冗余.上面的示例中,每个副本都分布在至少2台不同的broker中.</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608631181435-40a87763-f28c-45c4-a25e-d85651c26d2e.png" alt="image.png"></p><h4 id="4-1-1-2-手动创建主题以及分区副本"><a href="#4-1-1-2-手动创建主题以及分区副本" class="headerlink" title="4.1.1.2 手动创建主题以及分区副本"></a>4.1.1.2 手动创建主题以及分区副本</h4><p>通过 <code>kafka-topics.sh</code> 脚本创建的主题会按照内部既定逻辑来分配分区和副本到Broker节点上.其实该脚本还提供一个 <code>replica-assignment</code> 参数来手动指定分区副本的分配方案.用法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">格式为: 分区1broker节点1:分区1broker节点2,分区2broker节点1:分区2broker节点2.副本集合用冒号隔开,分区之间用逗号隔开</span><br><span class="line">--replica-assignment broker_id_for_partition1_replica1:broker_id_for_partition1_replica2,broker_id_for_partition2_replica1:broker_id_for_partition2_replica2.......</span><br></pre></td></tr></table></figure><p>例如下面这个实例通过手动方式创建了一个和 <code>topic-test</code> 一样分区副本分配的 <code>topic-test-same</code> 主题.</p><p>下面是刚创建的自动分配的topic-test主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic &quot;topic-test&quot;</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br></pre></td></tr></table></figure><p>通过 <code>--replica-assignment</code> 手动指定分区副本分配情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-test-same --replica-assignment 153:152,154:153,152:154,153:154</span><br></pre></td></tr></table></figure><blockquote><p>–replica-assignment参数其实就是逗号隔开的所有分区的Replicas副本集合.副本集合内部用:冒号隔开</p></blockquote><p>查看 <code>topic-test-same</code> 分区信息.和 <code>topic-test</code> 主题分区副本分配一致</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic &quot;topic-test-same&quot;</span><br><span class="line">Topic:topic-test-same   PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test-same  Partition: 0    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">    Topic: topic-test-same  Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test-same  Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test-same  Partition: 3    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br></pre></td></tr></table></figure><p>手动分配分区副本需要遵循以下原则,否则会报错:</p><ul><li>同一个分区内的副本不能有重复.比如153:153</li><li>分区之间所指定的副本数量要相同.比如153:154,152,154:152</li><li>不能跳过一个分区.比如153:154,,154:152</li></ul><h4 id="4-1-1-3-自定义相关参数"><a href="#4-1-1-3-自定义相关参数" class="headerlink" title="4.1.1.3 自定义相关参数"></a>4.1.1.3 自定义相关参数</h4><p>在创建主题时,还可以通过 <code>config</code> 参数设置要创建主题的相关参数.可以覆盖原本的默认配置参数. <code>config</code> 可以指定多个参数.用法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--config 参数名=值 --config 参数名=值 ......</span><br></pre></td></tr></table></figure><p>下面示例使用 <code>config</code> 参数创建主题 <code>topic-config</code>.并且携带2个参数 :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-config --replication-factor 1 --partitions 1 --config cleanup.policy=compact --config max.message.bytes=10000</span><br><span class="line">Created topic &quot;topic-config&quot;.</span><br></pre></td></tr></table></figure><p>查看主题信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:1    ReplicationFactor:1 Configs:cleanup.policy=compact,max.message.bytes=10000</span><br><span class="line">    Topic: topic-config Partition: 0    Leader: 154 Replicas: 154   Isr: 154</span><br></pre></td></tr></table></figure><p>通过zk也能查看到config信息,config信息保存在 <code>/config/topics/TOPIC_NAME</code> 目录下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] get /config/topics/topic-config</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;config&quot;:&#123;&quot;max.message.bytes&quot;:&quot;10000&quot;,&quot;cleanup.policy&quot;:&quot;compact&quot;&#125;&#125;</span><br></pre></td></tr></table></figure><h4 id="4-1-1-4-总结"><a href="#4-1-1-4-总结" class="headerlink" title="4.1.1.4 总结"></a>4.1.1.4 总结</h4><p>创建主题时需要遵循几个原则</p><ul><li>主题名不能重复,否则会报错.(使用 <code>if-not-exists</code> 参数可以避免出现报错信息,但是不会成功创建一个同名主题)</li><li>主题名不推荐使用__双下划线开头的命名,双下划线开头主题一般看做Kafka的内部主题</li><li>主题名由大小写祖母,数字,点号,下划线,连接线等组成.不能只有特殊符号</li></ul><p><code>kafka-topics.sh</code> 创建主题信息支持以下参数:</p><ul><li><p><code>--create</code> 创建主题</p></li><li><ul><li><code>--replica-assignment</code> 手动创建主题的分区副本分配</li><li><code>--config</code> 手动指定参数</li></ul></li></ul><h3 id="4-1-2-查看主题的分区和副本信息"><a href="#4-1-2-查看主题的分区和副本信息" class="headerlink" title="4.1.2 查看主题的分区和副本信息"></a>4.1.2 查看主题的分区和副本信息</h3><h4 id="4-1-2-1-查看具体某个topic的信息"><a href="#4-1-2-1-查看具体某个topic的信息" class="headerlink" title="4.1.2.1.查看具体某个topic的信息"></a>4.1.2.1.查看具体某个topic的信息</h4><p><code>kafka-topics.sh</code> 脚本提供了 <code>--describe</code> 参数来查看一个topic的信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic &quot;topic-test&quot;</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br></pre></td></tr></table></figure><p>在上面的示例中,命令行提供了以下几个信息:</p><p>一共有3个broker节点:152,153,154</p><p><code>PartitionCount</code> 表示一共有3个分区</p><p><code>ReplicationFactor</code> 副本因子为2</p><p><code>Leader</code> 表示某个分区对应的leader副本在具体的Broker节点</p><p><code>Replicas</code> 表示分区内所有AR副本的集合</p><p><code>Isr</code> 表示ISR副本集合</p><h4 id="4-1-2-2-查看所有topic的信息"><a href="#4-1-2-2-查看所有topic的信息" class="headerlink" title="4.1.2.2 查看所有topic的信息"></a>4.1.2.2 查看所有topic的信息</h4><p>如果 <code>kafka-topics.sh</code> 脚本没有指定具体的 <code>--topic</code> 字段.则会展示所有的topic主题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe | head</span><br><span class="line">Topic:__consumer_offsets    PartitionCount:50   ReplicationFactor:1 Configs:segment.bytes=104857600,cleanup.policy=compact,compression.type=producer</span><br><span class="line">    Topic: __consumer_offsets   Partition: 0    Leader: 153 Replicas: 153   Isr: 153</span><br><span class="line">    Topic: __consumer_offsets   Partition: 1    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: __consumer_offsets   Partition: 2    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">    Topic: __consumer_offsets   Partition: 3    Leader: 153 Replicas: 153   Isr: 153</span><br><span class="line">    Topic: __consumer_offsets   Partition: 4    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: __consumer_offsets   Partition: 5    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">    Topic: __consumer_offsets   Partition: 6    Leader: 153 Replicas: 153   Isr: 153</span><br><span class="line">    Topic: __consumer_offsets   Partition: 7    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: __consumer_offsets   Partition: 8    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">  .....略.......</span><br></pre></td></tr></table></figure><h4 id="4-1-2-3-zookeeper查看topic信息"><a href="#4-1-2-3-zookeeper查看topic信息" class="headerlink" title="4.1.2.3 zookeeper查看topic信息"></a>4.1.2.3 zookeeper查看topic信息</h4><p>zookeeper提供了 <code>zkCli.sh</code> 客户端.使用客户端连接zookeeper:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/zookeeper-3.4.10/bin/zkCli.sh  -server localhost:2181</span><br><span class="line">[zk: localhost:2181(CONNECTED) 0]</span><br></pre></td></tr></table></figure><p>zookeeer的 <code>/brokers/topics</code> 目录下保存了主题的分区副本分片方案.通过查看这个目录即可查看主题的分区和副本信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] get /brokers/topics/topic-test</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:&#123;&quot;2&quot;:[152,154],&quot;1&quot;:[154,153],&quot;3&quot;:[153,154],&quot;0&quot;:[153,152]&#125;&#125;</span><br></pre></td></tr></table></figure><p>如上示例所示, <code>&quot;2&quot;:[152,154]</code> 表示分区2分配了2个副本,分别在152和153这2个broker节点上.</p><h4 id="4-1-2-4-查看kafka集群当前所有主题"><a href="#4-1-2-4-查看kafka集群当前所有主题" class="headerlink" title="4.1.2.4 查看kafka集群当前所有主题"></a>4.1.2.4 查看kafka集群当前所有主题</h4><p><code>--list</code> 参数可以列出当前的所有topic</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --list</span><br><span class="line">__consumer_offsets</span><br><span class="line">delivery_message</span><br><span class="line">example</span><br><span class="line">example1</span><br><span class="line">goods_center</span><br><span class="line">hsq-aliapp</span><br><span class="line">hsq-wxapp</span><br><span class="line">hsq_online_test</span><br><span class="line">monitor_report_app_log</span><br><span class="line">sample_consumer_dlq</span><br><span class="line">tidb_test</span><br><span class="line">topic-config</span><br><span class="line">topic-demo</span><br><span class="line">topic-demo1</span><br><span class="line">topic-test</span><br><span class="line">topic-test-same</span><br></pre></td></tr></table></figure><h4 id="4-1-2-5-查看主题其他详细信息"><a href="#4-1-2-5-查看主题其他详细信息" class="headerlink" title="4.1.2.5 查看主题其他详细信息"></a>4.1.2.5 查看主题其他详细信息</h4><p><code>kafka-topics.sh</code> 脚本的 <code>describe</code> 参数还支持很多额外的指令,用于查看更详细的信息.</p><p>1.<strong><code>--topics-with-overrides</code></strong> 参数表示查看覆盖配置的主题,列出包含了与集群不一样配置的主题.下面列出了 <code>topic-config</code> 这个主题,这个主题使用了 <code>--config</code> 参数创建</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topics-with-overrides</span><br><span class="line"></span><br><span class="line">Topic:topic-config  PartitionCount:1    ReplicationFactor:1 Configs:cleanup.policy=compact,max.message.bytes=10000</span><br></pre></td></tr></table></figure><p>2.<strong><code>--under-replicated-paritions</code></strong> 参数列出包含失效副本的分区.失效副本的分区可能正在进行同步操作,也有可能同步发生异常.此时分区的ISR集合小于AR集合.失效副本的分区是重点监控对象,因为这可能意味着集群中的某个broker已经失效或者同步效率降低等.</p><p>正常情况下此命令不会出现任何信息.例如查看主题 <code>topic-demo</code> 的失效副本信息,但是没有任何输出信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo --under-replicated-partitions</span><br></pre></td></tr></table></figure><p>此时将153这个节点下线.再次查看:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo --under-replicated-partitions</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 152,154</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 154 Replicas: 153,154,152   Isr: 154,152</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 154,152</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 152,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>可以看到Leader和ISR集合中都没有了153这个节点.将153节点上线.此时再次查询,恢复正常.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo --under-replicated-partitions</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>\3. <strong><code>unavailable-partitions</code></strong> 参数可以查看主题中没有leader副本的分区.这些分区已经处于离线状态,对于生产者或者消费者来说不可用.</p><p>同样正常情况下,该命令没有展示任何信息.</p><p>例如,下面的 <code>topic-test</code> 主题有4个分区,每个分区有2个副本.其中分区1和分区3的副本ISR是153和154这2个节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 153 Replicas: 153,152   Isr: 152,153</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: 153 Replicas: 153,154   Isr: 154,153</span><br></pre></td></tr></table></figure><p>现在停掉153和154这2个节点的kafka进程.使用 <code>unavailable-partitions</code> 参数查看分区信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test --unavailable-partitions</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: -1  Replicas: 154,153   Isr: 154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: -1  Replicas: 153,154   Isr: 154</span><br><span class="line">  </span><br><span class="line">  [hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 152 Replicas: 153,152   Isr: 152</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: -1  Replicas: 154,153   Isr: 154</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: -1  Replicas: 153,154   Isr: 154</span><br></pre></td></tr></table></figure><p>leader显示为-1,表示没有可用leader</p><p>节点恢复后,再次执行该命令,没有任何显示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test --unavailable-partitions</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><h4 id="4-1-2-6-总结"><a href="#4-1-2-6-总结" class="headerlink" title="4.1.2.6 总结"></a>4.1.2.6 总结</h4><p><code>kafka-topics.sh</code> 查看主题信息支持以下参数:</p><ul><li><p><code>--describe</code> </p></li><li><ul><li>默认展示所有topic的分区副本信息</li><li><code>--topic TOPIC_NAME</code> 展示具体某个topic主题的分区副本信息</li><li><code>--topics-with-overrides</code> 列出覆盖配置参数的主题</li><li><code>--under-replicated-partitions</code> 列出失效副本的主题分区信息</li><li><code>--unavailable-partitions</code> 列出没有副本的主题分区</li></ul></li><li><p><code>--list</code> 列出kafka集群下的所有topic主题名称</p></li></ul><h3 id="4-1-3-修改主题"><a href="#4-1-3-修改主题" class="headerlink" title="4.1.3 修改主题"></a>4.1.3 修改主题</h3><h4 id="4-1-3-1-修改主题分区数量"><a href="#4-1-3-1-修改主题分区数量" class="headerlink" title="4.1.3.1 修改主题分区数量"></a>4.1.3.1 修改主题分区数量</h4><p>当一个主题被修改后,依然允许我们对其做一定的修改,比如修改分区个数,修改配置等.这个功能就是 <code>kafka-topic.sh</code> 脚本中的 <code>alter</code> 指令提供的.</p><p>以 <code>topic-config</code> 主题为例,该主题下只有一个分区.将分区修改为3:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --partitions 3</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Adding partitions succeeded!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:3    ReplicationFactor:1 Configs:cleanup.policy=compact,max.message.bytes=10000</span><br><span class="line">    Topic: topic-config Partition: 0    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: topic-config Partition: 1    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">    Topic: topic-config Partition: 2    Leader: 153 Replicas: 153   Isr: 153</span><br></pre></td></tr></table></figure><p><code>--partition</code> 参数表示扩展后的分区个数.</p><blockquote><p>注意告警信息.如果主题中的消息包含key(key不为Null)时,根据key计算分区的行为就会受到影响.当分区数为1时,所以key的消息都会发送到这个分区.当分区扩展到3,会根据消息的key来计算区号.原本发往分区0的消息可能会发送到分区1或者2.此外,还会影响既定消息的顺序.</p></blockquote><p>对于基于key计算的主题,不建议修改分区数量.在一开始就设置好分区数量.另外需要注意的是,Kafka不支持减少分区.只能增加不能减少.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --partitions 1</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line"></span><br><span class="line">Error while executing topic command : The number of partitions for a topic can only be increased</span><br></pre></td></tr></table></figure><blockquote><p>不支持减少分区主要是考虑到保障kafka的消息可靠性和顺序性,事务性问题.</p></blockquote><p>如果修改一个不存在的主题分区,则会报错.添加 <code>--if-exists</code> 参数会忽略一些异常</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-none-exist  --partitions 3</span><br><span class="line">Error while executing topic command : Topic topic-none-exist does not exist on ZK path localhost:2181</span><br><span class="line"></span><br><span class="line">#使用--if-exists参数,没有报错,但是不会产生任何效果</span><br><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-none-exist  --if-exists --partitions 3</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><h4 id=""><a href="#" class="headerlink" title=" "></a> </h4><h4 id="4-1-3-2-修改主题配置"><a href="#4-1-3-2-修改主题配置" class="headerlink" title="4.1.3.2 修改主题配置"></a>4.1.3.2 修改主题配置</h4><p>还可以使用 <code>kafka-topics.sh</code> 脚本的 <code>alter</code> 指令修改主题的配置.在创建主题的时候通过 <code>config</code> 参数来设置要创建的主题相关参数.在创建完主题之后,还可以通过 <code>alter</code> 和 <code>config</code> 配合增加或者修改一些配置文件覆盖原有的值</p><p>下面例子演示修改主题 <code>topic-config</code> 的 <code>max.message.bytes</code> 配置.从10000修改到20000</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --config max.message.bytes=20000</span><br><span class="line">WARNING: Altering topic configuration from this script has been deprecated and may be removed in future releases.</span><br><span class="line">         Going forward, please use kafka-configs.sh for this functionality</span><br><span class="line">Updated config for topic &quot;topic-config&quot;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:3    ReplicationFactor:1 Configs:max.message.bytes=20000,cleanup.policy=compact</span><br></pre></td></tr></table></figure><p>通过 <code>alter</code> 也可以删除创建主题时候的自定义配置.使用 <code>--delete-config</code> 参数.下面这个例子中删除了 <code>max.message.bytes</code> 配置.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --delete-config max.message.bytes</span><br><span class="line">WARNING: Altering topic configuration from this script has been deprecated and may be removed in future releases.</span><br><span class="line">         Going forward, please use kafka-configs.sh for this functionality</span><br><span class="line">Updated config for topic &quot;topic-config&quot;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:3    ReplicationFactor:1 Configs:cleanup.policy=compact</span><br></pre></td></tr></table></figure><blockquote><p>注意.在对config配置进行增删改查时候,都会提示建议使用kafka-configs.sh这个脚本来实现.该脚本的使用方式下面马上讲到</p></blockquote><h3 id="4-1-4-配置管理"><a href="#4-1-4-配置管理" class="headerlink" title="4.1.4 配置管理"></a>4.1.4 配置管理</h3><p><code>kafka-configs.sh</code> 脚本专门用来对配置进行操作.可以在运行状态下动态更改配置.也可以查询主题的相关配置.而且该脚本不仅可以支持主题相关配置修改,还可以修改broker,用户和客户端这3个类型的配置</p><p><code>kafka-configs.sh</code> 脚本使用 <code>entity-type</code> 参数指定操作配置的类型, <code>entity-name</code> 参数指定操作配置的名称.</p><h4 id="4-1-4-1-查询配置"><a href="#4-1-4-1-查询配置" class="headerlink" title="4.1.4.1 查询配置"></a>4.1.4.1 查询配置</h4><p>下面这个例子查看主题 <code>topic-config</code> 的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics --entity-name topic-config</span><br><span class="line">Configs for topic &apos;topic-config&apos; are cleanup.policy=compact</span><br></pre></td></tr></table></figure><p><code>--entity-type</code> 指定查看的实体类型.支持以下几种类型:</p><ul><li>topics</li><li>clients</li><li>users</li><li>brokers</li></ul><p><code>--entity-name</code> 配置的实体名称:</p><ul><li>topic name (主题名称)</li><li>client id (客户端ID)</li><li>user principal name (用户名)</li><li>broker id (kafka节点ID)</li></ul><p>如果不指定 <code>--entity-name</code> 参数则会查询所有的 <code>entity-type</code> 对应的所有配置信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics</span><br><span class="line">Configs for topic &apos;topic-config&apos; are</span><br><span class="line">Configs for topic &apos;__consumer_offsets&apos; are segment.bytes=104857600,cleanup.policy=compact,compression.type=producer</span><br><span class="line">......</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>通过zookeeper也可以查询主题的配置信息.路径为 <code>/config/topics/TOPIC_NAME</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] get /config/topics/topic-config</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;config&quot;:&#123;&quot;cleanup.policy&quot;:&quot;compact&quot;,&quot;max.message.bytes&quot;:&quot;20000&quot;&#125;&#125;</span><br></pre></td></tr></table></figure><h4 id="4-1-4-2-修改配置"><a href="#4-1-4-2-修改配置" class="headerlink" title="4.1.4.2 修改配置"></a>4.1.4.2 修改配置</h4><p>使用 <code>alter</code> 对配置进行变更.需要配合 <code>add-config</code> 或者 <code>delete-config</code> 这2个参数一起使用.</p><p><code>add-config</code> 参数实现配置的增,改</p><p>下面的例子中,为主题 <code>topic-config</code> 添加 <code>max.message.bytes</code> 参数配置和 <code>cleanup.policy</code> 参数配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name topic-config --add-config cleanup.policy=compact,max.message.bytes=20000</span><br><span class="line">Completed Updating config for entity: topic &apos;topic-config&apos;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics --entity-name topic-config</span><br><span class="line">Configs for topic &apos;topic-config&apos; are cleanup.policy=compact,max.message.bytes=20000</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p><code>delete-config</code> 参数可以实现配置删除.</p><p>下面的例子中,删除上面的2个配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name topic-config --delete-config cleanup.policy,max.message.bytes</span><br><span class="line">Completed Updating config for entity: topic &apos;topic-config&apos;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics --entity-name topic-config</span><br><span class="line">Configs for topic &apos;topic-config&apos; are</span><br></pre></td></tr></table></figure><h3 id="4-1-5-删除主题"><a href="#4-1-5-删除主题" class="headerlink" title="4.1.5 删除主题"></a>4.1.5 删除主题</h3><p>如果确定不再使用一个主题,那么最好的方式是将其删除.这样可以释放一些资源,比如磁盘,文件句柄等. <code>kafka-topics.sh</code> 脚本中的 <code>delete</code> 命令可以用来删除主题.比如下面删除主题 <code>topic-demo1</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --delete --topic topic-demo1</span><br><span class="line">Topic topic-demo1 is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br></pre></td></tr></table></figure><blockquote><p>注意.必须将kafka服务器配置文件的delete.topic.enable选项设置为true才能删除.这个参数的默认值是false.删除主题的操作会被忽略.主题并没有被删除</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --list | grep topic-demo1</span><br><span class="line">topic-demo1</span><br></pre></td></tr></table></figure><p>编辑配置文件 <code>/opt/kafka/config/server.properties</code> 修改下面的参数为true</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Switch to enable topic deletion or not, default value is false</span><br><span class="line">delete.topic.enable=true</span><br></pre></td></tr></table></figure><p>如果删除一个kafka的内部主题,那么会报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --delete --topic __consumer_offsets</span><br><span class="line">Error while executing topic command : Topic __consumer_offsets is a kafka internal topic and is not allowed to be marked for deletion.</span><br></pre></td></tr></table></figure><p>删除一个不存在的主题也会报错,此时可以通过 <code>if-exists</code> 参数来忽略异常.</p><h3 id="4-1-5-总结"><a href="#4-1-5-总结" class="headerlink" title="4.1.5 总结"></a>4.1.5 总结</h3><p>下面这张图是 <code>kafka-topics.sh</code> 脚本的常用参数</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608805092662-042718a3-9a6a-489e-a987-db4e38217171.png" alt="image.png"></p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608805117734-2bd94456-e966-41b1-9465-dde20a8a9129.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;4-1-1-创建主题&quot;&gt;&lt;a href=&quot;#4-1-1-创建主题&quot; class=&quot;headerlink&quot; title=&quot;4.1.1 创建主题&quot;&gt;&lt;/a&gt;4.1.1 创建主题&lt;/h3&gt;&lt;h4 id=&quot;4-1-1-1-自动创建主题以及分区副本&quot;&gt;&lt;a href=&quot;#4-1-1-1-自动创建主题以及分区副本&quot; class=&quot;headerlink&quot; title=&quot;4.1.1.1 自动创建主题以及分区副本&quot;&gt;&lt;/a&gt;4.1.1.1 自动创建主题以及分区副本&lt;/h4&gt;&lt;p&gt;在之前的笔记中提到了创建主题的一个简单示例.kafka提供 &lt;code&gt;kafka-topics.sh&lt;/code&gt; 脚本来创建主题.下面这个示例创建了一个 &lt;code&gt;topic-test&lt;/code&gt; 的主题,包含4个分区和2个副本.&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;/opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-test --replication-factor 2 --partitions 4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Created topic &amp;quot;topic-test&amp;quot;.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;分区创建完成后,会在kafka的 &lt;code&gt;log.dirs&lt;/code&gt; 或者 &lt;code&gt;log.dir&lt;/code&gt; 的目录下创建相应的主题分区.下面是在其中一台Broker节点的信息展示:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev152 ~]$ ls /opt/logs/kafka/ | grep &amp;quot;topic-test&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;可以看到152节点中创建了2个文件夹 topic-test-0 和 topic-test-2,对应主题 topic-test的2个分区编号为0和2的分区，命名方式可以概括为 &lt;code&gt;&amp;lt;topic&amp;gt;-&amp;lt;partition&amp;gt;&lt;/code&gt; .严谨地说,其实这类文件夹对应的不是分区,分区同主题一样是一个逻辑的概念而没有物理上的存在.并且这里我们也只是看到了2个分区,而我们创建的是4个分区,其余2个分区被分配到了153和154节点中，参考如下:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;#153节点&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev153 ~]$ ls /opt/logs/kafka/ | grep &amp;quot;topic-test&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#154节点&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev154 ~]$ ls /opt/logs/kafka/ | grep &amp;quot;topic-test&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;三个broker节点一共创建了8个文件夹,这个数字8实质上是分区数4与副本因子2的乘积.每个副本(或者更确切地说应该是日志,副本与日志一一对应)才真正对应 了一个命名形式.&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="4-主题和分区" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch索引迁移</title>
    <link href="https://jesse.top/2020/09/30/elk/Elasticsearch%E7%B4%A2%E5%BC%95%E8%BF%81%E7%A7%BB/"/>
    <id>https://jesse.top/2020/09/30/elk/Elasticsearch索引迁移/</id>
    <published>2020-09-30T14:59:58.000Z</published>
    <updated>2021-01-19T14:39:47.543Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Elasticsearch索引迁移"><a href="#Elasticsearch索引迁移" class="headerlink" title="Elasticsearch索引迁移"></a>Elasticsearch索引迁移</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>旧Elasticsearch版本:2.4.4</p><p>新Elasticsearch版本:2.4.4</p><p>近期dev环境服务器迁移到一台新的物理机,所以需要迁移部分Elasticsearch索引数据.</p><p>Elasticsearch索引迁移有许多方法.测试过elasticsearch-exporter.但是没有成功.报错如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[work@docker elasticsearch-exporter]$ node exporter.js -a 10.0.0.250 -b 10.0.0.101 -p 9200 -q 9200 -i mid_mg_gc_datasource_items -j mid_mg_gc_datasource_items</span><br><span class="line">Elasticsearch Exporter - Version 1.4.0</span><br><span class="line">Reading source statistics from ElasticSearch</span><br><span class="line">The source driver has not reported any documents that can be exported. Not exporting.</span><br><span class="line">Number of calls:0</span><br><span class="line">Fetched Entries:0 documents</span><br><span class="line">Processed Entries:0 documents</span><br><span class="line">Source DB Size:0 documents</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="Elasticsearch-dump"><a href="#Elasticsearch-dump" class="headerlink" title="Elasticsearch-dump"></a>Elasticsearch-dump</h3><p>本次使用elasticsearch-dump进行索引迁移.在github上可以找到具体使用方法:<a href="https://github.com/elasticsearch-dump/elasticsearch-dump" target="_blank" rel="noopener">elasticsearch-dump</a></p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install elasticdump</span><br></pre></td></tr></table></figure><p>这里稍微踩了个坑,如果报错:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pm WARN deprecated nomnom@1.8.1: Package no longer supported. Contact support@npmjs.com for more info.</span><br><span class="line">npm WARN saveError ENOENT: no such file or directory, open &apos;/home/work/package.json&apos;</span><br><span class="line">npm WARN enoent ENOENT: no such file or directory, open &apos;/home/work/package.json&apos;</span><br><span class="line">npm WARN work No description</span><br><span class="line">npm WARN work No repository field.</span><br><span class="line">npm WARN work No README data</span><br><span class="line">npm WARN work No license field.</span><br></pre></td></tr></table></figure><p>则需要初始化一下npm</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">work@docker ~]$ npm init</span><br></pre></td></tr></table></figure><p>安装完成后,进入到<code>elasticsearch dump</code>的<code>bin</code>目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[work@docker ~]$ cd node_modules/elasticdump/bin/</span><br></pre></td></tr></table></figure><h3 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h3><p>查看elasticsearchdump的具体用法</p><p>[work@docker bin]$ ./elasticdump –help</p><p><code>elaticsearchdump</code>支持两个ES跨版本迁移索引,还支持索引备份到文件,以及从文件恢复到Elasticsearch</p><h5 id="迁移mid-gm-gc-brand这个索引数据"><a href="#迁移mid-gm-gc-brand这个索引数据" class="headerlink" title="迁移mid_gm_gc_brand这个索引数据"></a>迁移mid_gm_gc_brand这个索引数据</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[work@docker bin]$ ./elasticdump --input=http://10.0.0.250:9200/mid_mg_gc_brand --output=http://10.0.0.101:9200/mid_mg_gc_brand --type=analyzer</span><br><span class="line"></span><br><span class="line">[work@docker bin]$ ./elasticdump --input=http://10.0.0.250:9200/mid_mg_gc_brand --</span><br><span class="line"></span><br><span class="line">[work@docker bin]$ ./elasticdump --input=http://10.0.0.250:9200/mid_mg_gc_brand --output=http://10.0.0.101:9200/mid_mg_gc_brand --type=data</span><br></pre></td></tr></table></figure><blockquote><p>文档中的type类型有settings, analyzer, data, mapping, alias, template</p></blockquote><h5 id="查看新服务器上的索引-迁移成功"><a href="#查看新服务器上的索引-迁移成功" class="headerlink" title="查看新服务器上的索引.迁移成功"></a>查看新服务器上的索引.迁移成功</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl -Ssl &apos;http://10.0.0.101:9200/_cat/indices?v&apos; | grep &apos;mid_mg*&apos;</span><br><span class="line">yellow open   mid_mg_gc_datasource_items             5   1         96            1     87.3kb         87.3kb</span><br><span class="line">yellow open   mid_mg_gc_brand                        5   1        966            0    312.1kb        312.1kb</span><br><span class="line">yellow open   mid_mg_gc_synonyms                     5   1        116            0     82.1kb         82.1kb</span><br><span class="line"></span><br><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl -Ssl &apos;http://10.0.0.250:9200/_cat/indices?v&apos; | grep &apos;mid_mg*&apos;</span><br><span class="line">yellow open   mid_mg_gc_datasource_items             3   1         92            9     99.1kb         99.1kb</span><br><span class="line">yellow open   mid_mg_gc_brand                        3   1        966            0    345.4kb        345.4kb</span><br><span class="line">yellow open   mid_mg_gc_synonyms                     3   1        116           16    106.1kb        106.1kb</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Elasticsearch索引迁移&quot;&gt;&lt;a href=&quot;#Elasticsearch索引迁移&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearch索引迁移&quot;&gt;&lt;/a&gt;Elasticsearch索引迁移&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;旧Elasticsearch版本:2.4.4&lt;/p&gt;
&lt;p&gt;新Elasticsearch版本:2.4.4&lt;/p&gt;
&lt;p&gt;近期dev环境服务器迁移到一台新的物理机,所以需要迁移部分Elasticsearch索引数据.&lt;/p&gt;
&lt;p&gt;Elasticsearch索引迁移有许多方法.测试过elasticsearch-exporter.但是没有成功.报错如下:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[work@docker elasticsearch-exporter]$ node exporter.js -a 10.0.0.250 -b 10.0.0.101 -p 9200 -q 9200 -i mid_mg_gc_datasource_items -j mid_mg_gc_datasource_items&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Elasticsearch Exporter - Version 1.4.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Reading source statistics from ElasticSearch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;The source driver has not reported any documents that can be exported. Not exporting.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Number of calls:	0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Fetched Entries:	0 documents&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Processed Entries:	0 documents&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Source DB Size:		0 documents&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>Openvpn客户端无法连接OpenSSL</title>
    <link href="https://jesse.top/2020/09/22/Linux-Service/Openvpn%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5%20OpenSSL-%20error/"/>
    <id>https://jesse.top/2020/09/22/Linux-Service/Openvpn客户端无法连接 OpenSSL- error/</id>
    <published>2020-09-22T14:59:58.000Z</published>
    <updated>2021-01-19T14:42:39.686Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Openvpn客户端无法连接OpenSSL-error"><a href="#Openvpn客户端无法连接OpenSSL-error" class="headerlink" title="Openvpn客户端无法连接OpenSSL: error"></a>Openvpn客户端无法连接OpenSSL: error</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>今天阿里云的Openvpn服务器部署了docker服务后,需要升级内存配置.服务器升级重启后,发现客户端无法连接Openvpn.</p><hr><h3 id="故障表现"><a href="#故障表现" class="headerlink" title="故障表现"></a>故障表现</h3><p>在openvpn客户端日志中发现下面报错信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2020-09-22 16:00:49 WARNING: No server certificate verification method has been enabled.  See http://openvpn.net/howto.html#mitm for more info.</span><br></pre></td></tr></table></figure><p>openvpn服务端日志报错信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS: Initial packet from [AF_INET]27.115.51.166:26184, sid=c0cb2b12 4b3187b2</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 OpenSSL: error:14089086:SSL routines:ssl3_get_client_certificate:certificate verify failed</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS_ERROR: BIO read tls_read_plaintext error</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS object -&gt; incoming plaintext read error</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS handshake failed</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 SIGUSR1[soft,tls-error] received, client-instance restarting</span><br><span class="line">Tue Sep 22 15:48:06 2020 27.115.51.166:33480 TLS: Initial packet from [AF_INET]27.115.51.166:33480, sid=11b9760e 97f6d068</span><br><span class="line">Tue Sep 22 15:48:07 2020 27.115.51.166:33480 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx</span><br></pre></td></tr></table></figure><p><strong>日志关键字</strong></p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OpenSSL: error:14089086:SSL routines:ssl3_get_client_certificate:certificate verify failed</span><br></pre></td></tr></table></figure><hr><h3 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h3><p>服务器重启后需要注意的几个问题:</p><p>1.检查以下几个服务是否启动:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl status openvpn@server</span><br><span class="line">systemctl status iptables</span><br></pre></td></tr></table></figure><p>2.由于docker服务会初始化iptables,所以docker启动后需要手动添加一条iptables规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A POSTROUTING -s 10.111.255.0/24 -o eth0 -j MASQUERADE</span><br></pre></td></tr></table></figure><p>3.检查ip转发功能</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysctl.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br></pre></td></tr></table></figure><p>4.检查阿里云的安全组规则,是否开通了1094的udp协议</p><hr><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>最终解决方案还是要靠google解决,在openvpn的wiki上找到了解决方案,</p><p>具体网站链接:<a href="https://community.openvpn.net/openvpn/wiki/CertificateRevocationListExpired?__cf_chl_jschl_tk__=40e85ba16653b7db84d828db819eafbc2b5a9faf-1600761449-0-AUAdZiIXwLUqBJDJAcDe9htVtUTZlGJm8m_RYLUsxLu2he8Myk5WXwzQn-CZdZyBDJRHn9clM-6y0ITsnKk0Pru3AwB7EOc0LhjyrV9unNnBv0V9_skxNC2n3per9e1TQJjcmmtwnaNl23Sp5D8p9FZYyX5PO-vtkdp1i7dyh_x1KSFwZqibI8Zt4saVoABGkfMJ3nKeUJpIIlnEhRGhfJrQwQZqvvG6EAS1CJUHRR8uqKNyqmEz90RmqcLGc8ytoOTIBYhJMs8OsPk_dA2nKA47QObzI5-4SEUZkC5ZaxhNCfkRBGx9kwimWfBJFtxJkPzI5_vkXONWXhoB0wi5cfE" target="_blank" rel="noopener">openvpn wiki</a></p><p>解决问题很简单:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#需要进入到下面这个目录下</span><br><span class="line">cd /etc/openvpn/easy-rsa/3</span><br><span class="line"></span><br><span class="line">#更新一下crl文件.</span><br><span class="line">[root@dwd-Dnsmasq 3]# ./easyrsa gen-crl</span><br><span class="line"></span><br><span class="line">Using configuration from ./openssl-1.0.cnf</span><br><span class="line"></span><br><span class="line">An updated CRL has been created.</span><br><span class="line">CRL file: /etc/openvpn/easy-rsa/3/pki/crl.pem</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Openvpn客户端无法连接OpenSSL-error&quot;&gt;&lt;a href=&quot;#Openvpn客户端无法连接OpenSSL-error&quot; class=&quot;headerlink&quot; title=&quot;Openvpn客户端无法连接OpenSSL: error&quot;&gt;&lt;/a&gt;Openvpn客户端无法连接OpenSSL: error&lt;/h2&gt;&lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;今天阿里云的Openvpn服务器部署了docker服务后,需要升级内存配置.服务器升级重启后,发现客户端无法连接Openvpn.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;故障表现&quot;&gt;&lt;a href=&quot;#故障表现&quot; class=&quot;headerlink&quot; title=&quot;故障表现&quot;&gt;&lt;/a&gt;故障表现&lt;/h3&gt;&lt;p&gt;在openvpn客户端日志中发现下面报错信息:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;2020-09-22 16:00:49 WARNING: No server certificate verification method has been enabled.  See http://openvpn.net/howto.html#mitm for more info.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;openvpn服务端日志报错信息:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;ue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS: Initial packet from [AF_INET]27.115.51.166:26184, sid=c0cb2b12 4b3187b2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 OpenSSL: error:14089086:SSL routines:ssl3_get_client_certificate:certificate verify failed&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS_ERROR: BIO read tls_read_plaintext error&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS object -&amp;gt; incoming plaintext read error&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS handshake failed&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 SIGUSR1[soft,tls-error] received, client-instance restarting&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:48:06 2020 27.115.51.166:33480 TLS: Initial packet from [AF_INET]27.115.51.166:33480, sid=11b9760e 97f6d068&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:48:07 2020 27.115.51.166:33480 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;日志关键字&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-Service" scheme="https://jesse.top/categories/Linux-Service/"/>
    
    
      <category term="Linux" scheme="https://jesse.top/tags/Linux/"/>
    
      <category term="openvpn" scheme="https://jesse.top/tags/openvpn/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://jesse.top/2020/09/16/SRE%E8%BF%90%E7%BB%B4/SRE%E8%BF%90%E7%BB%B4%E7%AC%94%E8%AE%B0/"/>
    <id>https://jesse.top/2020/09/16/SRE运维/SRE运维笔记/</id>
    <published>2020-09-16T14:30:54.835Z</published>
    <updated>2020-09-16T14:30:54.836Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SRE运维笔记-chapter-One-概念介绍"><a href="#SRE运维笔记-chapter-One-概念介绍" class="headerlink" title="SRE运维笔记-chapter One-概念介绍"></a>SRE运维笔记-chapter One-概念介绍</h2><h3 id="SRE概念"><a href="#SRE概念" class="headerlink" title="SRE概念"></a>SRE概念</h3><p>SRE(sitereliabilityengineering).中文翻译为站点可靠性工程师.SRE概念中比较重要的特性在于:</p><p>1.engineer表示SRE是工程师,使用软件工程手段设计,研发和维护业务软件系统.</p><p>2.SRE的关注焦点在于<strong>可靠性</strong>,专注于软件系统架构设计,运维流程优化,让业务软件系统运行更可靠.</p><p>3.SRE主要工作是运维业务服务,</p><h4 id="SRE团队职责"><a href="#SRE团队职责" class="headerlink" title="SRE团队职责:"></a>SRE团队职责:</h4><ul><li>可用性改进</li><li>延迟优化</li><li>性能优化</li><li>效率优化</li><li>变更管理</li><li>监控</li><li>紧急事务处理</li><li>容量规划与管理</li></ul><h3 id="SRE方法论"><a href="#SRE方法论" class="headerlink" title="SRE方法论"></a>SRE方法论</h3><ul><li><p><strong>确保长期关注研发工作</strong></p><p>运维工作限制在50%以内,剩余的时间花在研发项目上.</p></li><li><p><strong>在保障服务SLO的前提下最大化迭代速度</strong></p><ul><li>错误预算: 1-可靠性目标.<ul><li>如果一个服务的可靠性目标是99.99%,那么错误预算就是0.01%.</li></ul></li></ul></li><li><p><strong>监控系统</strong></p><p>监控系统不应该依赖人来分析警报信息.而是应该由系统自动分析.仅当需要用户执行某种操作时,才需要通知用户</p><p>监控系统需要具备三类输出:</p><ul><li>紧急警报(alert)</li><li>工单:接受工单的用户应该执行某种操作,但是并非立即执行</li><li>日志</li></ul></li><li><p><strong>应急事件处理</strong></p><ul><li>可靠性:MTTF(平均失败时间),MTTR(平均恢复时间),MTBF(平均故障间隔时间).</li><li>任何需要人工操作的事情都只会延长恢复时间,一个可以自动恢复的系统即使有更多故障发生,也比事事都要人工干预的系统可用性更高.当不可避免需要人工介入时,最佳方法是事先预案,并且记录在<strong>运维手册(playbook)</strong>中,这能降低<strong>MTTR(平均恢复)</strong>时间.</li></ul></li><li><p><strong>变更管理</strong></p><p> 大概70%的生产事故是由某种变更触发,变更管理的最佳实践是使用自动化完成以下几个项目:</p><ul><li>采用渐进式发布机制</li><li>迅速而准确的检测到问题的发生</li><li>安全迅速的回滚</li></ul></li><li><p><strong>需求预测和容量规划</strong></p><ul><li>必须有一个准确的自然增城需求预测模型,需求预测的时间应该超过资源获取时间</li><li>规划中必须有准确的非自然增长的需求来源统计</li><li>必须有周期性压力测试,以便准确的将系统原始资源与业务容量对应起来</li></ul></li><li><p><strong>资源部署</strong></p></li><li><p><strong>效率与性能</strong></p><p>一个业务总体资源使用情况是由以下几个因素驱动的:</p><ul><li>用户需求(流量)</li><li>可用容量</li><li>软件资源使用效率</li></ul><p>SRE需要通过模型预测用户需求,合理部署和配置可用容量,改进软件以提高资源使用效率,这3个因素能够推动一个服务器的效率提升.</p><blockquote><p>软件系统在负载上升的时候,会导致延迟升高.SRE的目标是根据一个预设的延迟目标部署和维护足够的容量,SRE和研发团队应该共同监控和优化整个系统的性能.</p></blockquote></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;SRE运维笔记-chapter-One-概念介绍&quot;&gt;&lt;a href=&quot;#SRE运维笔记-chapter-One-概念介绍&quot; class=&quot;headerlink&quot; title=&quot;SRE运维笔记-chapter One-概念介绍&quot;&gt;&lt;/a&gt;SRE运维笔记-chapte
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>kong利用request-transformer插件重写URL</title>
    <link href="https://jesse.top/2020/09/10/Linux-Web/kong%E5%88%A9%E7%94%A8request-transformer%E6%8F%92%E4%BB%B6%E9%87%8D%E5%86%99URL/"/>
    <id>https://jesse.top/2020/09/10/Linux-Web/kong利用request-transformer插件重写URL/</id>
    <published>2020-09-10T04:59:58.000Z</published>
    <updated>2020-09-10T14:58:43.601Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kong利用request-transformer插件重写URL"><a href="#kong利用request-transformer插件重写URL" class="headerlink" title="kong利用request-transformer插件重写URL"></a>kong利用request-transformer插件重写URL</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>最近业务整合后有一个需求,将URL:<a href="http://www.abc.com/api/item/111" target="_blank" rel="noopener">www.abc.com/api/item/111</a> 想重写成<a href="http://www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转" target="_blank" rel="noopener">www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转</a>.</p><p>使用Nginx的rewrite redirect指令可以实现URL重写需求,但是redirect会跳转到新域名,不符合需求.</p><p>刚好该业务的的前端是用Kong网关处理,所以研究kong的插件实现这个需求</p><hr><h3 id="request-transformer介绍"><a href="#request-transformer介绍" class="headerlink" title="request-transformer介绍"></a>request-transformer介绍</h3><p><strong>request-transformer</strong>是Kong官方的插件,允许修改重写用户的请求,还可以使用正则表达式匹配URL,并将匹配到的字符串保存在变量中,然后使用模板将变量转换成用户的请求</p><p>简而言之:<strong>就是重写用户的请求</strong>,包括URL,args,headers,methods等等</p><p>官方地址: <a href="https://docs.konghq.com/hub/kong-inc/request-transformer/" target="_blank" rel="noopener">reuqest-transformer官方地址</a></p><p>github项目地址: <a href="https://github.com/Kong/kong-plugin-request-transformer" target="_blank" rel="noopener">request-transformer github</a></p><a id="more"></a><hr><h3 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h3><blockquote><p>kong使用的是2.1.3最新版本,试过使用Kong1.0版本插件无法生效</p></blockquote><p>这里举2个例子说明</p><ul><li><p>将URL:/v4/jkf/branch/qrcode&amp;code=100006 重写为 /v4/jkf/branch/qrcode?code=100006.也就是将<code>&amp;</code>转换为<code>?</code></p><ul><li>首先配置Service和Route.具体配置方法略过,这里主要关心一下Route中的Path设置:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/v4/jkf/branch/qrcode\&amp;code=(?&lt;code&gt;\d+)</span><br></pre></td></tr></table></figure><p>该PATH表示:</p><p>1.匹配/v4/jkf/branch/qrcode&amp;code=任意长度数字.</p><p>2.正则<code>\d</code>表示匹配数字,并且将匹配到的数字保存为<code>code</code>变量</p><p>3.<code>\&amp;</code>表示转义URL中的<code>$</code>符号</p><blockquote><p>关闭route中的script path可选项</p></blockquote><ul><li><p>其次在该route下添加<code>request-transformer</code>插件,表示该插件只应用到此条route下.并且配置插件参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://localhost:8001/routes/21292565-e7ae-40ff-a465-f449c16b7819/plugins \</span><br><span class="line">      --data &quot;name=request-transformer&quot; \</span><br><span class="line">      --data &quot;config.replace.uri=/jkf/branch/qrcode&quot; \</span><br><span class="line">      --data &quot;config.add.querystring=code:\$(uri_captures.code)&quot;</span><br></pre></td></tr></table></figure><p>对于上面的命令行解释如下</p><ol><li><code>21292565-e7ae-40ff-a465-f449c16b7819</code>就是刚才创建的路由ID</li><li><code>config.replace.uri</code>表示将route匹配到的PATH重写为<code>/jkf/branch/qrcode</code></li><li><code>onfig.add.querystring</code>表示给URL添加args参数</li><li><code>code:\$(uri_captures.code)</code>.参数名是<code>code</code>,<code>uri_captures.code</code>表示获取route的PATH中code变量的值,由于命令行shell环境关系,所以要在变量符号<code>$</code>前进行转义.</li></ol></li></ul></li></ul><pre><code>或者也可以使用konga的UI管理平台添加和编辑插件</code></pre><p><img src="https://img2.jesse.top/image-20200910160826719.png" alt=""></p><p>  <img src="https://img2.jesse.top/image-20200910160949836.png" alt="image-20200910160949836"></p><ul><li><p>最后,<code>reload</code>Kong进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker ~]# docker exec kong kong reload</span><br></pre></td></tr></table></figure><p><strong>验证</strong></p><p>本地访问网站:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">✘ huangyong@huangyong-Macbook-Pro  ~  curl -XGET https://m.devapi.xxx.com/v4/jkf/branch/qrcode\&amp;code\=100006</span><br></pre></td></tr></table></figure><p>Kong和后端nginx日志如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">172.18.0.1 - - [10/Sep/2020:16:52:11 +0800] &quot;GET /jkf/branch/qrcode?code=100006 HTTP/1.1&quot; 200 163 &quot;-&quot; &quot;curl/7.54.0&quot; &quot;10.0.4.9, 172.18.0.2&quot; 10.0.4.9, 172.18.0.2, 172.18.0.1 &quot;2b7dcdc621d1928f456d561f31e95b25&quot;0.065 0.065</span><br></pre></td></tr></table></figure><p>可以看到已经成功重写了URL</p></li></ul><hr><ul><li>第二个例子,将/api/item/111 重写为/open/item/itemdetail?id=111</li></ul><p>将URL后面的数字拼接到id的值,作为参数拼接成URL后,传递给后端</p><p>1.添加Service和Routes,Routes的PATH部分如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#该PATH只匹配/api/item/数字格式的URL.并且将\d+正则匹配到的数字保存为变量id</span><br><span class="line">/api/item/(?&lt;id&gt;\d+)$</span><br></pre></td></tr></table></figure><p>2.添加和配置插件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://localhost:8001/routes/38fce1b7-2a36-42cb-9619-f30539889137/plugins \</span><br><span class="line">      --data &quot;name=request-transformer&quot; \</span><br><span class="line">      --data &quot;config.replace.uri=/open/item/itemdetail&quot; \</span><br><span class="line">      --data &quot;config.add.querystring=id:\$(uri_captures.id)&quot;</span><br></pre></td></tr></table></figure><p>3.重载kong</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker ~]# docker exec kong kong reload</span><br></pre></td></tr></table></figure><p>4.验证</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huangyong@huangyong-Macbook-Pro  ~  curl -XGET https://m.devapi.xxx.com/api/item/111</span><br></pre></td></tr></table></figure><p>后端日志如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">172.18.0.1 - - [10/Sep/2020:17:11:43 +0800] &quot;GET /open/item/itemdetail?id=111 HTTP/1.1&quot; 200 160 &quot;-&quot; &quot;curl/7.54.0&quot; &quot;10.0.4.9, 172.18.0.2&quot; 10.0.4.9, 172.18.0.2, 172.18.0.1 &quot;54ac589d36f90c1ef99ba6a43c4d488e&quot;0.105 0.105</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kong利用request-transformer插件重写URL&quot;&gt;&lt;a href=&quot;#kong利用request-transformer插件重写URL&quot; class=&quot;headerlink&quot; title=&quot;kong利用request-transformer插件重写URL&quot;&gt;&lt;/a&gt;kong利用request-transformer插件重写URL&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;最近业务整合后有一个需求,将URL:&lt;a href=&quot;http://www.abc.com/api/item/111&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;www.abc.com/api/item/111&lt;/a&gt; 想重写成&lt;a href=&quot;http://www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;使用Nginx的rewrite redirect指令可以实现URL重写需求,但是redirect会跳转到新域名,不符合需求.&lt;/p&gt;
&lt;p&gt;刚好该业务的的前端是用Kong网关处理,所以研究kong的插件实现这个需求&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;request-transformer介绍&quot;&gt;&lt;a href=&quot;#request-transformer介绍&quot; class=&quot;headerlink&quot; title=&quot;request-transformer介绍&quot;&gt;&lt;/a&gt;request-transformer介绍&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;request-transformer&lt;/strong&gt;是Kong官方的插件,允许修改重写用户的请求,还可以使用正则表达式匹配URL,并将匹配到的字符串保存在变量中,然后使用模板将变量转换成用户的请求&lt;/p&gt;
&lt;p&gt;简而言之:&lt;strong&gt;就是重写用户的请求&lt;/strong&gt;,包括URL,args,headers,methods等等&lt;/p&gt;
&lt;p&gt;官方地址: &lt;a href=&quot;https://docs.konghq.com/hub/kong-inc/request-transformer/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;reuqest-transformer官方地址&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;github项目地址: &lt;a href=&quot;https://github.com/Kong/kong-plugin-request-transformer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;request-transformer github&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>zabbix监控vmware主机以及GuestOS</title>
    <link href="https://jesse.top/2020/08/26/%E7%9B%91%E6%8E%A7/zabbix%E7%9B%91%E6%8E%A7vmware%E4%B8%BB%E6%9C%BA%E4%BB%A5%E5%8F%8AGuestOS/"/>
    <id>https://jesse.top/2020/08/26/监控/zabbix监控vmware主机以及GuestOS/</id>
    <published>2020-08-26T01:20:58.000Z</published>
    <updated>2020-08-26T23:55:20.314Z</updated>
    
    <content type="html"><![CDATA[<h3 id="zabbix监控vmware主机以及GuestOS"><a href="#zabbix监控vmware主机以及GuestOS" class="headerlink" title="zabbix监控vmware主机以及GuestOS"></a>zabbix监控vmware主机以及GuestOS</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>ESXI主机无法安装zabbix agent,所以不能使用传统的agent客户端监控vmware主机,但是Zabbix有自导的vmware hypervisors监控模板.Zabbix 通过 vmware collector 进程来监控虚拟机,使用SOAP协议从vmware web服务器获取必要的监控信息.</p><hr><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p>1.在zabbix服务器修改<code>zabbix_server.conf</code>配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">StartVMwareCollectors=6</span><br><span class="line">VMwareCacheSize=50M</span><br><span class="line">VMwareFrequency=10</span><br><span class="line">VMwarePerfFrequency=60</span><br><span class="line">VMwareTimeout=300</span><br></pre></td></tr></table></figure><a id="more"></a><p><strong>说明</strong>: </p><p><strong>StartVMwareCollectors</strong>：vmware 收集器实例的数量。<br>此值取决于要监控的 VMware 服务的数量。在大多数情况下，这应该是：<code>servicenum &lt; StartVMwareCollectors &lt; (servicenum * 2)</code>其中 servicenum 是 VMware 服务的数量。</p><p>例如：如果您有 1 个 VMware 服务要将 StartVMwareCollectors 设置为 2，那么如果您有 3 个 VMware 服务，请将其设置为 5。请注意，在大多数情况下，此值不应小于 2，不应大于 VMware 数量的 2 倍服务。</p><p>2.重启zabbix服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart zabbix_server</span><br></pre></td></tr></table></figure><hr><h4 id="Esxi物理主机配置"><a href="#Esxi物理主机配置" class="headerlink" title="Esxi物理主机配置"></a>Esxi物理主机配置</h4><p>1.登陆Esxi web界面: <a href="https://172.16.0.55" target="_blank" rel="noopener">https://172.16.0.55</a><br>2.在<code>manage</code>—<code>system</code>—-<code>advanced settings</code>.修改<code>Config.HostAgent.plugins.solo.enableMob</code>的值为True</p><p><img src="https://img2.jesse.top/image-20200818112727513.png" alt="image-20200818112727513"></p><p>3.访问:<a href="https://172.16.0.55/mob/?moid=ha-host&amp;doPath=hardware.systemInfo" target="_blank" rel="noopener">https://172.16.0.55/mob/?moid=ha-host&amp;doPath=hardware.systemInfo</a><br>记录UUID<br><img src="https://img2.jesse.top/image-20200818112949235.png" alt="image-20200818112949235"></p><p>4.在zabbix添加主机</p><p><img src="https://img2.jesse.top/image-20200818113114835.png" alt="image-20200818113114835"></p><ul><li><strong>主机名称</strong>:上面查到的UUID</li><li><strong>IP地址</strong>:Esxi的IP地址</li><li><strong>端口</strong>:80</li></ul><p><strong>模板</strong>:</p><p><img src="https://img2.jesse.top/image-20200818113315388.png" alt="image-20200818113315388"></p><p><strong>宏</strong></p><p><img src="https://img2.jesse.top/image-20200818113428859.png" alt="image-20200818113428859"></p><ul><li><strong>password</strong>: Esxi主机密码</li><li><p><strong>URL</strong>: <a href="https://Esxi_IP/sdk" target="_blank" rel="noopener">https://Esxi_IP/sdk</a> </p></li><li><p><strong>username</strong>: ESXI主机用户名</p></li><li><strong>UUID</strong>: 上文记录的UUID</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;zabbix监控vmware主机以及GuestOS&quot;&gt;&lt;a href=&quot;#zabbix监控vmware主机以及GuestOS&quot; class=&quot;headerlink&quot; title=&quot;zabbix监控vmware主机以及GuestOS&quot;&gt;&lt;/a&gt;zabbix监控vmware主机以及GuestOS&lt;/h3&gt;&lt;h4 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h4&gt;&lt;p&gt;ESXI主机无法安装zabbix agent,所以不能使用传统的agent客户端监控vmware主机,但是Zabbix有自导的vmware hypervisors监控模板.Zabbix 通过 vmware collector 进程来监控虚拟机,使用SOAP协议从vmware web服务器获取必要的监控信息.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h4&gt;&lt;p&gt;1.在zabbix服务器修改&lt;code&gt;zabbix_server.conf&lt;/code&gt;配置文件&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;StartVMwareCollectors=6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwareCacheSize=50M&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwareFrequency=10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwarePerfFrequency=60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwareTimeout=300&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="监控" scheme="https://jesse.top/categories/%E7%9B%91%E6%8E%A7/"/>
    
    
      <category term="zabbix监控" scheme="https://jesse.top/tags/zabbix%E7%9B%91%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>搭建Anaconda和jupyter notebook</title>
    <link href="https://jesse.top/2020/08/25/Linux-Service/%E6%90%AD%E5%BB%BAAnaconda%E5%92%8Cjupyter%20notebook/"/>
    <id>https://jesse.top/2020/08/25/Linux-Service/搭建Anaconda和jupyter notebook/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.309Z</updated>
    
    <content type="html"><![CDATA[<h2 id="搭建Anaconda和jupyter-notebook"><a href="#搭建Anaconda和jupyter-notebook" class="headerlink" title="搭建Anaconda和jupyter notebook"></a>搭建Anaconda和jupyter notebook</h2><h3 id="一-什么是Anaconda"><a href="#一-什么是Anaconda" class="headerlink" title="一.什么是Anaconda"></a>一.什么是Anaconda</h3><p>Anaconda可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。</p><h2 id="2-特点"><a href="#2-特点" class="headerlink" title="2. 特点"></a>2. 特点</h2><p>Anaconda具有如下特点：</p><ul><li>开源</li><li>安装过程简单</li><li>高性能使用Python和R语言</li><li>免费的社区支持</li></ul><p>其特点的实现主要基于Anaconda拥有的：</p><ul><li>conda包</li><li>环境管理器</li><li>1,000+开源库</li></ul><a id="more"></a><hr><h3 id="3-Anaconda安装"><a href="#3-Anaconda安装" class="headerlink" title="3.Anaconda安装"></a>3.Anaconda安装</h3><p><a href="https://www.anaconda.com/" target="_blank" rel="noopener">Anaconda官方</a>提供了三种不同的版本,除了Individual Edition个人版免费以外,其他2种版本都是收费的.所以这里选择Anaconda个人版.</p><h4 id="3-1-Docker安装-有坑-弃用了"><a href="#3-1-Docker安装-有坑-弃用了" class="headerlink" title="3.1 Docker安装(有坑,弃用了)"></a>3.1 Docker安装(有坑,弃用了)</h4><p>我刚开始选择的是用Docker运行,使用的是Anaconda的镜像:<a href="https://hub.docker.com/r/continuumio/anaconda3" target="_blank" rel="noopener">continuumio/anaconda3</a>.用Dockerfile在此基础之上安装了多个python扩展模块自定义了一个镜像.Dockerfile内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">FROM continuumio/anaconda3:latest</span><br><span class="line">LABEL author=jessehuang</span><br><span class="line">LABEL description=&quot;自定义制作annaconda镜像&quot;</span><br><span class="line"></span><br><span class="line">#更新debian源.使用清华大学的debian 10 brust的源</span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y apt-transport-https ca-certificates</span><br><span class="line"></span><br><span class="line">ADD sources.list /etc/apt/sources.list</span><br><span class="line"></span><br><span class="line">#安装python模块的依赖.否则thriftpy的安装会出现问题</span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \</span><br><span class="line">    python-dev \</span><br><span class="line">    vim \</span><br><span class="line">    gcc \</span><br><span class="line">    g++ \</span><br><span class="line">    libsasl2-dev </span><br><span class="line"></span><br><span class="line">#安装python扩展模块</span><br><span class="line"></span><br><span class="line">ADD requirement.txt /tmp/requirement.txt </span><br><span class="line">RUN pip install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r /tmp/requirement.txt</span><br><span class="line"></span><br><span class="line">RUN /opt/conda/bin/conda install jupyter -y --quiet &amp;&amp; mkdir /opt/notebooks </span><br><span class="line">EXPOSE 8888</span><br><span class="line"></span><br><span class="line">#启动命令</span><br><span class="line">CMD [&quot;/opt/conda/bin/jupyter&quot;,&quot;notebook&quot;, &quot;--notebook-dir=/opt/notebooks&quot;,&quot;--ip=&apos;*&apos;&quot;,&quot;--port=8888&quot;,&quot;--no-browser&quot;,&quot;--allow-root&quot;]</span><br></pre></td></tr></table></figure><p>requirement.txt内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pyecharts</span><br><span class="line">pymysql</span><br><span class="line">psycopg2-binary</span><br><span class="line">six</span><br><span class="line">bit_array</span><br><span class="line">thriftpy</span><br><span class="line">thrift_sasl==0.2.1</span><br><span class="line">impyla</span><br><span class="line">jupyter_contrib_nbextensions</span><br></pre></td></tr></table></figure><p>docker部署的anaconda在运行jupyter notebook的时候提示<code>kernel restarting</code>.容器日志内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[I 07:25:59.512 NotebookApp] Restoring connection for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.541 NotebookApp] Starting buffering for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.565 NotebookApp] Restoring connection for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.595 NotebookApp] Starting buffering for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.620 NotebookApp] Restoring connection for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[W 07:26:04.401 NotebookApp] Replacing stale connection: 63b3942c-a225-4692-8989-893af2c992cc:743009a3e3684e64a9b9fcceedc30775</span><br><span class="line">[W 07:26:05.062 NotebookApp] Replacing stale connection: a9543d90-08b5-4395-8a3c-f7bbec9f44a1:0bc5b6496d59406b82ab54a181db7215</span><br></pre></td></tr></table></figure><p>这个故障在搜遍了Google和baidu后也无解,尝试了网上各种解决办法也不行.所以果断弃掉,转而使用Linux虚拟机部署</p><h3 id="3-2-Linux部署安装-成功"><a href="#3-2-Linux部署安装-成功" class="headerlink" title="3.2 Linux部署安装(成功)"></a>3.2 Linux部署安装(成功)</h3><p>Anaconda的<a href="https://docs.anaconda.com/anaconda/install/linux/" target="_blank" rel="noopener">安装官方文档</a>.建议已root用户安装,或者你的普通用户有执行<code>/home/$(USER)/anaconda3/bin/python3</code>的sudo权限</p><p>1.首先下载python的anaconda安装脚本,建议选择python3的版本</p><p><a href="https://www.anaconda.com/products/individual#linux" target="_blank" rel="noopener">https://www.anaconda.com/products/individual#linux</a></p><p>2.安装依赖包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install libXcomposite libXcursor libXi libXtst libXrandr alsa-lib mesa-libEGL libXdamage mesa-libGL libXScrnSaver</span><br></pre></td></tr></table></figure><p>3.执行下载下来的脚本文件.根据提示,一直选择默认就可以了,官方不建议更改安装路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash ~/Anaconda3-2020.02-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><blockquote><p>We recommend you accept the default install location. Do not choose the path as /usr for the Anaconda/Miniconda installation.</p></blockquote><p>4.如果提示<code>Thank you for installing Anaconda&lt;2 or 3&gt;!</code>则说明安装完成,应用环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><p>5.验证安装结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">condal list #显示已经安装的包和版本好</span><br></pre></td></tr></table></figure><p>至此,Anaconda和anaconda自带的jupyter notebook都已经安装完了</p><hr><h3 id="4-jupyter-notebook-扩展模块安装"><a href="#4-jupyter-notebook-扩展模块安装" class="headerlink" title="4.jupyter notebook 扩展模块安装"></a>4.jupyter notebook 扩展模块安装</h3><p>公司的BI团队需要使用jupyter notebook访问MySQL、pgsql、hive,画图等工作,所以需要安装python的部分扩展模块.首先查看服务器上pip版本.需要使用python3的pip来安装模块</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#显示pip版本是python3</span><br><span class="line">(base) [root@anaconda ~]# pip --version</span><br><span class="line">pip 20.0.2 from /root/anaconda3/lib/python3.7/site-packages/pip (python 3.7)</span><br><span class="line">(base) [root@anaconda ~]#</span><br></pre></td></tr></table></figure><ol><li>安装依赖文件,否则安装<code>thriftpy</code>模块会报错</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install gcc-c++ gcc python3-devel python-dev cyrus-sasl cyrus-sasl-devel cyrus-sasl-lib</span><br></pre></td></tr></table></figure><blockquote><p>如果是ubuntu系统,则需要安装如下安装包: apt-get install -y python-dev gcc g++ libsasl2-dev</p></blockquote><ol start="2"><li>编写requirement文件,将所需要安装的python扩展模块加入到文件中</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pyecharts</span><br><span class="line">pymysql</span><br><span class="line">psycopg2-binary</span><br><span class="line">six</span><br><span class="line">bit_array</span><br><span class="line">thriftpy</span><br><span class="line">thrift_sasl==0.2.1</span><br><span class="line">impyla</span><br><span class="line">jupyter_contrib_nbextensions</span><br></pre></td></tr></table></figure><p>3.安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r requirement.txt</span><br></pre></td></tr></table></figure><p>安装完依赖包<code>jupyter_contrib_nbextensions</code>以后,安装一个服务:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter-contrib-nbextension install --user</span><br></pre></td></tr></table></figure><p>4.初始化jupyter配置文件,会在默认路径下初始化一个配置文件<code>/root/.jupyter/jupyter_notebook_config.py</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure><p>5.编辑文件,修改如下配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.ip=&apos;当前服务器IP&apos;</span><br><span class="line">c.NotebookApp.notebook_dir = u&apos;/data/notebooks&apos; #jupyter笔记的工作目录</span><br><span class="line">c.NotebookApp.open_browser = False #服务端无需启动浏览器</span><br><span class="line">c.NotebookApp.port = 80 #监听端口,默认是8888</span><br><span class="line">c.NotebookApp.allow_root = True  #允许root用户运行jupyter notebook</span><br><span class="line">c.NotebookApp.allow_origin = &apos;*&apos; #允许所有来源访问,解决跨域问题</span><br><span class="line">c.NotebookApp.quit_button = False #关闭jupyter notebook浏览器界面的quit按钮功能.因为quit按钮会关闭jupyter服务</span><br></pre></td></tr></table></figure><p>6.启动服务.记下pid号,后面要重启</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup jupyter-notebook --config=/root/.jupyter/jupyter_notebook_config.py &amp;</span><br></pre></td></tr></table></figure><p>7.打开浏览器,输入IP地址,此时会进入jupyter的界面.要求输入Token,或者使用Token设置一个密码</p><p><img src="https://img2.jesse.top/image-20200708105619499.png" alt="image-20200708105619499"></p><p>8.根据提示,使用命令<code>jupyter notebook list</code>查看Token</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">base) root@anaconda:/# jupyter notebook list</span><br><span class="line">Currently running servers:</span><br><span class="line">http://localhost:8888/?token=5469d940ce3a70950299e5c907e1d47b09acc61457348cd9 :: /data/notebooks</span><br></pre></td></tr></table></figure><p>9.拿到Token后,在浏览器中下方位置设置一个新密码.</p><p><img src="https://img2.jesse.top/image-20200708105756748.png" alt="image-20200708105756748"></p><p>10.设置完密码后,成功登陆了jupyter的工作界面.此时kill掉jupyter进程,重启</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#关闭进程</span><br><span class="line">kill $PID</span><br><span class="line"></span><br><span class="line">#重新启动</span><br><span class="line">nohup jupyter-notebook --config=/root/.jupyter/jupyter_notebook_config.py &amp;</span><br></pre></td></tr></table></figure><p>11.此时再次打开浏览器,就提示输入密码</p><p><img src="https://img2.jesse.top/image-20200708110014017.png" alt="image-20200708110014017"></p><p>12.输入密码后,成功登陆</p><p><img src="https://img2.jesse.top/image-20200708110048546.png" alt="image-20200708110048546"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;搭建Anaconda和jupyter-notebook&quot;&gt;&lt;a href=&quot;#搭建Anaconda和jupyter-notebook&quot; class=&quot;headerlink&quot; title=&quot;搭建Anaconda和jupyter notebook&quot;&gt;&lt;/a&gt;搭建Anaconda和jupyter notebook&lt;/h2&gt;&lt;h3 id=&quot;一-什么是Anaconda&quot;&gt;&lt;a href=&quot;#一-什么是Anaconda&quot; class=&quot;headerlink&quot; title=&quot;一.什么是Anaconda&quot;&gt;&lt;/a&gt;一.什么是Anaconda&lt;/h3&gt;&lt;p&gt;Anaconda可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。&lt;/p&gt;
&lt;h2 id=&quot;2-特点&quot;&gt;&lt;a href=&quot;#2-特点&quot; class=&quot;headerlink&quot; title=&quot;2. 特点&quot;&gt;&lt;/a&gt;2. 特点&lt;/h2&gt;&lt;p&gt;Anaconda具有如下特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;开源&lt;/li&gt;
&lt;li&gt;安装过程简单&lt;/li&gt;
&lt;li&gt;高性能使用Python和R语言&lt;/li&gt;
&lt;li&gt;免费的社区支持&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其特点的实现主要基于Anaconda拥有的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;conda包&lt;/li&gt;
&lt;li&gt;环境管理器&lt;/li&gt;
&lt;li&gt;1,000+开源库&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Linux-Service" scheme="https://jesse.top/categories/Linux-Service/"/>
    
    
      <category term="Anaconda" scheme="https://jesse.top/tags/Anaconda/"/>
    
  </entry>
  
  <entry>
    <title>Kibana图表制作</title>
    <link href="https://jesse.top/2020/08/25/elk/Kibana%E5%9B%BE%E8%A1%A8%E5%88%B6%E4%BD%9C/"/>
    <id>https://jesse.top/2020/08/25/elk/Kibana图表制作/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kibana图表制作"><a href="#Kibana图表制作" class="headerlink" title="Kibana图表制作"></a>Kibana图表制作</h2><p>kibana的可视化可以制作各种统计分析图表,然后合并展示到dashbord中.下面介绍一些常用的nginx的访问图表.</p><blockquote><p>在kibana7.7版本中可以配置kibana web界面的中文语言.</p></blockquote><p>编辑kibana配置文件,<code>/etc/kibana/kibana.yml</code>,加入以下配置: </p><p><code>i18n.locale: &quot;zh-CN&quot;</code></p><h4 id="一-统计过去XXX时间的访问量"><a href="#一-统计过去XXX时间的访问量" class="headerlink" title="一.统计过去XXX时间的访问量"></a>一.统计过去XXX时间的访问量</h4><p>在可视化界面,添加仪表盘图表.指定nginx的openapi access访问日志索引.</p><p>无需进行任何配置,选择右上角的时间范围,会显示计数,也就是日志量的计数</p><p><img src="https://img2.jesse.top/image-20200723154706247.png" alt="image-20200723154706247"></p><a id="more"></a><h3 id="openapi-nginx流量图"><a href="#openapi-nginx流量图" class="headerlink" title="openapi-nginx流量图"></a>openapi-nginx流量图</h3><p><img src="https://img2.jesse.top/image-20200806155157188.png" alt="image-20200806155157188"></p><h4 id="添加城市访问地图"><a href="#添加城市访问地图" class="headerlink" title="添加城市访问地图"></a>添加城市访问地图</h4><p>在<code>map</code>界面新建一个地图.在<code>road map</code>的地图上添加图层.</p><p><img src="https://img2.jesse.top/image-20200723155049422.png" alt="image-20200723155049422"></p><p>选择数据源.选择文档类型:</p><p><img src="https://img2.jesse.top/image-20200723160002287.png" alt="image-20200723160002287"></p><p>选择索引后,选择<code>geoip.location</code>字段,此时客户端地图分布自动展现出现,而且会自动计数</p><p><img src="https://img2.jesse.top/image-20200723160127185.png" alt="image-20200723160127185"></p><h4 id="Nginx状态码统计图"><a href="#Nginx状态码统计图" class="headerlink" title="Nginx状态码统计图"></a>Nginx状态码统计图</h4><p>添加饼图,使用<code>status</code>指标来统计各状态码的次数.</p><p><img src="https://img2.jesse.top/image-20200723163110993.png" alt="image-20200723163110993"></p><blockquote><p>如果没有status字段,需要去刷新索引</p></blockquote><h4 id="Nginx访问客户端TOP5"><a href="#Nginx访问客户端TOP5" class="headerlink" title="Nginx访问客户端TOP5"></a>Nginx访问客户端TOP5</h4><p>添加垂直条形图.使用geoip关键词统计各IP的访问次数,并且按降序排序</p><p><img src="https://img2.jesse.top/image-20200723163406793.png" alt="image-20200723163406793"></p><h4 id="nginx请求URL的TOP5"><a href="#nginx请求URL的TOP5" class="headerlink" title="nginx请求URL的TOP5"></a>nginx请求URL的TOP5</h4><p>添加数据图表,使用request关键词统计各URL的访问次数,并按降序排序</p><p><img src="https://img2.jesse.top/image-20200723163554578.png" alt="image-20200723163554578"></p><h4 id="nginx的cost请求时间TOP10"><a href="#nginx的cost请求时间TOP10" class="headerlink" title="nginx的cost请求时间TOP10"></a>nginx的cost请求时间TOP10</h4><p>添加垂直条形图.使用responsetime关键词做聚合,统计请求最慢的cost时间</p><p><img src="https://img2.jesse.top/image-20200723163717829.png" alt="image-20200723163717829"></p><hr><h4 id="在Dashboard中将多个图表聚合成一个大盘"><a href="#在Dashboard中将多个图表聚合成一个大盘" class="headerlink" title="在Dashboard中将多个图表聚合成一个大盘"></a>在Dashboard中将多个图表聚合成一个大盘</h4><p><img src="https://img2.jesse.top/image-20200723163906182.png" alt="image-20200723163906182"></p><p>​                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kibana图表制作&quot;&gt;&lt;a href=&quot;#Kibana图表制作&quot; class=&quot;headerlink&quot; title=&quot;Kibana图表制作&quot;&gt;&lt;/a&gt;Kibana图表制作&lt;/h2&gt;&lt;p&gt;kibana的可视化可以制作各种统计分析图表,然后合并展示到dashbord中.下面介绍一些常用的nginx的访问图表.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在kibana7.7版本中可以配置kibana web界面的中文语言.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;编辑kibana配置文件,&lt;code&gt;/etc/kibana/kibana.yml&lt;/code&gt;,加入以下配置: &lt;/p&gt;
&lt;p&gt;&lt;code&gt;i18n.locale: &amp;quot;zh-CN&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&quot;一-统计过去XXX时间的访问量&quot;&gt;&lt;a href=&quot;#一-统计过去XXX时间的访问量&quot; class=&quot;headerlink&quot; title=&quot;一.统计过去XXX时间的访问量&quot;&gt;&lt;/a&gt;一.统计过去XXX时间的访问量&lt;/h4&gt;&lt;p&gt;在可视化界面,添加仪表盘图表.指定nginx的openapi access访问日志索引.&lt;/p&gt;
&lt;p&gt;无需进行任何配置,选择右上角的时间范围,会显示计数,也就是日志量的计数&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2.jesse.top/image-20200723154706247.png&quot; alt=&quot;image-20200723154706247&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>ELK收集mysql5.7慢日志</title>
    <link href="https://jesse.top/2020/08/25/elk/ELK%E6%94%B6%E9%9B%86mysql5.7%E6%85%A2%E6%97%A5%E5%BF%97/"/>
    <id>https://jesse.top/2020/08/25/elk/ELK收集mysql5.7慢日志/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.312Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ELK收集mysql5-7慢日志"><a href="#ELK收集mysql5-7慢日志" class="headerlink" title="ELK收集mysql5.7慢日志"></a>ELK收集mysql5.7慢日志</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>公司ELK平台计划收集生产业务的所有mysql慢日志.由于所有环境中均使用mysql5.7版本,所以其他mysql版本的慢日志格式不在讨论范围之内.</p><p>慢日志的grok正则匹配我折腾了很久,网上的大多文档中给出的logstash的grok正则其实并不能正确的解析到mysql慢日志的字段.</p><p>这个博客的grok正则经过实践可行.而且filebeat,logstash的filter配置也是参考这个博客配置的:<a href="https://www.cnblogs.com/minseo/p/10441913.html" target="_blank" rel="noopener">博客地址</a></p><hr><h3 id="MySQL慢日志"><a href="#MySQL慢日志" class="headerlink" title="MySQL慢日志"></a>MySQL慢日志</h3><p>慢日志格式如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[work@msf-mysql-master log]$ sudo head slow_2019072103.log</span><br><span class="line">/usr/local/mysql5.7/bin/mysqld, Version: 5.7.24-log (MySQL Community Server (GPL)). started with:</span><br><span class="line">Tcp port: 3306  Unix socket: /mysql_log/msf/tmp/mysql.sock</span><br><span class="line">Time                 Id Command    Argument</span><br><span class="line"># Time: 2019-07-21T08:54:04.145255+08:00</span><br><span class="line"># User@Host: u_msf[u_msf] @  [10.111.10.40]  Id: 131421254</span><br><span class="line"># Query_time: 1.595300  Lock_time: 0.000031 Rows_sent: 20  Rows_examined: 809259</span><br><span class="line">use msf_prod;</span><br><span class="line">SET timestamp=1563670444;</span><br><span class="line">SELECT `id`,`type`,`honey`,`remark`,`created_at` FROM `t_log_user_honey` WHERE `user_id` = &apos;1000014423&apos; ORDER BY `created_at` DESC LIMIT 20 OFFSET 0;</span><br><span class="line"># Time: 2019-07-21T10:51:06.184010+08:00</span><br></pre></td></tr></table></figure><a id="more"></a><p>每个日志文件的格式为<code>slow_日期.log</code> .7天切割一次新的日志文件</p><p>每个日志的开头三行是不需要的内容,所以需要filebeat排除</p><p>每一条慢日志有以下几行组成:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Time: 2019-07-21T08:54:04.145255+08:00</span><br><span class="line"># User@Host: u_msf[u_msf] @  [10.111.10.40]  Id: 131421254</span><br><span class="line"># Query_time: 1.595300  Lock_time: 0.000031 Rows_sent: 20  Rows_examined: 809259</span><br><span class="line">use msf_prod;</span><br><span class="line">SET timestamp=1563670444;</span><br><span class="line">SELECT `id`,`type`,`honey`,`remark`,`created_at` FROM `t_log_user_honey` WHERE `user_id` = &apos;1000014423&apos; ORDER BY `created_at` DESC LIMIT 20 OFFSET 0;</span><br></pre></td></tr></table></figure><p>第一行Time时间不需要,所以也需要filebeat排除.</p><p>从第二行开始匹配,有些慢日志可能没有<code>use database;</code>的语句.所以需要分别针对对待</p><hr><h3 id="filebeat配置"><a href="#filebeat配置" class="headerlink" title="filebeat配置"></a>filebeat配置</h3><p>filebeat需要开启多行日志功能.并且排除特定的字段.除此之外,和其他的日志收集配置一样.下面是生产环境中filebeat的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">- type: log</span><br><span class="line"></span><br><span class="line">  # Change to true to enable this input configuration.</span><br><span class="line">  enabled: true</span><br><span class="line"></span><br><span class="line">  # Paths that should be crawled and fetched. Glob based paths.</span><br><span class="line">  paths:</span><br><span class="line">    - /mysql_log/msf/log/slow_*.log</span><br><span class="line">  exclude_lines: [&apos;^\# Time&apos;,&apos;^\/usr&apos;,&apos;^Tcp&apos;,&apos;^Time&apos;]</span><br><span class="line">  multiline.pattern: &apos;^\# Time|^\# User&apos;</span><br><span class="line">  multiline.negate: true</span><br><span class="line">  multiline.match: after</span><br><span class="line">  fields:</span><br><span class="line">    project: msf</span><br><span class="line">    type: mysql</span><br><span class="line">    level: slow</span><br></pre></td></tr></table></figure><hr><h3 id="logstash配置"><a href="#logstash配置" class="headerlink" title="logstash配置"></a>logstash配置</h3><p>logstash需要使用正则匹配2种格式的慢日志.当一种grok匹配到了后,logstash就不会再接着往下匹配了,所以每条日志只会匹配一种grok规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">      if [fields][type] == &quot;mysql&quot; &#123;</span><br><span class="line">          grok &#123;</span><br><span class="line">            #有use database语句</span><br><span class="line">            match =&gt; [ &quot;message&quot; , &quot;^#\s+User@Host:\s+%&#123;USER:user&#125;\[[^\]]+\]\s+@\s+(?:(?&lt;clienthost&gt;\S*) )?\[(?:%&#123;IPV4:clientip&#125;)?\]\s+Id:\s+%&#123;NUMBER:row_id:int&#125;\n#\s+Query_time:\s+%&#123;NUMBER:query_time:float&#125;\s+Lock_time:\s+%&#123;NUMBER:lock_time:float&#125;\s+Rows_sent:\s+%&#123;NUMBER:rows_sent:int&#125;\s+Rows_examined:\s+%&#123;NUMBER:rows_examined:int&#125;\n\s*(?:use %&#123;DATA:database&#125;;\s*\n)?SET\s+timestamp=%&#123;NUMBER:timestamp&#125;;\n\s*(?&lt;sql&gt;(?&lt;action&gt;\w+)\b.*;)\s*(?:\n#\s+Time)?.*$&quot; ]</span><br><span class="line"></span><br><span class="line">            remove_field =&gt; [&quot;message&quot;] #删除原始日志,我试过写在mutate中,发现不起作用</span><br><span class="line">&#125;</span><br><span class="line">            #无use database语句</span><br><span class="line">          grok &#123;</span><br><span class="line">            match =&gt; [ &quot;message&quot; , &quot;^#\s+User@Host:\s+%&#123;USER:user&#125;\[[^\]]+\]\s+@\s+(?:(?&lt;clienthost&gt;\S*) )?\[(?:%&#123;IPV4:clientip&#125;)?\]\s+Id:\s+%&#123;NUMBER:row_id:int&#125;\n#\s+Query_time:\s+%&#123;NUMBER:query_time:float&#125;\s+Lock_time:\s+%&#123;NUMBER:lock_time:float&#125;\s+Rows_sent:\s+%&#123;NUMBER:rows_sent:int&#125;\s+Rows_examined:\s+%&#123;NUMBER:rows_examined:int&#125;\nSET\s+timestamp=%&#123;NUMBER:timestamp&#125;;\n\s*(?&lt;sql&gt;(?&lt;action&gt;\w+)\b.*;)\s*(?:\n#\s+Time)?.*$&quot; ]</span><br><span class="line">           remove_field =&gt; [&quot;message&quot;]  #删除原始日志,我试过写在mutate中,发现不起作用</span><br><span class="line">    &#125;</span><br><span class="line">        date &#123;</span><br><span class="line">            match =&gt; [&quot;timestamp_mysql&quot;, &quot;UNIX&quot;]</span><br><span class="line">            target =&gt; &quot;@timestamp&quot;</span><br><span class="line">        &#125;</span><br><span class="line">       mutate &#123;</span><br><span class="line">            remove_field =&gt; &quot;@version&quot;  #删除filebeat传输过来的无用字段,可以视情况删除</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><blockquote><p>也可以将2个match语句写入到一个grok中,但是我试过好像不行,有些慢日志并不能正常解析</p></blockquote><p>实际上无论是grok debbuger在线工具还是Kibana的dev tool均提供了grok的在线调试工具,可以检测和调试grok的正则匹配.例如使用上述中的grok正则检测一下是否能正常匹配到前文提到的mysql原始日志数据.可以使用kibana的dev tool工具中的Grok Debugger工具来校验:</p><p><img src="https://img2.jesse.top/image-20200729142559717.png" alt="image-20200729142559717"></p><p>上面的结构化数据输出中可以看到grok正则能正常解析原始日志中的数据,并且以json格式将日志内容映射给各字段.</p><hr><h3 id="Kibana展示"><a href="#Kibana展示" class="headerlink" title="Kibana展示"></a>Kibana展示</h3><p>我尝试在kibana中使用图形化展示query_time(也就是SQL执行时间)最长的TOP5的SQL语句,制作成可视化图表,方便动态展示.但是发现可视化的聚合图形并不能满足这个需求.</p><p>但是Kibana的Discover界面通过选定字段,也能对query_time进行排序.例如下面的截图中先选定<code>query_time</code>和<code>sql</code>这2个字段,然后再对<code>query_time</code>进行排序(在右边的query_time字段下有个倒三角形表示倒序排序).</p><p><img src="https://img2.jesse.top/image-20200729143159670.png" alt="image-20200729143159670"></p><p>然后将这个discover的筛选结果保存到Dashboard中,方便以后查看:</p><p><img src="https://img2.jesse.top/image-20200729143309655.png" alt="image-20200729143309655"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ELK收集mysql5-7慢日志&quot;&gt;&lt;a href=&quot;#ELK收集mysql5-7慢日志&quot; class=&quot;headerlink&quot; title=&quot;ELK收集mysql5.7慢日志&quot;&gt;&lt;/a&gt;ELK收集mysql5.7慢日志&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;公司ELK平台计划收集生产业务的所有mysql慢日志.由于所有环境中均使用mysql5.7版本,所以其他mysql版本的慢日志格式不在讨论范围之内.&lt;/p&gt;
&lt;p&gt;慢日志的grok正则匹配我折腾了很久,网上的大多文档中给出的logstash的grok正则其实并不能正确的解析到mysql慢日志的字段.&lt;/p&gt;
&lt;p&gt;这个博客的grok正则经过实践可行.而且filebeat,logstash的filter配置也是参考这个博客配置的:&lt;a href=&quot;https://www.cnblogs.com/minseo/p/10441913.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;博客地址&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;MySQL慢日志&quot;&gt;&lt;a href=&quot;#MySQL慢日志&quot; class=&quot;headerlink&quot; title=&quot;MySQL慢日志&quot;&gt;&lt;/a&gt;MySQL慢日志&lt;/h3&gt;&lt;p&gt;慢日志格式如下:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[work@msf-mysql-master log]$ sudo head slow_2019072103.log&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/usr/local/mysql5.7/bin/mysqld, Version: 5.7.24-log (MySQL Community Server (GPL)). started with:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tcp port: 3306  Unix socket: /mysql_log/msf/tmp/mysql.sock&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Time                 Id Command    Argument&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# Time: 2019-07-21T08:54:04.145255+08:00&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# User@Host: u_msf[u_msf] @  [10.111.10.40]  Id: 131421254&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# Query_time: 1.595300  Lock_time: 0.000031 Rows_sent: 20  Rows_examined: 809259&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;use msf_prod;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;SET timestamp=1563670444;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;SELECT `id`,`type`,`honey`,`remark`,`created_at` FROM `t_log_user_honey` WHERE `user_id` = &amp;apos;1000014423&amp;apos; ORDER BY `created_at` DESC LIMIT 20 OFFSET 0;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# Time: 2019-07-21T10:51:06.184010+08:00&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>使用goreplay收集线上真实http流量</title>
    <link href="https://jesse.top/2020/08/25/Linux-Web/%E4%BD%BF%E7%94%A8goreplay%E6%94%B6%E9%9B%86%E7%BA%BF%E4%B8%8A%E7%9C%9F%E5%AE%9Ehttp%E6%B5%81%E9%87%8F/"/>
    <id>https://jesse.top/2020/08/25/Linux-Web/使用goreplay收集线上真实http流量/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.311Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用goreplay收集线上真实http流量"><a href="#使用goreplay收集线上真实http流量" class="headerlink" title="使用goreplay收集线上真实http流量"></a>使用goreplay收集线上真实http流量</h2><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>在很多场景中,我们需要将线上服务器的真实Http请求复制转发到某台服务器中(或者测试环境中),并且前提是不影响线上生产业务进行.</p><p>例如:</p><ol><li>通常可能会通过ab等压测工具来对单一http接口进行压测。但如果是需要http服务整体压测，使用ab来压测工作量大且不方便，通过线上流量复制引流，通过将真实请求流量放大N倍来进行压测，能对服务有一个较为全面的检验.</li><li>将线上流量引入到测试环境中,测试某个中间件或者数据库的压力</li><li>上线前在预发布环境，使用线上真实的请求，检查是否准备发布的版本，是否具备发布标准</li><li>用线上的流量转发到预发，检查相同流量下一些指标的反馈情况，检查核心数据是否有改善、优化.</li></ol><a id="more"></a><hr><h3 id="goreplay介绍"><a href="#goreplay介绍" class="headerlink" title="goreplay介绍"></a>goreplay介绍</h3><p>goreplay项目请参考github:<a href="https://github.com/buger/goreplay" target="_blank" rel="noopener">goreplay介绍</a></p><p>goreplay是一款开源网络监控工具,可以在不影响业务的情况下,记录服务器真实流量,将该流量用来做镜像,压力测试,监控和分析等用途.</p><p>简单来说就是goreplay抓取线上真实的流量，并将捕捉到的流量转发到测试服务器上(或者保存到本地文件中)</p><p>goreplay大致工作流程如下:</p><p><img src="https://img2.jesse.top/20200629110035.png" alt=""></p><hr><h3 id="goreplay常见使用方式"><a href="#goreplay常见使用方式" class="headerlink" title="goreplay常见使用方式"></a>goreplay常见使用方式</h3><p>goreplay使用文档参考:<a href="https://github.com/buger/goreplay/wiki" target="_blank" rel="noopener">goreplay文档</a></p><p>常用的一些命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-input-raw 抓取指定端口的流量 gor --input-raw :8080</span><br><span class="line">-output-stdout 打印到控制台</span><br><span class="line">-output-file 将请求写到文件中 gor --input-raw :80 --output-file ./requests.gor</span><br><span class="line">-input-file 从文件中读取请求，与上一条命令呼应 gor --input-file ./requests.gor</span><br><span class="line">-exit-after 5s 持续时间</span><br><span class="line">-http-allow-url url白名单，其他请求将会被丢弃</span><br><span class="line">-http-allow-method 根据请求方式过滤</span><br><span class="line">-http-disallow-url 遇上一个url相反，黑名单，其他的请求会被捕获到</span><br></pre></td></tr></table></figure><blockquote><p>更多命令可以使用 ./gor –help查看</p></blockquote><hr><h3 id="goreplay安装"><a href="#goreplay安装" class="headerlink" title="goreplay安装"></a>goreplay安装</h3><p>在github上下载Linux的二进制文件: <a href="https://github.com/buger/goreplay/releases" target="_blank" rel="noopener">goreplay安装</a></p><blockquote><p>注意.虽然在github上提供了rpm安装包,但是实际安装发现无法安装:</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# rpm -ivh gor-1.0.0-1.x86_64.rpm</span><br><span class="line">Preparing...                          ################################# [100%]</span><br><span class="line">package goreplay-1.0.0-1.x86_64 is intended for a different operating system</span><br></pre></td></tr></table></figure><p>下载github上的二进制文件,解压后是一个gor的二进制可执行文件.复制到PATH变量路径下即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# wget https://github.com/buger/goreplay/releases/download/v1.0.0/gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# ls</span><br><span class="line"> gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# tar -xf gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# ls</span><br><span class="line">gor </span><br><span class="line">[root@dwd-tongji-3 ~]# ll gor</span><br><span class="line">-rwxr-xr-x 1 501 games 17779040 Mar 30  2019 gor</span><br><span class="line">[root@dwd-tongji-3 ~]# cp gor /usr/local/bin/</span><br></pre></td></tr></table></figure><hr><h3 id="goreplay简单实践"><a href="#goreplay简单实践" class="headerlink" title="goreplay简单实践"></a>goreplay简单实践</h3><h4 id="1-将本地http的流量保存到本地文件中"><a href="#1-将本地http的流量保存到本地文件中" class="headerlink" title="1.将本地http的流量保存到本地文件中."></a>1.将本地http的流量保存到本地文件中.</h4><p>为了简便起见,以下命令都在root用户下执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## 1.开启一个screen窗口</span><br><span class="line">[root@dwd-tongji-3 ~]# screen -S GOR</span><br><span class="line">## 2.将80流量保存到本地的文件</span><br><span class="line">[root@dwd-tongji-3 ~]# gor --input-raw :80 --output-file /data/requests.gor</span><br><span class="line">Version: 1.0.0</span><br></pre></td></tr></table></figure><p>默认情况下goreplay会以块文件存储,将流量保存为多个块文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# ls /data/requests_* | more</span><br><span class="line">/data/requests_0.gor</span><br><span class="line">/data/requests_100.gor</span><br><span class="line">/data/requests_101.gor</span><br><span class="line">/data/requests_102.gor</span><br><span class="line">/data/requests_103.gor</span><br><span class="line">/data/requests_104.gor</span><br><span class="line">/data/requests_105.gor</span><br><span class="line">/data/requests_106.gor</span><br><span class="line">/data/requests_107.gor</span><br><span class="line">/data/requests_108.gor</span><br><span class="line">/data/requests_109.gor</span><br><span class="line">/data/requests_10.gor</span><br></pre></td></tr></table></figure><p>使用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>[root@dwd-tongji-3 ~]#./gor –input-raw :80 –output-file /data/gor.gor –output-file-append</p><p>[root@dwd-tongji-3 ~]# ll /data -h<br>total 1.4M<br>-rw-r—– 1 root root 1.4M Jun 29 15:13 gor.gor<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 2.将http的请求打印到终端</span><br></pre></td></tr></table></figure></p><p>[root@dwd-tongji-3 ~]#gor –input-raw :8000 –output-stdout<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 3.将http的请求转发到测试环境</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-http=”<a href="http://beta:80&quot;" target="_blank" rel="noopener">http://beta:80&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在测试服务器上的nginx查看日志,.发现流量已经进来了</span><br></pre></td></tr></table></figure></p><p>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik.php HTTP/1.1” 200 5 “<a href="https://2021001151691008.hybrid.alipay-eco.com/2021001151691008/0.2.2006111453.18/index.html#pages/index/index?appid=2021001151691008&amp;taskId=415&quot;" target="_blank" rel="noopener">https://2021001151691008.hybrid.alipay-eco.com/2021001151691008/0.2.2006111453.18/index.html#pages/index/index?appid=2021001151691008&amp;taskId=415&quot;</a> “Mozilla/5.0 (iPhone; CPU iPhone OS 13_3_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/17D50 Ariver/1.0.12 AliApp(AP/10.1.95.7030) Nebula WK RVKType(0) AlipayDefined(nt:4G,ws:375|667|2.0) AlipayClient/10.1.95.7030 Language/zh-Hans Region/CN NebulaX/1.0.0” “112.96.179.238”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik.php HTTP/1.1” 200 5 “<a href="https://servicewechat.com/wxa090d3923fde0d4b/132/page-frame.html&quot;" target="_blank" rel="noopener">https://servicewechat.com/wxa090d3923fde0d4b/132/page-frame.html&quot;</a> “Mozilla/5.0 (iPhone; CPU iPhone OS 13_5_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 MicroMessenger/7.0.13(0x17000d29) NetType/4G Language/zh_CN” “14.106.171.11”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">####  也可以将流量输出到多个终端</span><br><span class="line"></span><br><span class="line">* 输出到多个http服务器</span><br></pre></td></tr></table></figure></p><p>gor –input-tcp :28020 –output-http “<a href="http://staging.com&quot;" target="_blank" rel="noopener">http://staging.com&quot;</a>  –output-http “<a href="http://dev.com&quot;" target="_blank" rel="noopener">http://dev.com&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 输出到文件或者Http服务器</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-file requests.log –output-http “<a href="http://staging.com&quot;" target="_blank" rel="noopener">http://staging.com&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">####  4.将流量从文件重放到http服务器</span><br><span class="line"></span><br><span class="line">1.首先将请求流量保存到本地文件</span><br></pre></td></tr></table></figure></p><p>sudo ./gor –input-raw :8000 –output-file=requests.gor<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2.再开一个窗口,运行gor,将请求流量从文件中重放</span><br></pre></td></tr></table></figure></p><p>./gor –input-file requests.gor –output-http=”<a href="http://localhost:8001&quot;" target="_blank" rel="noopener">http://localhost:8001&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 压力测试</span><br><span class="line"></span><br><span class="line">goreplay支持将捕获到的生产实际请求流量减少或者放大重播以用于测试环境的压力测试.压力测试一般针对Input流量减少或者放大.例如下面的例子</span><br></pre></td></tr></table></figure></p><h1 id="Replay-from-file-on-2x-speed"><a href="#Replay-from-file-on-2x-speed" class="headerlink" title="Replay from file on 2x speed"></a>Replay from file on 2x speed</h1><p>#将请求流量以2倍的速度放大重播<br>gor –input-file “requests.gor|200%” –output-http “staging.com”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">当然也也支持10%,20%等缩小请求流量</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 限速</span><br><span class="line"></span><br><span class="line">如果受限于测试环境的服务器资源压力,只想重播一部分流量到测试环境中,而不需要所有的实际生产流量,那么就可以用限速功能.有两种策略可以实现限流</span><br><span class="line"></span><br><span class="line">1.随机丢弃请求流量</span><br><span class="line"></span><br><span class="line">2.基于Header或者URL丢弃一定的流量(百分比)</span><br><span class="line"></span><br><span class="line">#####  随机丢弃请求流量</span><br><span class="line"></span><br><span class="line">input和output两端都支持限速,有两种限速算法:**百分比**或者**绝对值**</span><br><span class="line"></span><br><span class="line">* 百分比: input端支持缩小或者放大请求流量,基于指定的策略随机丢弃请求流量</span><br><span class="line">* 绝对值: 如果单位时间(秒)内达到临界值,则丢弃剩余请求流量,下一秒临界值还原</span><br><span class="line"></span><br><span class="line">**用法**:</span><br><span class="line"></span><br><span class="line">在output终端使用&quot;|&quot;运算符指定限速阈值,例如:</span><br><span class="line"></span><br><span class="line">* 使用绝对值限速</span><br></pre></td></tr></table></figure></p><h1 id="staging-server-will-not-get-more-than-ten-requests-per-second"><a href="#staging-server-will-not-get-more-than-ten-requests-per-second" class="headerlink" title="staging.server will not get more than ten requests per second"></a>staging.server will not get more than ten requests per second</h1><p>#staging服务每秒只接收10个请求<br>gor –input-tcp :28020 –output-http “<a href="http://staging.com|10&quot;" target="_blank" rel="noopener">http://staging.com|10&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 使用百分比限速</span><br></pre></td></tr></table></figure></p><h1 id="replay-server-will-not-get-more-than-10-of-requests"><a href="#replay-server-will-not-get-more-than-10-of-requests" class="headerlink" title="replay server will not get more than 10% of requests"></a>replay server will not get more than 10% of requests</h1><h1 id="useful-for-high-load-environments"><a href="#useful-for-high-load-environments" class="headerlink" title="useful for high-load environments"></a>useful for high-load environments</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##### 基于Header或者URL参数限速</span><br><span class="line"></span><br><span class="line">如果header或者URL参数中有唯一值,例如(API key),则可以转发指定百分比的流量到后端,例如:</span><br></pre></td></tr></table></figure></p><h1 id="Limit-based-on-header-value"><a href="#Limit-based-on-header-value" class="headerlink" title="Limit based on header value"></a>Limit based on header value</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%” –http-header-limiter “X-API-KEY: 10%”</p><h1 id="Limit-based-on-URL-param-value"><a href="#Limit-based-on-URL-param-value" class="headerlink" title="Limit based on URL param value"></a>Limit based on URL param value</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%” –http-param-limiter “api_key: 10%”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">###  过滤</span><br><span class="line"></span><br><span class="line">如果只想捕获指定的URL路径请求,或者http头部,或者Http方法,则可以使用过滤功能</span><br><span class="line"></span><br><span class="line">下面是几个例子</span><br><span class="line"></span><br><span class="line">* 只捕获某个URL</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-being-sent-to-the-api-endpoint"><a href="#only-forward-requests-being-sent-to-the-api-endpoint" class="headerlink" title="only forward requests being sent to the /api endpoint"></a>only forward requests being sent to the /api endpoint</h1><p>gor –input-raw :8080 –output-http staging.com –http-allow-url /api<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 拒绝某个URL</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-NOT-being-sent-to-the-api…-endpoint"><a href="#only-forward-requests-NOT-being-sent-to-the-api…-endpoint" class="headerlink" title="only forward requests NOT being sent to the /api… endpoint"></a>only forward requests NOT being sent to the /api… endpoint</h1><p>gor –input-raw :8080 –output-http staging.com –http-disallow-url /api<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 基于正则表达式过滤头部</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-with-an-api-version-of-1-0x"><a href="#only-forward-requests-with-an-api-version-of-1-0x" class="headerlink" title="only forward requests with an api version of 1.0x"></a>only forward requests with an api version of 1.0x</h1><p>gor –input-raw :8080 –output-http staging.com –http-allow-header api-version:^1.0\d</p><h1 id="only-forward-requests-NOT-containing-User-Agent-header-value-“Replayed-by-Gor”"><a href="#only-forward-requests-NOT-containing-User-Agent-header-value-“Replayed-by-Gor”" class="headerlink" title="only forward requests NOT containing User-Agent header value “Replayed by Gor”"></a>only forward requests NOT containing User-Agent header value “Replayed by Gor”</h1><p>gor –input-raw :8080 –output-http staging.com –http-disallow-header “User-Agent: Replayed by Gor”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 过滤HTTP请求方法</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-http “<a href="http://staging.server&quot;" target="_blank" rel="noopener">http://staging.server&quot;</a> \<br>    –http-allow-method GET \<br>    –http-allow-method OPTIONS<br><code>`</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;使用goreplay收集线上真实http流量&quot;&gt;&lt;a href=&quot;#使用goreplay收集线上真实http流量&quot; class=&quot;headerlink&quot; title=&quot;使用goreplay收集线上真实http流量&quot;&gt;&lt;/a&gt;使用goreplay收集线上真实http流量&lt;/h2&gt;&lt;h3 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;背景介绍&lt;/h3&gt;&lt;p&gt;在很多场景中,我们需要将线上服务器的真实Http请求复制转发到某台服务器中(或者测试环境中),并且前提是不影响线上生产业务进行.&lt;/p&gt;
&lt;p&gt;例如:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通常可能会通过ab等压测工具来对单一http接口进行压测。但如果是需要http服务整体压测，使用ab来压测工作量大且不方便，通过线上流量复制引流，通过将真实请求流量放大N倍来进行压测，能对服务有一个较为全面的检验.&lt;/li&gt;
&lt;li&gt;将线上流量引入到测试环境中,测试某个中间件或者数据库的压力&lt;/li&gt;
&lt;li&gt;上线前在预发布环境，使用线上真实的请求，检查是否准备发布的版本，是否具备发布标准&lt;/li&gt;
&lt;li&gt;用线上的流量转发到预发，检查相同流量下一些指标的反馈情况，检查核心数据是否有改善、优化.&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="goreplay" scheme="https://jesse.top/tags/goreplay/"/>
    
  </entry>
  
</feed>

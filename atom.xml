<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jesse&#39;s home</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jesse.top/"/>
  <updated>2021-01-05T14:54:58.225Z</updated>
  <id>https://jesse.top/</id>
  
  <author>
    <name>Jesse</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>kafka-4.2分区管理</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/4.2%E5%88%86%E5%8C%BA%E7%AE%A1%E7%90%86/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/4-主题和分区/4.2分区管理/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:54:58.225Z</updated>
    
    <content type="html"><![CDATA[<h3 id="4-2-1-优选副本的选举"><a href="#4-2-1-优选副本的选举" class="headerlink" title="4.2.1 优选副本的选举"></a>4.2.1 优选副本的选举</h3><h4 id="4-2-1-1-什么是优先副本"><a href="#4-2-1-1-什么是优先副本" class="headerlink" title="4.2.1.1 什么是优先副本"></a>4.2.1.1 什么是优先副本</h4><p>分区使用多副本机制来提升可靠性,但是只有leader副本对外提供读写服务.而follower副本只负责在内部进行消息的同步.如果一个分区的leader副本不可用,那么就意味着整个分区变得不可用.此时就需要从剩余的follower副本中挑选一个新的leader副本继续对外提供服务.</p><blockquote><p>broker节点中的Leader副本个数决定了这个节点负载的高低</p></blockquote><p>在创建主题的时候,主题的分区和副本会尽可能的均匀分布在kafka集群的各个broker节点.对应的Leader副本的分配也比较均匀.例如下面的 <code>topic-demo</code> 主题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 152,153,154</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 152,153,154</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 152,153,154</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 152,153,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><a id="more"></a> <p>可以看到,leader副本均匀分布在所有的broker节点.另外,同一个分区,在同一台broker节点只能存在一个副本.所以leader副本所在的broker节点叫做分区的leader节点.而follower副本所在的broker节点叫做分区的follower节点.</p><p>可以想象的是,随着时间的推移,kafka集群中不可避免的出现节点宕机或者崩溃的情况.当分区的Leader节点发生故障时,其中一个follower节点就会成为新的Leader节点.这样导致集群中的节点之间负载不均衡,从而影响kafka整个集群的稳定性和健壮性.</p><p>即使原来的Leader节点恢复后,加入到集群时,也只能成为一个新的follower节点,而不会自动”抢班夺权”变成leader.</p><p>例如刚才的 <code>topic-demo</code> 分区重启152节点后,leader分布如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 153 Replicas: 152,153,154   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 154 Replicas: 152,154,153   Isr: 153,154,152</span><br></pre></td></tr></table></figure><p>尽管kafka非常均匀的将leader副本分布在其他另外2个几点.但是此时152节点的负载几乎为零.</p><p>为了有效的治理负载失衡的情况,kafka引入了<strong>优先副本(preferred replica)</strong>的概念.所谓的优先副本就是在AR集合列表中的第一个副本为优先副本,理想情况下优先副本就是该分区的leader副本.所以也可以称之为 <code>preferred leader</code> .</p><p>比如上面的例子中,分区0的AR集合(Replicas)是[152,153,154].那么分区0的优先副本就是152.<strong>Kafka会确保所有主题的优先副本均匀分布.这样就保证了所有分区的leader均衡分布.</strong></p><p><strong></strong></p><h4 id="4-2-1-2-优先副本选举"><a href="#4-2-1-2-优先副本选举" class="headerlink" title="4.2.1.2 优先副本选举"></a>4.2.1.2 优先副本选举</h4><p>所谓的优先副本选举是指通过一定的方式促使优先副本选举为Leader副本,促进集群的负载均衡.这一行为也称之为”分区平衡”.</p><p>kafka broker端(server.properties配置文件)有个 <code>auto.leader.rebalance.enble</code> 参数.默认为true.也就是分区自动平衡功能.Kafka会启动一个定时任务,轮询所有的broker节点,自动执行优先副本选举动作.</p><p>不过在生产环境中建议将该配置设置为 <code>false</code> .因为kafka自动平衡分区可能在某些关键高分期时刻引起负面性能问题.也有可能引起客户端的阻塞.为了防止出现此类情况,建议针对副本不均衡的问题进行相应监控和告警,然后在合适的时间通过手动来执行分区平衡.</p><p>Kafka中的 <code>kafka-preferred-replica-election.sh</code> 脚本提供了对分区leader副本进行重新平衡的功能.优先副本选举过程是一个安全的过程,kafka客户端会自动感知leader副本的变更.</p><p>命令用法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181</span><br></pre></td></tr></table></figure><p>但是这样一来会对kafka集群的所有主题和分区都执行一遍优先副本的选举操作.如果集群中包含大量的分区,那么可能选举会失败,并且会对性能造成一定的应用.比较建议的是使用 <code>path-to-json-file</code> 参数来小批量的对部分指定的主题分区进行优先副本的选举操作.该参数指定一个JSON文件,这个JSON文件保存需要执行优先副本选举的分区清单.</p><p>举个例子,对上面的 <code>topic-demo</code> 分区进行优先副本选举操作.先创建一个JSON文件,文件名可以任意:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">  &quot;partitions&quot;: [</span><br><span class="line">    &#123; </span><br><span class="line">        &quot;partition&quot;:0,</span><br><span class="line">        &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;partition&quot;:1,</span><br><span class="line">        &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;   &quot;partition&quot;:2,</span><br><span class="line">            &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;   &quot;partition&quot;:3,</span><br><span class="line">            &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;</span><br><span class="line">   ]</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>将上述内容保存为 <code>election.json</code> 文件.然后执行下列命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-preferred-replica-election.sh --zookeeper localhost:2181 --path-to-json-file ~/election.json</span><br><span class="line"> </span><br><span class="line">Created preferred replica election path with &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:0&#125;,&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:1&#125;,&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:2&#125;,&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:3&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic-demo,0], [topic-demo,1], [topic-demo,2], [topic-demo,3])</span><br></pre></td></tr></table></figure><p>提示优先副本选举成功.下列结果显示leader副本已经均衡分配到所有Broker节点了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 153,154,152</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>在实际生产环境中,建议使用这种方式来分批的执行优先副本选举操作.杜绝直接粗暴的进行所有分区的优先副本选举.另外,这类操作也应该需要避开业务高峰期,以免对性能造成负面影响,或者出现意外故障.</p><h3 id="4-2-2-分区重分配"><a href="#4-2-2-分区重分配" class="headerlink" title="4.2.2 分区重分配"></a>4.2.2 分区重分配</h3><p>当集群中一个Broker节点宕机,该节点的所有副本都处于丢失状态.kafka并不会自动将这些失效的分区副本自动迁移到集群其他broker节点.另外当集群中新增一台Broker节点时,只有新创建的主题分区才能被分配到这个节点上,而之前的主题分区并不会自动的加入到新节点(因为在创建时,并没有这个节点).这就导致新节点负载和原有节点负载之间严重不均衡.</p><p>为了解决这些问题,需要让分区副本再次进行合理的分配.也就是所谓的分区重分配.kafka提供了 <code>kafka-reassign-paritions.sh</code> 脚本执行分区重分配的工作.可以在集群节点失效或者扩容时使用.使用需要3个步骤:</p><ul><li>创建一个包含主题清单的JSON文件</li><li>根据主题清单和Broker节点清单生成一份重分配方案</li><li>执行具体重分配工作</li></ul><blockquote><p>要执行分区重分配,前提是broker节点清单数量要大于或者等于副本因子数量,否则会报错</p><p>Partitions reassignment failed due to replication factor: 3 larger than available brokers: 2</p></blockquote><p>下面创建一个4分区,2个副本因子的主题 <code>topic-reassign</code> 举例.假定要将152这个broker节点下线.下线之前需要将该节点上的分区副本迁移出去.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-reassign --replication-factor 2 --partitions 4</span><br><span class="line">Created topic &quot;topic-reassign&quot;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$  kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,153   Isr: 152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>第一步,创建一个JSON文件(文件名假定为reassign.json).文件内容是主题清单:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">    &quot;topics&quot;:[</span><br><span class="line">      &#123; </span><br><span class="line">                &quot;topic&quot;:&quot;topic-reassign&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;version&quot;:1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第二步,根据这个JSON文件和指定要分配的broker节点列表生成一份候选重分配方案:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --generate --topics-to-move-json-file ~/reassign.json --broker-list 153,154</span><br><span class="line"> </span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[153,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,153]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file project.json</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[153,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,153]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>在上面的例子中有以下几个参数:</p><p><code>--zookeeper</code> 这个参数已经非常熟悉了</p><p><code>--generate</code> 指令类型参数,类似于kafka-topics.sh脚本中的 <code>--create</code> , <code>--list</code> . <code>--describe</code> 等</p><p><code>--topics-to-move-json-file</code> 指定主题清单文件路径</p><p><code>--broker-list</code> 指定要分配的broker节点列表</p><p>上面的例子中打印了2个JSON格式内容:</p><p><code>Current partition replica assignment</code> 表示目前的分区副本分配情况,在执行分区重分配前最好备份这个内容,以便后续回滚操作</p><p><code>Proposed partition reassignment configuration</code> 表示候选重分配方案.这里只是一个方案,并没有真正执行.</p><p>将第二个Json内容格式化输出后,我们发现这个方案正如我们计划的那样,将该主题的所有分区下的AR副本集合分配到153和154节点,所有副本已经从即将要下线的152节点迁移走.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;:1,</span><br><span class="line">    &quot;partitions&quot;:[</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:1,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                153</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:0,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                153,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:3,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                153</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:2,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                153,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第三步,将 <code>Proposed partition reassignment configuration</code> JSON文件内容保存在一个文件中(假定为project.json).然后执行具体的重分配的动作,命令如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file project.json</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[153,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,153]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><blockquote><p>这里仍然打印了之前的副本分配方案,并且提示保存到JSON文件,以便回滚</p></blockquote><p>这里使用了2个不同的命令参数:</p><ul><li><code>--execute</code> 指令类型参数,执行重分配动作</li><li><code>--reassignment-json-file</code> 指定重分配方案文件路径</li></ul><p>再次查看 <code>topic-reassign</code> 主题分区副本分配情况,所有的副本都从152迁移出去,此时该节点可以顺利下线</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 153 Replicas: 154,153   Isr: 153,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>当然,我们也可以直接编写第二个JSON文件来自定义重分配方案,这样就不需要执行上面的第一步和第二步操作了.</p><p>分区重分配的基本原理是为每个分区添加新副本(增加副本数量),新副本会从leader副本复制所有的数据.复制完成后,控制器将旧副本从副本清单里移除.(恢复成原来的副本数量).</p><blockquote><p>所以,分区重分配需要确保有足够的空间,并且避免在业务高峰期操作</p></blockquote><p>从刚才的主题分区结果可以看到,大部分的分区leader副本都集中在153这个broker节点.这样负载非常不均衡,我们可以继续借助 <code>kafka-preferred-replica-election.sh</code> 脚本执行一次优先副本选举.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-preferred-replica-election.sh --zookeeper localhost:2181 --path-to-json-file election.json</span><br><span class="line"></span><br><span class="line">Created preferred replica election path with &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic-reassign,0], [topic-reassign,1], [topic-reassign,2], [topic-reassign,3])</span><br><span class="line"></span><br><span class="line">#选举完成后,副本分配均衡</span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 154 Replicas: 154,153   Isr: 153,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><blockquote><p>和优先副本选举一样,分区重分配对集群的性能有很大的影响.需要占用额外的磁盘,网络IO等资源.在生产环境执行操作时应该分批次执行.</p></blockquote><h3 id="4-2-3-复制限流"><a href="#4-2-3-复制限流" class="headerlink" title="4.2.3 复制限流"></a>4.2.3 复制限流</h3><p>我们了解分区重分配的本质在于数据复制,先增加新副本,进行数据同步,然后删除旧副本.如果副本数据量太大必然会占用很多额外的资源,从而影响集群整体性能.kafka有限流机制,可以对副本之间的复制流量进行限制.</p><p>副本复制限流有2种实现方式:</p><ul><li><code>kafka-config.sh</code> </li><li><code>kafka-reassign-partitions.sh</code> </li></ul><p>前者的实现方式有点繁琐,这里介绍后者的使用方式.</p><p><code>kafka-reassign-partitions.sh</code> 的实现方式非常简单,只需要一个throttle参数即可.例如上面的例子中副本都在153和154节点,现在继续使用分区重分配,让副本从153节点迁移到152节点.但是这次使用限流工具</p><p>首先,修改 <code>project.json</code> 文件,将153替换成152</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &apos;s/153/152/g&apos; project.json</span><br></pre></td></tr></table></figure><p>然后,执行分区重分配,这里使用 <code>--throttle</code> 参数,指定一个限流速度(单位是B/s)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file project.json --throttle 10</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,153]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[154,153]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[153,154]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value.</span><br><span class="line">The throttle limit was set to 10 B/s</span><br><span class="line">Successfully started reassignment of partitions.</span><br></pre></td></tr></table></figure><p>上面的示例输出中提示以下3点信息:</p><ol><li>需要周期性的使用 <code>--verify</code> 参数来周期性的查看副本复制进度,直到分区重分配完成,也就是说需要显示的使用这种方式确保分区重分配完成后解除限流的设置</li><li>限流的速度为10B/s</li><li>如果想要修改限流速度,重复此条执行命令,修改throttle的值即可</li></ol><p>接下来使用 <code>--verify</code> 参数查看复制进度.下面的示例显示复制已经完成,并且限流已被解除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --verify --reassignment-json-file project.json</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition [topic-reassign,1] completed successfully</span><br><span class="line">Reassignment of partition [topic-reassign,0] completed successfully</span><br><span class="line">Reassignment of partition [topic-reassign,3] completed successfully</span><br><span class="line">Reassignment of partition [topic-reassign,2] completed successfully</span><br><span class="line">Throttle was removed.</span><br></pre></td></tr></table></figure><p>此时153的副本已经被移除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 152 Replicas: 152,154   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 154 Replicas: 154,152   Isr: 154,152</span><br></pre></td></tr></table></figure><h3 id="4-2-4-修改副本因子"><a href="#4-2-4-修改副本因子" class="headerlink" title="4.2.4 修改副本因子"></a>4.2.4 修改副本因子</h3><p>上面的例子中分区重分配,将副本从一个broker节点中移除,此时kafka集群的broker节点数量只剩下2个.副本因子也只有2个.这里有个问题,此时153节点重启,或者新增broker节点后,如何将新增的broker节点加入进群,扩展副本数量呢?或者还有一种情况,当创建主题和分区后,想要修改副本因子呢?</p><p><code>kafka-reassign-parition.sh</code> 脚本同样实现了修改副本因子的功能..仔细观察一下分区重分配案例中的 <code>project.json</code> 文件内容:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ cat project.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;:1,</span><br><span class="line">    &quot;partitions&quot;:[</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:1,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                152</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:0,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                152,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:3,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                152</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:2,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                152,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>json文件中的副本集合(replicas)都是2个副本,我们可以很简单的添加一个副本.比如对于分区0而言,可以将153节点添加进去.(其他分区也是如此)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:1,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                152,</span><br><span class="line">                153</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br></pre></td></tr></table></figure><p>执行 <code>kafka-reassign-partition.sh</code> 脚本,执行命令的方法和参数和分区重分片几乎一致:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file add.json</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[152,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,154]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>查看副本分配情况.副本数量已经增加到了3个</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,154,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 154 Replicas: 153,154,152   Isr: 154,152,153</span><br></pre></td></tr></table></figure><blockquote><p>虽然副本因子增加到3个,但是Leader还是没有分配到新的153这个broker节点.此时可以通过优先副本选举重新分配</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-preferred-replica-election.sh --zookeeper localhost:2181 --path-to-json-file election.json</span><br><span class="line">Created preferred replica election path with &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic-reassign,0], [topic-reassign,1], [topic-reassign,2], [topic-reassign,3])</span><br><span class="line"></span><br><span class="line">#再次查看topic-reassign主题,分区副本分配均衡</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,154,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 153 Replicas: 153,154,152   Isr: 154,152,153</span><br></pre></td></tr></table></figure><p>重点: <strong>与修改分区数量不同,副本数还可以减少</strong>,修改方法和命令几乎一样,只需要编辑JSON配置文件即可.这里就不再演示</p><h3 id="4-2-5-如何选择合适的分区数量"><a href="#4-2-5-如何选择合适的分区数量" class="headerlink" title="4.2.5 如何选择合适的分区数量"></a>4.2.5 如何选择合适的分区数量</h3><p>如何选择合适的分区数量是需要经常面对的问题,但是这个问题似乎并没有权威的标准答案,需要根据实际的业务场景,硬件资源,应用软件,负载等情况做具体考量.这一章节主要介绍与本问题相关的一些决策因素,以供参考</p><h4 id="4-2-5-1-性能测试工具"><a href="#4-2-5-1-性能测试工具" class="headerlink" title="4.2.5.1 性能测试工具"></a>4.2.5.1 性能测试工具</h4><p>在生产环境中设定分区数量需要考虑性能因素.所以性能测试工具必不可少,kafka本身提供了用于生产者性能测试的 <code>kafka-producer-pref-test.sh</code> 脚本和用于消费者性能测试的 <code>kafka-consumer-perf-test.sh</code>脚本</p><p>首先创建一个用于测试的分区为1,副本为1的 <code>topic-1</code> 的主题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Topic:topic-1   PartitionCount:1    ReplicationFactor:1 Configs:</span><br><span class="line">    Topic: topic-1  Partition: 0    Leader: 153 Replicas: 153   Isr: 153</span><br></pre></td></tr></table></figure><p>其次.我们往这个主题发送100万条消息,并且每条消息大小为1024B,生产者对应的acks参数为1:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-producer-perf-test.sh --topic topic-1 --num-records 1000000 --record-size 1024 --throughput -1 --producer-props bootstrap.servers=localhost:9092 acks=1</span><br><span class="line">76666 records sent, 15333.2 records/sec (14.97 MB/sec), 1517.5 ms avg latency, 2061.0 max latency.</span><br><span class="line">119400 records sent, 23880.0 records/sec (23.32 MB/sec), 1353.6 ms avg latency, 1631.0 max latency.</span><br><span class="line">124560 records sent, 24912.0 records/sec (24.33 MB/sec), 1231.2 ms avg latency, 1375.0 max latency.</span><br><span class="line">146520 records sent, 29304.0 records/sec (28.62 MB/sec), 1066.6 ms avg latency, 1146.0 max latency.</span><br><span class="line">156795 records sent, 31359.0 records/sec (30.62 MB/sec), 972.3 ms avg latency, 1051.0 max latency.</span><br><span class="line">133365 records sent, 26673.0 records/sec (26.05 MB/sec), 1141.1 ms avg latency, 1322.0 max latency.</span><br><span class="line">159945 records sent, 31989.0 records/sec (31.24 MB/sec), 964.3 ms avg latency, 1178.0 max latency.</span><br><span class="line">1000000 records sent, 26148.576210 records/sec (25.54 MB/sec), 1143.54 ms avg latency, 2061.00 ms max latency, 1114 ms 50th, 1654 ms 95th, 1869 ms 99th, 2036 ms 99.9th.</span><br></pre></td></tr></table></figure><p>示例中使用了多个参数:</p><p><code>num-records</code>:  指定发送消息的总条数</p><p><code>record-size</code>: 设置每条消息的字节数</p><p><code>throughtput</code> : 限流控制,-1表示不限流,大于0表示限流值</p><p><code>producer-props</code> : 指定生产者的配置,可以同时指定多组配置</p><p>除此之外还有其他参数,比如 <code>print-metrics</code> 在测试完成之后,打印很多指标信息.有兴趣可以执行 <code>--help</code> 查看更多参数信息.</p><p>回过头再看看上面示例中的压测结果信息,以第一条和最后一条为例:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">76666 records sent, 15333.2 records/sec (14.97 MB/sec), 1517.5 ms avg latency, 2061.0 max latency.</span><br><span class="line">1114 ms 50th, 1654 ms 95th, 1869 ms 99th, 2036 ms 99.9th</span><br></pre></td></tr></table></figure><p><strong>records sent: 表示发送的消息综述</strong></p><p><strong>records/sec: 吞吐量,表示每秒发送的消息数量</strong></p><p><strong>MB/sec: 吞吐量,表示每秒发送的消息大小</strong></p><p><strong>avg latency: 表示消息处理的平均耗时</strong></p><p><strong>max latency: 表示消息处理的最大耗时</strong></p><p><strong>50th,95th,99th,99.th</strong> 表示50%,95%,99%,99.9%时消息处理耗时</p><p><strong></strong></p><p>消费者压测工具的脚本使用也比较简单,下面的简单实例演示了消费主题 <code>topic-1</code> 中的100万条消息.命令使用方法:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-consumer-perf-test.sh --topic topic-1 --messages 1000000 --broker-list localhost:9092</span><br><span class="line">start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec</span><br><span class="line">2020-12-26 14:07:29:612, 2020-12-26 14:07:35:272, 977.0264, 172.6195, 1000475, 176762.3675</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p><strong>data.consumed.in.MB: 消费的消息总量,单位为MB</strong></p><p><strong>MB.sec</strong>: 按字节大小计算的消费吞吐量(单位:MB/s)</p><p><strong>data.consumed.in.nMsg</strong>: 消费的消息消息总数</p><p><strong>nMsg.sec</strong>: 按消息个数计算的吞独量(单位n/s)</p><blockquote><p>可以创建多个分区,比如10,100,200,500等(副本数量都为1)来测试生产和消费的性能表现,</p></blockquote><h4 id="4-2-5-2-分区数量越多不代表吞独量越高"><a href="#4-2-5-2-分区数量越多不代表吞独量越高" class="headerlink" title="4.2.5.2 分区数量越多不代表吞独量越高"></a>4.2.5.2 分区数量越多不代表吞独量越高</h4><p>消息中间件的性能一般是指吞吐量(还包括延迟),吞吐量会受到硬件资源,消息大小,消息压缩,消息发送方式(同步,异步),副本因子等参数影响.分区数量越多不一定吞吐量越高,超过一定的临界值后,kafka的吞吐量会不升反降.</p><h4 id="4-2-5-3-分区数量的上限"><a href="#4-2-5-3-分区数量的上限" class="headerlink" title="4.2.5.3 分区数量的上限"></a>4.2.5.3 分区数量的上限</h4><p>一味的增加分区数量并不能使吞吐量得到提升,并且分区的数量也不能一直增加,如果超过一定的临界值还会引起kafka进程的崩溃.</p><p><strong>每次创建一个分区,都会消耗一个Linux系统的文件描述符.</strong></p><p>通过kafka的pid编号,可以查看当前kafka进程占用的文件描述符数量:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ ls /proc/22858/fd/ | wc -l</span><br><span class="line">173</span><br></pre></td></tr></table></figure><p>此时创建一个分区数量为400个的topic-demo4的主题.由于分区会平均创建在集群内的3个broker节点,所以需要统计一下152这个本地节点的分区数量.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics --describe --topic topic-demo4 | grep -Eo &quot;Leader:\s[0-9]+&quot; | sort | uniq -c</span><br><span class="line">    134 Leader: 152</span><br><span class="line">    133 Leader: 153</span><br><span class="line">    133 Leader: 154</span><br></pre></td></tr></table></figure><p>可以看到152这个节点创建了134个分区.接下来看看系统文件描述符的数量.正好增加了134个文件描述符</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ ls /proc/22858/fd/ | wc -l</span><br><span class="line">307</span><br></pre></td></tr></table></figure><p>可以想见的是,一旦分区数量超过了操作系统规定的文件描述符上限,kafka进程就会崩溃</p><h4 id="4-2-5-4-分区数量的考量"><a href="#4-2-5-4-分区数量的考量" class="headerlink" title="4.2.5.4 分区数量的考量"></a>4.2.5.4 分区数量的考量</h4><p>如何选择合适的分区数量,一个恰当的答案就是视具体情况而定.</p><p>从吞吐量方面考虑,增加合适的分区数量可以在一定程度上提升整体吞吐量,但是超过临界值之后吞吐量不升反降.在投入生产环境之前,应该对吞吐量进行相关的测试,以找到合适的分区数量</p><p>分区数量太多会影响系统可用性,当broker发生故障时,broker节点上的所有分区的leader副本不可用,此时如果有大量的分区要进行leader角色切换,这个切换的过程会耗费相当的时间,并且这个时间段内分区会变的不可用.并且分区数量太多不仅为增加日志清理的耗时,而且在被删除时也会消费更多时间.</p><p>一个好的建议是,创建主题之前对分区数量性能进行充分压测,在创建主题之后,还需要对其进行追踪,监控,调优.如果分区数量较少,还能通过增加分区数量,或者增加broker进行分区重分配等改进.</p><p>最后,一个通用的准则是,建议分区数量设定为集群中broker的倍数,例如集群中有3个broker节点,可以设定分区数为3,6,9等.</p><h3 id="4-2-6-总结"><a href="#4-2-6-总结" class="headerlink" title="4.2.6 总结"></a>4.2.6 总结</h3><p><code>kafka-topics.sh</code> 查看,创建主题分区,副本</p><p><code>kafka-configs.sh</code> 修改主题配置文件</p><p><code>kafka_perferred-replica-elections.sh</code> 优先副本选举</p><p><code>kafka-reassign-partitions.sh</code> 分区重分配,副本复制限流,修改副本因子数量</p><p><code>kafka-producer-perf-test.sh</code> 生产者分区数和吞吐量性能压测</p><p><code>kafka-consumer-perf-test.sh</code> 消费者性能压测</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;4-2-1-优选副本的选举&quot;&gt;&lt;a href=&quot;#4-2-1-优选副本的选举&quot; class=&quot;headerlink&quot; title=&quot;4.2.1 优选副本的选举&quot;&gt;&lt;/a&gt;4.2.1 优选副本的选举&lt;/h3&gt;&lt;h4 id=&quot;4-2-1-1-什么是优先副本&quot;&gt;&lt;a href=&quot;#4-2-1-1-什么是优先副本&quot; class=&quot;headerlink&quot; title=&quot;4.2.1.1 什么是优先副本&quot;&gt;&lt;/a&gt;4.2.1.1 什么是优先副本&lt;/h4&gt;&lt;p&gt;分区使用多副本机制来提升可靠性,但是只有leader副本对外提供读写服务.而follower副本只负责在内部进行消息的同步.如果一个分区的leader副本不可用,那么就意味着整个分区变得不可用.此时就需要从剩余的follower副本中挑选一个新的leader副本继续对外提供服务.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;broker节点中的Leader副本个数决定了这个节点负载的高低&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在创建主题的时候,主题的分区和副本会尽可能的均匀分布在kafka集群的各个broker节点.对应的Leader副本的分配也比较均匀.例如下面的 &lt;code&gt;topic-demo&lt;/code&gt; 主题:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev152 ~]$&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="4-主题和分区" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-4.1主题管理</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/4.1%E4%B8%BB%E9%A2%98%E7%AE%A1%E7%90%86/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/4-主题和分区/4.1主题管理/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:55:25.320Z</updated>
    
    <content type="html"><![CDATA[<h3 id="4-1-1-创建主题"><a href="#4-1-1-创建主题" class="headerlink" title="4.1.1 创建主题"></a>4.1.1 创建主题</h3><h4 id="4-1-1-1-自动创建主题以及分区副本"><a href="#4-1-1-1-自动创建主题以及分区副本" class="headerlink" title="4.1.1.1 自动创建主题以及分区副本"></a>4.1.1.1 自动创建主题以及分区副本</h4><p>在之前的笔记中提到了创建主题的一个简单示例.kafka提供 <code>kafka-topics.sh</code> 脚本来创建主题.下面这个示例创建了一个 <code>topic-test</code> 的主题,包含4个分区和2个副本.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-test --replication-factor 2 --partitions 4</span><br><span class="line"></span><br><span class="line">Created topic &quot;topic-test&quot;.</span><br></pre></td></tr></table></figure><p>分区创建完成后,会在kafka的 <code>log.dirs</code> 或者 <code>log.dir</code> 的目录下创建相应的主题分区.下面是在其中一台Broker节点的信息展示:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ ls /opt/logs/kafka/ | grep &quot;topic-test&quot;</span><br><span class="line">topic-test-0</span><br><span class="line">topic-test-2</span><br></pre></td></tr></table></figure><p>可以看到152节点中创建了2个文件夹 topic-test-0 和 topic-test-2,对应主题 topic-test的2个分区编号为0和2的分区，命名方式可以概括为 <code>&lt;topic&gt;-&lt;partition&gt;</code> .严谨地说,其实这类文件夹对应的不是分区,分区同主题一样是一个逻辑的概念而没有物理上的存在.并且这里我们也只是看到了2个分区,而我们创建的是4个分区,其余2个分区被分配到了153和154节点中，参考如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#153节点</span><br><span class="line">[hadoop@bi-dev153 ~]$ ls /opt/logs/kafka/ | grep &quot;topic-test&quot;</span><br><span class="line">topic-test-0</span><br><span class="line">topic-test-1</span><br><span class="line">topic-test-3</span><br><span class="line"></span><br><span class="line">#154节点</span><br><span class="line">[hadoop@bi-dev154 ~]$ ls /opt/logs/kafka/ | grep &quot;topic-test&quot;</span><br><span class="line">topic-test-1</span><br><span class="line">topic-test-2</span><br><span class="line">topic-test-3</span><br></pre></td></tr></table></figure><p>三个broker节点一共创建了8个文件夹,这个数字8实质上是分区数4与副本因子2的乘积.每个副本(或者更确切地说应该是日志,副本与日志一一对应)才真正对应 了一个命名形式.</p><a id="more"></a> <p><strong>主题</strong>,<strong>分区,副本和日志</strong>的关系如下图所示.<strong>主题</strong>和<strong>分区</strong>是提供给上层用户的抽象,而在副本层面(或者更确切的说是Log日志层面)才会实际物理存在.</p><p>同一个分区中的多个副本必须分布在不同broker中,并且一个分区副本同时存在多个broker中,这样才能提供有效的数据冗余.上面的示例中,每个副本都分布在至少2台不同的broker中.</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608631181435-40a87763-f28c-45c4-a25e-d85651c26d2e.png" alt="image.png"></p><h4 id="4-1-1-2-手动创建主题以及分区副本"><a href="#4-1-1-2-手动创建主题以及分区副本" class="headerlink" title="4.1.1.2 手动创建主题以及分区副本"></a>4.1.1.2 手动创建主题以及分区副本</h4><p>通过 <code>kafka-topics.sh</code> 脚本创建的主题会按照内部既定逻辑来分配分区和副本到Broker节点上.其实该脚本还提供一个 <code>replica-assignment</code> 参数来手动指定分区副本的分配方案.用法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">格式为: 分区1broker节点1:分区1broker节点2,分区2broker节点1:分区2broker节点2.副本集合用冒号隔开,分区之间用逗号隔开</span><br><span class="line">--replica-assignment broker_id_for_partition1_replica1:broker_id_for_partition1_replica2,broker_id_for_partition2_replica1:broker_id_for_partition2_replica2.......</span><br></pre></td></tr></table></figure><p>例如下面这个实例通过手动方式创建了一个和 <code>topic-test</code> 一样分区副本分配的 <code>topic-test-same</code> 主题.</p><p>下面是刚创建的自动分配的topic-test主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic &quot;topic-test&quot;</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br></pre></td></tr></table></figure><p>通过 <code>--replica-assignment</code> 手动指定分区副本分配情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-test-same --replica-assignment 153:152,154:153,152:154,153:154</span><br></pre></td></tr></table></figure><blockquote><p>–replica-assignment参数其实就是逗号隔开的所有分区的Replicas副本集合.副本集合内部用:冒号隔开</p></blockquote><p>查看 <code>topic-test-same</code> 分区信息.和 <code>topic-test</code> 主题分区副本分配一致</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic &quot;topic-test-same&quot;</span><br><span class="line">Topic:topic-test-same   PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test-same  Partition: 0    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">    Topic: topic-test-same  Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test-same  Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test-same  Partition: 3    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br></pre></td></tr></table></figure><p>手动分配分区副本需要遵循以下原则,否则会报错:</p><ul><li>同一个分区内的副本不能有重复.比如153:153</li><li>分区之间所指定的副本数量要相同.比如153:154,152,154:152</li><li>不能跳过一个分区.比如153:154,,154:152</li></ul><h4 id="4-1-1-3-自定义相关参数"><a href="#4-1-1-3-自定义相关参数" class="headerlink" title="4.1.1.3 自定义相关参数"></a>4.1.1.3 自定义相关参数</h4><p>在创建主题时,还可以通过 <code>config</code> 参数设置要创建主题的相关参数.可以覆盖原本的默认配置参数. <code>config</code> 可以指定多个参数.用法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--config 参数名=值 --config 参数名=值 ......</span><br></pre></td></tr></table></figure><p>下面示例使用 <code>config</code> 参数创建主题 <code>topic-config</code>.并且携带2个参数 :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-config --replication-factor 1 --partitions 1 --config cleanup.policy=compact --config max.message.bytes=10000</span><br><span class="line">Created topic &quot;topic-config&quot;.</span><br></pre></td></tr></table></figure><p>查看主题信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:1    ReplicationFactor:1 Configs:cleanup.policy=compact,max.message.bytes=10000</span><br><span class="line">    Topic: topic-config Partition: 0    Leader: 154 Replicas: 154   Isr: 154</span><br></pre></td></tr></table></figure><p>通过zk也能查看到config信息,config信息保存在 <code>/config/topics/TOPIC_NAME</code> 目录下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] get /config/topics/topic-config</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;config&quot;:&#123;&quot;max.message.bytes&quot;:&quot;10000&quot;,&quot;cleanup.policy&quot;:&quot;compact&quot;&#125;&#125;</span><br></pre></td></tr></table></figure><h4 id="4-1-1-4-总结"><a href="#4-1-1-4-总结" class="headerlink" title="4.1.1.4 总结"></a>4.1.1.4 总结</h4><p>创建主题时需要遵循几个原则</p><ul><li>主题名不能重复,否则会报错.(使用 <code>if-not-exists</code> 参数可以避免出现报错信息,但是不会成功创建一个同名主题)</li><li>主题名不推荐使用__双下划线开头的命名,双下划线开头主题一般看做Kafka的内部主题</li><li>主题名由大小写祖母,数字,点号,下划线,连接线等组成.不能只有特殊符号</li></ul><p><code>kafka-topics.sh</code> 创建主题信息支持以下参数:</p><ul><li><p><code>--create</code> 创建主题</p></li><li><ul><li><code>--replica-assignment</code> 手动创建主题的分区副本分配</li><li><code>--config</code> 手动指定参数</li></ul></li></ul><h3 id="4-1-2-查看主题的分区和副本信息"><a href="#4-1-2-查看主题的分区和副本信息" class="headerlink" title="4.1.2 查看主题的分区和副本信息"></a>4.1.2 查看主题的分区和副本信息</h3><h4 id="4-1-2-1-查看具体某个topic的信息"><a href="#4-1-2-1-查看具体某个topic的信息" class="headerlink" title="4.1.2.1.查看具体某个topic的信息"></a>4.1.2.1.查看具体某个topic的信息</h4><p><code>kafka-topics.sh</code> 脚本提供了 <code>--describe</code> 参数来查看一个topic的信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic &quot;topic-test&quot;</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br></pre></td></tr></table></figure><p>在上面的示例中,命令行提供了以下几个信息:</p><p>一共有3个broker节点:152,153,154</p><p><code>PartitionCount</code> 表示一共有3个分区</p><p><code>ReplicationFactor</code> 副本因子为2</p><p><code>Leader</code> 表示某个分区对应的leader副本在具体的Broker节点</p><p><code>Replicas</code> 表示分区内所有AR副本的集合</p><p><code>Isr</code> 表示ISR副本集合</p><h4 id="4-1-2-2-查看所有topic的信息"><a href="#4-1-2-2-查看所有topic的信息" class="headerlink" title="4.1.2.2 查看所有topic的信息"></a>4.1.2.2 查看所有topic的信息</h4><p>如果 <code>kafka-topics.sh</code> 脚本没有指定具体的 <code>--topic</code> 字段.则会展示所有的topic主题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe | head</span><br><span class="line">Topic:__consumer_offsets    PartitionCount:50   ReplicationFactor:1 Configs:segment.bytes=104857600,cleanup.policy=compact,compression.type=producer</span><br><span class="line">    Topic: __consumer_offsets   Partition: 0    Leader: 153 Replicas: 153   Isr: 153</span><br><span class="line">    Topic: __consumer_offsets   Partition: 1    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: __consumer_offsets   Partition: 2    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">    Topic: __consumer_offsets   Partition: 3    Leader: 153 Replicas: 153   Isr: 153</span><br><span class="line">    Topic: __consumer_offsets   Partition: 4    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: __consumer_offsets   Partition: 5    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">    Topic: __consumer_offsets   Partition: 6    Leader: 153 Replicas: 153   Isr: 153</span><br><span class="line">    Topic: __consumer_offsets   Partition: 7    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: __consumer_offsets   Partition: 8    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">  .....略.......</span><br></pre></td></tr></table></figure><h4 id="4-1-2-3-zookeeper查看topic信息"><a href="#4-1-2-3-zookeeper查看topic信息" class="headerlink" title="4.1.2.3 zookeeper查看topic信息"></a>4.1.2.3 zookeeper查看topic信息</h4><p>zookeeper提供了 <code>zkCli.sh</code> 客户端.使用客户端连接zookeeper:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/zookeeper-3.4.10/bin/zkCli.sh  -server localhost:2181</span><br><span class="line">[zk: localhost:2181(CONNECTED) 0]</span><br></pre></td></tr></table></figure><p>zookeeer的 <code>/brokers/topics</code> 目录下保存了主题的分区副本分片方案.通过查看这个目录即可查看主题的分区和副本信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] get /brokers/topics/topic-test</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:&#123;&quot;2&quot;:[152,154],&quot;1&quot;:[154,153],&quot;3&quot;:[153,154],&quot;0&quot;:[153,152]&#125;&#125;</span><br></pre></td></tr></table></figure><p>如上示例所示, <code>&quot;2&quot;:[152,154]</code> 表示分区2分配了2个副本,分别在152和153这2个broker节点上.</p><h4 id="4-1-2-4-查看kafka集群当前所有主题"><a href="#4-1-2-4-查看kafka集群当前所有主题" class="headerlink" title="4.1.2.4 查看kafka集群当前所有主题"></a>4.1.2.4 查看kafka集群当前所有主题</h4><p><code>--list</code> 参数可以列出当前的所有topic</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --list</span><br><span class="line">__consumer_offsets</span><br><span class="line">delivery_message</span><br><span class="line">example</span><br><span class="line">example1</span><br><span class="line">goods_center</span><br><span class="line">hsq-aliapp</span><br><span class="line">hsq-wxapp</span><br><span class="line">hsq_online_test</span><br><span class="line">monitor_report_app_log</span><br><span class="line">sample_consumer_dlq</span><br><span class="line">tidb_test</span><br><span class="line">topic-config</span><br><span class="line">topic-demo</span><br><span class="line">topic-demo1</span><br><span class="line">topic-test</span><br><span class="line">topic-test-same</span><br></pre></td></tr></table></figure><h4 id="4-1-2-5-查看主题其他详细信息"><a href="#4-1-2-5-查看主题其他详细信息" class="headerlink" title="4.1.2.5 查看主题其他详细信息"></a>4.1.2.5 查看主题其他详细信息</h4><p><code>kafka-topics.sh</code> 脚本的 <code>describe</code> 参数还支持很多额外的指令,用于查看更详细的信息.</p><p>1.<strong><code>--topics-with-overrides</code></strong> 参数表示查看覆盖配置的主题,列出包含了与集群不一样配置的主题.下面列出了 <code>topic-config</code> 这个主题,这个主题使用了 <code>--config</code> 参数创建</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topics-with-overrides</span><br><span class="line"></span><br><span class="line">Topic:topic-config  PartitionCount:1    ReplicationFactor:1 Configs:cleanup.policy=compact,max.message.bytes=10000</span><br></pre></td></tr></table></figure><p>2.<strong><code>--under-replicated-paritions</code></strong> 参数列出包含失效副本的分区.失效副本的分区可能正在进行同步操作,也有可能同步发生异常.此时分区的ISR集合小于AR集合.失效副本的分区是重点监控对象,因为这可能意味着集群中的某个broker已经失效或者同步效率降低等.</p><p>正常情况下此命令不会出现任何信息.例如查看主题 <code>topic-demo</code> 的失效副本信息,但是没有任何输出信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo --under-replicated-partitions</span><br></pre></td></tr></table></figure><p>此时将153这个节点下线.再次查看:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo --under-replicated-partitions</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 152,154</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 154 Replicas: 153,154,152   Isr: 154,152</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 154,152</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 152,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>可以看到Leader和ISR集合中都没有了153这个节点.将153节点上线.此时再次查询,恢复正常.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo --under-replicated-partitions</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>\3. <strong><code>unavailable-partitions</code></strong> 参数可以查看主题中没有leader副本的分区.这些分区已经处于离线状态,对于生产者或者消费者来说不可用.</p><p>同样正常情况下,该命令没有展示任何信息.</p><p>例如,下面的 <code>topic-test</code> 主题有4个分区,每个分区有2个副本.其中分区1和分区3的副本ISR是153和154这2个节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 153 Replicas: 153,152   Isr: 152,153</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: 153 Replicas: 153,154   Isr: 154,153</span><br></pre></td></tr></table></figure><p>现在停掉153和154这2个节点的kafka进程.使用 <code>unavailable-partitions</code> 参数查看分区信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test --unavailable-partitions</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: -1  Replicas: 154,153   Isr: 154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: -1  Replicas: 153,154   Isr: 154</span><br><span class="line">  </span><br><span class="line">  [hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 152 Replicas: 153,152   Isr: 152</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: -1  Replicas: 154,153   Isr: 154</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: -1  Replicas: 153,154   Isr: 154</span><br></pre></td></tr></table></figure><p>leader显示为-1,表示没有可用leader</p><p>节点恢复后,再次执行该命令,没有任何显示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test --unavailable-partitions</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><h4 id="4-1-2-6-总结"><a href="#4-1-2-6-总结" class="headerlink" title="4.1.2.6 总结"></a>4.1.2.6 总结</h4><p><code>kafka-topics.sh</code> 查看主题信息支持以下参数:</p><ul><li><p><code>--describe</code> </p></li><li><ul><li>默认展示所有topic的分区副本信息</li><li><code>--topic TOPIC_NAME</code> 展示具体某个topic主题的分区副本信息</li><li><code>--topics-with-overrides</code> 列出覆盖配置参数的主题</li><li><code>--under-replicated-partitions</code> 列出失效副本的主题分区信息</li><li><code>--unavailable-partitions</code> 列出没有副本的主题分区</li></ul></li><li><p><code>--list</code> 列出kafka集群下的所有topic主题名称</p></li></ul><h3 id="4-1-3-修改主题"><a href="#4-1-3-修改主题" class="headerlink" title="4.1.3 修改主题"></a>4.1.3 修改主题</h3><h4 id="4-1-3-1-修改主题分区数量"><a href="#4-1-3-1-修改主题分区数量" class="headerlink" title="4.1.3.1 修改主题分区数量"></a>4.1.3.1 修改主题分区数量</h4><p>当一个主题被修改后,依然允许我们对其做一定的修改,比如修改分区个数,修改配置等.这个功能就是 <code>kafka-topic.sh</code> 脚本中的 <code>alter</code> 指令提供的.</p><p>以 <code>topic-config</code> 主题为例,该主题下只有一个分区.将分区修改为3:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --partitions 3</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Adding partitions succeeded!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:3    ReplicationFactor:1 Configs:cleanup.policy=compact,max.message.bytes=10000</span><br><span class="line">    Topic: topic-config Partition: 0    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: topic-config Partition: 1    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">    Topic: topic-config Partition: 2    Leader: 153 Replicas: 153   Isr: 153</span><br></pre></td></tr></table></figure><p><code>--partition</code> 参数表示扩展后的分区个数.</p><blockquote><p>注意告警信息.如果主题中的消息包含key(key不为Null)时,根据key计算分区的行为就会受到影响.当分区数为1时,所以key的消息都会发送到这个分区.当分区扩展到3,会根据消息的key来计算区号.原本发往分区0的消息可能会发送到分区1或者2.此外,还会影响既定消息的顺序.</p></blockquote><p>对于基于key计算的主题,不建议修改分区数量.在一开始就设置好分区数量.另外需要注意的是,Kafka不支持减少分区.只能增加不能减少.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --partitions 1</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line"></span><br><span class="line">Error while executing topic command : The number of partitions for a topic can only be increased</span><br></pre></td></tr></table></figure><blockquote><p>不支持减少分区主要是考虑到保障kafka的消息可靠性和顺序性,事务性问题.</p></blockquote><p>如果修改一个不存在的主题分区,则会报错.添加 <code>--if-exists</code> 参数会忽略一些异常</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-none-exist  --partitions 3</span><br><span class="line">Error while executing topic command : Topic topic-none-exist does not exist on ZK path localhost:2181</span><br><span class="line"></span><br><span class="line">#使用--if-exists参数,没有报错,但是不会产生任何效果</span><br><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-none-exist  --if-exists --partitions 3</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><h4 id=""><a href="#" class="headerlink" title=" "></a> </h4><h4 id="4-1-3-2-修改主题配置"><a href="#4-1-3-2-修改主题配置" class="headerlink" title="4.1.3.2 修改主题配置"></a>4.1.3.2 修改主题配置</h4><p>还可以使用 <code>kafka-topics.sh</code> 脚本的 <code>alter</code> 指令修改主题的配置.在创建主题的时候通过 <code>config</code> 参数来设置要创建的主题相关参数.在创建完主题之后,还可以通过 <code>alter</code> 和 <code>config</code> 配合增加或者修改一些配置文件覆盖原有的值</p><p>下面例子演示修改主题 <code>topic-config</code> 的 <code>max.message.bytes</code> 配置.从10000修改到20000</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --config max.message.bytes=20000</span><br><span class="line">WARNING: Altering topic configuration from this script has been deprecated and may be removed in future releases.</span><br><span class="line">         Going forward, please use kafka-configs.sh for this functionality</span><br><span class="line">Updated config for topic &quot;topic-config&quot;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:3    ReplicationFactor:1 Configs:max.message.bytes=20000,cleanup.policy=compact</span><br></pre></td></tr></table></figure><p>通过 <code>alter</code> 也可以删除创建主题时候的自定义配置.使用 <code>--delete-config</code> 参数.下面这个例子中删除了 <code>max.message.bytes</code> 配置.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --delete-config max.message.bytes</span><br><span class="line">WARNING: Altering topic configuration from this script has been deprecated and may be removed in future releases.</span><br><span class="line">         Going forward, please use kafka-configs.sh for this functionality</span><br><span class="line">Updated config for topic &quot;topic-config&quot;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:3    ReplicationFactor:1 Configs:cleanup.policy=compact</span><br></pre></td></tr></table></figure><blockquote><p>注意.在对config配置进行增删改查时候,都会提示建议使用kafka-configs.sh这个脚本来实现.该脚本的使用方式下面马上讲到</p></blockquote><h3 id="4-1-4-配置管理"><a href="#4-1-4-配置管理" class="headerlink" title="4.1.4 配置管理"></a>4.1.4 配置管理</h3><p><code>kafka-configs.sh</code> 脚本专门用来对配置进行操作.可以在运行状态下动态更改配置.也可以查询主题的相关配置.而且该脚本不仅可以支持主题相关配置修改,还可以修改broker,用户和客户端这3个类型的配置</p><p><code>kafka-configs.sh</code> 脚本使用 <code>entity-type</code> 参数指定操作配置的类型, <code>entity-name</code> 参数指定操作配置的名称.</p><h4 id="4-1-4-1-查询配置"><a href="#4-1-4-1-查询配置" class="headerlink" title="4.1.4.1 查询配置"></a>4.1.4.1 查询配置</h4><p>下面这个例子查看主题 <code>topic-config</code> 的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics --entity-name topic-config</span><br><span class="line">Configs for topic &apos;topic-config&apos; are cleanup.policy=compact</span><br></pre></td></tr></table></figure><p><code>--entity-type</code> 指定查看的实体类型.支持以下几种类型:</p><ul><li>topics</li><li>clients</li><li>users</li><li>brokers</li></ul><p><code>--entity-name</code> 配置的实体名称:</p><ul><li>topic name (主题名称)</li><li>client id (客户端ID)</li><li>user principal name (用户名)</li><li>broker id (kafka节点ID)</li></ul><p>如果不指定 <code>--entity-name</code> 参数则会查询所有的 <code>entity-type</code> 对应的所有配置信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics</span><br><span class="line">Configs for topic &apos;topic-config&apos; are</span><br><span class="line">Configs for topic &apos;__consumer_offsets&apos; are segment.bytes=104857600,cleanup.policy=compact,compression.type=producer</span><br><span class="line">......</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>通过zookeeper也可以查询主题的配置信息.路径为 <code>/config/topics/TOPIC_NAME</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] get /config/topics/topic-config</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;config&quot;:&#123;&quot;cleanup.policy&quot;:&quot;compact&quot;,&quot;max.message.bytes&quot;:&quot;20000&quot;&#125;&#125;</span><br></pre></td></tr></table></figure><h4 id="4-1-4-2-修改配置"><a href="#4-1-4-2-修改配置" class="headerlink" title="4.1.4.2 修改配置"></a>4.1.4.2 修改配置</h4><p>使用 <code>alter</code> 对配置进行变更.需要配合 <code>add-config</code> 或者 <code>delete-config</code> 这2个参数一起使用.</p><p><code>add-config</code> 参数实现配置的增,改</p><p>下面的例子中,为主题 <code>topic-config</code> 添加 <code>max.message.bytes</code> 参数配置和 <code>cleanup.policy</code> 参数配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name topic-config --add-config cleanup.policy=compact,max.message.bytes=20000</span><br><span class="line">Completed Updating config for entity: topic &apos;topic-config&apos;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics --entity-name topic-config</span><br><span class="line">Configs for topic &apos;topic-config&apos; are cleanup.policy=compact,max.message.bytes=20000</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p><code>delete-config</code> 参数可以实现配置删除.</p><p>下面的例子中,删除上面的2个配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name topic-config --delete-config cleanup.policy,max.message.bytes</span><br><span class="line">Completed Updating config for entity: topic &apos;topic-config&apos;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics --entity-name topic-config</span><br><span class="line">Configs for topic &apos;topic-config&apos; are</span><br></pre></td></tr></table></figure><h3 id="4-1-5-删除主题"><a href="#4-1-5-删除主题" class="headerlink" title="4.1.5 删除主题"></a>4.1.5 删除主题</h3><p>如果确定不再使用一个主题,那么最好的方式是将其删除.这样可以释放一些资源,比如磁盘,文件句柄等. <code>kafka-topics.sh</code> 脚本中的 <code>delete</code> 命令可以用来删除主题.比如下面删除主题 <code>topic-demo1</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --delete --topic topic-demo1</span><br><span class="line">Topic topic-demo1 is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br></pre></td></tr></table></figure><blockquote><p>注意.必须将kafka服务器配置文件的delete.topic.enable选项设置为true才能删除.这个参数的默认值是false.删除主题的操作会被忽略.主题并没有被删除</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --list | grep topic-demo1</span><br><span class="line">topic-demo1</span><br></pre></td></tr></table></figure><p>编辑配置文件 <code>/opt/kafka/config/server.properties</code> 修改下面的参数为true</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Switch to enable topic deletion or not, default value is false</span><br><span class="line">delete.topic.enable=true</span><br></pre></td></tr></table></figure><p>如果删除一个kafka的内部主题,那么会报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --delete --topic __consumer_offsets</span><br><span class="line">Error while executing topic command : Topic __consumer_offsets is a kafka internal topic and is not allowed to be marked for deletion.</span><br></pre></td></tr></table></figure><p>删除一个不存在的主题也会报错,此时可以通过 <code>if-exists</code> 参数来忽略异常.</p><h3 id="4-1-5-总结"><a href="#4-1-5-总结" class="headerlink" title="4.1.5 总结"></a>4.1.5 总结</h3><p>下面这张图是 <code>kafka-topics.sh</code> 脚本的常用参数</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608805092662-042718a3-9a6a-489e-a987-db4e38217171.png" alt="image.png"></p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608805117734-2bd94456-e966-41b1-9465-dde20a8a9129.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;4-1-1-创建主题&quot;&gt;&lt;a href=&quot;#4-1-1-创建主题&quot; class=&quot;headerlink&quot; title=&quot;4.1.1 创建主题&quot;&gt;&lt;/a&gt;4.1.1 创建主题&lt;/h3&gt;&lt;h4 id=&quot;4-1-1-1-自动创建主题以及分区副本&quot;&gt;&lt;a href=&quot;#4-1-1-1-自动创建主题以及分区副本&quot; class=&quot;headerlink&quot; title=&quot;4.1.1.1 自动创建主题以及分区副本&quot;&gt;&lt;/a&gt;4.1.1.1 自动创建主题以及分区副本&lt;/h4&gt;&lt;p&gt;在之前的笔记中提到了创建主题的一个简单示例.kafka提供 &lt;code&gt;kafka-topics.sh&lt;/code&gt; 脚本来创建主题.下面这个示例创建了一个 &lt;code&gt;topic-test&lt;/code&gt; 的主题,包含4个分区和2个副本.&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;/opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-test --replication-factor 2 --partitions 4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Created topic &amp;quot;topic-test&amp;quot;.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;分区创建完成后,会在kafka的 &lt;code&gt;log.dirs&lt;/code&gt; 或者 &lt;code&gt;log.dir&lt;/code&gt; 的目录下创建相应的主题分区.下面是在其中一台Broker节点的信息展示:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev152 ~]$ ls /opt/logs/kafka/ | grep &amp;quot;topic-test&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;可以看到152节点中创建了2个文件夹 topic-test-0 和 topic-test-2,对应主题 topic-test的2个分区编号为0和2的分区，命名方式可以概括为 &lt;code&gt;&amp;lt;topic&amp;gt;-&amp;lt;partition&amp;gt;&lt;/code&gt; .严谨地说,其实这类文件夹对应的不是分区,分区同主题一样是一个逻辑的概念而没有物理上的存在.并且这里我们也只是看到了2个分区,而我们创建的是4个分区,其余2个分区被分配到了153和154节点中，参考如下:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;#153节点&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev153 ~]$ ls /opt/logs/kafka/ | grep &amp;quot;topic-test&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#154节点&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev154 ~]$ ls /opt/logs/kafka/ | grep &amp;quot;topic-test&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;三个broker节点一共创建了8个文件夹,这个数字8实质上是分区数4与副本因子2的乘积.每个副本(或者更确切地说应该是日志,副本与日志一一对应)才真正对应 了一个命名形式.&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="4-主题和分区" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-3.1消费者与消费组</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/3-%E6%B6%88%E8%B4%B9%E8%80%85/3.1%20%E6%B6%88%E8%B4%B9%E8%80%85%E4%B8%8E%E6%B6%88%E8%B4%B9%E7%BB%84/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/3-消费者/3.1 消费者与消费组/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:49:55.160Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-1-消费者与消费组"><a href="#3-1-消费者与消费组" class="headerlink" title="3.1 消费者与消费组"></a>3.1 消费者与消费组</h2><h3 id="1-消费者和消费组介绍"><a href="#1-消费者和消费组介绍" class="headerlink" title="1.消费者和消费组介绍"></a>1.消费者和消费组介绍</h3><p>消费者( Consumer)负责订阅Kafka中的主题( Topic)，并且从订阅的主题上拉取消息.与其他一些消息中间件不同的是:在 Kafka的消费理念中还有一层消费组( Consumer Group)的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者 。</p><p>以下图为例,某个主题中共有 4 个分区( Partition) : PO、 Pl、 P2、 P3。 有两个消费组 A和 B 都订阅了这个主题，消费组 A 中有 4 个消费者 (CO、 Cl、 C2 和 C3)，消费组 B 中有 2个消费者 CC4 和 CS) 。按照 Kafka默认的规则，最后的分配结果是消费组 A 中的每一个消费 者分配到1个分区，消费组 B 中的每一个消费者分配到 2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。换言之 每一个分区只能被一个消费组中的一个消费者所消费.</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608547700645-c525d96a-ac02-471f-9276-ee885e471c86.png" alt="image.png"></p><a id="more"></a> <p>假设目前某消费组内只有一个消费者 co，订阅了一个主题，这个主题包含 7 个分区: PO、 Pl、 P2、 P3、 P4、PS、 P6o 也就是说，这个消费者co订阅了7个分区，具体分配情形参考图3-2。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608547782387-e3cd0cab-f862-465d-bfd7-96c7fa613b21.png" alt="image.png"></p><p>​                                        </p><p>此时消费组内又加入了一个新的消费者 Cl，按照既定的逻辑，需要将原来消费者 co 的部分分区分配给消费者 Cl 消费 ， 如图 3-3 所示 。 消费者 co 和 Cl 各自负责消费所分配到的分区 ，彼此之间并无逻辑上的干扰 </p><p>消费者与消费组这种模型可以让整体的消费能力具备横向伸缩性，我们 可以增加(或减少) 消费者的个数来提高 (或降低〕整体的消费能力 。 对于分区数固定的情况， 一昧地增加消费者并不会让消费能力 一直得到提升，<strong>如果消费者过多，出现了消费者的个数大于分区个数的情况，**</strong>就会有消费者分配不到任何分区**。</p><h3 id="2-两种消息投递模式"><a href="#2-两种消息投递模式" class="headerlink" title="2.两种消息投递模式"></a>2.两种消息投递模式</h3><p>对于消息中间件而言,一般有两种消息投递模式:<strong>点对点</strong>(P2P, Point-to-Point)模式和<strong>发**</strong>布/订阅**( Pub/Sub)模式.</p><p><strong>点对点模式</strong>是基于队列的，消息生产者发送消息到队列，消息消费者从队列中接收消息。</p><p><strong>发布订阅模式</strong>定义了如何向一个内容节点发布和订阅消息,这个内容节点称为主题(Topic),主题可以认为是消息传递的中介,消息发布者将消息发布到某个主题,而消息订阅者从主题中订阅消息.主题使得消息的订阅者和发布者互相保持独立,不需要进行接触即可保证消息的传递,发布/订阅模式在消息的一对多广播时采用.Kafka同时支持两种消息投递模式，而这正是得益于消费者与消费组模型的契合:</p><ul><li>如果所有的消费者都隶属于同一个消费组,那么所有的消息都会被均衡地投递给每一个消费者,即每条消息只会被一个消费者处理,这就相当于点对点模式的应用 。</li><li>如果所有的消费者都隶属于不同的消费组,那么所有的消息都会被广播给所有的消费者,即每条消息会被所有的消费者处理,这就相当于发布/订阅模式的应用.</li></ul><p>消费组是一个逻辑上的概念，它将旗下的消费者归为一类 ，每一个消费者只隶属于一个消费组。每一个消费组都会有一个固定的名称，消费者在进行消费前需要指定其所属消费组的名称，这个可以通过消费者客户端参数 group.id来配置，默认值为空宇符串。</p><p>消费者并非逻辑上的概念它是实际的应用实例它可以是一个线程，也可以是一个进程。同一个消费组内的消费者既可以部署在同一台机器上，也可以部署在不同的机器上。</p><p>​                                        </p><p>​                                        </p><p>​                                                                                </p><p>​                                        </p><p>​                                        </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-1-消费者与消费组&quot;&gt;&lt;a href=&quot;#3-1-消费者与消费组&quot; class=&quot;headerlink&quot; title=&quot;3.1 消费者与消费组&quot;&gt;&lt;/a&gt;3.1 消费者与消费组&lt;/h2&gt;&lt;h3 id=&quot;1-消费者和消费组介绍&quot;&gt;&lt;a href=&quot;#1-消费者和消费组介绍&quot; class=&quot;headerlink&quot; title=&quot;1.消费者和消费组介绍&quot;&gt;&lt;/a&gt;1.消费者和消费组介绍&lt;/h3&gt;&lt;p&gt;消费者( Consumer)负责订阅Kafka中的主题( Topic)，并且从订阅的主题上拉取消息.与其他一些消息中间件不同的是:在 Kafka的消费理念中还有一层消费组( Consumer Group)的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者 。&lt;/p&gt;
&lt;p&gt;以下图为例,某个主题中共有 4 个分区( Partition) : PO、 Pl、 P2、 P3。 有两个消费组 A和 B 都订阅了这个主题，消费组 A 中有 4 个消费者 (CO、 Cl、 C2 和 C3)，消费组 B 中有 2个消费者 CC4 和 CS) 。按照 Kafka默认的规则，最后的分配结果是消费组 A 中的每一个消费 者分配到1个分区，消费组 B 中的每一个消费者分配到 2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。换言之 每一个分区只能被一个消费组中的一个消费者所消费.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.nlark.com/yuque/0/2020/png/2992889/1608547700645-c525d96a-ac02-471f-9276-ee885e471c86.png&quot; alt=&quot;image.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="3-消费者" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/3-%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-1.1基本概念介绍</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/1.1%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/1-概念介绍/1.1 基本概念介绍/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:48:01.269Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-1-基本概念介绍"><a href="#1-1-基本概念介绍" class="headerlink" title="1.1 基本概念介绍"></a>1.1 基本概念介绍</h1><h3 id="kafka特性"><a href="#kafka特性" class="headerlink" title="kafka特性"></a>kafka特性</h3><ul><li><strong>消息系统</strong>: Kafka和传统的消息中间件都具备流量削峰,缓冲,异步通信,扩展性等.另外,Kafka还提供了大多数消息中间件难以实现的消息顺序保障及回溯消费的功能</li><li><strong>存储系统</strong>: 消息持久化到存盘,可以实现永久存储</li><li><strong>流式处理平台</strong>: Kafka提供了流式处理类库</li></ul><a id="more"></a><h3 id="Kafka架构"><a href="#Kafka架构" class="headerlink" title="Kafka架构"></a>Kafka架构</h3><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608447536390-cba7d090-f67a-435c-8d7d-704fb446e573.png" alt="image.png"></p><p>一个Kafka体系主要包括:</p><ul><li>producer: 生产者</li><li>broker: kafka节点服务器</li><li>consumer: 消费者</li><li>zookeeper: 负责管理kafka集群元数据,集群选举等</li></ul><p>producer将消息发送到Broker,Broker负责将受到的消息存储到磁盘中,Consumer负责从Broker订阅并消费消息.</p><h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><ul><li>Kafka 通过 <em>topic</em> 对存储的流数据进行分类。</li><li>每条记录中包含一个key，一个value和一个timestamp（时间戳).</li><li>kafka保留所有的发布记录(无论是否已经被消费过).通过一个可配置的参数—保留期限来控制记录存在时间.</li></ul><blockquote><p>举个例子， 如果保留策略设置为2天，一条记录发布后两天内，可以随时被消费，两天过后这条记录会被抛弃并释放磁盘空间。</p></blockquote><h3 id="kafka核心API"><a href="#kafka核心API" class="headerlink" title="kafka核心API"></a>kafka核心API</h3><ul><li><a href="https://kafka.apachecn.org/documentation.html#producerapi" target="_blank" rel="noopener">Producer API</a> : 允许一个应用程序发布一串流式的数据到一个或者多个Kafka topic。</li><li><a href="https://kafka.apachecn.org/documentation.html#consumerapi" target="_blank" rel="noopener">Consumer API</a>: 允许一个应用程序订阅一个或多个 topic ，并且对发布给他们的流式数据进行处理。</li><li><a href="https://kafka.apachecn.org/documentation/streams" target="_blank" rel="noopener">Streams API</a>: 允许一个应用程序作为一个<em>流处理器</em>，消费一个或者多个topic产生的输入流，然后生产一个输出流到一个或多个topic中去，在输入输出流中进行有效的转换。</li><li><a href="https://kafka.apachecn.org/documentation.html#connect" target="_blank" rel="noopener">Connector API</a>: 允许构建并运行可重用的生产者或者消费者，将Kafka topics连接到已存在的应用程序或者数据系统。比如，连接到一个关系型数据库，捕捉表（table）的所有变更内容。</li></ul><h3 id="理解topics和Partition和offset"><a href="#理解topics和Partition和offset" class="headerlink" title="理解topics和Partition和offset"></a>理解topics和Partition和offset</h3><p><strong>Topic</strong>: 就是数据主题，生产者将消息发送到特点的主题.消费者负责订阅主题并进行消费.</p><p><strong>Partition</strong>: 一个Topic可以划分成多个partition(分区).但是一个分区只属于单个主题.很多时候也会将partition称为主题分区(Topic-Partition).同一个主题下的不同分区包含的消息也不同.分区在存储层面可以看做一个追加的日志(Log)文件.</p><p>一个主题的分区可以在不同的节点服务器上,所有的消息会均匀的分配到不同的分区中(也就是不同的节点服务器),这样可以提高磁盘IO和性能.在创建主题的时候可以设置分区数量,当然也可以在主题创建完成后去修改分区数量.通过增加分区的数量实现水平扩展.</p><p>好比是为公路运输，不同的起始点和目的地需要修不同高速公路（主题），高速公路上可以提供多条车道（分区），流量大的公路多修几条车道保证畅通，流量小的公路少修几条车道避免浪费。收费站好比消费者，车多的时候多开几个一起收费避免堵在路上，车少的时候开几个让汽车并道就好了</p><p>Kafka中的Topics总是多订阅者模式，一个topic可以拥有一个或者多个消费者来订阅它的数据。对于每一个topic， Kafka集群都会维持一个分区日志，如下所示：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608119181003-5ca1cc98-c0ec-4c00-b7b0-d0b916204ab0.png" alt="image.png"></p><p>每个partition分区都是有序切不可变的记录集.并且不断的追加到结构化的commit log文件.</p><p><strong>Offset</strong>: 消息被存储到分区的日志文件时会分片一个偏移量(offset).offset是消息在分区中的唯一表示.kafka通过它来保障消息在分区内的顺序.</p><p>不过Offset并不跨越分区,也就是说Kafka保证的是分区有序,而不是主题有序.</p><p>在每一个消费者中唯一保存的元数据是offset（偏移量）即消费在log中的位置.偏移量由消费者所控制:通常在读取记录后，消费者会以线性的方式增加偏移量，但是实际上，由于这个位置由消费者控制，所以消费者可以采用任何顺序来消费记录。例如，一个消费者可以重置到一个旧的偏移量，从而重新处理过去的数据；也可以跳过最近的记录，从”现在”开始消费。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608120423518-7cddb09d-e7fa-4f35-809f-908a36b5a4d1.png?x-oss-process=image%2Fresize%2Cw_1500" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-1-基本概念介绍&quot;&gt;&lt;a href=&quot;#1-1-基本概念介绍&quot; class=&quot;headerlink&quot; title=&quot;1.1 基本概念介绍&quot;&gt;&lt;/a&gt;1.1 基本概念介绍&lt;/h1&gt;&lt;h3 id=&quot;kafka特性&quot;&gt;&lt;a href=&quot;#kafka特性&quot; class=&quot;headerlink&quot; title=&quot;kafka特性&quot;&gt;&lt;/a&gt;kafka特性&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;消息系统&lt;/strong&gt;: Kafka和传统的消息中间件都具备流量削峰,缓冲,异步通信,扩展性等.另外,Kafka还提供了大多数消息中间件难以实现的消息顺序保障及回溯消费的功能&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;存储系统&lt;/strong&gt;: 消息持久化到存盘,可以实现永久存储&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;流式处理平台&lt;/strong&gt;: Kafka提供了流式处理类库&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="1-概念介绍" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-1.2 生产与消费简单实例</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/1.2%20%E7%94%9F%E4%BA%A7%E4%B8%8E%E6%B6%88%E8%B4%B9%E7%AE%80%E5%8D%95%E5%AE%9E%E4%BE%8B/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/1-概念介绍/1.2 生产与消费简单实例/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:48:34.104Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-2-生产与消费简单实例"><a href="#1-2-生产与消费简单实例" class="headerlink" title="1.2 生产与消费简单实例"></a>1.2 生产与消费简单实例</h1><h3 id="创建topic"><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h3><p>kafka提供了许多实用的脚本工具,存放在$KAFKA_HOME的bin目录下.其中与主题相关的就是kafka-topic.sh脚本.例如.下面创建一个分区数为4,副本为3的主题topic-demon<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-demo --replication-factor 3 --partitions 4</span><br><span class="line"></span><br><span class="line">Created topic <span class="string">"topic-demo"</span>.</span><br></pre></td></tr></table></figure></p><p><code>--zoopkeer</code> 指定kafka连接的zookeeper服务地址<br><code>--topic</code> 指定一个topic主题<br><code>--replication-factor</code>  指定副本因子数量<br><code>--partition</code> 指定分区数量<br><code>--create</code> 表示创建</p><a id="more"></a> <p>下面命令展示了刚创建的主题信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 bin]$ ./kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demoPartitionCount:4ReplicationFactor:3Configs:</span><br><span class="line">Topic: topic-demoPartition: 0Leader: 152Replicas: 152,153,154Isr: 152,153,154</span><br><span class="line">Topic: topic-demoPartition: 1Leader: 153Replicas: 153,154,152Isr: 153,154,152</span><br><span class="line">Topic: topic-demoPartition: 2Leader: 154Replicas: 154,152,153Isr: 154,152,153</span><br><span class="line">Topic: topic-demoPartition: 3Leader: 152Replicas: 152,154,153Isr: 152,154,153</span><br></pre></td></tr></table></figure></p><p>上面的命令结果表示 <code>topic-demon</code> 这个主题一共有4个分区,存放在3台Kafka broker服务器节点.3个broker均是ISR集合,没有OSR集合</p><blockquote><p>在任意一台kafka集群内的节点服务器上执行上述命令,会得到完全相同的结果</p></blockquote><h3 id="创建consumer"><a href="#创建consumer" class="headerlink" title="创建consumer"></a>创建consumer</h3><p><code>kafka-console-consumer.sh</code> 在任意一台kafka集群内的节点服务器上可以通过控制台创建一个 <code>consumer</code> 消费者.示例如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev154 bin]$ ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-demo</span><br></pre></td></tr></table></figure></p><p><code>--bootstrap-server</code> 指定连接的kafka集群地址<br><code>--topic</code> 指定消费者订阅的主题</p><h3 id="创建producer"><a href="#创建producer" class="headerlink" title="创建producer"></a>创建producer</h3><p><code>kafka-console-producer.sh</code> 在任意一台kafka集群内的节点服务器上可以通过控制台创建一个 <code>producer</code> 消费者.示例如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev153 bin]$ ./kafka-console-producer.sh --broker-list localhost:9092 --topic topic-demo</span><br></pre></td></tr></table></figure></p><p><code>--broker-list</code> 指定连接的kafka集群地址<br><code>--topic</code> 指定发小时时的主题<br>在弹出的shell终端中,输入 <code>hello world!</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;hello,world!</span><br></pre></td></tr></table></figure></p><p>回到 <code>consumer</code> 的shell终端界面,发现消费到了刚生产的消息:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev154 bin]$ ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-demo</span><br><span class="line">hello,world!</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-2-生产与消费简单实例&quot;&gt;&lt;a href=&quot;#1-2-生产与消费简单实例&quot; class=&quot;headerlink&quot; title=&quot;1.2 生产与消费简单实例&quot;&gt;&lt;/a&gt;1.2 生产与消费简单实例&lt;/h1&gt;&lt;h3 id=&quot;创建topic&quot;&gt;&lt;a href=&quot;#创建topic&quot; class=&quot;headerlink&quot; title=&quot;创建topic&quot;&gt;&lt;/a&gt;创建topic&lt;/h3&gt;&lt;p&gt;kafka提供了许多实用的脚本工具,存放在$KAFKA_HOME的bin目录下.其中与主题相关的就是kafka-topic.sh脚本.例如.下面创建一个分区数为4,副本为3的主题topic-demon&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;./kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-demo --replication-factor 3 --partitions 4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Created topic &lt;span class=&quot;string&quot;&gt;&quot;topic-demo&quot;&lt;/span&gt;.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;--zoopkeer&lt;/code&gt; 指定kafka连接的zookeeper服务地址&lt;br&gt;&lt;code&gt;--topic&lt;/code&gt; 指定一个topic主题&lt;br&gt;&lt;code&gt;--replication-factor&lt;/code&gt;  指定副本因子数量&lt;br&gt;&lt;code&gt;--partition&lt;/code&gt; 指定分区数量&lt;br&gt;&lt;code&gt;--create&lt;/code&gt; 表示创建&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="1-概念介绍" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-2.1Kafka副本</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/2-%E5%89%AF%E6%9C%AC%E4%BB%8B%E7%BB%8D/2.1%20%20Kafka%E5%89%AF%E6%9C%AC/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/2-副本介绍/2.1  Kafka副本/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:49:02.886Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2-1-Kafka副本"><a href="#2-1-Kafka副本" class="headerlink" title="2.1  Kafka副本"></a>2.1  Kafka副本</h1><h3 id="副本介绍"><a href="#副本介绍" class="headerlink" title="副本介绍"></a>副本介绍</h3><p>Kafka为分区引入了副本(Replica)机制.通过增加副本数量提升容灾能力.一个Topic主题可以有多个分区,一个分区又可以有多个副本.这多个副本中，只有一个是leader，而其他的都是follower副本。仅有leader副本可以对外提供服务。所以副本之间是一主多从的关系,而且每个副本中保存的相同的消息.(严格来说,同一时刻副本之间的消息并非能一定完全同步)</p><p>多个follower副本通常存放在和leader副本不同的broker中。通过这样的机制实现了高可用，当某台机器挂掉后，其他follower副本也能迅速”转正“，开始对外提供服务。</p><p>在kafka中，实现副本的目的就是冗余备份，且仅仅是冗余备份，所有的读写请求都是由leader副本进行处理的。follower副本仅有一个功能，那就是从leader副本拉取消息，尽量让自己跟leader副本的内容一致。</p><blockquote><p>follower副本之所以不能对外提供服务,主要是为了保障数据一致性</p></blockquote><p>下图是一个多副本架构图.</p><p>Kafka集群中有4个broker，某个主题中有3个分区，且副本因子（即副本个数）也为3，如此每个分区便有1个leader副本和2个follower副本。生产者和消费者只与leader副本进行交互，而follower副本只负责消息的同步，很多时候follower副本中的消息相对leader副本而言会有一定的滞后。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608450120303-2d66cd2e-611a-4e3e-a507-53b4f81adfa0.png" alt="image.png"></p><h3 id="副本同步"><a href="#副本同步" class="headerlink" title="副本同步"></a>副本同步</h3><p><strong>AR</strong>: 分区内的所有副本统称.</p><p><strong>ISR</strong>: In-Sync Replicas.所有与Leader副本保持一定程度同步的副本(包括Leader副本).一起组成ISR</p><p><strong>OSR</strong>: Out-of-Sync Replicas: 与leader副本同步滞后过多的副本(不包括leader副本),一起注册呢个OSR</p><p><strong>AR = ISR + OSR.</strong></p><blockquote><p>正常情况下,所有的follower副本都应该与leader副本保持一定程度的同步,即AR = ISR,OSR集合为空</p></blockquote><p>Leader副本负责维护和跟踪ISR集合中所有follower副本的滞后状态,当follower副本落后太多或者失效时,leader副本会把它从ISR集合中剔除,如果OSR的follower副本追上了leader副本,那会从OSR转移到ISR.</p><blockquote><p>默认情况下,只有ISR集合中的follower副本才有资格被选举为新的Leader</p></blockquote><h3 id="HW和LEO"><a href="#HW和LEO" class="headerlink" title="HW和LEO"></a>HW和LEO</h3><p><strong>HW(High Watermark)</strong>: 俗称高水位.它标识了一个特点的消息偏移量(offset).消费者只能拉取这个offset之前的信息.</p><p><strong>LEO(Log End Offset)</strong>: 标识当前日志文件中下一条代写入消息的offset. </p><p><strong></strong></p><p>下面一张图能说明这两个概念</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608450932841-d4f33228-91b2-484f-b28e-24d0762ef670.png" alt="image.png"></p><p>上面的图代表一个日志文件.这个日志文件中有 9 条消息，第一条消息的 offset（LogStartOffset）为0，最后一条消息的offset为8，offset为9的消息用虚线框表示，代表下一条待写入的消息。日志文件的HW为6，表示消费者只能拉取到offset在0至5之间的消息，而offset为6的消息对消费者而言是不可见的。</p><p>offset为9的位置即为当前日志文件的LEO，LEO的大小相当于当前日志分区中最后一条消息的offset值加1。分区ISR集合中的每个副本都会维护自身的LEO，<strong>而ISR集合中最小的LEO即为分区的HW，对消费者而言只能消费HW之前的消息。</strong></p><h3 id="ISR和HW-LEO的关系"><a href="#ISR和HW-LEO的关系" class="headerlink" title="ISR和HW,LEO的关系"></a>ISR和HW,LEO的关系</h3><p>为了让读者更好地理解ISR集合，以及HW和LEO之间的关系，下面通过一个简单的示例来进行相关的说明。如图1-5所示，假设某个分区的ISR集合中有3个副本，即一个leader副本和2个follower副本，此时分区的LEO和HW都为3。消息3和消息4从生产者发出之后会被先存入leader副本</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451481234-2309dd97-7662-49d0-a43a-42b822f7a7d4.png" alt="image.png"></p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451501313-69357f66-f23b-4d07-940f-ef9f0afb9260.png" alt="image.png"></p><p>在同步过程中，不同的 follower 副本的同步效率也不尽相同。如图 所示，在某一时刻follower1完全跟上了leader副本而follower2只同步了消息3，如此leader副本的LEO为5，follower1的LEO为5，follower2的LEO为4，那么当前分区的HW取最小值4，此时消费者可以消费到offset为0至3之间的消息。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451550216-2bc62531-a164-40a1-b039-e9e911401d02.png" alt="image.png"></p><p>如果所有的副本都成功写入了消息3和消息4，整个分区的HW和LEO都变为5，因此消费者可以消费到offset为4的消息了。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451625357-266f1693-1d4b-4524-9f64-89256893555e.png" alt="image.png"></p><p>Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的 follower 副本都复制完，这条消息才会被确认为已成功提交，这种复制方式极大地影响了性能。而在异步复制方式下，follower副本异步地从leader副本中复制数据，数据只要被leader副本写入就被认为已经成功提交。在这种情况下，如果follower副本都还没有复制完而落后于leader副本，突然leader副本宕机，则会造成数据丢失。Kafka使用的这种ISR的方式则有效地权衡了数据可靠性和性能之间的关系。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;2-1-Kafka副本&quot;&gt;&lt;a href=&quot;#2-1-Kafka副本&quot; class=&quot;headerlink&quot; title=&quot;2.1  Kafka副本&quot;&gt;&lt;/a&gt;2.1  Kafka副本&lt;/h1&gt;&lt;h3 id=&quot;副本介绍&quot;&gt;&lt;a href=&quot;#副本介绍&quot; class
      
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="2-副本介绍" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/2-%E5%89%AF%E6%9C%AC%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://jesse.top/2020/09/16/SRE%E8%BF%90%E7%BB%B4/SRE%E8%BF%90%E7%BB%B4%E7%AC%94%E8%AE%B0-chapter%20Three-%E6%8B%A5%E6%8A%B1%E9%A3%8E%E9%99%A9/"/>
    <id>https://jesse.top/2020/09/16/SRE运维/SRE运维笔记-chapter Three-拥抱风险/</id>
    <published>2020-09-16T14:30:54.835Z</published>
    <updated>2020-09-16T14:30:54.835Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SRE运维笔记-chapter-Three-拥抱风险"><a href="#SRE运维笔记-chapter-Three-拥抱风险" class="headerlink" title="SRE运维笔记-chapter Three-拥抱风险"></a>SRE运维笔记-chapter Three-拥抱风险</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;SRE运维笔记-chapter-Three-拥抱风险&quot;&gt;&lt;a href=&quot;#SRE运维笔记-chapter-Three-拥抱风险&quot; class=&quot;headerlink&quot; title=&quot;SRE运维笔记-chapter Three-拥抱风险&quot;&gt;&lt;/a&gt;SRE运维笔记-
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://jesse.top/2020/09/16/SRE%E8%BF%90%E7%BB%B4/SRE%E8%BF%90%E7%BB%B4%E7%AC%94%E8%AE%B0/"/>
    <id>https://jesse.top/2020/09/16/SRE运维/SRE运维笔记/</id>
    <published>2020-09-16T14:30:54.835Z</published>
    <updated>2020-09-16T14:30:54.836Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SRE运维笔记-chapter-One-概念介绍"><a href="#SRE运维笔记-chapter-One-概念介绍" class="headerlink" title="SRE运维笔记-chapter One-概念介绍"></a>SRE运维笔记-chapter One-概念介绍</h2><h3 id="SRE概念"><a href="#SRE概念" class="headerlink" title="SRE概念"></a>SRE概念</h3><p>SRE(sitereliabilityengineering).中文翻译为站点可靠性工程师.SRE概念中比较重要的特性在于:</p><p>1.engineer表示SRE是工程师,使用软件工程手段设计,研发和维护业务软件系统.</p><p>2.SRE的关注焦点在于<strong>可靠性</strong>,专注于软件系统架构设计,运维流程优化,让业务软件系统运行更可靠.</p><p>3.SRE主要工作是运维业务服务,</p><h4 id="SRE团队职责"><a href="#SRE团队职责" class="headerlink" title="SRE团队职责:"></a>SRE团队职责:</h4><ul><li>可用性改进</li><li>延迟优化</li><li>性能优化</li><li>效率优化</li><li>变更管理</li><li>监控</li><li>紧急事务处理</li><li>容量规划与管理</li></ul><h3 id="SRE方法论"><a href="#SRE方法论" class="headerlink" title="SRE方法论"></a>SRE方法论</h3><ul><li><p><strong>确保长期关注研发工作</strong></p><p>运维工作限制在50%以内,剩余的时间花在研发项目上.</p></li><li><p><strong>在保障服务SLO的前提下最大化迭代速度</strong></p><ul><li>错误预算: 1-可靠性目标.<ul><li>如果一个服务的可靠性目标是99.99%,那么错误预算就是0.01%.</li></ul></li></ul></li><li><p><strong>监控系统</strong></p><p>监控系统不应该依赖人来分析警报信息.而是应该由系统自动分析.仅当需要用户执行某种操作时,才需要通知用户</p><p>监控系统需要具备三类输出:</p><ul><li>紧急警报(alert)</li><li>工单:接受工单的用户应该执行某种操作,但是并非立即执行</li><li>日志</li></ul></li><li><p><strong>应急事件处理</strong></p><ul><li>可靠性:MTTF(平均失败时间),MTTR(平均恢复时间),MTBF(平均故障间隔时间).</li><li>任何需要人工操作的事情都只会延长恢复时间,一个可以自动恢复的系统即使有更多故障发生,也比事事都要人工干预的系统可用性更高.当不可避免需要人工介入时,最佳方法是事先预案,并且记录在<strong>运维手册(playbook)</strong>中,这能降低<strong>MTTR(平均恢复)</strong>时间.</li></ul></li><li><p><strong>变更管理</strong></p><p> 大概70%的生产事故是由某种变更触发,变更管理的最佳实践是使用自动化完成以下几个项目:</p><ul><li>采用渐进式发布机制</li><li>迅速而准确的检测到问题的发生</li><li>安全迅速的回滚</li></ul></li><li><p><strong>需求预测和容量规划</strong></p><ul><li>必须有一个准确的自然增城需求预测模型,需求预测的时间应该超过资源获取时间</li><li>规划中必须有准确的非自然增长的需求来源统计</li><li>必须有周期性压力测试,以便准确的将系统原始资源与业务容量对应起来</li></ul></li><li><p><strong>资源部署</strong></p></li><li><p><strong>效率与性能</strong></p><p>一个业务总体资源使用情况是由以下几个因素驱动的:</p><ul><li>用户需求(流量)</li><li>可用容量</li><li>软件资源使用效率</li></ul><p>SRE需要通过模型预测用户需求,合理部署和配置可用容量,改进软件以提高资源使用效率,这3个因素能够推动一个服务器的效率提升.</p><blockquote><p>软件系统在负载上升的时候,会导致延迟升高.SRE的目标是根据一个预设的延迟目标部署和维护足够的容量,SRE和研发团队应该共同监控和优化整个系统的性能.</p></blockquote></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;SRE运维笔记-chapter-One-概念介绍&quot;&gt;&lt;a href=&quot;#SRE运维笔记-chapter-One-概念介绍&quot; class=&quot;headerlink&quot; title=&quot;SRE运维笔记-chapter One-概念介绍&quot;&gt;&lt;/a&gt;SRE运维笔记-chapte
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>kong利用request-transformer插件重写URL</title>
    <link href="https://jesse.top/2020/09/10/Linux-Web/kong%E5%88%A9%E7%94%A8request-transformer%E6%8F%92%E4%BB%B6%E9%87%8D%E5%86%99URL/"/>
    <id>https://jesse.top/2020/09/10/Linux-Web/kong利用request-transformer插件重写URL/</id>
    <published>2020-09-10T04:59:58.000Z</published>
    <updated>2020-09-10T14:58:43.601Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kong利用request-transformer插件重写URL"><a href="#kong利用request-transformer插件重写URL" class="headerlink" title="kong利用request-transformer插件重写URL"></a>kong利用request-transformer插件重写URL</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>最近业务整合后有一个需求,将URL:<a href="http://www.abc.com/api/item/111" target="_blank" rel="noopener">www.abc.com/api/item/111</a> 想重写成<a href="http://www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转" target="_blank" rel="noopener">www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转</a>.</p><p>使用Nginx的rewrite redirect指令可以实现URL重写需求,但是redirect会跳转到新域名,不符合需求.</p><p>刚好该业务的的前端是用Kong网关处理,所以研究kong的插件实现这个需求</p><hr><h3 id="request-transformer介绍"><a href="#request-transformer介绍" class="headerlink" title="request-transformer介绍"></a>request-transformer介绍</h3><p><strong>request-transformer</strong>是Kong官方的插件,允许修改重写用户的请求,还可以使用正则表达式匹配URL,并将匹配到的字符串保存在变量中,然后使用模板将变量转换成用户的请求</p><p>简而言之:<strong>就是重写用户的请求</strong>,包括URL,args,headers,methods等等</p><p>官方地址: <a href="https://docs.konghq.com/hub/kong-inc/request-transformer/" target="_blank" rel="noopener">reuqest-transformer官方地址</a></p><p>github项目地址: <a href="https://github.com/Kong/kong-plugin-request-transformer" target="_blank" rel="noopener">request-transformer github</a></p><a id="more"></a><hr><h3 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h3><blockquote><p>kong使用的是2.1.3最新版本,试过使用Kong1.0版本插件无法生效</p></blockquote><p>这里举2个例子说明</p><ul><li><p>将URL:/v4/jkf/branch/qrcode&amp;code=100006 重写为 /v4/jkf/branch/qrcode?code=100006.也就是将<code>&amp;</code>转换为<code>?</code></p><ul><li>首先配置Service和Route.具体配置方法略过,这里主要关心一下Route中的Path设置:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/v4/jkf/branch/qrcode\&amp;code=(?&lt;code&gt;\d+)</span><br></pre></td></tr></table></figure><p>该PATH表示:</p><p>1.匹配/v4/jkf/branch/qrcode&amp;code=任意长度数字.</p><p>2.正则<code>\d</code>表示匹配数字,并且将匹配到的数字保存为<code>code</code>变量</p><p>3.<code>\&amp;</code>表示转义URL中的<code>$</code>符号</p><blockquote><p>关闭route中的script path可选项</p></blockquote><ul><li><p>其次在该route下添加<code>request-transformer</code>插件,表示该插件只应用到此条route下.并且配置插件参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://localhost:8001/routes/21292565-e7ae-40ff-a465-f449c16b7819/plugins \</span><br><span class="line">      --data &quot;name=request-transformer&quot; \</span><br><span class="line">      --data &quot;config.replace.uri=/jkf/branch/qrcode&quot; \</span><br><span class="line">      --data &quot;config.add.querystring=code:\$(uri_captures.code)&quot;</span><br></pre></td></tr></table></figure><p>对于上面的命令行解释如下</p><ol><li><code>21292565-e7ae-40ff-a465-f449c16b7819</code>就是刚才创建的路由ID</li><li><code>config.replace.uri</code>表示将route匹配到的PATH重写为<code>/jkf/branch/qrcode</code></li><li><code>onfig.add.querystring</code>表示给URL添加args参数</li><li><code>code:\$(uri_captures.code)</code>.参数名是<code>code</code>,<code>uri_captures.code</code>表示获取route的PATH中code变量的值,由于命令行shell环境关系,所以要在变量符号<code>$</code>前进行转义.</li></ol></li></ul></li></ul><pre><code>或者也可以使用konga的UI管理平台添加和编辑插件</code></pre><p><img src="https://img2.jesse.top/image-20200910160826719.png" alt=""></p><p>  <img src="https://img2.jesse.top/image-20200910160949836.png" alt="image-20200910160949836"></p><ul><li><p>最后,<code>reload</code>Kong进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker ~]# docker exec kong kong reload</span><br></pre></td></tr></table></figure><p><strong>验证</strong></p><p>本地访问网站:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">✘ huangyong@huangyong-Macbook-Pro  ~  curl -XGET https://m.devapi.xxx.com/v4/jkf/branch/qrcode\&amp;code\=100006</span><br></pre></td></tr></table></figure><p>Kong和后端nginx日志如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">172.18.0.1 - - [10/Sep/2020:16:52:11 +0800] &quot;GET /jkf/branch/qrcode?code=100006 HTTP/1.1&quot; 200 163 &quot;-&quot; &quot;curl/7.54.0&quot; &quot;10.0.4.9, 172.18.0.2&quot; 10.0.4.9, 172.18.0.2, 172.18.0.1 &quot;2b7dcdc621d1928f456d561f31e95b25&quot;0.065 0.065</span><br></pre></td></tr></table></figure><p>可以看到已经成功重写了URL</p></li></ul><hr><ul><li>第二个例子,将/api/item/111 重写为/open/item/itemdetail?id=111</li></ul><p>将URL后面的数字拼接到id的值,作为参数拼接成URL后,传递给后端</p><p>1.添加Service和Routes,Routes的PATH部分如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#该PATH只匹配/api/item/数字格式的URL.并且将\d+正则匹配到的数字保存为变量id</span><br><span class="line">/api/item/(?&lt;id&gt;\d+)$</span><br></pre></td></tr></table></figure><p>2.添加和配置插件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://localhost:8001/routes/38fce1b7-2a36-42cb-9619-f30539889137/plugins \</span><br><span class="line">      --data &quot;name=request-transformer&quot; \</span><br><span class="line">      --data &quot;config.replace.uri=/open/item/itemdetail&quot; \</span><br><span class="line">      --data &quot;config.add.querystring=id:\$(uri_captures.id)&quot;</span><br></pre></td></tr></table></figure><p>3.重载kong</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker ~]# docker exec kong kong reload</span><br></pre></td></tr></table></figure><p>4.验证</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huangyong@huangyong-Macbook-Pro  ~  curl -XGET https://m.devapi.xxx.com/api/item/111</span><br></pre></td></tr></table></figure><p>后端日志如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">172.18.0.1 - - [10/Sep/2020:17:11:43 +0800] &quot;GET /open/item/itemdetail?id=111 HTTP/1.1&quot; 200 160 &quot;-&quot; &quot;curl/7.54.0&quot; &quot;10.0.4.9, 172.18.0.2&quot; 10.0.4.9, 172.18.0.2, 172.18.0.1 &quot;54ac589d36f90c1ef99ba6a43c4d488e&quot;0.105 0.105</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kong利用request-transformer插件重写URL&quot;&gt;&lt;a href=&quot;#kong利用request-transformer插件重写URL&quot; class=&quot;headerlink&quot; title=&quot;kong利用request-transformer插件重写URL&quot;&gt;&lt;/a&gt;kong利用request-transformer插件重写URL&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;最近业务整合后有一个需求,将URL:&lt;a href=&quot;http://www.abc.com/api/item/111&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;www.abc.com/api/item/111&lt;/a&gt; 想重写成&lt;a href=&quot;http://www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;使用Nginx的rewrite redirect指令可以实现URL重写需求,但是redirect会跳转到新域名,不符合需求.&lt;/p&gt;
&lt;p&gt;刚好该业务的的前端是用Kong网关处理,所以研究kong的插件实现这个需求&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;request-transformer介绍&quot;&gt;&lt;a href=&quot;#request-transformer介绍&quot; class=&quot;headerlink&quot; title=&quot;request-transformer介绍&quot;&gt;&lt;/a&gt;request-transformer介绍&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;request-transformer&lt;/strong&gt;是Kong官方的插件,允许修改重写用户的请求,还可以使用正则表达式匹配URL,并将匹配到的字符串保存在变量中,然后使用模板将变量转换成用户的请求&lt;/p&gt;
&lt;p&gt;简而言之:&lt;strong&gt;就是重写用户的请求&lt;/strong&gt;,包括URL,args,headers,methods等等&lt;/p&gt;
&lt;p&gt;官方地址: &lt;a href=&quot;https://docs.konghq.com/hub/kong-inc/request-transformer/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;reuqest-transformer官方地址&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;github项目地址: &lt;a href=&quot;https://github.com/Kong/kong-plugin-request-transformer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;request-transformer github&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>zabbix监控vmware主机以及GuestOS</title>
    <link href="https://jesse.top/2020/08/26/%E7%9B%91%E6%8E%A7/zabbix%E7%9B%91%E6%8E%A7vmware%E4%B8%BB%E6%9C%BA%E4%BB%A5%E5%8F%8AGuestOS/"/>
    <id>https://jesse.top/2020/08/26/监控/zabbix监控vmware主机以及GuestOS/</id>
    <published>2020-08-26T01:20:58.000Z</published>
    <updated>2020-08-26T23:55:20.314Z</updated>
    
    <content type="html"><![CDATA[<h3 id="zabbix监控vmware主机以及GuestOS"><a href="#zabbix监控vmware主机以及GuestOS" class="headerlink" title="zabbix监控vmware主机以及GuestOS"></a>zabbix监控vmware主机以及GuestOS</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>ESXI主机无法安装zabbix agent,所以不能使用传统的agent客户端监控vmware主机,但是Zabbix有自导的vmware hypervisors监控模板.Zabbix 通过 vmware collector 进程来监控虚拟机,使用SOAP协议从vmware web服务器获取必要的监控信息.</p><hr><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p>1.在zabbix服务器修改<code>zabbix_server.conf</code>配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">StartVMwareCollectors=6</span><br><span class="line">VMwareCacheSize=50M</span><br><span class="line">VMwareFrequency=10</span><br><span class="line">VMwarePerfFrequency=60</span><br><span class="line">VMwareTimeout=300</span><br></pre></td></tr></table></figure><a id="more"></a><p><strong>说明</strong>: </p><p><strong>StartVMwareCollectors</strong>：vmware 收集器实例的数量。<br>此值取决于要监控的 VMware 服务的数量。在大多数情况下，这应该是：<code>servicenum &lt; StartVMwareCollectors &lt; (servicenum * 2)</code>其中 servicenum 是 VMware 服务的数量。</p><p>例如：如果您有 1 个 VMware 服务要将 StartVMwareCollectors 设置为 2，那么如果您有 3 个 VMware 服务，请将其设置为 5。请注意，在大多数情况下，此值不应小于 2，不应大于 VMware 数量的 2 倍服务。</p><p>2.重启zabbix服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart zabbix_server</span><br></pre></td></tr></table></figure><hr><h4 id="Esxi物理主机配置"><a href="#Esxi物理主机配置" class="headerlink" title="Esxi物理主机配置"></a>Esxi物理主机配置</h4><p>1.登陆Esxi web界面: <a href="https://172.16.0.55" target="_blank" rel="noopener">https://172.16.0.55</a><br>2.在<code>manage</code>—<code>system</code>—-<code>advanced settings</code>.修改<code>Config.HostAgent.plugins.solo.enableMob</code>的值为True</p><p><img src="https://img2.jesse.top/image-20200818112727513.png" alt="image-20200818112727513"></p><p>3.访问:<a href="https://172.16.0.55/mob/?moid=ha-host&amp;doPath=hardware.systemInfo" target="_blank" rel="noopener">https://172.16.0.55/mob/?moid=ha-host&amp;doPath=hardware.systemInfo</a><br>记录UUID<br><img src="https://img2.jesse.top/image-20200818112949235.png" alt="image-20200818112949235"></p><p>4.在zabbix添加主机</p><p><img src="https://img2.jesse.top/image-20200818113114835.png" alt="image-20200818113114835"></p><ul><li><strong>主机名称</strong>:上面查到的UUID</li><li><strong>IP地址</strong>:Esxi的IP地址</li><li><strong>端口</strong>:80</li></ul><p><strong>模板</strong>:</p><p><img src="https://img2.jesse.top/image-20200818113315388.png" alt="image-20200818113315388"></p><p><strong>宏</strong></p><p><img src="https://img2.jesse.top/image-20200818113428859.png" alt="image-20200818113428859"></p><ul><li><strong>password</strong>: Esxi主机密码</li><li><p><strong>URL</strong>: <a href="https://Esxi_IP/sdk" target="_blank" rel="noopener">https://Esxi_IP/sdk</a> </p></li><li><p><strong>username</strong>: ESXI主机用户名</p></li><li><strong>UUID</strong>: 上文记录的UUID</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;zabbix监控vmware主机以及GuestOS&quot;&gt;&lt;a href=&quot;#zabbix监控vmware主机以及GuestOS&quot; class=&quot;headerlink&quot; title=&quot;zabbix监控vmware主机以及GuestOS&quot;&gt;&lt;/a&gt;zabbix监控vmware主机以及GuestOS&lt;/h3&gt;&lt;h4 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h4&gt;&lt;p&gt;ESXI主机无法安装zabbix agent,所以不能使用传统的agent客户端监控vmware主机,但是Zabbix有自导的vmware hypervisors监控模板.Zabbix 通过 vmware collector 进程来监控虚拟机,使用SOAP协议从vmware web服务器获取必要的监控信息.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h4&gt;&lt;p&gt;1.在zabbix服务器修改&lt;code&gt;zabbix_server.conf&lt;/code&gt;配置文件&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;StartVMwareCollectors=6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwareCacheSize=50M&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwareFrequency=10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwarePerfFrequency=60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwareTimeout=300&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="监控" scheme="https://jesse.top/categories/%E7%9B%91%E6%8E%A7/"/>
    
    
      <category term="zabbix监控" scheme="https://jesse.top/tags/zabbix%E7%9B%91%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>使用ElastAlert+ELK实现日志监控钉钉告警</title>
    <link href="https://jesse.top/2020/08/25/elk/%E4%BD%BF%E7%94%A8ElastAlert+ELK%E5%AE%9E%E7%8E%B0%E6%97%A5%E5%BF%97%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/"/>
    <id>https://jesse.top/2020/08/25/elk/使用ElastAlert+ELK实现日志监控告警/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-29T01:51:30.236Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用ElastAlert-ELK实现日志监控钉钉告警"><a href="#使用ElastAlert-ELK实现日志监控钉钉告警" class="headerlink" title="使用ElastAlert+ELK实现日志监控钉钉告警"></a>使用ElastAlert+ELK实现日志监控钉钉告警</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>目前公司使用ELK做日志收集和展示分析.所以想对一些关键日志进行监控告警.比如Nginx的5xx日志,比如php-fpm的Fatal严重错误日志等.通过监控ES的日志数据,然后使用Python调用钉钉接口来实现日志的告警</p><hr><h3 id="ElastAlert介绍"><a href="#ElastAlert介绍" class="headerlink" title="ElastAlert介绍"></a>ElastAlert介绍</h3><p>ElastAlert是一个开源的工具,用于从Elastisearch中检索数据,并根据匹配模式发出告警.github项目地址如下:<a href="https://github.com/Yelp/elastalert" target="_blank" rel="noopener">https://github.com/Yelp/elastalert</a></p><p>官方文档如下:<a href="https://elastalert.readthedocs.io/en/latest/elastalert.html" target="_blank" rel="noopener">https://elastalert.readthedocs.io/en/latest/elastalert.html</a></p><p>它支持多种监控模式和告警方式,具体可以查阅Github项目介绍.但是自带的ElastAlert并不支持钉钉告警,在github上有第三方的钉钉python项目.地址如下:<a href="https://github.com/xuyaoqiang/elastalert-dingtalk-plugin" target="_blank" rel="noopener">https://github.com/xuyaoqiang/elastalert-dingtalk-plugin</a></p><p>第三方的钉钉告警插件并没有艾特相关人员的功能,所以我再此基础上进行了二次开发,增加了这个功能</p><hr><a id="more"></a><h3 id="ElastAlert安装"><a href="#ElastAlert安装" class="headerlink" title="ElastAlert安装"></a>ElastAlert安装</h3><blockquote><p>新版的ElastAlert不支持python2了.所以需要安装Python3环境</p></blockquote><ul><li>安装依赖</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum -y install python3 python3-devel python3-libs python3-setuptools git gcc</span><br></pre></td></tr></table></figure><p>如果是Ubuntu系统:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt -y upgrade</span><br><span class="line">sudo apt install -y python3.6-dev</span><br><span class="line">sudo apt install -y libffi-dev libssl-dev</span><br><span class="line">sudo apt install -y python3-pip</span><br><span class="line">sudo apt install -y python3-venv</span><br></pre></td></tr></table></figure><ul><li>安装elastalert模块</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install elastalert  -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com</span><br></pre></td></tr></table></figure><ul><li>克隆ElastAlert项目</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/Yelp/elastalert.git</span><br><span class="line">cp -r elastalert /data/</span><br></pre></td></tr></table></figure><ul><li>安装模块</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /data/elastalert/</span><br><span class="line">pip3 install &quot;setuptools&gt;=11.3&quot;</span><br><span class="line"></span><br><span class="line">pip install -r requirements.txt  -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com</span><br></pre></td></tr></table></figure><ul><li>创建ElastAlert的索引</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo elastalert-create-index --index elastalert</span><br></pre></td></tr></table></figure><ul><li>修改ElastAlert的配置文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cp config.yaml.example config.yaml</span><br><span class="line">vim config.yaml</span><br><span class="line"></span><br><span class="line">rules_folder: rule  #rule匹配模式的目录,可以自定义一个/data/elastalert路径下的相对目录</span><br><span class="line">run_every:          #ElastAlert多久向Elasticsearch发送一次请求</span><br><span class="line">  minutes: 1</span><br><span class="line">buffer_time:        #如果某些日志源不是实时的，则ElastAlert将缓冲最近一段时间的结果.这个值默认是15,但是无法触发告警,设置为1正常</span><br><span class="line">  minutes: 1</span><br><span class="line">es_host: localhost      #ES集群节点,随便指定任意一台均可</span><br><span class="line">es_port: 9200           #ES端口号</span><br><span class="line">es_username: elastic    # 如果ES使用了X-pack安全验证,则需要配置此项,否则注释</span><br><span class="line">es_password: password   # 同上</span><br><span class="line">writeback_index: elastalert_status  #ElastAlert索引名</span><br><span class="line">alert_time_limit:       #如果告警发送失败,则会在下面时间范围内尝试重新发送</span><br><span class="line">  days: 2</span><br></pre></td></tr></table></figure><ul><li>配置钉钉报警</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/xuyaoqiang/elastalert-dingtalk-plugin.git</span><br><span class="line">cd elastalert-dingtalk-plugin</span><br><span class="line">pip3 install -r requirements.txt -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com</span><br><span class="line">cp -r elastalert_modules /data/elastalert/</span><br></pre></td></tr></table></figure><hr><h3 id="Rule规则"><a href="#Rule规则" class="headerlink" title="Rule规则"></a>Rule规则</h3><p>官方支持很多Rule模式,在<code>example_rules</code>目录下也有很多参考Rule可以参考.一般常用的是类型(type)是<code>frequence</code></p><p>rule的yaml配置要放在<code>config.yml</code>配置文件中定义的目录下,我这里是rule目录.</p><p>下面这个rule是监控Nginx的5XX状态码,并且调用钉钉告警</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">#rule名字,必须唯一</span><br><span class="line">name:  the count of servnginx log that reponse status code is 5xx and it appears greater than 5 in the period 1 minute</span><br><span class="line"></span><br><span class="line">#类型,官方提供多种类型</span><br><span class="line">type: frequency</span><br><span class="line"></span><br><span class="line">#ES索引,支持通配符</span><br><span class="line">index: logstash-*-nginx-access-*</span><br><span class="line"></span><br><span class="line">#在timeframe时间内,匹配到多少个结果便告警</span><br><span class="line">num_events: 1</span><br><span class="line"></span><br><span class="line">#监控周期.默认是minutes: 1</span><br><span class="line">timeframe:</span><br><span class="line">  seconds: 5  </span><br><span class="line">  </span><br><span class="line">#匹配模式.</span><br><span class="line">filter:</span><br><span class="line">- range:</span><br><span class="line">    status:</span><br><span class="line">      from: 500</span><br><span class="line">      to: 599</span><br><span class="line">      </span><br><span class="line">#告警方式,下面是调用第三方的钉钉告警</span><br><span class="line">alert:</span><br><span class="line">- &quot;elastalert_modules.dingtalk_alert.DingTalkAlerter&quot;</span><br><span class="line"></span><br><span class="line">#钉钉的webhook</span><br><span class="line">dingtalk_webhook: &quot;https://oapi.dingtalk.com/robot/send?access_token=&quot;  #参考地址,需要自行配置</span><br><span class="line">dingtalk_msgtype: text</span><br><span class="line"></span><br><span class="line">#原生的告警信息不友好,自定义告警内容的格式</span><br><span class="line">alert_text: &quot;</span><br><span class="line">域    名: &#123;&#125;\n</span><br><span class="line">调用方式: &#123;&#125;\n</span><br><span class="line">请求链接: &#123;&#125;\n</span><br><span class="line">状 态 码: &#123;&#125;\n</span><br><span class="line">后端服务器: &#123;&#125;\n</span><br><span class="line">数      量: &#123;&#125;</span><br><span class="line">&quot;</span><br><span class="line">alert_text_type: alert_text_only</span><br><span class="line"></span><br><span class="line">#告警内容</span><br><span class="line">alert_text_args:</span><br><span class="line">- domain</span><br><span class="line">- request_method</span><br><span class="line">- request</span><br><span class="line">- status</span><br><span class="line">- upstreamaddr</span><br><span class="line">- num_hits</span><br></pre></td></tr></table></figure><p>测试Rule文件是否正确.在elastalert目录下执行下个命令可以测试某个rule是否正常工作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/bin/elastalert-test-rule --config config.yaml rule/nginx.yaml</span><br></pre></td></tr></table></figure><p>这一步可能会有一些报错情况.一般都是扩展模块版本或者依赖关系的问题.比如下面这个问题,就需要执行<code>pip3 install jira==2.0.0</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/local/bin/elastalert-test-rule&quot;, line 11, in &lt;module&gt;</span><br><span class="line">    load_entry_point(&apos;elastalert==0.1.20&apos;, &apos;console_scripts&apos;, &apos;elastalert-test-rule&apos;)()</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 476, in load_entry_point</span><br><span class="line">    return get_distribution(dist).load_entry_point(group, name)</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 2700, in load_entry_point</span><br><span class="line">    return ep.load()</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 2318, in load</span><br><span class="line">    return self.resolve()</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 2324, in resolve</span><br><span class="line">    module = __import__(self.module_name, fromlist=[&apos;__name__&apos;], level=0)</span><br><span class="line">  File &quot;/usr/local/lib/python3.6/site-packages/elastalert/test_rule.py&quot;, line 20, in &lt;module&gt;</span><br><span class="line">    import elastalert.config</span><br><span class="line">  File &quot;/usr/local/lib/python3.6/site-packages/elastalert/config.py&quot;, line 99</span><br><span class="line">    raise EAException(&quot;Could not import module %s: %s&quot; % (module_name, e)), None, sys.exc_info()[2]</span><br><span class="line">                                                                          ^</span><br><span class="line">SyntaxError: invalid syntax</span><br><span class="line"></span><br><span class="line">或者下面这个错误</span><br><span class="line"></span><br><span class="line">[work@idc-function-elk10 elastalert]$ /usr/local/bin/elastalert-test-rule --config config.yaml rule/nginx.yaml</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 570, in _build_master</span><br><span class="line">    ws.require(__requires__)</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 888, in require</span><br><span class="line">    needed = self.resolve(parse_requirements(requirements))</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 779, in resolve</span><br><span class="line">    raise VersionConflict(dist, req).with_context(dependent_req)</span><br><span class="line">pkg_resources.VersionConflict: (elastalert 0.1.20 (/usr/local/lib/python3.6/site-packages), Requirement.parse(&apos;elastalert==0.2.4&apos;))</span><br><span class="line"></span><br><span class="line">During handling of the above exception, another exception occurred:</span><br><span class="line"></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/local/bin/elastalert-test-rule&quot;, line 6, in &lt;module&gt;</span><br><span class="line">    from pkg_resources import load_entry_point</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 3095, in &lt;module&gt;</span><br><span class="line">    @_call_aside</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 3079, in _call_aside</span><br><span class="line">    f(*args, **kwargs)</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 3108, in _initialize_master_working_set</span><br><span class="line">    working_set = WorkingSet._build_master()</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 572, in _build_master</span><br><span class="line">    return cls._build_from_requirements(__requires__)</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 585, in _build_from_requirements</span><br><span class="line">    dists = ws.resolve(reqs, Environment())</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 774, in resolve</span><br><span class="line">    raise DistributionNotFound(req, requirers)</span><br><span class="line">pkg_resources.DistributionNotFound: The &apos;jira&gt;=2.0.0&apos; distribution was not found and is required by elastalert</span><br></pre></td></tr></table></figure><hr><h3 id="执行ElastAlert"><a href="#执行ElastAlert" class="headerlink" title="执行ElastAlert"></a>执行ElastAlert</h3><p>一切没问题后,就可以执行ElastAlert.如果是针对单个Rule执行就使用下列命令.(在ElastAlert目录下)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk10 elastalert]# python3  -m elastalert.elastalert --verbose --rule /data/elastalert/rule/nginx.yaml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1 rules loaded</span><br><span class="line">INFO:elastalert:Starting up</span><br><span class="line">INFO:elastalert:Disabled rules are: []</span><br><span class="line">INFO:elastalert:Sleeping for 59.999906 seconds</span><br><span class="line">INFO:elastalert:Queried rule the count of servnginx log that reponse status code is 5xx and it appears greater than 5 in the period 1 minute from 2020-08-26 09:11 CST to 2020-08-26 09:26 CST: 10000 / 10000 hits (scrolling..)</span><br><span class="line">INFO:elastalert:Queried rule the count of servnginx log that reponse status code is 5xx and it appears greater than 5 in the period 1 minute from 2020-08-26 09:11 CST to 2020-08-26 09:26 CST: 20000 / 10000 hits (scrolling..)</span><br></pre></td></tr></table></figure><p>等待几秒钟后,钉钉会收到告警(我这里用的是200状态码测试).报警内容是Rule配置文件中自定义的格式和内容</p><p><img src="https://img2.jesse.top/image-20200826094746820.png" alt="image-20200826094746820"></p><hr><h3 id="Rule2-监控php-fpm的Fatal错误信息"><a href="#Rule2-监控php-fpm的Fatal错误信息" class="headerlink" title="Rule2. 监控php-fpm的Fatal错误信息"></a>Rule2. 监控php-fpm的Fatal错误信息</h3><p>fpm的错误日志也收集到了ELK中.我们期望只要pfm日志中出现”Fatal”关键字错误信息就立即告警.最初计划是用ElastAlert的黑名单(blacklist)类型的Rule.但是由于fpm的错误日志没有解析,而是直接保存原始日志,所以不符合要求.</p><p>参考github上我提的ISSUE:<a href="https://github.com/Yelp/elastalert/issues/2937" target="_blank" rel="noopener">balacklist query hits but no matches no alerts</a></p><p>也可以用Any类型的type.Rule文件如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk10 elastalert]# sed &apos;/^#/d&apos; rule/php-fpm.yaml | sed &apos;/^$/d&apos;</span><br><span class="line">name: monitor the fatal,error log in php-fpm log</span><br><span class="line">type: any</span><br><span class="line">index: logstash-*-fpm-error-*</span><br><span class="line">num_events: 1</span><br><span class="line">timeframe:</span><br><span class="line">  seconds: 5</span><br><span class="line">filter:</span><br><span class="line"> - query:</span><br><span class="line">     query_string:</span><br><span class="line">       query: &quot;message: \&quot;PHP Fatal\&quot;&quot;  #匹配Fatal关键字</span><br><span class="line">alert:</span><br><span class="line">- &quot;elastalert_modules.dingtalk_alert.DingTalkAlerter&quot;</span><br><span class="line">dingtalk_webhook: &quot;https://oapi.dingtalk.com/robot/send?access_token=&quot; </span><br><span class="line">dingtalk_msgtype: text</span><br><span class="line">alert_text: &quot;</span><br><span class="line"> 主机: &#123;&#125;\n</span><br><span class="line"> IP地址: &#123;&#125;\n</span><br><span class="line"> 业务线: &#123;&#125;\n</span><br><span class="line"> 日志类型: &#123;&#125;\n</span><br><span class="line"> 完整日志: &#123;&#125;</span><br><span class="line">&quot;</span><br><span class="line">alert_text_type: alert_text_only</span><br><span class="line">alert_text_args:</span><br><span class="line">  - host.name</span><br><span class="line">  - host.ip</span><br><span class="line">  - fields.project</span><br><span class="line">  - fields.type</span><br><span class="line">  - message</span><br></pre></td></tr></table></figure><hr><h3 id="启动ElastAlert"><a href="#启动ElastAlert" class="headerlink" title="启动ElastAlert"></a>启动ElastAlert</h3><p>开启一个Screen然后,使用nohup挂起执行.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python3  -m elastalert.elastalert --verbose &amp;</span><br></pre></td></tr></table></figure><hr><h3 id="钉钉告警二次开发"><a href="#钉钉告警二次开发" class="headerlink" title="钉钉告警二次开发"></a>钉钉告警二次开发</h3><p>当前日志告警只是简单的发送到告警群,由于没有艾特相关人员,所以大家还是无法第一时间看到告警信息,所以需要增加这个功能,大致思路是根据业务线来艾特相关负责人.</p><p>但是中台的业务线有些复杂,因为不同的项目负责人不同.所以需要特殊对待.</p><p><strong>准备工作</strong>:</p><p>日志告警中必须含有以下几个字段:</p><ul><li>业务线</li><li>日志类型</li><li>如果是Nginx日志,则需要有Nginx的域名</li></ul><p>修改原生的钉钉告警的alert动态方法,内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">   </span><br><span class="line">   def alert(self, matches):</span><br><span class="line">        headers = &#123;</span><br><span class="line">            &quot;Content-Type&quot;: &quot;application/json&quot;,</span><br><span class="line">            &quot;Accept&quot;: &quot;application/json;charset=utf-8&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        #body拿到的是告警内容字符串</span><br><span class="line">        body = self.create_alert_body(matches) </span><br><span class="line">        #利用正则找到告警日志中的type关键字,也就是日志类别.当前主要有Nginx日志和fpm日志</span><br><span class="line">        res_type = re.findall(&quot;type: ([a-z]+)&quot;, body)</span><br><span class="line">        #找到业务线,当前有hsq,iqg,msf.bbh,mg等业务线</span><br><span class="line">        res_Project = re.findall(r&quot;业务线: ([a-z]+)&quot;, body)</span><br><span class="line">        #如果告警日志没有相关字段,则抛出异常</span><br><span class="line">        if not res_type or not res_Project:</span><br><span class="line">            raise EAException(&quot;告警字段中日志类型或者业务线没有配置&quot;)</span><br><span class="line">            </span><br><span class="line">        #将正则匹配到的列表类型转换为字符串</span><br><span class="line">        Type = &quot;&quot;.join(res_type)</span><br><span class="line">        Project = &quot;&quot;.join(res_Project)</span><br><span class="line">        </span><br><span class="line">        #根据相关业务,艾特相关人员</span><br><span class="line">        if Project == &quot;hsq&quot;:</span><br><span class="line">            at_list = [&apos;1560xxxxxx&apos;]</span><br><span class="line">        elif Project == &quot;iqg&quot;:</span><br><span class="line">            at_list = [&quot;137xxxxxx&quot;]</span><br><span class="line">        elif Project == &quot;bbh&quot;:</span><br><span class="line">            at_list = [&quot;176xxxxxx&quot;]</span><br><span class="line">        elif Project == &quot;msf&quot;:</span><br><span class="line">            at_list = [&quot;180xxxxxx&quot;]</span><br><span class="line">        #如果是中台业务线,并且是Nginx的告警,则需要艾特具体人员</span><br><span class="line">        elif Project == &quot;mg&quot;:</span><br><span class="line">            if &quot;nginx&quot; in Type:</span><br><span class="line">              #匹配到域名</span><br><span class="line">                mg_Project = re.findall(&quot;domain: (.*)\.doweidu\.com&quot;, body)[0]</span><br><span class="line">                #如果是交易中台的域名</span><br><span class="line">                if mg_Project == &quot;trade&quot;: </span><br><span class="line">                    at_list = [&quot;177xxxxxx&quot;]</span><br><span class="line">                #如果是消息中台域名</span><br><span class="line">                elif mg_Project == &quot;message.center&quot;:</span><br><span class="line">                    at_list = [&quot;170xxxxxx&quot;]</span><br><span class="line">                #如果是商品中台域名</span><br><span class="line">                elif mg_Project == &quot;goods.center&quot;:</span><br><span class="line">                    at_list = [&quot;186xxxxxx&quot;]</span><br><span class="line">                #否则艾特中台负责人</span><br><span class="line">                else:</span><br><span class="line">                    at_list = [&quot;186xxxxx&quot;]</span><br><span class="line">            else:</span><br><span class="line">                at_list = [&quot;186xxxxx&quot;]</span><br><span class="line"></span><br><span class="line">#为了防止遗漏,如果没有at_list变量,则艾特我本人.使用locals().keys()可以判断某个变量是否被定义</span><br><span class="line">        if (not &quot;at_list&quot; in locals().keys()): at_list = [&quot;17749739691&quot;]</span><br><span class="line">        payload = &#123;</span><br><span class="line">            &quot;msgtype&quot;: self.dingtalk_msgtype,</span><br><span class="line">            &quot;text&quot;: &#123;</span><br><span class="line">                &quot;content&quot;: body</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;at&quot;: &#123;</span><br><span class="line">                &quot;atMobiles&quot;: at_list,   #艾特相关人员</span><br><span class="line">                &quot;isAtAll&quot;:False</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        try:</span><br><span class="line">            response = requests.post(self.dingtalk_webhook_url, </span><br><span class="line">                        data=json.dumps(payload, cls=DateTimeEncoder),</span><br><span class="line">                        headers=headers)</span><br><span class="line">            response.raise_for_status()</span><br><span class="line">        except RequestException as e:</span><br><span class="line">            raise EAException(&quot;Error request to Dingtalk: &#123;0&#125;&quot;.format(str(e)))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;使用ElastAlert-ELK实现日志监控钉钉告警&quot;&gt;&lt;a href=&quot;#使用ElastAlert-ELK实现日志监控钉钉告警&quot; class=&quot;headerlink&quot; title=&quot;使用ElastAlert+ELK实现日志监控钉钉告警&quot;&gt;&lt;/a&gt;使用ElastAlert+ELK实现日志监控钉钉告警&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;目前公司使用ELK做日志收集和展示分析.所以想对一些关键日志进行监控告警.比如Nginx的5xx日志,比如php-fpm的Fatal严重错误日志等.通过监控ES的日志数据,然后使用Python调用钉钉接口来实现日志的告警&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;ElastAlert介绍&quot;&gt;&lt;a href=&quot;#ElastAlert介绍&quot; class=&quot;headerlink&quot; title=&quot;ElastAlert介绍&quot;&gt;&lt;/a&gt;ElastAlert介绍&lt;/h3&gt;&lt;p&gt;ElastAlert是一个开源的工具,用于从Elastisearch中检索数据,并根据匹配模式发出告警.github项目地址如下:&lt;a href=&quot;https://github.com/Yelp/elastalert&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Yelp/elastalert&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;官方文档如下:&lt;a href=&quot;https://elastalert.readthedocs.io/en/latest/elastalert.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://elastalert.readthedocs.io/en/latest/elastalert.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;它支持多种监控模式和告警方式,具体可以查阅Github项目介绍.但是自带的ElastAlert并不支持钉钉告警,在github上有第三方的钉钉python项目.地址如下:&lt;a href=&quot;https://github.com/xuyaoqiang/elastalert-dingtalk-plugin&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/xuyaoqiang/elastalert-dingtalk-plugin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;第三方的钉钉告警插件并没有艾特相关人员的功能,所以我再此基础上进行了二次开发,增加了这个功能&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>ELK使用x-pack插件实现权限控制</title>
    <link href="https://jesse.top/2020/08/25/elk/ELK%E4%BD%BF%E7%94%A8x-pack%E6%8F%92%E4%BB%B6%E5%AE%9E%E7%8E%B0%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6/"/>
    <id>https://jesse.top/2020/08/25/elk/ELK使用x-pack插件实现权限控制/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.312Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ELK使用x-pack插件实现权限控制"><a href="#ELK使用x-pack插件实现权限控制" class="headerlink" title="ELK使用x-pack插件实现权限控制"></a>ELK使用x-pack插件实现权限控制</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>Elsticsearch集群搭建完成,使用Kibana可以直接访问WEB界面,或者使用curl命令行工具也可以直接访问ES的索引,并且可以直接对索引进行增删改查的操作.这在生产中可能就会有严重的安全隐患.</p><p>为了防止Kibana的重要管理功能配置,或者ES的重要索引数据被认为的误删,误配置.就有必要对ES和Kibana进行权限管控.</p><p>而Xpack插件就非常方便,完美的实现了这个功能</p><hr><h3 id="Xpack介绍"><a href="#Xpack介绍" class="headerlink" title="Xpack介绍"></a>Xpack介绍</h3><p>Xpack能够对网络流量进行加密、创建和管理用户、定义能够保护索引和集群级别访问权限的角色，并且使用 Spaces 为 Kibana提供全面保护.在Elastic Stack7.x中已经免费开放基础版本功能.但是更高版本的X-PACK仍然需要付费购买.</p><p>而且在7.x版本中Xpack默认就已经安装了,无需另行安装插件.</p><p>x-pack详细介绍请点击<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.7/setup-xpack.html" target="_blank" rel="noopener">官方文档</a></p><p>x-pack免费版提供一下功能</p><ul><li>TLS 功能。 可对通信进行加密；</li><li>文件和原生 Realm。 可用于创建和管理用户；</li><li>基于角色的访问控制。 可用于控制用户对集群 API 和索引的访问权限；</li><li>通过针对 Kibana Spaces 的安全功能，还可允许在Kibana 中实现多租户</li></ul><a id="more"></a><hr><h3 id="ELK集群中配置Xpack"><a href="#ELK集群中配置Xpack" class="headerlink" title="ELK集群中配置Xpack"></a>ELK集群中配置Xpack</h3><p>主要参考博客:<a href="http://www.eryajf.net/3500.html" target="_blank" rel="noopener">http://www.eryajf.net/3500.html</a></p><p>一.在任意一台ES节点中生成证书</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#直接回车生成证书,无需设置密码</span><br><span class="line">[root@idc-function-elk10 bin]# ./elasticsearch-certutil ca</span><br><span class="line">[root@idc-function-elk10 bin]# ./elasticsearch-certutil cert --ca elastic-stack-ca.p12</span><br></pre></td></tr></table></figure><p>二.将生成的证书拷贝到<code>/etc/elasticsearch/</code>目录下(我这里是拷贝到Ansible的目录下)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk10 bin]# cd ..</span><br><span class="line"></span><br><span class="line">#拷贝elastic-certificates.p12和elastic-stack-ca.p12证书文件到ES的目录下</span><br><span class="line">[root@idc-function-elk10 elasticsearch]# ls</span><br><span class="line">bin  elastic-certificates.p12  elastic-stack-ca.p12  jdk  lib  LICENSE.txt  modules  NOTICE.txt  plugins  README.asciidoc</span><br></pre></td></tr></table></figure><p>我这里使用Ansible来发布</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- name: 拷贝x-pack证书文件</span><br><span class="line">        copy: src=files/&#123;&#123; item &#125;&#125; dest=/etc/elasticsearch/&#123;&#123; item &#125;&#125; owner=elasticsearch  group=elasticsearch</span><br><span class="line">        with_items:</span><br><span class="line">           - elastic-certificates.p12</span><br><span class="line">           - elastic-stack-ca.p12</span><br></pre></td></tr></table></figure><p>三.修改ES配置文件.在所有节点的ES配置文件中新增下面配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/elasticsearch/elasticsearch.yml</span><br><span class="line"></span><br><span class="line">xpack.security.enabled: true</span><br><span class="line">xpack.security.transport.ssl.enabled: true</span><br><span class="line">xpack.security.transport.ssl.verification_mode: certificate</span><br><span class="line">xpack.security.transport.ssl.keystore.path: /etc/elasticsearch/elastic-certificates.p12</span><br><span class="line">xpack.security.transport.ssl.truststore.path: /etc/elasticsearch/elastic-certificates.p12</span><br></pre></td></tr></table></figure><p>我仍然使用Ansible将配置文件同步到所有ES节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- name: 拷贝elasticsearch配置文件</span><br><span class="line">        template: src=templates/elasticsearch.yml.j2 dest=/etc/elasticsearch/elasticsearch.yml owner=elasticsearch  group=elasticsearch</span><br></pre></td></tr></table></figure><blockquote><p>ES配置文件同步完成后,重启ES服务</p></blockquote><p>四.为内置账号添加密码.</p><p>ES中内置了几个管理其他集成组件的账号即：<code>apm_system</code>, <code>beats_system</code>, <code>elastic</code>, <code>kibana</code>, <code>logstash_system</code>, <code>remote_monitoring_user</code>，使用之前，首先需要设置一下密码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive</span><br></pre></td></tr></table></figure><p>为了简便起见,我使用了同一个密码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Enter password for [elastic]: [root@idc-function-elk10 bin]# ./elasticsearch-setup-passwords interactive</span><br><span class="line">Initiating the setup of passwords for reserved users elastic,apm_system,kibana,logstash_system,beats_system,remote_monitoring_user.</span><br><span class="line">You will be prompted to enter passwords as the process progresses.</span><br><span class="line">Please confirm that you would like to continue [y/N]y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Enter password for [elastic]:</span><br><span class="line">Reenter password for [elastic]:</span><br><span class="line">Enter password for [apm_system]:</span><br><span class="line">Reenter password for [apm_system]:</span><br><span class="line">Enter password for [kibana]:</span><br><span class="line">Reenter password for [kibana]:</span><br><span class="line">Enter password for [logstash_system]:</span><br><span class="line">Reenter password for [logstash_system]:</span><br><span class="line">Enter password for [beats_system]:</span><br><span class="line">Reenter password for [beats_system]:</span><br><span class="line">Enter password for [remote_monitoring_user]:</span><br><span class="line">Reenter password for [remote_monitoring_user]:</span><br><span class="line">Changed password for user [apm_system]</span><br><span class="line">Changed password for user [kibana]</span><br><span class="line">Changed password for user [logstash_system]</span><br><span class="line">Changed password for user [beats_system]</span><br><span class="line">Changed password for user [remote_monitoring_user]</span><br><span class="line">Changed password for user [elastic]</span><br></pre></td></tr></table></figure><p>此时,登陆ES的集群管理页面就需要账号密码认证了.需要使用elastic账号和密码进行登录</p><p><img src="https://img2.jesse.top/image-20200812145302139.png" alt="image-20200812145302139"></p><p>五.由于ES开启了身份验证,所以此时Kibana和logstash都无法访问ES了.在logstash配置文件的output中添加ES的账号密码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">output &#123;</span><br><span class="line">        elasticsearch &#123;</span><br><span class="line">            hosts =&gt; [&quot;172.16.20.101:9200&quot;,&quot;172.16.20.110:9200&quot;,&quot;172.16.20.107:9200&quot;,&quot;172.16.20.108:9200&quot;,&quot;172.16.20.102:9200&quot;,&quot;172.16.20.103:9200&quot;,&quot;172.16.20.104:9200&quot;,&quot;172.16.20.105:9200&quot;,&quot;172.16.20.106:9200&quot;,&quot;172.16.20.109:9200&quot;]</span><br><span class="line">            index =&gt; &quot;logstash-%&#123;[fields][project]&#125;-%&#123;[fields][type]&#125;-%&#123;[fields][level]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">            user =&gt; &quot;elastic&quot;</span><br><span class="line">            password =&gt; &quot;上一步设置的密码&quot;</span><br></pre></td></tr></table></figure><blockquote><p>使用Ansible将配置文件同步到所有logstash节点,然后重启logstash</p></blockquote><p>六. 为Kibana添加ES的elastic账号密码</p><p>在kibana服务器中配置访问ES集群密文账号.在弹出的用户密码中我写的是elastic的用户密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk05 bin]# ./kibana-keystore create --allow-root</span><br><span class="line">Created Kibana keystore in /var/lib/kibana/kibana.keystore</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk05 bin]# ./kibana-keystore add elasticsearch.username --allow-root</span><br><span class="line">Enter value for elasticsearch.username: *******</span><br><span class="line">[root@idc-function-elk05 bin]# ./kibana-keystore add elasticsearch.password --allow-root</span><br><span class="line">Enter value for elasticsearch.password: **********</span><br><span class="line"></span><br><span class="line">#重启kibana</span><br><span class="line">[root@idc-function-elk05 bin]# systemctl restart kibana</span><br></pre></td></tr></table></figure><p>PS: 有些文档中提到还需要在kibana.yml配置文件中新增以下内容,但是我没有添加也没问题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xpack.reporting.encryptionKey: &quot;a_random_string&quot;</span><br><span class="line">xpack.security.encryptionKey: &quot;something_at_least_32_characters&quot;</span><br></pre></td></tr></table></figure><hr><h3 id="Kibana角色权限控制"><a href="#Kibana角色权限控制" class="headerlink" title="Kibana角色权限控制"></a>Kibana角色权限控制</h3><p>安装配置了x-pack后,如果一切正常.此时登陆Kibana就需要用户密码认证.使用elastic管理员账号登陆,给Kibana配置角色和只读用户</p><p><img src="https://img2.jesse.top/image-20200812150302014.png" alt="image-20200812150302014"></p><p>登陆kibana以后,在管理页面中多了个安全性的功能菜单.</p><p><img src="https://img2.jesse.top/image-20200812150507260.png" alt="image-20200812150507260"></p><p>这篇博客详细介绍了Kibana的角色介绍:<a href="https://www.elastic.co/cn/blog/getting-started-with-elasticsearch-security" target="_blank" rel="noopener">https://www.elastic.co/cn/blog/getting-started-with-elasticsearch-security</a></p><p>一般只需要创建一个只读用户即可,防止用户对Kibana以及ES索引进行一些误操作.</p><p>1.创建一个只读的角色.该角色对默认的<code>logstash-*</code>有read权限.(如果有其他索引,可以自行添加).以及对所有工作区有read权限.</p><p><img src="https://img2.jesse.top/image-20200812150646871.png" alt="image-20200812150646871"></p><ol start="2"><li><p>然后添加一个账户,绑定该只读角色即可</p><p><img src="https://img2.jesse.top/image-20200812150801194.png" alt="image-20200812150801194"></p></li></ol><p>使用只读账号登陆Kibana以后,对Kibana只有只读权限,无法访问或者配置任何菜单功能</p><p><img src="https://img2.jesse.top/image-20200812150939231.png" alt="image-20200812150939231"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ELK使用x-pack插件实现权限控制&quot;&gt;&lt;a href=&quot;#ELK使用x-pack插件实现权限控制&quot; class=&quot;headerlink&quot; title=&quot;ELK使用x-pack插件实现权限控制&quot;&gt;&lt;/a&gt;ELK使用x-pack插件实现权限控制&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;Elsticsearch集群搭建完成,使用Kibana可以直接访问WEB界面,或者使用curl命令行工具也可以直接访问ES的索引,并且可以直接对索引进行增删改查的操作.这在生产中可能就会有严重的安全隐患.&lt;/p&gt;
&lt;p&gt;为了防止Kibana的重要管理功能配置,或者ES的重要索引数据被认为的误删,误配置.就有必要对ES和Kibana进行权限管控.&lt;/p&gt;
&lt;p&gt;而Xpack插件就非常方便,完美的实现了这个功能&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;Xpack介绍&quot;&gt;&lt;a href=&quot;#Xpack介绍&quot; class=&quot;headerlink&quot; title=&quot;Xpack介绍&quot;&gt;&lt;/a&gt;Xpack介绍&lt;/h3&gt;&lt;p&gt;Xpack能够对网络流量进行加密、创建和管理用户、定义能够保护索引和集群级别访问权限的角色，并且使用 Spaces 为 Kibana提供全面保护.在Elastic Stack7.x中已经免费开放基础版本功能.但是更高版本的X-PACK仍然需要付费购买.&lt;/p&gt;
&lt;p&gt;而且在7.x版本中Xpack默认就已经安装了,无需另行安装插件.&lt;/p&gt;
&lt;p&gt;x-pack详细介绍请点击&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/7.7/setup-xpack.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方文档&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;x-pack免费版提供一下功能&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TLS 功能。 可对通信进行加密；&lt;/li&gt;
&lt;li&gt;文件和原生 Realm。 可用于创建和管理用户；&lt;/li&gt;
&lt;li&gt;基于角色的访问控制。 可用于控制用户对集群 API 和索引的访问权限；&lt;/li&gt;
&lt;li&gt;通过针对 Kibana Spaces 的安全功能，还可允许在Kibana 中实现多租户&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>搭建Anaconda和jupyter notebook</title>
    <link href="https://jesse.top/2020/08/25/Linux-Service/%E6%90%AD%E5%BB%BAAnaconda%E5%92%8Cjupyter%20notebook/"/>
    <id>https://jesse.top/2020/08/25/Linux-Service/搭建Anaconda和jupyter notebook/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.309Z</updated>
    
    <content type="html"><![CDATA[<h2 id="搭建Anaconda和jupyter-notebook"><a href="#搭建Anaconda和jupyter-notebook" class="headerlink" title="搭建Anaconda和jupyter notebook"></a>搭建Anaconda和jupyter notebook</h2><h3 id="一-什么是Anaconda"><a href="#一-什么是Anaconda" class="headerlink" title="一.什么是Anaconda"></a>一.什么是Anaconda</h3><p>Anaconda可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。</p><h2 id="2-特点"><a href="#2-特点" class="headerlink" title="2. 特点"></a>2. 特点</h2><p>Anaconda具有如下特点：</p><ul><li>开源</li><li>安装过程简单</li><li>高性能使用Python和R语言</li><li>免费的社区支持</li></ul><p>其特点的实现主要基于Anaconda拥有的：</p><ul><li>conda包</li><li>环境管理器</li><li>1,000+开源库</li></ul><a id="more"></a><hr><h3 id="3-Anaconda安装"><a href="#3-Anaconda安装" class="headerlink" title="3.Anaconda安装"></a>3.Anaconda安装</h3><p><a href="https://www.anaconda.com/" target="_blank" rel="noopener">Anaconda官方</a>提供了三种不同的版本,除了Individual Edition个人版免费以外,其他2种版本都是收费的.所以这里选择Anaconda个人版.</p><h4 id="3-1-Docker安装-有坑-弃用了"><a href="#3-1-Docker安装-有坑-弃用了" class="headerlink" title="3.1 Docker安装(有坑,弃用了)"></a>3.1 Docker安装(有坑,弃用了)</h4><p>我刚开始选择的是用Docker运行,使用的是Anaconda的镜像:<a href="https://hub.docker.com/r/continuumio/anaconda3" target="_blank" rel="noopener">continuumio/anaconda3</a>.用Dockerfile在此基础之上安装了多个python扩展模块自定义了一个镜像.Dockerfile内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">FROM continuumio/anaconda3:latest</span><br><span class="line">LABEL author=jessehuang</span><br><span class="line">LABEL description=&quot;自定义制作annaconda镜像&quot;</span><br><span class="line"></span><br><span class="line">#更新debian源.使用清华大学的debian 10 brust的源</span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y apt-transport-https ca-certificates</span><br><span class="line"></span><br><span class="line">ADD sources.list /etc/apt/sources.list</span><br><span class="line"></span><br><span class="line">#安装python模块的依赖.否则thriftpy的安装会出现问题</span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \</span><br><span class="line">    python-dev \</span><br><span class="line">    vim \</span><br><span class="line">    gcc \</span><br><span class="line">    g++ \</span><br><span class="line">    libsasl2-dev </span><br><span class="line"></span><br><span class="line">#安装python扩展模块</span><br><span class="line"></span><br><span class="line">ADD requirement.txt /tmp/requirement.txt </span><br><span class="line">RUN pip install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r /tmp/requirement.txt</span><br><span class="line"></span><br><span class="line">RUN /opt/conda/bin/conda install jupyter -y --quiet &amp;&amp; mkdir /opt/notebooks </span><br><span class="line">EXPOSE 8888</span><br><span class="line"></span><br><span class="line">#启动命令</span><br><span class="line">CMD [&quot;/opt/conda/bin/jupyter&quot;,&quot;notebook&quot;, &quot;--notebook-dir=/opt/notebooks&quot;,&quot;--ip=&apos;*&apos;&quot;,&quot;--port=8888&quot;,&quot;--no-browser&quot;,&quot;--allow-root&quot;]</span><br></pre></td></tr></table></figure><p>requirement.txt内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pyecharts</span><br><span class="line">pymysql</span><br><span class="line">psycopg2-binary</span><br><span class="line">six</span><br><span class="line">bit_array</span><br><span class="line">thriftpy</span><br><span class="line">thrift_sasl==0.2.1</span><br><span class="line">impyla</span><br><span class="line">jupyter_contrib_nbextensions</span><br></pre></td></tr></table></figure><p>docker部署的anaconda在运行jupyter notebook的时候提示<code>kernel restarting</code>.容器日志内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[I 07:25:59.512 NotebookApp] Restoring connection for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.541 NotebookApp] Starting buffering for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.565 NotebookApp] Restoring connection for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.595 NotebookApp] Starting buffering for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.620 NotebookApp] Restoring connection for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[W 07:26:04.401 NotebookApp] Replacing stale connection: 63b3942c-a225-4692-8989-893af2c992cc:743009a3e3684e64a9b9fcceedc30775</span><br><span class="line">[W 07:26:05.062 NotebookApp] Replacing stale connection: a9543d90-08b5-4395-8a3c-f7bbec9f44a1:0bc5b6496d59406b82ab54a181db7215</span><br></pre></td></tr></table></figure><p>这个故障在搜遍了Google和baidu后也无解,尝试了网上各种解决办法也不行.所以果断弃掉,转而使用Linux虚拟机部署</p><h3 id="3-2-Linux部署安装-成功"><a href="#3-2-Linux部署安装-成功" class="headerlink" title="3.2 Linux部署安装(成功)"></a>3.2 Linux部署安装(成功)</h3><p>Anaconda的<a href="https://docs.anaconda.com/anaconda/install/linux/" target="_blank" rel="noopener">安装官方文档</a>.建议已root用户安装,或者你的普通用户有执行<code>/home/$(USER)/anaconda3/bin/python3</code>的sudo权限</p><p>1.首先下载python的anaconda安装脚本,建议选择python3的版本</p><p><a href="https://www.anaconda.com/products/individual#linux" target="_blank" rel="noopener">https://www.anaconda.com/products/individual#linux</a></p><p>2.安装依赖包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install libXcomposite libXcursor libXi libXtst libXrandr alsa-lib mesa-libEGL libXdamage mesa-libGL libXScrnSaver</span><br></pre></td></tr></table></figure><p>3.执行下载下来的脚本文件.根据提示,一直选择默认就可以了,官方不建议更改安装路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash ~/Anaconda3-2020.02-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><blockquote><p>We recommend you accept the default install location. Do not choose the path as /usr for the Anaconda/Miniconda installation.</p></blockquote><p>4.如果提示<code>Thank you for installing Anaconda&lt;2 or 3&gt;!</code>则说明安装完成,应用环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><p>5.验证安装结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">condal list #显示已经安装的包和版本好</span><br></pre></td></tr></table></figure><p>至此,Anaconda和anaconda自带的jupyter notebook都已经安装完了</p><hr><h3 id="4-jupyter-notebook-扩展模块安装"><a href="#4-jupyter-notebook-扩展模块安装" class="headerlink" title="4.jupyter notebook 扩展模块安装"></a>4.jupyter notebook 扩展模块安装</h3><p>公司的BI团队需要使用jupyter notebook访问MySQL、pgsql、hive,画图等工作,所以需要安装python的部分扩展模块.首先查看服务器上pip版本.需要使用python3的pip来安装模块</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#显示pip版本是python3</span><br><span class="line">(base) [root@anaconda ~]# pip --version</span><br><span class="line">pip 20.0.2 from /root/anaconda3/lib/python3.7/site-packages/pip (python 3.7)</span><br><span class="line">(base) [root@anaconda ~]#</span><br></pre></td></tr></table></figure><ol><li>安装依赖文件,否则安装<code>thriftpy</code>模块会报错</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install gcc-c++ gcc python3-devel python-dev cyrus-sasl cyrus-sasl-devel cyrus-sasl-lib</span><br></pre></td></tr></table></figure><blockquote><p>如果是ubuntu系统,则需要安装如下安装包: apt-get install -y python-dev gcc g++ libsasl2-dev</p></blockquote><ol start="2"><li>编写requirement文件,将所需要安装的python扩展模块加入到文件中</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pyecharts</span><br><span class="line">pymysql</span><br><span class="line">psycopg2-binary</span><br><span class="line">six</span><br><span class="line">bit_array</span><br><span class="line">thriftpy</span><br><span class="line">thrift_sasl==0.2.1</span><br><span class="line">impyla</span><br><span class="line">jupyter_contrib_nbextensions</span><br></pre></td></tr></table></figure><p>3.安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r requirement.txt</span><br></pre></td></tr></table></figure><p>安装完依赖包<code>jupyter_contrib_nbextensions</code>以后,安装一个服务:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter-contrib-nbextension install --user</span><br></pre></td></tr></table></figure><p>4.初始化jupyter配置文件,会在默认路径下初始化一个配置文件<code>/root/.jupyter/jupyter_notebook_config.py</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure><p>5.编辑文件,修改如下配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.ip=&apos;当前服务器IP&apos;</span><br><span class="line">c.NotebookApp.notebook_dir = u&apos;/data/notebooks&apos; #jupyter笔记的工作目录</span><br><span class="line">c.NotebookApp.open_browser = False #服务端无需启动浏览器</span><br><span class="line">c.NotebookApp.port = 80 #监听端口,默认是8888</span><br><span class="line">c.NotebookApp.allow_root = True  #允许root用户运行jupyter notebook</span><br><span class="line">c.NotebookApp.allow_origin = &apos;*&apos; #允许所有来源访问,解决跨域问题</span><br><span class="line">c.NotebookApp.quit_button = False #关闭jupyter notebook浏览器界面的quit按钮功能.因为quit按钮会关闭jupyter服务</span><br></pre></td></tr></table></figure><p>6.启动服务.记下pid号,后面要重启</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup jupyter-notebook --config=/root/.jupyter/jupyter_notebook_config.py &amp;</span><br></pre></td></tr></table></figure><p>7.打开浏览器,输入IP地址,此时会进入jupyter的界面.要求输入Token,或者使用Token设置一个密码</p><p><img src="https://img2.jesse.top/image-20200708105619499.png" alt="image-20200708105619499"></p><p>8.根据提示,使用命令<code>jupyter notebook list</code>查看Token</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">base) root@anaconda:/# jupyter notebook list</span><br><span class="line">Currently running servers:</span><br><span class="line">http://localhost:8888/?token=5469d940ce3a70950299e5c907e1d47b09acc61457348cd9 :: /data/notebooks</span><br></pre></td></tr></table></figure><p>9.拿到Token后,在浏览器中下方位置设置一个新密码.</p><p><img src="https://img2.jesse.top/image-20200708105756748.png" alt="image-20200708105756748"></p><p>10.设置完密码后,成功登陆了jupyter的工作界面.此时kill掉jupyter进程,重启</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#关闭进程</span><br><span class="line">kill $PID</span><br><span class="line"></span><br><span class="line">#重新启动</span><br><span class="line">nohup jupyter-notebook --config=/root/.jupyter/jupyter_notebook_config.py &amp;</span><br></pre></td></tr></table></figure><p>11.此时再次打开浏览器,就提示输入密码</p><p><img src="https://img2.jesse.top/image-20200708110014017.png" alt="image-20200708110014017"></p><p>12.输入密码后,成功登陆</p><p><img src="https://img2.jesse.top/image-20200708110048546.png" alt="image-20200708110048546"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;搭建Anaconda和jupyter-notebook&quot;&gt;&lt;a href=&quot;#搭建Anaconda和jupyter-notebook&quot; class=&quot;headerlink&quot; title=&quot;搭建Anaconda和jupyter notebook&quot;&gt;&lt;/a&gt;搭建Anaconda和jupyter notebook&lt;/h2&gt;&lt;h3 id=&quot;一-什么是Anaconda&quot;&gt;&lt;a href=&quot;#一-什么是Anaconda&quot; class=&quot;headerlink&quot; title=&quot;一.什么是Anaconda&quot;&gt;&lt;/a&gt;一.什么是Anaconda&lt;/h3&gt;&lt;p&gt;Anaconda可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。&lt;/p&gt;
&lt;h2 id=&quot;2-特点&quot;&gt;&lt;a href=&quot;#2-特点&quot; class=&quot;headerlink&quot; title=&quot;2. 特点&quot;&gt;&lt;/a&gt;2. 特点&lt;/h2&gt;&lt;p&gt;Anaconda具有如下特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;开源&lt;/li&gt;
&lt;li&gt;安装过程简单&lt;/li&gt;
&lt;li&gt;高性能使用Python和R语言&lt;/li&gt;
&lt;li&gt;免费的社区支持&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其特点的实现主要基于Anaconda拥有的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;conda包&lt;/li&gt;
&lt;li&gt;环境管理器&lt;/li&gt;
&lt;li&gt;1,000+开源库&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Linux-Service" scheme="https://jesse.top/categories/Linux-Service/"/>
    
    
      <category term="Anaconda" scheme="https://jesse.top/tags/Anaconda/"/>
    
  </entry>
  
  <entry>
    <title>ELK收集mysql5.7慢日志</title>
    <link href="https://jesse.top/2020/08/25/elk/ELK%E6%94%B6%E9%9B%86mysql5.7%E6%85%A2%E6%97%A5%E5%BF%97/"/>
    <id>https://jesse.top/2020/08/25/elk/ELK收集mysql5.7慢日志/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.312Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ELK收集mysql5-7慢日志"><a href="#ELK收集mysql5-7慢日志" class="headerlink" title="ELK收集mysql5.7慢日志"></a>ELK收集mysql5.7慢日志</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>公司ELK平台计划收集生产业务的所有mysql慢日志.由于所有环境中均使用mysql5.7版本,所以其他mysql版本的慢日志格式不在讨论范围之内.</p><p>慢日志的grok正则匹配我折腾了很久,网上的大多文档中给出的logstash的grok正则其实并不能正确的解析到mysql慢日志的字段.</p><p>这个博客的grok正则经过实践可行.而且filebeat,logstash的filter配置也是参考这个博客配置的:<a href="https://www.cnblogs.com/minseo/p/10441913.html" target="_blank" rel="noopener">博客地址</a></p><hr><h3 id="MySQL慢日志"><a href="#MySQL慢日志" class="headerlink" title="MySQL慢日志"></a>MySQL慢日志</h3><p>慢日志格式如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[work@msf-mysql-master log]$ sudo head slow_2019072103.log</span><br><span class="line">/usr/local/mysql5.7/bin/mysqld, Version: 5.7.24-log (MySQL Community Server (GPL)). started with:</span><br><span class="line">Tcp port: 3306  Unix socket: /mysql_log/msf/tmp/mysql.sock</span><br><span class="line">Time                 Id Command    Argument</span><br><span class="line"># Time: 2019-07-21T08:54:04.145255+08:00</span><br><span class="line"># User@Host: u_msf[u_msf] @  [10.111.10.40]  Id: 131421254</span><br><span class="line"># Query_time: 1.595300  Lock_time: 0.000031 Rows_sent: 20  Rows_examined: 809259</span><br><span class="line">use msf_prod;</span><br><span class="line">SET timestamp=1563670444;</span><br><span class="line">SELECT `id`,`type`,`honey`,`remark`,`created_at` FROM `t_log_user_honey` WHERE `user_id` = &apos;1000014423&apos; ORDER BY `created_at` DESC LIMIT 20 OFFSET 0;</span><br><span class="line"># Time: 2019-07-21T10:51:06.184010+08:00</span><br></pre></td></tr></table></figure><a id="more"></a><p>每个日志文件的格式为<code>slow_日期.log</code> .7天切割一次新的日志文件</p><p>每个日志的开头三行是不需要的内容,所以需要filebeat排除</p><p>每一条慢日志有以下几行组成:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Time: 2019-07-21T08:54:04.145255+08:00</span><br><span class="line"># User@Host: u_msf[u_msf] @  [10.111.10.40]  Id: 131421254</span><br><span class="line"># Query_time: 1.595300  Lock_time: 0.000031 Rows_sent: 20  Rows_examined: 809259</span><br><span class="line">use msf_prod;</span><br><span class="line">SET timestamp=1563670444;</span><br><span class="line">SELECT `id`,`type`,`honey`,`remark`,`created_at` FROM `t_log_user_honey` WHERE `user_id` = &apos;1000014423&apos; ORDER BY `created_at` DESC LIMIT 20 OFFSET 0;</span><br></pre></td></tr></table></figure><p>第一行Time时间不需要,所以也需要filebeat排除.</p><p>从第二行开始匹配,有些慢日志可能没有<code>use database;</code>的语句.所以需要分别针对对待</p><hr><h3 id="filebeat配置"><a href="#filebeat配置" class="headerlink" title="filebeat配置"></a>filebeat配置</h3><p>filebeat需要开启多行日志功能.并且排除特定的字段.除此之外,和其他的日志收集配置一样.下面是生产环境中filebeat的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">- type: log</span><br><span class="line"></span><br><span class="line">  # Change to true to enable this input configuration.</span><br><span class="line">  enabled: true</span><br><span class="line"></span><br><span class="line">  # Paths that should be crawled and fetched. Glob based paths.</span><br><span class="line">  paths:</span><br><span class="line">    - /mysql_log/msf/log/slow_*.log</span><br><span class="line">  exclude_lines: [&apos;^\# Time&apos;,&apos;^\/usr&apos;,&apos;^Tcp&apos;,&apos;^Time&apos;]</span><br><span class="line">  multiline.pattern: &apos;^\# Time|^\# User&apos;</span><br><span class="line">  multiline.negate: true</span><br><span class="line">  multiline.match: after</span><br><span class="line">  fields:</span><br><span class="line">    project: msf</span><br><span class="line">    type: mysql</span><br><span class="line">    level: slow</span><br></pre></td></tr></table></figure><hr><h3 id="logstash配置"><a href="#logstash配置" class="headerlink" title="logstash配置"></a>logstash配置</h3><p>logstash需要使用正则匹配2种格式的慢日志.当一种grok匹配到了后,logstash就不会再接着往下匹配了,所以每条日志只会匹配一种grok规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">      if [fields][type] == &quot;mysql&quot; &#123;</span><br><span class="line">          grok &#123;</span><br><span class="line">            #有use database语句</span><br><span class="line">            match =&gt; [ &quot;message&quot; , &quot;^#\s+User@Host:\s+%&#123;USER:user&#125;\[[^\]]+\]\s+@\s+(?:(?&lt;clienthost&gt;\S*) )?\[(?:%&#123;IPV4:clientip&#125;)?\]\s+Id:\s+%&#123;NUMBER:row_id:int&#125;\n#\s+Query_time:\s+%&#123;NUMBER:query_time:float&#125;\s+Lock_time:\s+%&#123;NUMBER:lock_time:float&#125;\s+Rows_sent:\s+%&#123;NUMBER:rows_sent:int&#125;\s+Rows_examined:\s+%&#123;NUMBER:rows_examined:int&#125;\n\s*(?:use %&#123;DATA:database&#125;;\s*\n)?SET\s+timestamp=%&#123;NUMBER:timestamp&#125;;\n\s*(?&lt;sql&gt;(?&lt;action&gt;\w+)\b.*;)\s*(?:\n#\s+Time)?.*$&quot; ]</span><br><span class="line"></span><br><span class="line">            remove_field =&gt; [&quot;message&quot;] #删除原始日志,我试过写在mutate中,发现不起作用</span><br><span class="line">&#125;</span><br><span class="line">            #无use database语句</span><br><span class="line">          grok &#123;</span><br><span class="line">            match =&gt; [ &quot;message&quot; , &quot;^#\s+User@Host:\s+%&#123;USER:user&#125;\[[^\]]+\]\s+@\s+(?:(?&lt;clienthost&gt;\S*) )?\[(?:%&#123;IPV4:clientip&#125;)?\]\s+Id:\s+%&#123;NUMBER:row_id:int&#125;\n#\s+Query_time:\s+%&#123;NUMBER:query_time:float&#125;\s+Lock_time:\s+%&#123;NUMBER:lock_time:float&#125;\s+Rows_sent:\s+%&#123;NUMBER:rows_sent:int&#125;\s+Rows_examined:\s+%&#123;NUMBER:rows_examined:int&#125;\nSET\s+timestamp=%&#123;NUMBER:timestamp&#125;;\n\s*(?&lt;sql&gt;(?&lt;action&gt;\w+)\b.*;)\s*(?:\n#\s+Time)?.*$&quot; ]</span><br><span class="line">           remove_field =&gt; [&quot;message&quot;]  #删除原始日志,我试过写在mutate中,发现不起作用</span><br><span class="line">    &#125;</span><br><span class="line">        date &#123;</span><br><span class="line">            match =&gt; [&quot;timestamp_mysql&quot;, &quot;UNIX&quot;]</span><br><span class="line">            target =&gt; &quot;@timestamp&quot;</span><br><span class="line">        &#125;</span><br><span class="line">       mutate &#123;</span><br><span class="line">            remove_field =&gt; &quot;@version&quot;  #删除filebeat传输过来的无用字段,可以视情况删除</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><blockquote><p>也可以将2个match语句写入到一个grok中,但是我试过好像不行,有些慢日志并不能正常解析</p></blockquote><p>实际上无论是grok debbuger在线工具还是Kibana的dev tool均提供了grok的在线调试工具,可以检测和调试grok的正则匹配.例如使用上述中的grok正则检测一下是否能正常匹配到前文提到的mysql原始日志数据.可以使用kibana的dev tool工具中的Grok Debugger工具来校验:</p><p><img src="https://img2.jesse.top/image-20200729142559717.png" alt="image-20200729142559717"></p><p>上面的结构化数据输出中可以看到grok正则能正常解析原始日志中的数据,并且以json格式将日志内容映射给各字段.</p><hr><h3 id="Kibana展示"><a href="#Kibana展示" class="headerlink" title="Kibana展示"></a>Kibana展示</h3><p>我尝试在kibana中使用图形化展示query_time(也就是SQL执行时间)最长的TOP5的SQL语句,制作成可视化图表,方便动态展示.但是发现可视化的聚合图形并不能满足这个需求.</p><p>但是Kibana的Discover界面通过选定字段,也能对query_time进行排序.例如下面的截图中先选定<code>query_time</code>和<code>sql</code>这2个字段,然后再对<code>query_time</code>进行排序(在右边的query_time字段下有个倒三角形表示倒序排序).</p><p><img src="https://img2.jesse.top/image-20200729143159670.png" alt="image-20200729143159670"></p><p>然后将这个discover的筛选结果保存到Dashboard中,方便以后查看:</p><p><img src="https://img2.jesse.top/image-20200729143309655.png" alt="image-20200729143309655"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ELK收集mysql5-7慢日志&quot;&gt;&lt;a href=&quot;#ELK收集mysql5-7慢日志&quot; class=&quot;headerlink&quot; title=&quot;ELK收集mysql5.7慢日志&quot;&gt;&lt;/a&gt;ELK收集mysql5.7慢日志&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;公司ELK平台计划收集生产业务的所有mysql慢日志.由于所有环境中均使用mysql5.7版本,所以其他mysql版本的慢日志格式不在讨论范围之内.&lt;/p&gt;
&lt;p&gt;慢日志的grok正则匹配我折腾了很久,网上的大多文档中给出的logstash的grok正则其实并不能正确的解析到mysql慢日志的字段.&lt;/p&gt;
&lt;p&gt;这个博客的grok正则经过实践可行.而且filebeat,logstash的filter配置也是参考这个博客配置的:&lt;a href=&quot;https://www.cnblogs.com/minseo/p/10441913.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;博客地址&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;MySQL慢日志&quot;&gt;&lt;a href=&quot;#MySQL慢日志&quot; class=&quot;headerlink&quot; title=&quot;MySQL慢日志&quot;&gt;&lt;/a&gt;MySQL慢日志&lt;/h3&gt;&lt;p&gt;慢日志格式如下:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[work@msf-mysql-master log]$ sudo head slow_2019072103.log&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/usr/local/mysql5.7/bin/mysqld, Version: 5.7.24-log (MySQL Community Server (GPL)). started with:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tcp port: 3306  Unix socket: /mysql_log/msf/tmp/mysql.sock&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Time                 Id Command    Argument&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# Time: 2019-07-21T08:54:04.145255+08:00&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# User@Host: u_msf[u_msf] @  [10.111.10.40]  Id: 131421254&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# Query_time: 1.595300  Lock_time: 0.000031 Rows_sent: 20  Rows_examined: 809259&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;use msf_prod;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;SET timestamp=1563670444;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;SELECT `id`,`type`,`honey`,`remark`,`created_at` FROM `t_log_user_honey` WHERE `user_id` = &amp;apos;1000014423&amp;apos; ORDER BY `created_at` DESC LIMIT 20 OFFSET 0;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# Time: 2019-07-21T10:51:06.184010+08:00&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>Filebeat+logstash收集Nginx访问日志</title>
    <link href="https://jesse.top/2020/08/25/elk/Filebeat+logstash%E6%94%B6%E9%9B%86Nginx%E8%AE%BF%E9%97%AE%E6%97%A5%E5%BF%97/"/>
    <id>https://jesse.top/2020/08/25/elk/Filebeat+logstash收集Nginx访问日志/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Filebeat-logstash收集Nginx访问日志"><a href="#Filebeat-logstash收集Nginx访问日志" class="headerlink" title="Filebeat+logstash收集Nginx访问日志"></a>Filebeat+logstash收集Nginx访问日志</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境:"></a>环境:</h3><p><strong>Filebeat</strong>: 7.0</p><p><strong>Logstash</strong>:7.0</p><p><strong>elasticsearch</strong>:7.0</p><hr><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>logstash默认自带了apache标准日志格式的grok正则表达式:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">COMMONAPACHELOG %&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;NOTSPACE:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;(?:%&#123;WORD:verb&#125; %&#123;NOTSPACE:request&#125;(?: HTTP/%&#123;NUMBER:httpversion&#125;)?|%&#123;DATA:rawrequest&#125;)&quot; %&#123;NUMBER:response&#125; (?:%&#123;NUMBER:bytes&#125;|-)</span><br><span class="line">COMBINEDAPACHELOG %&#123;COMMONAPACHELOG&#125; %&#123;QS:referrer&#125; %&#123;QS:agent&#125;</span><br></pre></td></tr></table></figure><p>对于 nginx 标准日志格式，可以发现只是最后多了一个 <code>$http_x_forwarded_for</code> 变量。所以 nginx 标准日志的 grok 正则定义是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MAINNGINXLOG %&#123;COMBINEDAPACHELOG&#125; %&#123;QS:x_forwarded_for&#125;</span><br></pre></td></tr></table></figure><p>如果Nginx日志格式不是标准的日志格式,则需要自行编写grok正则,匹配日志内容.</p><p>但是grok正则表达式不容易上手,非常难写.复杂不说,而且logstash使用正则表达式来处理日志格式,其性能也会受到很大的影响.</p><p>所以这里推荐另外一种收集方式.</p><p>本文档参考博客:<a href="https://www.popyone.com/post/13.html" target="_blank" rel="noopener">https://www.popyone.com/post/13.html</a></p><p>本文档参考书籍:<strong>ELK权威指南中文版第二版</strong></p><a id="more"></a><hr><h3 id="json格式的Nginx日志"><a href="#json格式的Nginx日志" class="headerlink" title="json格式的Nginx日志"></a>json格式的Nginx日志</h3><p>对 logstash 来说，nginx 日志还有另一种更简便的处理方式。就是自定义日志格式时，通过手工拼写，直接输出成 JSON 格式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">log_format json escape=json  <span class="string">'&#123;"@timestamp":"$time_iso8601",'</span></span><br><span class="line">                    <span class="string">'"@source":"$server_addr",'</span></span><br><span class="line">                    <span class="string">'"hostname":"$hostname",'</span></span><br><span class="line">                    <span class="string">'"ip":"$http_x_forwarded_for",'</span></span><br><span class="line">                    <span class="string">'"client":"$remote_addr",'</span></span><br><span class="line">                    <span class="string">'"request_method":"$request_method",'</span></span><br><span class="line">                    <span class="string">'"scheme":"$scheme",'</span></span><br><span class="line">                    <span class="string">'"domain":"$server_name",'</span></span><br><span class="line">                    <span class="string">'"client_host":"$host",'</span></span><br><span class="line">                    <span class="string">'"referer":"$http_referer",'</span></span><br><span class="line">                    <span class="string">'"request":"$request_uri",'</span></span><br><span class="line">                    <span class="string">'"args":"$args",'</span></span><br><span class="line">                    <span class="string">'"size":$body_bytes_sent,'</span></span><br><span class="line">                    <span class="string">'"status": $status,'</span></span><br><span class="line">                    <span class="string">'"responsetime":$request_time,'</span></span><br><span class="line">                    <span class="string">'"upstreamtime":"$upstream_response_time",'</span></span><br><span class="line">                    <span class="string">'"upstreamaddr":"$upstream_addr",'</span></span><br><span class="line">                    <span class="string">'"http_user_agent":"$http_user_agent"'</span></span><br><span class="line">                    <span class="string">'&#125;'</span>;</span><br></pre></td></tr></table></figure><blockquote><p>escape=json 参数，在配置日志格式时加上此参数可以不转义变量内容</p></blockquote><p>nginx虚拟主机的access_log应用了Json的格式后,日志格式自动转换成了json格式了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;@timestamp&quot;:&quot;2019-01-31T11:26:48+08:00&quot;,&quot;@source&quot;:&quot;192.168.12.200&quot;,&quot;hostname&quot;:&quot;proxy.server&quot;,&quot;ip&quot;:&quot;&quot;,&quot;client&quot;:&quot;183.3.239.170&quot;,&quot;request_method&quot;:&quot;GET&quot;,&quot;scheme&quot;:&quot;http&quot;,&quot;domain&quot;:&quot;www.51hangyu.com&quot;,&quot;referer&quot;:&quot;http://www.51hangyu.com/chat&quot;,&quot;request&quot;:&quot;/static/static/css/main.97f6b853.css&quot;,&quot;args&quot;:&quot;&quot;,&quot;size&quot;:460413,&quot;status&quot;: 200,&quot;responsetime&quot;:0.108,&quot;upstreamtime&quot;:&quot;0.002&quot;,&quot;upstreamaddr&quot;:&quot;192.168.12.15:80&quot;,&quot;http_user_agent&quot;:&quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)&quot;&#125;</span><br></pre></td></tr></table></figure><p>这样一来在客户端就自动转换成了json格式,无需logstash通过正则去过滤和匹配日志内容,大大提高logstash的收集性能</p><hr><h3 id="Filebeat配置"><a href="#Filebeat配置" class="headerlink" title="Filebeat配置"></a>Filebeat配置</h3><p>filebeat.yml配置如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">- type: log</span><br><span class="line"></span><br><span class="line">  # Change to true to enable this input configuration.</span><br><span class="line">  enabled: true</span><br><span class="line"></span><br><span class="line">  # Paths that should be crawled and fetched. Glob based paths.</span><br><span class="line">  paths:</span><br><span class="line">    - /data/logs/nginx/hsq_openapi_beta.access.log</span><br><span class="line"></span><br><span class="line">#默认这个值是FALSE的，也就是我们的json日志解析后会被放在json键上。设为TRUE，所有的keys就会被放到根节点</span><br><span class="line">  json.keys_under_root: true</span><br><span class="line"></span><br><span class="line">#是否要覆盖原有的key，这是关键配置，将keys_under_root设为TRUE后，再将overwrite_keys也设为TRUE，就能把filebeat默认的key值给覆盖了</span><br><span class="line">  json.overwrite_keys: true</span><br><span class="line">  exclude_lines: [&apos;HEAD&apos;]    #排除HEAD访问日志,因为HEAD是阿里云SLB的健康检查访问,而且没有外网IP,所有不需要上传</span><br><span class="line">  json.message_key: request_method #HEAD是request_mothod的字段值,所以需要指定字段名</span><br><span class="line">#fields字段用于打标签和索引,在logstash里判断日志来源</span><br><span class="line">  fields:</span><br><span class="line">     type: nginx-openapi</span><br><span class="line">     project: msf</span><br><span class="line">     level: access</span><br><span class="line">     </span><br><span class="line">---output配置,将日志输出到logstash-----</span><br><span class="line">output.logstash:</span><br><span class="line">  # The Logstash hosts</span><br><span class="line">  hosts: [&quot;172.16.20.107:5044&quot;]</span><br></pre></td></tr></table></figure><hr><h3 id="logstash配置"><a href="#logstash配置" class="headerlink" title="logstash配置"></a>logstash配置</h3><p>在logstash的con.d下新建一个Nginx_access的配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk07 elasticsearch]# mkdir /etc/logstash/conf.d/nginx_access.conf</span><br></pre></td></tr></table></figure><p>配置文件内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk07 elasticsearch]# cat /etc/logstash/conf.d/nginx_access.conf</span><br><span class="line">input &#123;</span><br><span class="line">    beats &#123;</span><br><span class="line">      port =&gt; 5044</span><br><span class="line">      codec =&gt; json</span><br><span class="line">      client_inactivity_timeout =&gt; 600</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">filter &#123;</span><br><span class="line">   if  &quot;openapi-nginx-access&quot; in [tags] &#123;</span><br><span class="line">       date &#123;</span><br><span class="line">           match =&gt; [ &quot;timestamp&quot; ,&quot;dd/MMM/yyyy:HH:mm:ss Z&quot; ]</span><br><span class="line">           target =&gt; &quot;@timestamp&quot;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">      mutate &#123;</span><br><span class="line">        remove_field =&gt; &quot;timestamp&quot;</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">      geoip &#123;</span><br><span class="line">        source =&gt; &quot;ip&quot;</span><br><span class="line">        target =&gt; &quot;geoip&quot;</span><br><span class="line">        database =&gt; &quot;/etc/logstash/GeoLite2/GeoLite2-City.mmdb&quot;</span><br><span class="line">        add_field =&gt; [&quot;[geoip][coordinates]&quot;,&quot;%&#123;[geoip][longitude]&#125;&quot;]</span><br><span class="line">        add_field =&gt; [&quot;[geoip][coordinates]&quot;,&quot;%&#123;[geoip][latitude]&#125;&quot;]</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">      mutate &#123;</span><br><span class="line">        remove_field =&gt; &quot;geoip.continent_code&quot;</span><br><span class="line">        remove_field =&gt; &quot;geoip.country_code2&quot;</span><br><span class="line">        remove_field =&gt; &quot;geoip.country_code3&quot;</span><br><span class="line">        convert =&gt; [ &quot;[geoip][coordinates]&quot;,&quot;float&quot; ]</span><br><span class="line">        convert =&gt; [ &quot;status&quot;,&quot;integer&quot; ]</span><br><span class="line">        convert =&gt; [ &quot;size&quot;,&quot;integer&quot; ]</span><br><span class="line">        convert =&gt; [ &quot;upstreatime&quot;,&quot;float&quot; ]</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line">    elasticsearch &#123;</span><br><span class="line">            hosts =&gt; [&quot;172.16.20.101:9200&quot;,&quot;172.16.20.110:9200&quot;,&quot;172.16.20.107:9200&quot;,&quot;172.16.20.108:9200&quot;,&quot;172.16.20.102:9200&quot;,&quot;172.16.20.103:9200&quot;,&quot;172.16.20.104:9200&quot;,&quot;172.16.20.105:9200&quot;,&quot;172.16.20.106:9200&quot;,&quot;172.16.20.109:9200&quot;]</span><br><span class="line">            index =&gt; &quot;logstash-%&#123;[fields][project]&#125;-%&#123;[fields][type]&#125;-%&#123;[fields][level]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">            user =&gt; &quot;elastic&quot;</span><br><span class="line">            password =&gt; &quot;password&quot;</span><br><span class="line">            #flush_size =&gt; 20000</span><br><span class="line">            #idle_flush_time =&gt; 10</span><br><span class="line">            sniffing =&gt; true</span><br><span class="line">            template_overwrite =&gt; true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>建议logstash先将日志输出到控制台测试,确定无误后再输出到es.输出到控制台只需要将output字段修改为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; output &#123;stdout &#123;&#125;&#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><hr><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>打开es集群的http界面,可以看到产生了索引分片</p><p><img src="https://img2.jesse.top/image-20200706112236216.png" alt="image-20200706112236216"></p><p>打开kibana配置了logstash-hsq-nginx-access-*的索引以后,可以看到展示的日志:</p><p><img src="https://img2.jesse.top/image-20200706112436147.png" alt="image-20200706112436147"></p><hr><h3 id="多个http-x-forwarded-fors上游IP的Nginx访问日志"><a href="#多个http-x-forwarded-fors上游IP的Nginx访问日志" class="headerlink" title="多个http_x_forwarded_fors上游IP的Nginx访问日志"></a>多个http_x_forwarded_fors上游IP的Nginx访问日志</h3><p>iqg业务的Nginx经过了2层代理(阿里云SLB—–&gt;Kong—-&gt;本地Nginx).所以上游IP有2个,使用json格式转换后,Nginx的访问日志内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;@timestamp&quot;:&quot;2020-08-14T03:15:38+08:00&quot;,&quot;@source&quot;:&quot;10.111.10.35&quot;,&quot;hostname&quot;:&quot;iqg-new1&quot;,&quot;ip&quot;:&quot;183.192.43.174, 100.117.85.32&quot;,&quot;client&quot;:&quot;10.111.30.194&quot;,&quot;request_method&quot;:&quot;GET&quot;,&quot;scheme&quot;:&quot;http&quot;,&quot;domain&quot;:&quot;api.v4.iqianggou.com&quot;,&quot;client_host&quot;:&quot;v4.api.iqg-new1&quot;,&quot;referer&quot;:&quot;&quot;,&quot;request&quot;:&quot;/open/user/clientupload?......&#125;</span><br></pre></td></tr></table></figure><p>可以看到IP字段包含了用户真实IP和阿里云SLB的IP.这样的访问日志上传到Logstash后,geoip模块无法处理IP字段.所以需要在logstash中解析和处理IP字段,</p><p>但是还有一个更好的办法,不需要logstash去单独处理,减少logstash的负担.可以在nginx本地将处理IP字段.</p><p>在nginx.conf配置文件的http字段中添加以下配置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">......</span><br><span class="line">map $http_x_forwarded_for  $clientRealIp &#123;</span><br><span class="line">                    &quot;&quot;      $remote_addr;</span><br><span class="line">                            ~^(?P&lt;firstAddr&gt;[0-9\.|:|a-f\.|:|A-F\.|:]+),?.*$  $firstAddr;</span><br><span class="line">         &#125;</span><br><span class="line">......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上map函数解析<code>$http_x_forwarded_for</code>字段,当字段为空时,使用<code>$remote_addr</code>映射为<code>$clientRealIp</code>.否则使用<code>$firstAddr</code>映射.</p><p>同时修改json日志格式,使用<code>$clientRealIp</code>替代<code>$http_x_forwarded_for</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">log_format json escape=json  &apos;&#123;&quot;@timestamp&quot;:&quot;$time_iso8601&quot;,&apos;</span><br><span class="line">                    &apos;&quot;@source&quot;:&quot;$server_addr&quot;,&apos;</span><br><span class="line">                    &apos;&quot;hostname&quot;:&quot;$hostname&quot;,&apos;</span><br><span class="line">                    &apos;&quot;ip&quot;:&quot;$clientRealIp&quot;,&apos;</span><br><span class="line">                    &apos;&quot;client&quot;:&quot;$remote_addr&quot;,&apos;</span><br><span class="line">                    &apos;&quot;request_method&quot;:&quot;$request_method&quot;,&apos;</span><br><span class="line">                    &apos;&quot;scheme&quot;:&quot;$scheme&quot;,&apos;</span><br><span class="line">                    &apos;&quot;domain&quot;:&quot;$server_name&quot;,&apos;</span><br><span class="line">                    &apos;&quot;client_host&quot;:&quot;$host&quot;,&apos;</span><br><span class="line">                    &apos;&quot;referer&quot;:&quot;$http_referer&quot;,&apos;</span><br><span class="line">                    &apos;&quot;request&quot;:&quot;$request_uri&quot;,&apos;</span><br><span class="line">                    &apos;&quot;args&quot;:&quot;$args&quot;,&apos;</span><br><span class="line">                    &apos;&quot;size&quot;:$body_bytes_sent,&apos;</span><br><span class="line">                    &apos;&quot;status&quot;: $status,&apos;</span><br><span class="line">                    &apos;&quot;responsetime&quot;:$request_time,&apos;</span><br><span class="line">                    &apos;&quot;upstreamtime&quot;:&quot;$upstream_response_time&quot;,&apos;</span><br><span class="line">                    &apos;&quot;upstreamaddr&quot;:&quot;$upstream_addr&quot;,&apos;</span><br><span class="line">                    &apos;&quot;http_user_agent&quot;:&quot;$http_user_agent&quot;&apos;</span><br><span class="line">                    &apos;&#125;&apos;;</span><br></pre></td></tr></table></figure><p>重启Nginx</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo nginx -t</span><br><span class="line">sudo nginx -s reload</span><br></pre></td></tr></table></figure><p>再次查看日志,发现此时ip字段只保留了公网IP地址,去掉了阿里云的SLB内网地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;@timestamp&quot;:&quot;2020-08-14T09:46:59+08:00&quot;,&quot;@source&quot;:&quot;10.111.10.35&quot;,&quot;hostname&quot;:&quot;iqg-new1&quot;,&quot;ip&quot;:&quot;113.247.47.77&quot;,&quot;client&quot;:&quot;10.111.30.194&quot;,&quot;request_method&quot;:&quot;GET&quot;,&quot;scheme&quot;:&quot;http&quot;,&quot;domain&quot;:&quot;api.v4.iqianggou.com&quot;,&quot;client_host&quot;:&quot;v4.api.iqg-new1&quot;,&quot;referer&quot;:&quot;https://servicewechat.com/wxa36b4671d95ba753/86/page-frame.html&quot;,&quot;request&quot;:&quot;/api/item?......&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Filebeat-logstash收集Nginx访问日志&quot;&gt;&lt;a href=&quot;#Filebeat-logstash收集Nginx访问日志&quot; class=&quot;headerlink&quot; title=&quot;Filebeat+logstash收集Nginx访问日志&quot;&gt;&lt;/a&gt;Filebeat+logstash收集Nginx访问日志&lt;/h2&gt;&lt;h3 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境:&quot;&gt;&lt;/a&gt;环境:&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Filebeat&lt;/strong&gt;: 7.0&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Logstash&lt;/strong&gt;:7.0&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;elasticsearch&lt;/strong&gt;:7.0&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;logstash默认自带了apache标准日志格式的grok正则表达式:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;COMMONAPACHELOG %&amp;#123;IPORHOST:clientip&amp;#125; %&amp;#123;USER:ident&amp;#125; %&amp;#123;NOTSPACE:auth&amp;#125; \[%&amp;#123;HTTPDATE:timestamp&amp;#125;\] &amp;quot;(?:%&amp;#123;WORD:verb&amp;#125; %&amp;#123;NOTSPACE:request&amp;#125;(?: HTTP/%&amp;#123;NUMBER:httpversion&amp;#125;)?|%&amp;#123;DATA:rawrequest&amp;#125;)&amp;quot; %&amp;#123;NUMBER:response&amp;#125; (?:%&amp;#123;NUMBER:bytes&amp;#125;|-)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;COMBINEDAPACHELOG %&amp;#123;COMMONAPACHELOG&amp;#125; %&amp;#123;QS:referrer&amp;#125; %&amp;#123;QS:agent&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;对于 nginx 标准日志格式，可以发现只是最后多了一个 &lt;code&gt;$http_x_forwarded_for&lt;/code&gt; 变量。所以 nginx 标准日志的 grok 正则定义是：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;MAINNGINXLOG %&amp;#123;COMBINEDAPACHELOG&amp;#125; %&amp;#123;QS:x_forwarded_for&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果Nginx日志格式不是标准的日志格式,则需要自行编写grok正则,匹配日志内容.&lt;/p&gt;
&lt;p&gt;但是grok正则表达式不容易上手,非常难写.复杂不说,而且logstash使用正则表达式来处理日志格式,其性能也会受到很大的影响.&lt;/p&gt;
&lt;p&gt;所以这里推荐另外一种收集方式.&lt;/p&gt;
&lt;p&gt;本文档参考博客:&lt;a href=&quot;https://www.popyone.com/post/13.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.popyone.com/post/13.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文档参考书籍:&lt;strong&gt;ELK权威指南中文版第二版&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>生产环境部署ELK7+ES冷热数据分离</title>
    <link href="https://jesse.top/2020/08/25/elk/%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2ELK7+ES%E5%86%B7%E7%83%AD%E6%95%B0%E6%8D%AE%E5%88%86%E7%A6%BB/"/>
    <id>https://jesse.top/2020/08/25/elk/生产环境部署ELK7+ES冷热数据分离/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.314Z</updated>
    
    <content type="html"><![CDATA[<h2 id="生产环境部署ELK7-ES冷热数据分离"><a href="#生产环境部署ELK7-ES冷热数据分离" class="headerlink" title="生产环境部署ELK7+ES冷热数据分离"></a>生产环境部署ELK7+ES冷热数据分离</h2><h3 id="需求整理"><a href="#需求整理" class="headerlink" title="需求整理"></a>需求整理</h3><p>当前公司还没有日志收集系统,.线上故障排错需要开发和运维人员通过跳板机登陆到业务服务器去查看日志,如果集群下服务器节点较多,可能需要登陆到每一台服务器才能找到准确的故障信息.这种方式会带来许多问题:</p><ul><li>效率低下</li><li>需要给研发人员分配生产服务器账号,有安全隐患</li><li>查看日志的方式极为麻烦,复杂</li><li>每台业务服务器需要大容量磁盘保存日志文件</li></ul><hr><h3 id="Elasticsearch冷热数据分离"><a href="#Elasticsearch冷热数据分离" class="headerlink" title="Elasticsearch冷热数据分离"></a>Elasticsearch冷热数据分离</h3><p>公司目前共有4个APP产品,每天的nginx日志+业务日志+php日志总共预计在1.5T左右.一周的日志数据量在10T左右.Elasticsearch规划了20T的SSD磁盘用来一周的日志数据(7天日志+1个副本).这些数据保存在Elasticsearch的热节点.</p><p>考虑到1周以后的日志数据查询频率非常低,所以可以将这部分的日志迁移和归档到Elasticsearch的冷节点.并且冷节点日志无需索引.所以预留了10T的SAS磁盘保留1周的冷数据.</p><p>总共是20TSSD磁盘的热节点和10TSAS机械磁盘的冷节点</p><a id="more"></a><hr><h3 id="Elasticsearch集群节点规划"><a href="#Elasticsearch集群节点规划" class="headerlink" title="Elasticsearch集群节点规划"></a>Elasticsearch集群节点规划</h3><ul><li><strong>热节点规划</strong></li></ul><p>ES集群的DATA节点负责保存收集到的日志数据,索引,整理和查询日志等工作.所以data节点对硬件资源要求较高.</p><p>计划使用3台物理服务器(VMware虚拟化集群),每台物理机的配置是32核134G,每台物理机有2个VM.一共计划6个elasticsearch的节点.</p><p>每个物理服务器上使用4块1.92T的SSD磁盘.平均每个VM虚拟机可以使用16核67G内存4TSSD磁盘.</p><blockquote><p>后期又增加了一台服务器,配置为16核48G内存2TSSD</p></blockquote><p>7个节点一共有20TSSD磁盘空间,刚好满足7天的日志数据的需求.</p><blockquote><p>为了避免磁盘IO竞争.每个节点使用独立的单块SSD磁盘,</p></blockquote><ul><li><strong>冷节点规划</strong></li></ul><p>集群冷节点可以适当放低硬件资源.计划使用2台VM作为ES冷节点.每台服务器为8核32G.为了更充分利用带宽和CPU资源.以及合理规划ES的节点角色.冷节点同时又作为集群的master节点和logstash的角色.</p><blockquote><p>这样热节点就有充分资源作为data角色</p></blockquote><hr><h3 id="ELK架构规划"><a href="#ELK架构规划" class="headerlink" title="ELK架构规划"></a>ELK架构规划</h3><p>为了ELK的高性能和数据传输安全性,完整的架构如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Filebeat------&gt;logstash-------&gt;Elasticsearch--------&gt;kibana-------&gt;nginx</span><br></pre></td></tr></table></figure><h5 id="以下是各组件作用介绍"><a href="#以下是各组件作用介绍" class="headerlink" title="以下是各组件作用介绍:"></a>以下是各组件作用介绍:</h5><ul><li>Filebeat: 轻量级的日志收集Agent,部署在每台业务服务器</li><li>logstash(5节点): 从filebeat费日志数据,并对数据进行加工处理,传输给ES.由于集群资源本身的稳定和健壮,logstash并不需要使用持久化数据特性.</li><li>Elasticsearch(7节点):保存和索引数据.7个热节点和2个冷节点</li><li>kibana:从Elasticsearch获取数据并在web界面展示</li><li>nginx: kibana的代理服务器</li></ul><p>官网要求ELK组件必须使用相同的版本,这次部署的是ELK的最新版.7.7版本</p><hr><h3 id="ELK服务器信息"><a href="#ELK服务器信息" class="headerlink" title="ELK服务器信息"></a>ELK服务器信息</h3><table><thead><tr><th>主机名</th><th>IP地址</th><th>操作系统</th><th>运行组件</th><th>功能</th><th>配置</th></tr></thead><tbody><tr><td>idc-function-elk01</td><td>172.16.20.101</td><td>CentOS7.7</td><td>logstash,es</td><td>logstash节点,data,master节点</td><td>16核80G3.84TSSD</td></tr><tr><td>idc-function-elk02</td><td>172.16.20.102</td><td>CentOS7.7</td><td>es</td><td>ES data节点</td><td>16核54G3.84TSSD</td></tr><tr><td>idc-function-elk03</td><td>172.16.20.103</td><td>CentOS7.7</td><td>logstash,es</td><td>logstash节点,data节点</td><td>16核80G3.84TSSD</td></tr><tr><td>idc-function-elk04</td><td>172.16.20.104</td><td>CentOS7.7</td><td>es</td><td>ES data节点</td><td>16核54G3.84TSSD</td></tr><tr><td>idc-function-elk05</td><td>172.16.20.105</td><td>CentOS7.7</td><td>logstash,es</td><td>logstash节点,data节点</td><td>16核80G3.84TSSD</td></tr><tr><td>idc-function-elk06</td><td>172.16.20.106</td><td>CentOS7.7</td><td>es</td><td>ES data节点</td><td>16核54G3.84TSSD</td></tr><tr><td>idc-function-elk07</td><td>172.16.20.107</td><td>CentOS7.7</td><td>es</td><td>ES data节点</td><td>16核54G1.92TSSD</td></tr><tr><td>idc-function-elk08</td><td>172.16.20.108</td><td>CentOS7.7</td><td>logstash,es</td><td>ES的master,冷节点,logstash节点</td><td>8核32G9.6TSAS</td></tr><tr><td>idc-function-elk09</td><td>172.16.20.109</td><td>CentOS7.7</td><td>logstash,es</td><td>ES的主master节点,冷节点,logstash节点</td><td>16核32G5TSAS</td></tr><tr><td>idc-function-docker</td><td>172.16.20.30</td><td>CentOS7.7</td><td>nginx</td><td>一台现有的服务器</td><td></td></tr><tr><td>idc-function-elk10</td><td>172.16.20.110</td><td>CentOS7.7</td><td>logstash,es</td><td>仅仅作为ES的master节点,,logstash节点</td><td>8核8G200GSAS</td></tr></tbody></table><hr><h3 id="部署安装"><a href="#部署安装" class="headerlink" title="部署安装"></a>部署安装</h3><h4 id="部署elasticsearch"><a href="#部署elasticsearch" class="headerlink" title="部署elasticsearch"></a>部署elasticsearch</h4><ul><li><strong>部署JAVA环境</strong>.</li></ul><p>官网要求需要JDK8或者11版本.这里部署的是11版本.在Oracle有rpm包.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@idc-function-elk01 ~]$ java -version</span><br><span class="line">java version &quot;11.0.7&quot; 2020-04-14 LTS</span><br><span class="line">Java(TM) SE Runtime Environment 18.9 (build 11.0.7+8-LTS)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.7+8-LTS, mixed mode)</span><br></pre></td></tr></table></figure><ul><li><strong>安装elasticsearch</strong></li></ul><p>官网的yum安装非常慢.先从elasticsearch中文社区下载rpm包.然后使用yum本地安装</p><p>中文社区下载中心:<a href="https://elasticsearch.cn/download/" target="_blank" rel="noopener">https://elasticsearch.cn/download/</a></p><ul><li><strong>配置elasticserch</strong></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#以下是ES的配置文件</span></span><br><span class="line"><span class="string">[root@idc-function-elk01</span> <span class="string">~]#</span> <span class="string">cat</span> <span class="string">/etc/elasticsearch/elasticsearch.yml</span> <span class="string">| sed '/^#/d'</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#ES集群名.每个节点需要定义同一个集群名</span></span><br><span class="line"><span class="string">cluster.name: dwd-elk  </span></span><br><span class="line"><span class="string">#节点名,一般和主机名一致</span></span><br><span class="line"><span class="string">node.name: idc-function-elk01</span></span><br><span class="line"><span class="string">#节点是否为master节点</span></span><br><span class="line"><span class="string">node.master: true</span></span><br><span class="line"><span class="string">#节点是否为data节点</span></span><br><span class="line"><span class="string">node.data: true</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#添加属性,标志位热节点.如果是冷节点,该值为cold</span></span><br><span class="line"><span class="string">node.attr.box_type: hot </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#数据存储路径,指定多块磁盘</span></span><br><span class="line"><span class="string">path.data: /data,/data1</span></span><br><span class="line"><span class="string">#日志存储路径</span></span><br><span class="line"><span class="string">path.logs: /data/logs/elasticsearch</span></span><br><span class="line"><span class="string">#是否需要锁定内存,不适应swap虚拟内存</span></span><br><span class="line"><span class="string">bootstrap.memory_lock: true</span></span><br><span class="line"><span class="string">#绑定IP.本机的内网IP</span></span><br><span class="line"><span class="string">network.host: 172.16.20.101</span></span><br><span class="line"><span class="string">#默认端口</span></span><br><span class="line"><span class="string">http.port: 9200</span></span><br><span class="line"><span class="string">#集群内其他ES节点地址</span></span><br><span class="line"><span class="string">discovery.seed_hosts: ["172.16.20.101","172.16.20.102","172.16.20.103","172.16.20.104","172.16.20.105","172.16.20.106","172.16.20.107","172.16.20.108","172.16.20.109"]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#初始化master节点,#master节点分布在不同的物理服务器</span></span><br><span class="line"><span class="string">cluster.initial_master_nodes: ["172.16.20.101","172.16.20.108","172.16.20.109"] </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#磁盘水位线</span></span><br><span class="line"><span class="string">cluster.routing.allocation.disk.watermark.low: 85%</span></span><br><span class="line"><span class="string">cluster.routing.allocation.disk.watermark.high: 90%</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#用于 fielddata 的最大内存，如果 fielddata 达到该阈值，就会把旧数据交换出去。该参数可以设置百分比或者绝对值。默认设置是不限制，所#以强烈建议设置该值</span></span><br><span class="line"><span class="string">indices.fielddata.cache.size: 10%</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#limit值需要大于indices.fielddata.cache.size的值。否则的话fielddata大小一到limit阈值就报错,就永远道不了size阈值,无法触发对##旧数据的交换任务</span></span><br><span class="line"><span class="string">indices.breaker.fielddata.limit: 30%</span></span><br><span class="line"><span class="string">[root@idc-function-elk01 ~]#</span></span><br></pre></td></tr></table></figure><ul><li><p><strong>优化系统和elasticsearch配置</strong></p><ul><li><strong>优化内核参数</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]<span class="comment"># cat /etc/sysctl.conf</span></span><br><span class="line"></span><br><span class="line">net.ipv4.ip_forward=1</span><br><span class="line">net.ipv6.conf.all.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.lo.disable_ipv6 = 1</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">net.ipv4.neigh.default.gc_stale_time=120</span><br><span class="line">net.ipv4.conf.all.rp_filter=0</span><br><span class="line">net.ipv4.conf.default.rp_filter=0</span><br><span class="line">net.ipv4.conf.default.arp_announce = 2</span><br><span class="line">net.ipv4.conf.lo.arp_announce=2</span><br><span class="line">net.ipv4.conf.all.arp_announce=2</span><br><span class="line">net.ipv4.tcp_max_tw_buckets = 5000</span><br><span class="line">net.ipv4.tcp_syncookies = 1</span><br><span class="line">net.ipv4.tcp_max_syn_backlog = 1024</span><br><span class="line">net.ipv4.tcp_synack_retries = 2</span><br><span class="line">kernel.core_uses_pid = 1 <span class="comment">#追加进程号到core文件名中</span></span><br><span class="line">fs.suid_dumpable = 2 <span class="comment">#确保设置属主的进程也可以生成core文件</span></span><br><span class="line">kernel.core_pattern = /tmp/core-%e-%s-%u-%g-%p-%t <span class="comment">#指定core文件生成的位置和文件名规则。</span></span><br><span class="line">vm.overcommit_memory = 1</span><br><span class="line">net.ipv4.conf.lo.arp_announce=2</span><br><span class="line">kernel.sysrq=1</span><br><span class="line">fs.file-max=6553500 <span class="comment">#最大文件句柄</span></span><br><span class="line">fs.nr_open=6553500  <span class="comment">#最大文件句柄</span></span><br><span class="line">net.core.somaxconn = 65535</span><br><span class="line">vm.max_map_count=262144  <span class="comment">#ES官网推荐</span></span><br><span class="line">net.ipv4.tcp_keepalive_time = 600</span><br><span class="line">net.ipv4.tcp_keepalive_probes = 3</span><br><span class="line">net.ipv4.tcp_keepalive_intvl = 15</span><br><span class="line">net.ipv4.tcp_tw_reuse = 1</span><br><span class="line">net.ipv4.tcp_tw_recycle = 0</span><br><span class="line">net.ipv4.tcp_syn_retries = 1</span><br><span class="line">net.ipv4.tcp_synack_retries = 1</span><br><span class="line">net.ipv4.tcp_timestamps=0</span><br><span class="line">net.ipv4.tcp_fin_timeout=30</span><br><span class="line">net.ipv4.ip_local_port_range = 10000 65000</span><br><span class="line">net.nf_conntrack_max = 655360</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_established = 1200</span><br></pre></td></tr></table></figure><ul><li><strong>设置文件句柄,内存</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]<span class="comment"># cat /etc/security/limits.conf</span></span><br><span class="line">elasticsearch soft memlock unlimited</span><br><span class="line">elasticsearch  hard memlock unlimited</span><br><span class="line">* soft nofile 655360</span><br><span class="line">* hard nofile 655360</span><br></pre></td></tr></table></figure><ul><li><strong>关闭Selinux和防火墙</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line"></span><br><span class="line">setenforce 0</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk01 ~]# cat /etc/selinux/config</span><br><span class="line"></span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure><ul><li><strong>创建目录,并且修改权限</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]# ll /data/elasticsearch/ -d</span><br><span class="line">drwxr-xr-x 3 elasticsearch elasticsearch 27 May 25 23:08 /data/elasticsearch/</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk01 ~]# ll /data/logs/elasticsearch/ -d</span><br><span class="line">drwxr-xr-x 2 elasticsearch elasticsearch 4096 May 26 00:00 /data/logs/elasticsearch/</span><br></pre></td></tr></table></figure><ul><li><strong>修改elasticsearch的systemctl启动文件,加入下面这一行</strong>:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]# cat /usr/lib/systemd/system/elasticsearch.service</span><br><span class="line"></span><br><span class="line">#新增这一行.否则启动的时候会提示在配置文件中开启了bootstrap.memory_lock: true.但是系统中没有检测到配置</span><br><span class="line">#unlimit memory</span><br><span class="line">LimitMEMLOCK=infinity</span><br></pre></td></tr></table></figure><ul><li><strong>修改elasticsearch的JVM内存</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]# cat /etc/elasticsearch/jvm.options</span><br><span class="line"></span><br><span class="line">-Xms32g</span><br><span class="line">-Xmx32g</span><br></pre></td></tr></table></figure><ul><li>启动elasticsearch</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable elasticsearch</span><br><span class="line">  systemctl start elasticsearch</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="Elasticsearch集群监控工具"><a href="#Elasticsearch集群监控工具" class="headerlink" title="Elasticsearch集群监控工具"></a>Elasticsearch集群监控工具</h3><p>这里推荐使用cerebro工具,是一个独立的小工具,无需安装到es插件.具体使用方法请参考:<a href="https://github.com/lmenezes/cerebro" target="_blank" rel="noopener">github项目地址</a></p><p>启动cerebro后.访问cerebro的9000端口.可以看到ES集群状态监控界面,界面还挺不错</p><p><img src="https://img2.jesse.top/image-20200724092916238.png" alt="image-20200724092916238"></p><blockquote><p>从上面的集群截图中可以看到,每个ES节点都有<code>hot</code>或者<code>cold</code>标签以区分节点属性.这就说明冷热节点属性配置正常</p></blockquote><p>但是每次登陆cerebro都需要选择连接的ES节点.无法实现登陆自动连接.这非常不方便,我在github上提交了相关<a href="https://github.com/lmenezes/cerebro/issues/450" target="_blank" rel="noopener">issue</a>.得到回复是使用以下URL:  <a href="http://localhost:9000/#/overview?host=http:%2F%2Flocalhost:9200" target="_blank" rel="noopener">http://localhost:9000/#/overview?host=http:%2F%2Flocalhost:9200</a></p><p>所以.可以配置Nginx反向代理:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line"> listen 80;</span><br><span class="line"> server_name elk.doweidu.com;</span><br><span class="line"></span><br><span class="line"> access_log /data/logs/nginx/elk_access.log main;</span><br><span class="line"> error_log /data/logs/nginx/elk_error.log;</span><br><span class="line"></span><br><span class="line">location / &#123;</span><br><span class="line">        proxy_set_header   Host  $host;</span><br><span class="line">        proxy_set_header   X-Real-IP $remote_addr;</span><br><span class="line">        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">   rewrite ^/(.*)  http://172.16.20.101:9000/#/overview?host=http:%2F%2Flocalhost:9200/$1 permanent;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>172.16.20.101:9000这个为cerebro所在服务器.localhost:9200是同台服务器的ES节点.如果ES节点为其他服务器,使用IP地址指定即可</p></blockquote><hr><h3 id="部署logstash"><a href="#部署logstash" class="headerlink" title="部署logstash"></a>部署logstash</h3><p>logstash也是运行在JVM的java环境中.安装方法和ES一样.</p><ul><li><strong>配置logstash文件</strong></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@idc-function-elk01</span> <span class="string">~]#</span> <span class="string">vim</span> <span class="string">/etc/logstash/logstash.yml</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#这个参数是指单次接收的块大小.可以根据实际场景和性能压力不断的调整和测试</span></span><br><span class="line"><span class="string">pipeline.batch.size:</span> <span class="number">1500</span></span><br><span class="line"><span class="string">pipeline.batch.delay:</span> <span class="number">10</span></span><br><span class="line"><span class="comment">#节点名称</span></span><br><span class="line"><span class="string">node.name:</span> <span class="string">idc-function-elk01</span></span><br><span class="line"><span class="comment">#数据路径</span></span><br><span class="line"><span class="string">path.data:</span> <span class="string">/data/logstash</span></span><br><span class="line"><span class="comment">#日志路径</span></span><br><span class="line"><span class="string">path.logs:</span> <span class="string">/data/logs/logstash</span></span><br><span class="line"><span class="comment">#绑定IP,本机内网IP地址</span></span><br><span class="line"><span class="string">http.host:</span> <span class="string">"172.16.20.101"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#下面这个对logstash性能比较重要的配置保持默认即可.默认就是CPU的内核数</span></span><br><span class="line"><span class="comment"># This defaults to the number of the host's CPU cores.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># pipeline.workers: 2</span></span><br></pre></td></tr></table></figure><ul><li><strong>修改JVM内存</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]# vim /etc/logstash/jvm.options</span><br><span class="line">-Xms16g</span><br><span class="line">-Xmx16g</span><br></pre></td></tr></table></figure><ul><li>systemctl启动logstash</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service logstash start</span><br></pre></td></tr></table></figure><hr><h3 id="部署到其他服务器"><a href="#部署到其他服务器" class="headerlink" title="部署到其他服务器"></a>部署到其他服务器</h3><p>如果通过克隆或者模板的方式部署到其他服务器,那么ES集群应该会报错,提示无法将节点加入到集群中.这是因为集群中所有节点的data数据目录拥有同一个ID.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">can&apos;t add node &#123;idc-function-elk03&#125;&#123;dt57hBjCQ9uCAp8G6sxJYw&#125;&#123;7-s_05ZMSl6CtpmMZGZEUA&#125;&#123;172.16.20.103&#125;&#123;172.16.20.103:9300&#125;&#123;dilmrt&#125;&#123;ml.machine_memory=61044445184, ml.max_open_jobs=20, xpack.installed=true, transform.node=true&#125;, found existing node &#123;idc-function-elk01&#125;&#123;dt57hBjCQ9uCAp8G6sxJYw&#125;&#123;biZPCqjCSx-PX3eU-CquUw&#125;&#123;172.16.20.101&#125;&#123;172.16.20.101:9300&#125;&#123;dilmrt&#125;&#123;ml.machine_memory=61044445184, ml.max_open_jobs=20, xpack.installed=true, transform.node=true&#125; with the same id but is a different node instance</span><br></pre></td></tr></table></figure><p>删除data数据路径下的内容,然后重启ES.等待集群自动同步</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk03 ~]# rm -rf /data/elasticsearch/nodes</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk03 ~]# ll /data/elasticsearch/</span><br><span class="line">total 0</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk03 ~]# systemctl restart elasticsearch</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk03 ~]# ll /data/elasticsearch/</span><br><span class="line">drwxr-xr-x 3 elasticsearch elasticsearch 15 May 26 17:30 nodes</span><br><span class="line">[root@idc-function-elk03 ~]#</span><br></pre></td></tr></table></figure><hr><h3 id="kibana安装"><a href="#kibana安装" class="headerlink" title="kibana安装"></a>kibana安装</h3><p>安装方式非常简单,直接下载rpm包安装即可.安装完成后修改<code>/etc/kibana/kibana.yaml</code>配置文件.新增以下配置.将kibana的web界面改为中文:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">i18n.locale: &quot;zh-CN&quot;</span><br></pre></td></tr></table></figure><blockquote><p>我这里是采用rpm包安装,如果是其他安装方式,请自行寻找配置文件路径</p></blockquote><p><strong>nginx反向代理配置</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">  server_name kibana.doweidu.com;</span><br><span class="line"></span><br><span class="line">  error_log /data/logs/nginx/kibana.error.log;</span><br><span class="line">  access_log /data/logs/nginx/kibana.access.log main;</span><br><span class="line"></span><br><span class="line">listen 443 ssl http2;</span><br><span class="line">  ssl_certificate /data/letsencrypt/kibana.doweidu.com/fullchain.cer;</span><br><span class="line">  ssl_certificate_key /data/letsencrypt/kibana.doweidu.com/kibana.doweidu.com.key;</span><br><span class="line">  include /data/letsencrypt/options-ssl-nginx.conf;</span><br><span class="line">  ssl_dhparam /data/letsencrypt/ssl-dhparams.pem;</span><br><span class="line"></span><br><span class="line">  location / &#123;</span><br><span class="line">    proxy_pass http://172.16.20.105:5601;</span><br><span class="line">    proxy_set_header Host $host;</span><br><span class="line">    proxy_redirect off;</span><br><span class="line">    proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">    proxy_connect_timeout    60;</span><br><span class="line">    proxy_read_timeout       60;</span><br><span class="line">    proxy_send_timeout       60;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">if ($host = kibana.doweidu.com) &#123;</span><br><span class="line">    return 301 https://$host$request_uri;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">listen 80;</span><br><span class="line">server_name kibana.doweidu.com;</span><br><span class="line">return 404;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h4 id="安装filebeat"><a href="#安装filebeat" class="headerlink" title="安装filebeat."></a>安装filebeat.</h4><p>对于有supervisor的服务器,采用二进制安装,否则使用rpm包安装</p><p>二进制包下载地址:<a href="http://repo.doweidu.com/ELK/filebeat-7.7.0-linux-x86_64.tar.gz" target="_blank" rel="noopener">http://repo.doweidu.com/ELK/filebeat-7.7.0-linux-x86_64.tar.gz</a></p><p>rpm包下载地址:<a href="http://repo.doweidu.com/ELK/filebeat-7.7.0-x86_64.rpm" target="_blank" rel="noopener">http://repo.doweidu.com/ELK/filebeat-7.7.0-x86_64.rpm</a></p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;生产环境部署ELK7-ES冷热数据分离&quot;&gt;&lt;a href=&quot;#生产环境部署ELK7-ES冷热数据分离&quot; class=&quot;headerlink&quot; title=&quot;生产环境部署ELK7+ES冷热数据分离&quot;&gt;&lt;/a&gt;生产环境部署ELK7+ES冷热数据分离&lt;/h2&gt;&lt;h3 id=&quot;需求整理&quot;&gt;&lt;a href=&quot;#需求整理&quot; class=&quot;headerlink&quot; title=&quot;需求整理&quot;&gt;&lt;/a&gt;需求整理&lt;/h3&gt;&lt;p&gt;当前公司还没有日志收集系统,.线上故障排错需要开发和运维人员通过跳板机登陆到业务服务器去查看日志,如果集群下服务器节点较多,可能需要登陆到每一台服务器才能找到准确的故障信息.这种方式会带来许多问题:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;效率低下&lt;/li&gt;
&lt;li&gt;需要给研发人员分配生产服务器账号,有安全隐患&lt;/li&gt;
&lt;li&gt;查看日志的方式极为麻烦,复杂&lt;/li&gt;
&lt;li&gt;每台业务服务器需要大容量磁盘保存日志文件&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&quot;Elasticsearch冷热数据分离&quot;&gt;&lt;a href=&quot;#Elasticsearch冷热数据分离&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearch冷热数据分离&quot;&gt;&lt;/a&gt;Elasticsearch冷热数据分离&lt;/h3&gt;&lt;p&gt;公司目前共有4个APP产品,每天的nginx日志+业务日志+php日志总共预计在1.5T左右.一周的日志数据量在10T左右.Elasticsearch规划了20T的SSD磁盘用来一周的日志数据(7天日志+1个副本).这些数据保存在Elasticsearch的热节点.&lt;/p&gt;
&lt;p&gt;考虑到1周以后的日志数据查询频率非常低,所以可以将这部分的日志迁移和归档到Elasticsearch的冷节点.并且冷节点日志无需索引.所以预留了10T的SAS磁盘保留1周的冷数据.&lt;/p&gt;
&lt;p&gt;总共是20TSSD磁盘的热节点和10TSAS机械磁盘的冷节点&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>使用goreplay收集线上真实http流量</title>
    <link href="https://jesse.top/2020/08/25/Linux-Web/%E4%BD%BF%E7%94%A8goreplay%E6%94%B6%E9%9B%86%E7%BA%BF%E4%B8%8A%E7%9C%9F%E5%AE%9Ehttp%E6%B5%81%E9%87%8F/"/>
    <id>https://jesse.top/2020/08/25/Linux-Web/使用goreplay收集线上真实http流量/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.311Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用goreplay收集线上真实http流量"><a href="#使用goreplay收集线上真实http流量" class="headerlink" title="使用goreplay收集线上真实http流量"></a>使用goreplay收集线上真实http流量</h2><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>在很多场景中,我们需要将线上服务器的真实Http请求复制转发到某台服务器中(或者测试环境中),并且前提是不影响线上生产业务进行.</p><p>例如:</p><ol><li>通常可能会通过ab等压测工具来对单一http接口进行压测。但如果是需要http服务整体压测，使用ab来压测工作量大且不方便，通过线上流量复制引流，通过将真实请求流量放大N倍来进行压测，能对服务有一个较为全面的检验.</li><li>将线上流量引入到测试环境中,测试某个中间件或者数据库的压力</li><li>上线前在预发布环境，使用线上真实的请求，检查是否准备发布的版本，是否具备发布标准</li><li>用线上的流量转发到预发，检查相同流量下一些指标的反馈情况，检查核心数据是否有改善、优化.</li></ol><a id="more"></a><hr><h3 id="goreplay介绍"><a href="#goreplay介绍" class="headerlink" title="goreplay介绍"></a>goreplay介绍</h3><p>goreplay项目请参考github:<a href="https://github.com/buger/goreplay" target="_blank" rel="noopener">goreplay介绍</a></p><p>goreplay是一款开源网络监控工具,可以在不影响业务的情况下,记录服务器真实流量,将该流量用来做镜像,压力测试,监控和分析等用途.</p><p>简单来说就是goreplay抓取线上真实的流量，并将捕捉到的流量转发到测试服务器上(或者保存到本地文件中)</p><p>goreplay大致工作流程如下:</p><p><img src="https://img2.jesse.top/20200629110035.png" alt=""></p><hr><h3 id="goreplay常见使用方式"><a href="#goreplay常见使用方式" class="headerlink" title="goreplay常见使用方式"></a>goreplay常见使用方式</h3><p>goreplay使用文档参考:<a href="https://github.com/buger/goreplay/wiki" target="_blank" rel="noopener">goreplay文档</a></p><p>常用的一些命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-input-raw 抓取指定端口的流量 gor --input-raw :8080</span><br><span class="line">-output-stdout 打印到控制台</span><br><span class="line">-output-file 将请求写到文件中 gor --input-raw :80 --output-file ./requests.gor</span><br><span class="line">-input-file 从文件中读取请求，与上一条命令呼应 gor --input-file ./requests.gor</span><br><span class="line">-exit-after 5s 持续时间</span><br><span class="line">-http-allow-url url白名单，其他请求将会被丢弃</span><br><span class="line">-http-allow-method 根据请求方式过滤</span><br><span class="line">-http-disallow-url 遇上一个url相反，黑名单，其他的请求会被捕获到</span><br></pre></td></tr></table></figure><blockquote><p>更多命令可以使用 ./gor –help查看</p></blockquote><hr><h3 id="goreplay安装"><a href="#goreplay安装" class="headerlink" title="goreplay安装"></a>goreplay安装</h3><p>在github上下载Linux的二进制文件: <a href="https://github.com/buger/goreplay/releases" target="_blank" rel="noopener">goreplay安装</a></p><blockquote><p>注意.虽然在github上提供了rpm安装包,但是实际安装发现无法安装:</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# rpm -ivh gor-1.0.0-1.x86_64.rpm</span><br><span class="line">Preparing...                          ################################# [100%]</span><br><span class="line">package goreplay-1.0.0-1.x86_64 is intended for a different operating system</span><br></pre></td></tr></table></figure><p>下载github上的二进制文件,解压后是一个gor的二进制可执行文件.复制到PATH变量路径下即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# wget https://github.com/buger/goreplay/releases/download/v1.0.0/gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# ls</span><br><span class="line"> gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# tar -xf gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# ls</span><br><span class="line">gor </span><br><span class="line">[root@dwd-tongji-3 ~]# ll gor</span><br><span class="line">-rwxr-xr-x 1 501 games 17779040 Mar 30  2019 gor</span><br><span class="line">[root@dwd-tongji-3 ~]# cp gor /usr/local/bin/</span><br></pre></td></tr></table></figure><hr><h3 id="goreplay简单实践"><a href="#goreplay简单实践" class="headerlink" title="goreplay简单实践"></a>goreplay简单实践</h3><h4 id="1-将本地http的流量保存到本地文件中"><a href="#1-将本地http的流量保存到本地文件中" class="headerlink" title="1.将本地http的流量保存到本地文件中."></a>1.将本地http的流量保存到本地文件中.</h4><p>为了简便起见,以下命令都在root用户下执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## 1.开启一个screen窗口</span><br><span class="line">[root@dwd-tongji-3 ~]# screen -S GOR</span><br><span class="line">## 2.将80流量保存到本地的文件</span><br><span class="line">[root@dwd-tongji-3 ~]# gor --input-raw :80 --output-file /data/requests.gor</span><br><span class="line">Version: 1.0.0</span><br></pre></td></tr></table></figure><p>默认情况下goreplay会以块文件存储,将流量保存为多个块文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# ls /data/requests_* | more</span><br><span class="line">/data/requests_0.gor</span><br><span class="line">/data/requests_100.gor</span><br><span class="line">/data/requests_101.gor</span><br><span class="line">/data/requests_102.gor</span><br><span class="line">/data/requests_103.gor</span><br><span class="line">/data/requests_104.gor</span><br><span class="line">/data/requests_105.gor</span><br><span class="line">/data/requests_106.gor</span><br><span class="line">/data/requests_107.gor</span><br><span class="line">/data/requests_108.gor</span><br><span class="line">/data/requests_109.gor</span><br><span class="line">/data/requests_10.gor</span><br></pre></td></tr></table></figure><p>使用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>[root@dwd-tongji-3 ~]#./gor –input-raw :80 –output-file /data/gor.gor –output-file-append</p><p>[root@dwd-tongji-3 ~]# ll /data -h<br>total 1.4M<br>-rw-r—– 1 root root 1.4M Jun 29 15:13 gor.gor<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 2.将http的请求打印到终端</span><br></pre></td></tr></table></figure></p><p>[root@dwd-tongji-3 ~]#gor –input-raw :8000 –output-stdout<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 3.将http的请求转发到测试环境</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-http=”<a href="http://beta:80&quot;" target="_blank" rel="noopener">http://beta:80&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在测试服务器上的nginx查看日志,.发现流量已经进来了</span><br></pre></td></tr></table></figure></p><p>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik.php HTTP/1.1” 200 5 “<a href="https://2021001151691008.hybrid.alipay-eco.com/2021001151691008/0.2.2006111453.18/index.html#pages/index/index?appid=2021001151691008&amp;taskId=415&quot;" target="_blank" rel="noopener">https://2021001151691008.hybrid.alipay-eco.com/2021001151691008/0.2.2006111453.18/index.html#pages/index/index?appid=2021001151691008&amp;taskId=415&quot;</a> “Mozilla/5.0 (iPhone; CPU iPhone OS 13_3_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/17D50 Ariver/1.0.12 AliApp(AP/10.1.95.7030) Nebula WK RVKType(0) AlipayDefined(nt:4G,ws:375|667|2.0) AlipayClient/10.1.95.7030 Language/zh-Hans Region/CN NebulaX/1.0.0” “112.96.179.238”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik.php HTTP/1.1” 200 5 “<a href="https://servicewechat.com/wxa090d3923fde0d4b/132/page-frame.html&quot;" target="_blank" rel="noopener">https://servicewechat.com/wxa090d3923fde0d4b/132/page-frame.html&quot;</a> “Mozilla/5.0 (iPhone; CPU iPhone OS 13_5_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 MicroMessenger/7.0.13(0x17000d29) NetType/4G Language/zh_CN” “14.106.171.11”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">####  也可以将流量输出到多个终端</span><br><span class="line"></span><br><span class="line">* 输出到多个http服务器</span><br></pre></td></tr></table></figure></p><p>gor –input-tcp :28020 –output-http “<a href="http://staging.com&quot;" target="_blank" rel="noopener">http://staging.com&quot;</a>  –output-http “<a href="http://dev.com&quot;" target="_blank" rel="noopener">http://dev.com&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 输出到文件或者Http服务器</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-file requests.log –output-http “<a href="http://staging.com&quot;" target="_blank" rel="noopener">http://staging.com&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">####  4.将流量从文件重放到http服务器</span><br><span class="line"></span><br><span class="line">1.首先将请求流量保存到本地文件</span><br></pre></td></tr></table></figure></p><p>sudo ./gor –input-raw :8000 –output-file=requests.gor<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2.再开一个窗口,运行gor,将请求流量从文件中重放</span><br></pre></td></tr></table></figure></p><p>./gor –input-file requests.gor –output-http=”<a href="http://localhost:8001&quot;" target="_blank" rel="noopener">http://localhost:8001&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 压力测试</span><br><span class="line"></span><br><span class="line">goreplay支持将捕获到的生产实际请求流量减少或者放大重播以用于测试环境的压力测试.压力测试一般针对Input流量减少或者放大.例如下面的例子</span><br></pre></td></tr></table></figure></p><h1 id="Replay-from-file-on-2x-speed"><a href="#Replay-from-file-on-2x-speed" class="headerlink" title="Replay from file on 2x speed"></a>Replay from file on 2x speed</h1><p>#将请求流量以2倍的速度放大重播<br>gor –input-file “requests.gor|200%” –output-http “staging.com”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">当然也也支持10%,20%等缩小请求流量</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 限速</span><br><span class="line"></span><br><span class="line">如果受限于测试环境的服务器资源压力,只想重播一部分流量到测试环境中,而不需要所有的实际生产流量,那么就可以用限速功能.有两种策略可以实现限流</span><br><span class="line"></span><br><span class="line">1.随机丢弃请求流量</span><br><span class="line"></span><br><span class="line">2.基于Header或者URL丢弃一定的流量(百分比)</span><br><span class="line"></span><br><span class="line">#####  随机丢弃请求流量</span><br><span class="line"></span><br><span class="line">input和output两端都支持限速,有两种限速算法:**百分比**或者**绝对值**</span><br><span class="line"></span><br><span class="line">* 百分比: input端支持缩小或者放大请求流量,基于指定的策略随机丢弃请求流量</span><br><span class="line">* 绝对值: 如果单位时间(秒)内达到临界值,则丢弃剩余请求流量,下一秒临界值还原</span><br><span class="line"></span><br><span class="line">**用法**:</span><br><span class="line"></span><br><span class="line">在output终端使用&quot;|&quot;运算符指定限速阈值,例如:</span><br><span class="line"></span><br><span class="line">* 使用绝对值限速</span><br></pre></td></tr></table></figure></p><h1 id="staging-server-will-not-get-more-than-ten-requests-per-second"><a href="#staging-server-will-not-get-more-than-ten-requests-per-second" class="headerlink" title="staging.server will not get more than ten requests per second"></a>staging.server will not get more than ten requests per second</h1><p>#staging服务每秒只接收10个请求<br>gor –input-tcp :28020 –output-http “<a href="http://staging.com|10&quot;" target="_blank" rel="noopener">http://staging.com|10&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 使用百分比限速</span><br></pre></td></tr></table></figure></p><h1 id="replay-server-will-not-get-more-than-10-of-requests"><a href="#replay-server-will-not-get-more-than-10-of-requests" class="headerlink" title="replay server will not get more than 10% of requests"></a>replay server will not get more than 10% of requests</h1><h1 id="useful-for-high-load-environments"><a href="#useful-for-high-load-environments" class="headerlink" title="useful for high-load environments"></a>useful for high-load environments</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##### 基于Header或者URL参数限速</span><br><span class="line"></span><br><span class="line">如果header或者URL参数中有唯一值,例如(API key),则可以转发指定百分比的流量到后端,例如:</span><br></pre></td></tr></table></figure></p><h1 id="Limit-based-on-header-value"><a href="#Limit-based-on-header-value" class="headerlink" title="Limit based on header value"></a>Limit based on header value</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%” –http-header-limiter “X-API-KEY: 10%”</p><h1 id="Limit-based-on-URL-param-value"><a href="#Limit-based-on-URL-param-value" class="headerlink" title="Limit based on URL param value"></a>Limit based on URL param value</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%” –http-param-limiter “api_key: 10%”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">###  过滤</span><br><span class="line"></span><br><span class="line">如果只想捕获指定的URL路径请求,或者http头部,或者Http方法,则可以使用过滤功能</span><br><span class="line"></span><br><span class="line">下面是几个例子</span><br><span class="line"></span><br><span class="line">* 只捕获某个URL</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-being-sent-to-the-api-endpoint"><a href="#only-forward-requests-being-sent-to-the-api-endpoint" class="headerlink" title="only forward requests being sent to the /api endpoint"></a>only forward requests being sent to the /api endpoint</h1><p>gor –input-raw :8080 –output-http staging.com –http-allow-url /api<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 拒绝某个URL</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-NOT-being-sent-to-the-api…-endpoint"><a href="#only-forward-requests-NOT-being-sent-to-the-api…-endpoint" class="headerlink" title="only forward requests NOT being sent to the /api… endpoint"></a>only forward requests NOT being sent to the /api… endpoint</h1><p>gor –input-raw :8080 –output-http staging.com –http-disallow-url /api<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 基于正则表达式过滤头部</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-with-an-api-version-of-1-0x"><a href="#only-forward-requests-with-an-api-version-of-1-0x" class="headerlink" title="only forward requests with an api version of 1.0x"></a>only forward requests with an api version of 1.0x</h1><p>gor –input-raw :8080 –output-http staging.com –http-allow-header api-version:^1.0\d</p><h1 id="only-forward-requests-NOT-containing-User-Agent-header-value-“Replayed-by-Gor”"><a href="#only-forward-requests-NOT-containing-User-Agent-header-value-“Replayed-by-Gor”" class="headerlink" title="only forward requests NOT containing User-Agent header value “Replayed by Gor”"></a>only forward requests NOT containing User-Agent header value “Replayed by Gor”</h1><p>gor –input-raw :8080 –output-http staging.com –http-disallow-header “User-Agent: Replayed by Gor”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 过滤HTTP请求方法</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-http “<a href="http://staging.server&quot;" target="_blank" rel="noopener">http://staging.server&quot;</a> \<br>    –http-allow-method GET \<br>    –http-allow-method OPTIONS<br><code>`</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;使用goreplay收集线上真实http流量&quot;&gt;&lt;a href=&quot;#使用goreplay收集线上真实http流量&quot; class=&quot;headerlink&quot; title=&quot;使用goreplay收集线上真实http流量&quot;&gt;&lt;/a&gt;使用goreplay收集线上真实http流量&lt;/h2&gt;&lt;h3 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;背景介绍&lt;/h3&gt;&lt;p&gt;在很多场景中,我们需要将线上服务器的真实Http请求复制转发到某台服务器中(或者测试环境中),并且前提是不影响线上生产业务进行.&lt;/p&gt;
&lt;p&gt;例如:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通常可能会通过ab等压测工具来对单一http接口进行压测。但如果是需要http服务整体压测，使用ab来压测工作量大且不方便，通过线上流量复制引流，通过将真实请求流量放大N倍来进行压测，能对服务有一个较为全面的检验.&lt;/li&gt;
&lt;li&gt;将线上流量引入到测试环境中,测试某个中间件或者数据库的压力&lt;/li&gt;
&lt;li&gt;上线前在预发布环境，使用线上真实的请求，检查是否准备发布的版本，是否具备发布标准&lt;/li&gt;
&lt;li&gt;用线上的流量转发到预发，检查相同流量下一些指标的反馈情况，检查核心数据是否有改善、优化.&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="goreplay" scheme="https://jesse.top/tags/goreplay/"/>
    
  </entry>
  
  <entry>
    <title>Filebeat+logstash收集Nginx错误日志</title>
    <link href="https://jesse.top/2020/08/25/elk/Filebeat+logstash%E6%94%B6%E9%9B%86Nginx%E9%94%99%E8%AF%AF%E6%97%A5%E5%BF%97/"/>
    <id>https://jesse.top/2020/08/25/elk/Filebeat+logstash收集Nginx错误日志/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Filebeat-logstash收集Nginx错误日志"><a href="#Filebeat-logstash收集Nginx错误日志" class="headerlink" title="Filebeat+logstash收集Nginx错误日志"></a>Filebeat+logstash收集Nginx错误日志</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>在上一个笔记&lt;Filebeat+logstash收集Nginx访问日志&gt;中分享了如何收集logstash的访问日志,这篇笔记主要是记录如何收集nginx的错误日志</p><p>Nginx 错误日志是运维人员最常见但又极其容易忽略的日志类型之一。Nginx 错误日志即没有统一明确的分隔符，也没有特别方便的正则模式，但通过 logstash 不同插件的组合，还是可以轻松做到数据处理。</p><hr><a id="more"></a><h3 id="logstash配置"><a href="#logstash配置" class="headerlink" title="logstash配置"></a>logstash配置</h3><p>在/etc/logstash/conf.d目录下编辑配置文件<code>nginx_error.conf</code>内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    beats &#123;</span><br><span class="line">      port =&gt; 5044</span><br><span class="line">      client_inactivity_timeout =&gt; 600</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">filter &#123;</span><br><span class="line">    if [fields][type] == &quot;nginx-error&quot; &#123;</span><br><span class="line">       grok &#123;</span><br><span class="line">      match =&gt; [</span><br><span class="line">          &quot;message&quot;, &quot;(?&lt;time_local&gt;%&#123;YEAR&#125;[./-]%&#123;MONTHNUM&#125;[./-]%&#123;MONTHDAY&#125;[- ]%&#123;TIME&#125;) \[%&#123;LOGLEVEL:log_level&#125;\] %&#123;POSINT:pid&#125;#%&#123;NUMBER&#125;: %&#123;GREEDYDATA:error_message&#125;(?:, client: (?&lt;client&gt;%&#123;IP&#125;|%&#123;HOSTNAME&#125;))(?:, server: %&#123;IPORHOST:server&#125;?)(?:, request: %&#123;QS:request&#125;)?(?:, upstream: (?&lt;upstream&gt;\&quot;%&#123;URI&#125;\&quot;|%&#123;QS&#125;))?(?:, host: %&#123;QS:request_host&#125;)?(?:, referrer: \&quot;%&#123;URI:referrer&#125;\&quot;)?&quot;,</span><br><span class="line">        &quot;message&quot;, &quot;(?&lt;time_local&gt;%&#123;YEAR&#125;[./-]%&#123;MONTHNUM&#125;[./-]%&#123;MONTHDAY&#125;[- ]%&#123;TIME&#125;) \[%&#123;LOGLEVEL:log_level&#125;\]\s&#123;1,&#125;%&#123;GREEDYDATA:error_message&#125;&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  date &#123;</span><br><span class="line">    match =&gt; [ &quot;timestamp&quot; , &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; ]</span><br><span class="line">  &#125;</span><br><span class="line">    mutate &#123;</span><br><span class="line">        convert =&gt; [ &quot;status&quot;, &quot;integer&quot; ]</span><br><span class="line">        convert =&gt; [ &quot;body_bytes&quot;,&quot;integer&quot; ]</span><br><span class="line">        remove_field =&gt; [&quot;message&quot;]</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  ruby &#123;</span><br><span class="line">    code =&gt; &quot;event.set(&apos;log_day&apos;, event.get(&apos;@timestamp&apos;).time.localtime.strftime(&apos;%Y%m%d&apos;))&quot;</span><br><span class="line">  &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">        elasticsearch &#123;</span><br><span class="line">            hosts =&gt; [&quot;172.16.20.107:9200&quot;]</span><br><span class="line">            index =&gt; &quot;logstash-%&#123;[fields][project]&#125;-%&#123;[fields][type]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">            #flush_size =&gt; 20000</span><br><span class="line">            #idle_flush_time =&gt; 10</span><br><span class="line">            #sniffing =&gt; true</span><br><span class="line">            #template_overwrite =&gt; true</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><hr><h3 id="Filebeat配置"><a href="#Filebeat配置" class="headerlink" title="Filebeat配置"></a>Filebeat配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">- type: log</span><br><span class="line"></span><br><span class="line">  # Change to true to enable this input configuration.</span><br><span class="line">  enabled: true</span><br><span class="line"></span><br><span class="line">  # Paths that should be crawled and fetched. Glob based paths.</span><br><span class="line">  paths:</span><br><span class="line">    - /data/logs/nginx/hsq_openapi_beta.error.log</span><br><span class="line"></span><br><span class="line">#fields字段用于打标签和索引,在logstash里判断日志来源</span><br><span class="line">  fields:</span><br><span class="line">     type: nginx-error</span><br><span class="line">     project: hsq</span><br><span class="line">     </span><br><span class="line">---output配置,将日志输出到logstash-----</span><br><span class="line">output.logstash:</span><br><span class="line">  # The Logstash hosts</span><br><span class="line">  hosts: [&quot;172.16.20.107:5044&quot;]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Filebeat-logstash收集Nginx错误日志&quot;&gt;&lt;a href=&quot;#Filebeat-logstash收集Nginx错误日志&quot; class=&quot;headerlink&quot; title=&quot;Filebeat+logstash收集Nginx错误日志&quot;&gt;&lt;/a&gt;Filebeat+logstash收集Nginx错误日志&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;在上一个笔记&amp;lt;Filebeat+logstash收集Nginx访问日志&amp;gt;中分享了如何收集logstash的访问日志,这篇笔记主要是记录如何收集nginx的错误日志&lt;/p&gt;
&lt;p&gt;Nginx 错误日志是运维人员最常见但又极其容易忽略的日志类型之一。Nginx 错误日志即没有统一明确的分隔符，也没有特别方便的正则模式，但通过 logstash 不同插件的组合，还是可以轻松做到数据处理。&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>记一次生产ELK性能优化</title>
    <link href="https://jesse.top/2020/08/25/elk/%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%94%9F%E4%BA%A7ELK%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    <id>https://jesse.top/2020/08/25/elk/记一次生产ELK性能优化/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-09-16T14:30:54.836Z</updated>
    
    <content type="html"><![CDATA[<h3 id="记一次生产ELK性能优化"><a href="#记一次生产ELK性能优化" class="headerlink" title="记一次生产ELK性能优化"></a>记一次生产ELK性能优化</h3><p>ES上线后遇到一些问题:</p><h3 id="一-内存压力过高"><a href="#一-内存压力过高" class="headerlink" title="一.内存压力过高"></a>一.内存压力过高</h3><p>目前ES节点的有两种规格内存.</p><p>80GB内存(节点同时运行了Logstash,分配了16G给logstash).</p><p>54GB内存(只运行ES,并且只作为data节点)</p><p>ES的JVM内存从32G下降到28G.建议JVM内存是总物理内存的一半左右,</p><p>节点内存使用情况如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&quot;mem&quot;: &#123; - </span><br><span class="line">          &quot;total&quot;: &quot;78.5gb&quot;,</span><br><span class="line">          &quot;total_in_bytes&quot;: 84296601600,</span><br><span class="line">          &quot;free&quot;: &quot;4.3gb&quot;,</span><br><span class="line">          &quot;free_in_bytes&quot;: 4688633856,</span><br><span class="line">          &quot;used&quot;: &quot;74.1gb&quot;,</span><br><span class="line">          &quot;used_in_bytes&quot;: 79607967744,</span><br><span class="line">          &quot;free_percent&quot;: 6,</span><br><span class="line">          &quot;used_percent&quot;: 94</span><br></pre></td></tr></table></figure><p>建议: ES节点内存建议配置高一点,虽然ES的JVM内存最大不超过32G.但是在其他方面仍然很吃内存</p><a id="more"></a><hr><h3 id="二-索引字段数"><a href="#二-索引字段数" class="headerlink" title="二.索引字段数"></a>二.索引字段数</h3><p>ES的单个索引默认最大字段数量是1000个.当超过这个字段数量时,会拒绝字段映射.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[2020-08-14T10:44:10,605][INFO ][o.e.a.b.TransportShardBulkAction] [idc-function-elk01] [logstash-mg-tc-netrcd-gateway-2020.08.14][0] mapping update rejected by primary</span><br><span class="line">java.lang.IllegalArgumentException: Limit of total fields [10000] in index [logstash-mg-tc-netrcd-gateway-2020.08.14] has been exceeded</span><br></pre></td></tr></table></figure><p>此时可以在索引模板中,修改默认字段数量.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PUT _template/logstash</span><br><span class="line">&#123;</span><br><span class="line">    &quot;index_patterns&quot;: &quot;logstash-*&quot;,</span><br><span class="line">    &quot;settings&quot;: &#123;</span><br><span class="line">        &quot;mapping.total_fields.limit&quot;:5000</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h3 id="三-ES线程和队列优化"><a href="#三-ES线程和队列优化" class="headerlink" title="三.ES线程和队列优化"></a>三.ES线程和队列优化</h3><p>ES日志有部分警告信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[2020-08-14T10:15:06,151][WARN ][o.e.c.r.a.AllocationService] [idc-function-elk01] failing shard [failed shard, shard [logstash-msf-internal-access-2020.08.14][0], node[3uwnN_KEQGiywi</span><br><span class="line">u9xR5Jng], [R], s[STARTED], a[id=ACE7jGGoS6yhS9T_Sn53UA], message [failed to perform indices:data/write/bulk[s] on replica [logstash-msf-internal-access-2020.08.14][0], node[3uwnN_KEQ</span><br><span class="line">Giywiu9xR5Jng], [R], s[STARTED], a[id=ACE7jGGoS6yhS9T_Sn53UA]], failure [RemoteTransportException[[idc-function-elk08][172.16.20.108:9300][indices:data/write/bulk[s][r]]]; nested: Cir</span><br><span class="line">cuitBreakingException[[parent] Data too large, data for [&lt;transport_request&gt;] would be [20720204958/19.2gb], which is larger than the limit of [20401094656/19gb], real usage: [2068074</span><br><span class="line">9096/19.2gb], new bytes reserved: [39455862/37.6mb], usages [request=197056/192.4kb, fielddata=1310/1.2kb, in_flight_requests=11463397212/10.6gb, accounting=54576960/52mb]]; ], markAs</span><br><span class="line">Stale [true]]</span><br><span class="line">org.elasticsearch.transport.RemoteTransportException: [idc-function-elk08][172.16.20.108:9300][indices:data/write/bulk[s][r]]</span><br><span class="line">Caused by: org.elasticsearch.common.breaker.CircuitBreakingException: [parent] Data too large, data for [&lt;transport_request&gt;] would be [20720204958/19.2gb], which is larger than the l</span><br><span class="line">imit of [20401094656/19gb], real usage: [20680749096/19.2gb], new bytes reserved: [39455862/37.6mb], usages [request=197056/192.4kb, fielddata=1310/1.2kb, in_flight_requests=114633972</span><br><span class="line">12/10.6gb, accounting=54576960/52mb]</span><br><span class="line">        at org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService.checkParentLimit(HierarchyCircuitBreakerService.java:347) ~[elasticsearch-7.7.0.jar:7.7.0]</span><br><span class="line">        at org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.addEstimateBytesAndMaybeBreak(ChildMemoryCircuitBreaker.java:128) ~[elasticsearch-7.7.0.jar:7.7.0]</span><br></pre></td></tr></table></figure><p>编辑<code>/etc/elasticsearch/elasticsearch.yml</code>配置文件.新增如下配置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">thread_pool:</span><br><span class="line">    write:</span><br><span class="line">        size: 32</span><br><span class="line">        queue_size: 10000</span><br><span class="line">processors: 32</span><br></pre></td></tr></table></figure><p>同时,修改<code>/etc/logstash/logstash.yml</code>配置文件.修改如下配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pipeline.batch.size: 10000</span><br><span class="line">pipeline.batch.delay: 100</span><br></pre></td></tr></table></figure><blockquote><p>以上这些配置都需要不断的调试,修改,观察,找到最适合的一个范围和效果</p></blockquote><hr><h3 id="四-集群最大分片-shard-数"><a href="#四-集群最大分片-shard-数" class="headerlink" title="四.集群最大分片(shard)数"></a>四.集群最大分片(shard)数</h3><p>今天收集新的日志报错,logstash日志内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2020-08-24T10:32:41,009][WARN ][logstash.outputs.elasticsearch][main][7ca4981ae091d6b8f604e5695405b2c74a9cb7fc4a72b338be4e2ede66a04d7d] Could not index event to Elasticsearch. &#123;:status=&gt;400, :action=&gt;[&quot;index&quot;, &#123;:_id=&gt;nil, :_index=&gt;&quot;logstash-hsq-search-netrcd-2020.08.24&quot;, :routing=&gt;nil, :_type=&gt;&quot;_doc&quot;&#125;, #&lt;LogStash::Event:0x4a0b491&gt;], :response=&gt;&#123;&quot;index&quot;=&gt;&#123;&quot;_index&quot;=&gt;&quot;logstash-hsq-search-netrcd-2020.08.24&quot;, &quot;_type&quot;=&gt;&quot;_doc&quot;, &quot;_id&quot;=&gt;nil, &quot;status&quot;=&gt;400, &quot;error&quot;=&gt;&#123;&quot;type&quot;=&gt;&quot;illegal_argument_exception&quot;, &quot;reason&quot;=&gt;&quot;Validation Failed: 1: this action would add [14] total shards, but this cluster currently has [8998]/[9000] maximum shards open;&quot;&#125;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure><p>看日志关键字发现当前集群总共分片数是8998个,如果再加上14个分片,那么就超过了最大的9000个.</p><p><strong>14和9000个分片是怎么来的?</strong></p><p>当前ES集群中有7个热节点,每个索引分片数量是7个,副本数是1..也就是说一个索引就要14个分片(包括副本)</p><p>另外,集群中还有2个冷节点,总共是9个节点.从Elasticsearch v7.0.0 开始，集群中的每个节点默认限制 1000 个shard.所以集群所有节点总共是9000个shard(分片)</p><p><strong>解决方案</strong></p><p>1.每个索引(Index)分配多少个分片(shared)合适?</p><p>配置Elasticsearch集群后,对于分片的数量通常比较难确定.分配过小或者过大对性能都不好.实际上每个分片都会消耗硬件资源:</p><ul><li>由于分片本质上是Lucene索引，因此会消耗文件句柄，内存和CPU资源。</li><li>每个搜索请求都将触摸索引中每个分片的副本，当分片分布在多个节点上时，这不是问题。当分片争夺相同的硬件资源时，就会出现争用并且性能会下降。</li></ul><p>我个人理解一般设置分片数量有几个方向可以考虑</p><ul><li>由于Elasticsearch的最大JVM一般在30-32G.所以一个分片的数量不能超过30G大小.如果一个索引最大是200G.那么就需要分片7个分片.</li><li>最好是按照日志创建索引,可以按天,或者周,甚至月来创建索引,如果一个月的日志太大,那么就按天创建索引</li><li>分片数量的配置最好是基于当前的日志量级,集群节点数量评估.或者可以规划为可以预见的增长幅度评估,千万不能盲目的为一年或者2年以后的日志量分配资源</li><li>如果不能确定索引容量,或者对于分片设置完全没有概念和把握,那么建议按照ES集群节点数量设置分片,有多少个节点,就设置多少分片,后期再观察和考虑是否需要增加或删减</li><li><p>生产环境中,最好设置为1个副本.副本有助于加速查询和健壮高可用行,但是也会占用磁盘容量空间,没必要设置2个或以上的副本.如果是做了冷热分离,对于冷数据如果没有太大的安全性要求,也可以不设置副本</p><p>2.每个节点的<code>maximum shards open</code>设置为多大合适</p></li></ul><p><strong>根据以下几个指标来评估:</strong></p><p>1.当前集群的shard分片数</p><p>2.当前集群索引量</p><p>3.索引保存时间</p><p>以我们生产集群为例,当前集群共有7个热节点,每个索引7个分片.每节点是一个分片.每天是55个索引,加上副本是110个索引,也就是每个节点,每天有110个shard分片.</p><p>热节点索引保留14天,单节点总共是110*14=1540个Shard.预留20%的空间,也就是1848个分片.所以给每个节点分配1800-2000个分片比较合适</p><p>另外,集群中还有2个冷节点.超过14天的索引会自动迁移到热节点(没有副本),并且保留14天后删除.所以冷节点14天总共分片数是55*7*14=5390个Shard.分摊到2个节点,平均每个节点的Shard是2695..留出20%的空间,每节点的Shard为3234.</p><p><strong>经过评估下来,ES热节点每节点Shard数量2000,冷节点3500</strong></p><p>编辑ES配置文件,添加以下配置,修改每节点的Shard数量</p><ul><li><p>热节点: <code>cluster.max_shards_per_node: 2000</code></p></li><li><p>冷节点: <code>cluster.max_shards_per_node: 3500</code></p></li></ul><p>以下是Ansible发布模板</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if es_role  is defined and es_role == &apos;hot&apos; %&#125;</span><br><span class="line">cluster.max_shards_per_node: 2000</span><br><span class="line">&#123;% elif es_role  is defined and es_role == &apos;cold&apos; %&#125;</span><br><span class="line">cluster.max_shards_per_node: 3500</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>重启ES后并没有生效.临时在Kibana的dev tool中设置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">PUT /_cluster/settings</span><br><span class="line">&#123;</span><br><span class="line">  &quot;transient&quot;: &#123;</span><br><span class="line">    &quot;cluster&quot;: &#123;</span><br><span class="line">      &quot;max_shards_per_node&quot;:3000</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>设置立即生效,在ELK集群面板中,总的shard分片数超过了9000的最大值,已经达到9,376 shards,</p><p>博客参考:<a href="https://studygolang.com/articles/25396" target="_blank" rel="noopener">https://studygolang.com/articles/25396</a></p><hr><h3 id="五-分片严重不均衡"><a href="#五-分片严重不均衡" class="headerlink" title="五.分片严重不均衡"></a>五.分片严重不均衡</h3><p>ES集群运行了将近一个月后,出现了分片验证不均衡的现象.当前7个Hot节点中,每个索引一共有7个分片,但是我们发现大部分索引的7个分片全部写入到ELK09这个节点,其他节点没有分配任何分片.如下图所示:</p><p><img src="https://img2.jesse.top/image-20200904103820376.png" alt="image-20200904103820376"></p><p>即使将elk09这个节点的磁盘警戒线和最高水位线下调到50%和80%,但是仍然不起作用,磁盘使用率依然上升到90%以上.</p><p>也尝试过,严格限定每个索引每节点最大分片数4个,但是ES会将所有4个分片全部写入到ELK09节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PUT _template/logstash</span><br><span class="line">&#123;</span><br><span class="line">    &quot;index_patterns&quot;: &quot;logstash-*&quot;,</span><br><span class="line">    &quot;settings&quot;: &#123;</span><br><span class="line">        &quot;index.routing.allocation.total_shards_per_node&quot;: 4 #每节点单索引最大分片数4个</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>之所以设置为4个,是因为每个索引7个分片在14天后会自动迁移到ES冷节点,而一共2个冷节点,所以每个节点4分片.(冷数据没有副本)</p></blockquote><p>观察nodes节点,elk09这个节点的内存,CPU,负载等各指标相比其他节点都高出一大截</p><p><img src="https://img2.jesse.top/image-20200904103802250.png" alt="image-20200904103802250"></p><p><strong>故障原因:</strong></p><p>故障原因在于其他热节点(elk01-elk06)有2块1.92T的SSD磁盘,总容量3.4T,这台elk09节点只有一块磁盘1.92T的SSD磁盘..这台elk09节点的磁盘容量只有其他节点的一半.</p><p>ELK默认是平均分配分片在节点中.当ES中数据较少时,分片会平均分配到各节点中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1245 759.7gb     1tb   3.9tb 4.9tb 20 172.16.20.108 172.16.20.108 idc-function-elk08</span><br><span class="line">1426 771.3gb 798.9gb   2.6tb 3.4tb 22 172.16.20.101 172.16.20.101 idc-function-elk01</span><br><span class="line">1426   770gb 795.6gb   2.7tb 3.4tb 22 172.16.20.102 172.16.20.102 idc-function-elk02</span><br><span class="line">1426 770.6gb 796.1gb   2.7tb 3.4tb 22 172.16.20.104 172.16.20.104 idc-function-elk04</span><br><span class="line">1426 770.7gb   811gb   2.6tb 3.4tb 22 172.16.20.105 172.16.20.105 idc-function-elk05</span><br><span class="line">1426 771.1gb 843.9gb   2.6tb 3.4tb 23 172.16.20.103 172.16.20.103 idc-function-elk03</span><br><span class="line">1426 770.9gb 797.1gb   2.6tb 3.4tb 22 172.16.20.106 172.16.20.106 idc-function-elk06</span><br><span class="line">1426 771.4gb 798.6gb 982.2gb 1.7tb 44 172.16.20.109 172.16.20.109 idc-function-elk09</span><br><span class="line">1244 792.5gb   869gb   4.1tb 4.9tb 16 172.16.20.107 172.16.20.107 idc-function-elk07</span><br></pre></td></tr></table></figure><p>但是当索引数据越来越大,elk09磁盘由于警戒线和水位线设置,所以索引分片更多写入到elk01-06这些节点,这就造成elk09节点上的分片数量和其他节点相比差距越来越大.</p><p>下图是故障发生时,可以看到每个节点的分片数在2787左右.但是ELK09这个节点的分片在2155个.</p><p><img src="https://img2.jesse.top/image-20200904103746474.png" alt="image-20200904103746474"></p><p>这就导致ES为了平衡各节点分配数量,将新索引的分片集中全部写入到elk09这个节点,以求集群各节点分片数量平衡,从而忽略了节点的磁盘使用率和硬件资源状态.</p><p><strong>解决办法</strong></p><p>有以下3种解决办法可以参考</p><p>1.给elk09添加一块同容量SSD磁盘,这样所有节点的磁盘容量一样,这也是最根本的解决办法</p><p>2.适当减少ES的索引保留天数,减少数据量,使得elk09单节点总数据量不超过单块磁盘容量(1.92T)中的50%左右区间.</p><p>3.调整集群参数<code>cluster.routing.allocation.balance.shard</code>和<code>cluster.routing.allocation.balance.index</code>.这2个值越大,集群更倾向于在集群中平均负载分片,所以这2个需要调小,具体什么值合适,需要不断的观察和测试.</p><p>关于这2个参数的介绍可以参考官网或者博客:</p><p><a href="https://www.bookstack.cn/read/ELKstack-guide-cn/elasticsearch-principle-shard-allocate.md" target="_blank" rel="noopener">https://www.bookstack.cn/read/ELKstack-guide-cn/elasticsearch-principle-shard-allocate.md</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;记一次生产ELK性能优化&quot;&gt;&lt;a href=&quot;#记一次生产ELK性能优化&quot; class=&quot;headerlink&quot; title=&quot;记一次生产ELK性能优化&quot;&gt;&lt;/a&gt;记一次生产ELK性能优化&lt;/h3&gt;&lt;p&gt;ES上线后遇到一些问题:&lt;/p&gt;
&lt;h3 id=&quot;一-内存压力过高&quot;&gt;&lt;a href=&quot;#一-内存压力过高&quot; class=&quot;headerlink&quot; title=&quot;一.内存压力过高&quot;&gt;&lt;/a&gt;一.内存压力过高&lt;/h3&gt;&lt;p&gt;目前ES节点的有两种规格内存.&lt;/p&gt;
&lt;p&gt;80GB内存(节点同时运行了Logstash,分配了16G给logstash).&lt;/p&gt;
&lt;p&gt;54GB内存(只运行ES,并且只作为data节点)&lt;/p&gt;
&lt;p&gt;ES的JVM内存从32G下降到28G.建议JVM内存是总物理内存的一半左右,&lt;/p&gt;
&lt;p&gt;节点内存使用情况如下:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;quot;mem&amp;quot;: &amp;#123; - &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;total&amp;quot;: &amp;quot;78.5gb&amp;quot;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;total_in_bytes&amp;quot;: 84296601600,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;free&amp;quot;: &amp;quot;4.3gb&amp;quot;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;free_in_bytes&amp;quot;: 4688633856,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;used&amp;quot;: &amp;quot;74.1gb&amp;quot;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;used_in_bytes&amp;quot;: 79607967744,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;free_percent&amp;quot;: 6,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;used_percent&amp;quot;: 94&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;建议: ES节点内存建议配置高一点,虽然ES的JVM内存最大不超过32G.但是在其他方面仍然很吃内存&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>Kibana图表制作</title>
    <link href="https://jesse.top/2020/08/25/elk/Kibana%E5%9B%BE%E8%A1%A8%E5%88%B6%E4%BD%9C/"/>
    <id>https://jesse.top/2020/08/25/elk/Kibana图表制作/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kibana图表制作"><a href="#Kibana图表制作" class="headerlink" title="Kibana图表制作"></a>Kibana图表制作</h2><p>kibana的可视化可以制作各种统计分析图表,然后合并展示到dashbord中.下面介绍一些常用的nginx的访问图表.</p><blockquote><p>在kibana7.7版本中可以配置kibana web界面的中文语言.</p></blockquote><p>编辑kibana配置文件,<code>/etc/kibana/kibana.yml</code>,加入以下配置: </p><p><code>i18n.locale: &quot;zh-CN&quot;</code></p><h4 id="一-统计过去XXX时间的访问量"><a href="#一-统计过去XXX时间的访问量" class="headerlink" title="一.统计过去XXX时间的访问量"></a>一.统计过去XXX时间的访问量</h4><p>在可视化界面,添加仪表盘图表.指定nginx的openapi access访问日志索引.</p><p>无需进行任何配置,选择右上角的时间范围,会显示计数,也就是日志量的计数</p><p><img src="https://img2.jesse.top/image-20200723154706247.png" alt="image-20200723154706247"></p><a id="more"></a><h3 id="openapi-nginx流量图"><a href="#openapi-nginx流量图" class="headerlink" title="openapi-nginx流量图"></a>openapi-nginx流量图</h3><p><img src="https://img2.jesse.top/image-20200806155157188.png" alt="image-20200806155157188"></p><h4 id="添加城市访问地图"><a href="#添加城市访问地图" class="headerlink" title="添加城市访问地图"></a>添加城市访问地图</h4><p>在<code>map</code>界面新建一个地图.在<code>road map</code>的地图上添加图层.</p><p><img src="https://img2.jesse.top/image-20200723155049422.png" alt="image-20200723155049422"></p><p>选择数据源.选择文档类型:</p><p><img src="https://img2.jesse.top/image-20200723160002287.png" alt="image-20200723160002287"></p><p>选择索引后,选择<code>geoip.location</code>字段,此时客户端地图分布自动展现出现,而且会自动计数</p><p><img src="https://img2.jesse.top/image-20200723160127185.png" alt="image-20200723160127185"></p><h4 id="Nginx状态码统计图"><a href="#Nginx状态码统计图" class="headerlink" title="Nginx状态码统计图"></a>Nginx状态码统计图</h4><p>添加饼图,使用<code>status</code>指标来统计各状态码的次数.</p><p><img src="https://img2.jesse.top/image-20200723163110993.png" alt="image-20200723163110993"></p><blockquote><p>如果没有status字段,需要去刷新索引</p></blockquote><h4 id="Nginx访问客户端TOP5"><a href="#Nginx访问客户端TOP5" class="headerlink" title="Nginx访问客户端TOP5"></a>Nginx访问客户端TOP5</h4><p>添加垂直条形图.使用geoip关键词统计各IP的访问次数,并且按降序排序</p><p><img src="https://img2.jesse.top/image-20200723163406793.png" alt="image-20200723163406793"></p><h4 id="nginx请求URL的TOP5"><a href="#nginx请求URL的TOP5" class="headerlink" title="nginx请求URL的TOP5"></a>nginx请求URL的TOP5</h4><p>添加数据图表,使用request关键词统计各URL的访问次数,并按降序排序</p><p><img src="https://img2.jesse.top/image-20200723163554578.png" alt="image-20200723163554578"></p><h4 id="nginx的cost请求时间TOP10"><a href="#nginx的cost请求时间TOP10" class="headerlink" title="nginx的cost请求时间TOP10"></a>nginx的cost请求时间TOP10</h4><p>添加垂直条形图.使用responsetime关键词做聚合,统计请求最慢的cost时间</p><p><img src="https://img2.jesse.top/image-20200723163717829.png" alt="image-20200723163717829"></p><hr><h4 id="在Dashboard中将多个图表聚合成一个大盘"><a href="#在Dashboard中将多个图表聚合成一个大盘" class="headerlink" title="在Dashboard中将多个图表聚合成一个大盘"></a>在Dashboard中将多个图表聚合成一个大盘</h4><p><img src="https://img2.jesse.top/image-20200723163906182.png" alt="image-20200723163906182"></p><p>​                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kibana图表制作&quot;&gt;&lt;a href=&quot;#Kibana图表制作&quot; class=&quot;headerlink&quot; title=&quot;Kibana图表制作&quot;&gt;&lt;/a&gt;Kibana图表制作&lt;/h2&gt;&lt;p&gt;kibana的可视化可以制作各种统计分析图表,然后合并展示到dashbord中.下面介绍一些常用的nginx的访问图表.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在kibana7.7版本中可以配置kibana web界面的中文语言.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;编辑kibana配置文件,&lt;code&gt;/etc/kibana/kibana.yml&lt;/code&gt;,加入以下配置: &lt;/p&gt;
&lt;p&gt;&lt;code&gt;i18n.locale: &amp;quot;zh-CN&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&quot;一-统计过去XXX时间的访问量&quot;&gt;&lt;a href=&quot;#一-统计过去XXX时间的访问量&quot; class=&quot;headerlink&quot; title=&quot;一.统计过去XXX时间的访问量&quot;&gt;&lt;/a&gt;一.统计过去XXX时间的访问量&lt;/h4&gt;&lt;p&gt;在可视化界面,添加仪表盘图表.指定nginx的openapi access访问日志索引.&lt;/p&gt;
&lt;p&gt;无需进行任何配置,选择右上角的时间范围,会显示计数,也就是日志量的计数&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2.jesse.top/image-20200723154706247.png&quot; alt=&quot;image-20200723154706247&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
</feed>

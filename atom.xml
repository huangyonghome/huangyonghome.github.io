<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jesse&#39;s home</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jesse.top/"/>
  <updated>2020-08-26T23:55:20.314Z</updated>
  <id>https://jesse.top/</id>
  
  <author>
    <name>Jesse</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>zabbix监控vmware主机以及GuestOS</title>
    <link href="https://jesse.top/2020/08/26/%E7%9B%91%E6%8E%A7/zabbix%E7%9B%91%E6%8E%A7vmware%E4%B8%BB%E6%9C%BA%E4%BB%A5%E5%8F%8AGuestOS/"/>
    <id>https://jesse.top/2020/08/26/监控/zabbix监控vmware主机以及GuestOS/</id>
    <published>2020-08-26T01:20:58.000Z</published>
    <updated>2020-08-26T23:55:20.314Z</updated>
    
    <content type="html"><![CDATA[<h3 id="zabbix监控vmware主机以及GuestOS"><a href="#zabbix监控vmware主机以及GuestOS" class="headerlink" title="zabbix监控vmware主机以及GuestOS"></a>zabbix监控vmware主机以及GuestOS</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>ESXI主机无法安装zabbix agent,所以不能使用传统的agent客户端监控vmware主机,但是Zabbix有自导的vmware hypervisors监控模板.Zabbix 通过 vmware collector 进程来监控虚拟机,使用SOAP协议从vmware web服务器获取必要的监控信息.</p><hr><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p>1.在zabbix服务器修改<code>zabbix_server.conf</code>配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">StartVMwareCollectors=6</span><br><span class="line">VMwareCacheSize=50M</span><br><span class="line">VMwareFrequency=10</span><br><span class="line">VMwarePerfFrequency=60</span><br><span class="line">VMwareTimeout=300</span><br></pre></td></tr></table></figure><a id="more"></a><p><strong>说明</strong>: </p><p><strong>StartVMwareCollectors</strong>：vmware 收集器实例的数量。<br>此值取决于要监控的 VMware 服务的数量。在大多数情况下，这应该是：<code>servicenum &lt; StartVMwareCollectors &lt; (servicenum * 2)</code>其中 servicenum 是 VMware 服务的数量。</p><p>例如：如果您有 1 个 VMware 服务要将 StartVMwareCollectors 设置为 2，那么如果您有 3 个 VMware 服务，请将其设置为 5。请注意，在大多数情况下，此值不应小于 2，不应大于 VMware 数量的 2 倍服务。</p><p>2.重启zabbix服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart zabbix_server</span><br></pre></td></tr></table></figure><hr><h4 id="Esxi物理主机配置"><a href="#Esxi物理主机配置" class="headerlink" title="Esxi物理主机配置"></a>Esxi物理主机配置</h4><p>1.登陆Esxi web界面: <a href="https://172.16.0.55" target="_blank" rel="noopener">https://172.16.0.55</a><br>2.在<code>manage</code>—<code>system</code>—-<code>advanced settings</code>.修改<code>Config.HostAgent.plugins.solo.enableMob</code>的值为True</p><p><img src="https://img2.jesse.top/image-20200818112727513.png" alt="image-20200818112727513"></p><p>3.访问:<a href="https://172.16.0.55/mob/?moid=ha-host&amp;doPath=hardware.systemInfo" target="_blank" rel="noopener">https://172.16.0.55/mob/?moid=ha-host&amp;doPath=hardware.systemInfo</a><br>记录UUID<br><img src="https://img2.jesse.top/image-20200818112949235.png" alt="image-20200818112949235"></p><p>4.在zabbix添加主机</p><p><img src="https://img2.jesse.top/image-20200818113114835.png" alt="image-20200818113114835"></p><ul><li><strong>主机名称</strong>:上面查到的UUID</li><li><strong>IP地址</strong>:Esxi的IP地址</li><li><strong>端口</strong>:80</li></ul><p><strong>模板</strong>:</p><p><img src="https://img2.jesse.top/image-20200818113315388.png" alt="image-20200818113315388"></p><p><strong>宏</strong></p><p><img src="https://img2.jesse.top/image-20200818113428859.png" alt="image-20200818113428859"></p><ul><li><strong>password</strong>: Esxi主机密码</li><li><p><strong>URL</strong>: <a href="https://Esxi_IP/sdk" target="_blank" rel="noopener">https://Esxi_IP/sdk</a> </p></li><li><p><strong>username</strong>: ESXI主机用户名</p></li><li><strong>UUID</strong>: 上文记录的UUID</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;zabbix监控vmware主机以及GuestOS&quot;&gt;&lt;a href=&quot;#zabbix监控vmware主机以及GuestOS&quot; class=&quot;headerlink&quot; title=&quot;zabbix监控vmware主机以及GuestOS&quot;&gt;&lt;/a&gt;zabbix监控vmware主机以及GuestOS&lt;/h3&gt;&lt;h4 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h4&gt;&lt;p&gt;ESXI主机无法安装zabbix agent,所以不能使用传统的agent客户端监控vmware主机,但是Zabbix有自导的vmware hypervisors监控模板.Zabbix 通过 vmware collector 进程来监控虚拟机,使用SOAP协议从vmware web服务器获取必要的监控信息.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h4&gt;&lt;p&gt;1.在zabbix服务器修改&lt;code&gt;zabbix_server.conf&lt;/code&gt;配置文件&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;StartVMwareCollectors=6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwareCacheSize=50M&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwareFrequency=10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwarePerfFrequency=60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwareTimeout=300&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="监控" scheme="https://jesse.top/categories/%E7%9B%91%E6%8E%A7/"/>
    
    
      <category term="zabbix监控" scheme="https://jesse.top/tags/zabbix%E7%9B%91%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>kibana索引及索引冷热分离管理</title>
    <link href="https://jesse.top/2020/08/25/elk/kibana%E7%B4%A2%E5%BC%95%E5%8F%8A%E7%B4%A2%E5%BC%95%E5%86%B7%E7%83%AD%E5%88%86%E7%A6%BB%E7%AE%A1%E7%90%86/"/>
    <id>https://jesse.top/2020/08/25/elk/kibana索引及索引冷热分离管理/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.313Z</updated>
    
    <content type="html"><![CDATA[<h3 id="kibana索引及索引冷热分离管理"><a href="#kibana索引及索引冷热分离管理" class="headerlink" title="kibana索引及索引冷热分离管理"></a>kibana索引及索引冷热分离管理</h3><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>本篇文档简单介绍一下kibana的索引管理.主要包括:</p><ul><li>索引模板</li><li>索引模式</li><li>索引生命周期</li><li>索引冷热分离管理</li><li>索引模板配置</li></ul><hr><h3 id="索引模板"><a href="#索引模板" class="headerlink" title="索引模板"></a>索引模板</h3><p>ES自带一个默认的logstash-*的索引模板,es内部维护了template，template定义好了mapping，只要index的名称被template匹配到，那么该index的mapping就按照template中定义的mapping自动创建。而且template中定义了index的shard分片数量、replica副本数量等等属性。</p><p>所以我们每次新建一个索引的时候,不需要手动创建mapping映射,也不需要手动设置副本和分片数量.</p><p>在Kibana的图形化界面中可以看到ES默认的索引模板</p><a id="more"></a><p><img src="https://img2.jesse.top/image-20200728094858891.png" alt="image-20200728094858891"></p><p>点击进入logstash索引模板,还能对模板进行编辑.比如设置以下重要2个参数:</p><p><code>refresh_interval</code> —-索引刷新间隔,一般在5-10秒范围内,周期可以设置长一点,有助于提高ES性能,减少不必要的刷新</p><p><code>number_of_shards</code>—–索引分片数量,在ES7版本中默认为1,在以前的版本默认为5.一般情况下设置为等同于ES data节点的数量</p><p><code>number_of_replicas</code>—-默认副本数量为1,如果有特殊需要,可以在下方设置.</p><p><img src="https://img2.jesse.top/image-20200728095100748.png" alt="image-20200728095100748"></p><p>由于ES默认索引模板的存在,所以logstash在向ES传输数据的时候,索引最好是以<code>logstash-</code>开头.例如:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">output &#123;</span><br><span class="line">        elasticsearch &#123;</span><br><span class="line">            hosts =&gt; [&quot;172.16.20.101:9200&quot;]</span><br><span class="line">            index =&gt; &quot;logstash-%&#123;[fields][project]&#125;-%&#123;[fields][type]&#125;-%&#123;[fields][level]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h3 id="索引模式"><a href="#索引模式" class="headerlink" title="索引模式"></a>索引模式</h3><p>当一个新建的索引传输到ES时,需要将这个索引关联到刚才提到的索引模板中.比如下面的<code>logstash-msf-mysql-slow-2020.02.28</code>的索引</p><p><img src="https://img2.jesse.top/image-20200728095631533.png" alt="image-20200728095631533"></p><p><strong>在Kibana的索引模式界面中,创建一个索引模式</strong></p><p><img src="https://img2.jesse.top/image-20200728100440229.png" alt="image-20200728100440229"></p><p><strong>由于每天创建一个索引,所以定义一个索引前缀,使用通配符匹配后面的日期</strong></p><p><code>logstash-msf-mysql-slow-*</code></p><p><img src="https://img2.jesse.top/image-20200728100507970.png" alt="image-20200728100507970"></p><p>默认使用日期来筛选字段</p><p><img src="https://img2.jesse.top/image-20200728100625572.png" alt="image-20200728100625572"></p><p><strong>创建完成后,字段已经自动映射</strong></p><p><img src="https://img2.jesse.top/image-20200728100714384.png" alt="image-20200728100714384"></p><p><strong>此时在discovery界面就能看到该索引并且该索引下的数据了</strong>.</p><hr><h3 id="索引生命周期"><a href="#索引生命周期" class="headerlink" title="索引生命周期"></a>索引生命周期</h3><p>Kibana的管理界面可以管理索引的生命周期,在下面的图示中,创建一个新的生命周期策略</p><p><img src="https://img2.jesse.top/image-20200729091321456.png" alt="image-20200729091321456"></p><p>可以管理索引的生命周期,例如,下面的截图中配置超过2天的索引自动删除</p><p><img src="https://img2.jesse.top/image-20200729092222991.png" alt="image-20200729092222991"></p><p><strong>保存策略,然后将该策略关联到索引模板.</strong></p><p><img src="https://img2.jesse.top/image-20200729092332714.png" alt="image-20200729092332714"></p><hr><h3 id="索引冷热分离管理"><a href="#索引冷热分离管理" class="headerlink" title="索引冷热分离管理"></a>索引冷热分离管理</h3><p>在&lt;生产环境部署ELK+冷热数据分离&gt;笔记中提到,当前业务每天大概1.5T左右的日志,规划了20T的SSD磁盘,刚好可以保留7天的日志(副本数为1).开发人员需要将日志保留更久(例如一个月),以便有些故障未能及时发现,可能数周后才去追踪日志,排查故障.</p><p>但是,将一个月的日志都存储在SSD的磁盘中成本较高.在这种场景中,可以使用冷热分离,将索引数据分开..例如,7天内的日志为热数据,该日志查询频率非常高,可以作为热数据,存储在ES的热节点,.7天后的日志查询频率就非常低,几乎很少查询,此时可以做为冷数据,存储在ES的冷节点.</p><p>ES冷节点通常硬件资源较低,CPU和内存配置相对热节点较低,磁盘通常采用机械磁盘,一方面冷节点无需太高配置,另一方面节省服务器成本费用</p><blockquote><p>在&lt;生产环境部署ELK+冷热数据分离&gt;笔记中的ES集群截图中,可以看到每个ES的data节点都有hot或者cold属性..该属性定义了ES的冷热节点角色</p></blockquote><p>还是在上面的Kibana索引生命周期中配置冷热分离.只需开启冷阶段配置即可.配置参考如下:</p><p><img src="https://img2.jesse.top/image-20200729102204264.png" alt="image-20200729102204264"></p><p>接着在Kibana的dev tool工具中,给索引模板新增以下配置,将索引数据分配到hot节点ES服务器:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">PUT _template/logstash</span><br><span class="line">&#123;</span><br><span class="line">    &quot;index_patterns&quot;: &quot;logstash-*&quot;,</span><br><span class="line">    &quot;settings&quot;: &#123;</span><br><span class="line">        &quot;index.number_of_replicas&quot;: &quot;1&quot;,</span><br><span class="line">         &quot;index.number_of_shards&quot;: &quot;7&quot;,</span><br><span class="line">         &quot;index.refresh_interval&quot;: &quot;10s&quot;,</span><br><span class="line">        &quot;index.routing.allocation.require.box_type&quot;: &quot;hot&quot;</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查看默认的logstash-*索引模板.可以看到设置已经生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;routing&quot;: &#123;</span><br><span class="line">     &quot;allocation&quot;: &#123;</span><br><span class="line">       &quot;require&quot;: &#123;</span><br><span class="line">         &quot;box_type&quot;: &quot;hot&quot;</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure><p>此后进来的索引数据,会自动存储到hot节点的ES服务器上.cold服务器不存储任何热数据</p><hr><p>PS: 为了测试冷热数据是否正常工作,也可以手动将索引从Hot节点迁移到cold节点,以索引<code>logstash-msf-fpm-error-2020.07.27</code>为例,在Kibana的<code>dev tool</code>工具中执行下面的语句,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PUT /logstash-msf-fpm-error-2020.07.27/_settings </span><br><span class="line">&#123; </span><br><span class="line">  &quot;settings&quot;: &#123; </span><br><span class="line">    &quot;index.routing.allocation.require.box_type&quot;: &quot;cold&quot;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>则该索引会增加下列属性:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;routing&quot;: &#123; - </span><br><span class="line">         &quot;allocation&quot;: &#123; - </span><br><span class="line">           &quot;require&quot;: &#123; - </span><br><span class="line">             &quot;box_type&quot;: &quot;cold&quot;</span><br><span class="line">           &#125;</span><br><span class="line">         &#125;</span><br></pre></td></tr></table></figure><blockquote><p>Kibana7版本的索引生命周期会自动迁移旧的索引数据到cold节点,所以无需手动或者写脚本定时迁移</p></blockquote><p>Kibana的索引生命周期生效后,旧的索引就会自动从hot节点迁移到cold节点:</p><p><img src="https://img2.jesse.top/image-20200729103131421.png" alt="image-20200729103131421"></p><blockquote><p>当配置了Kibana的索引生命周期后,logstash数据传输到ES时,只需要指定ES的Hot节点(或者master)节点即可,,cold节点(如果不是master)则无需写入到output配置中</p></blockquote><hr><h3 id="索引模板配置"><a href="#索引模板配置" class="headerlink" title="索引模板配置"></a>索引模板配置</h3><p>总结下来,索引模板需要做如下配置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">PUT _template/logstash</span><br><span class="line">&#123;</span><br><span class="line">    &quot;index_patterns&quot;: &quot;logstash-*&quot;,</span><br><span class="line">    &quot;settings&quot;: &#123;</span><br><span class="line">        &quot;index.number_of_replicas&quot;: &quot;1&quot;,  #副本数量</span><br><span class="line">         &quot;index.number_of_shards&quot;: &quot;7&quot;,   #分片数量</span><br><span class="line">         &quot;index.refresh_interval&quot;: &quot;10s&quot;, #索引刷新间隔</span><br><span class="line">        &quot;index.routing.allocation.require.box_type&quot;: &quot;hot&quot;, #索引分配节点</span><br><span class="line">        &quot;mapping.total_fields.limit&quot;:5000 #索引最大字段数量</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>索引设置效果如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;index&quot;: &#123;</span><br><span class="line">    &quot;lifecycle&quot;: &#123;</span><br><span class="line">      &quot;name&quot;: &quot;索引冷热分离&quot;  #索引生命周期策略</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;routing&quot;: &#123;</span><br><span class="line">      &quot;allocation&quot;: &#123;</span><br><span class="line">        &quot;require&quot;: &#123;</span><br><span class="line">          &quot;box_type&quot;: &quot;hot&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;mapping&quot;: &#123;</span><br><span class="line">      &quot;total_fields&quot;: &#123;</span><br><span class="line">        &quot;limit&quot;: &quot;5000&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;refresh_interval&quot;: &quot;10s&quot;,</span><br><span class="line">    &quot;number_of_shards&quot;: &quot;7&quot;,</span><br><span class="line">    &quot;number_of_replicas&quot;: &quot;1&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外,有时候索引找不到geoip.location字段.此时就需要在索引模板中手动映射字段</p><p>手动添加映射字段:</p><p>字段名: geoip.location<br>字段值: 地理坐标点</p><p><img src="https://img2.jesse.top/image-20200817153710670.png" alt="image-20200817153607943"></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;kibana索引及索引冷热分离管理&quot;&gt;&lt;a href=&quot;#kibana索引及索引冷热分离管理&quot; class=&quot;headerlink&quot; title=&quot;kibana索引及索引冷热分离管理&quot;&gt;&lt;/a&gt;kibana索引及索引冷热分离管理&lt;/h3&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;本篇文档简单介绍一下kibana的索引管理.主要包括:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;索引模板&lt;/li&gt;
&lt;li&gt;索引模式&lt;/li&gt;
&lt;li&gt;索引生命周期&lt;/li&gt;
&lt;li&gt;索引冷热分离管理&lt;/li&gt;
&lt;li&gt;索引模板配置&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&quot;索引模板&quot;&gt;&lt;a href=&quot;#索引模板&quot; class=&quot;headerlink&quot; title=&quot;索引模板&quot;&gt;&lt;/a&gt;索引模板&lt;/h3&gt;&lt;p&gt;ES自带一个默认的logstash-*的索引模板,es内部维护了template，template定义好了mapping，只要index的名称被template匹配到，那么该index的mapping就按照template中定义的mapping自动创建。而且template中定义了index的shard分片数量、replica副本数量等等属性。&lt;/p&gt;
&lt;p&gt;所以我们每次新建一个索引的时候,不需要手动创建mapping映射,也不需要手动设置副本和分片数量.&lt;/p&gt;
&lt;p&gt;在Kibana的图形化界面中可以看到ES默认的索引模板&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>Filebeat+logstash收集Nginx访问日志</title>
    <link href="https://jesse.top/2020/08/25/elk/Filebeat+logstash%E6%94%B6%E9%9B%86Nginx%E8%AE%BF%E9%97%AE%E6%97%A5%E5%BF%97/"/>
    <id>https://jesse.top/2020/08/25/elk/Filebeat+logstash收集Nginx访问日志/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Filebeat-logstash收集Nginx访问日志"><a href="#Filebeat-logstash收集Nginx访问日志" class="headerlink" title="Filebeat+logstash收集Nginx访问日志"></a>Filebeat+logstash收集Nginx访问日志</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境:"></a>环境:</h3><p><strong>Filebeat</strong>: 7.0</p><p><strong>Logstash</strong>:7.0</p><p><strong>elasticsearch</strong>:7.0</p><hr><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>logstash默认自带了apache标准日志格式的grok正则表达式:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">COMMONAPACHELOG %&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;NOTSPACE:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;(?:%&#123;WORD:verb&#125; %&#123;NOTSPACE:request&#125;(?: HTTP/%&#123;NUMBER:httpversion&#125;)?|%&#123;DATA:rawrequest&#125;)&quot; %&#123;NUMBER:response&#125; (?:%&#123;NUMBER:bytes&#125;|-)</span><br><span class="line">COMBINEDAPACHELOG %&#123;COMMONAPACHELOG&#125; %&#123;QS:referrer&#125; %&#123;QS:agent&#125;</span><br></pre></td></tr></table></figure><p>对于 nginx 标准日志格式，可以发现只是最后多了一个 <code>$http_x_forwarded_for</code> 变量。所以 nginx 标准日志的 grok 正则定义是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MAINNGINXLOG %&#123;COMBINEDAPACHELOG&#125; %&#123;QS:x_forwarded_for&#125;</span><br></pre></td></tr></table></figure><p>如果Nginx日志格式不是标准的日志格式,则需要自行编写grok正则,匹配日志内容.</p><p>但是grok正则表达式不容易上手,非常难写.复杂不说,而且logstash使用正则表达式来处理日志格式,其性能也会受到很大的影响.</p><p>所以这里推荐另外一种收集方式.</p><p>本文档参考博客:<a href="https://www.popyone.com/post/13.html" target="_blank" rel="noopener">https://www.popyone.com/post/13.html</a></p><p>本文档参考书籍:<strong>ELK权威指南中文版第二版</strong></p><a id="more"></a><hr><h3 id="json格式的Nginx日志"><a href="#json格式的Nginx日志" class="headerlink" title="json格式的Nginx日志"></a>json格式的Nginx日志</h3><p>对 logstash 来说，nginx 日志还有另一种更简便的处理方式。就是自定义日志格式时，通过手工拼写，直接输出成 JSON 格式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">log_format json escape=json  <span class="string">'&#123;"@timestamp":"$time_iso8601",'</span></span><br><span class="line">                    <span class="string">'"@source":"$server_addr",'</span></span><br><span class="line">                    <span class="string">'"hostname":"$hostname",'</span></span><br><span class="line">                    <span class="string">'"ip":"$http_x_forwarded_for",'</span></span><br><span class="line">                    <span class="string">'"client":"$remote_addr",'</span></span><br><span class="line">                    <span class="string">'"request_method":"$request_method",'</span></span><br><span class="line">                    <span class="string">'"scheme":"$scheme",'</span></span><br><span class="line">                    <span class="string">'"domain":"$server_name",'</span></span><br><span class="line">                    <span class="string">'"client_host":"$host",'</span></span><br><span class="line">                    <span class="string">'"referer":"$http_referer",'</span></span><br><span class="line">                    <span class="string">'"request":"$request_uri",'</span></span><br><span class="line">                    <span class="string">'"args":"$args",'</span></span><br><span class="line">                    <span class="string">'"size":$body_bytes_sent,'</span></span><br><span class="line">                    <span class="string">'"status": $status,'</span></span><br><span class="line">                    <span class="string">'"responsetime":$request_time,'</span></span><br><span class="line">                    <span class="string">'"upstreamtime":"$upstream_response_time",'</span></span><br><span class="line">                    <span class="string">'"upstreamaddr":"$upstream_addr",'</span></span><br><span class="line">                    <span class="string">'"http_user_agent":"$http_user_agent"'</span></span><br><span class="line">                    <span class="string">'&#125;'</span>;</span><br></pre></td></tr></table></figure><blockquote><p>escape=json 参数，在配置日志格式时加上此参数可以不转义变量内容</p></blockquote><p>nginx虚拟主机的access_log应用了Json的格式后,日志格式自动转换成了json格式了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;@timestamp&quot;:&quot;2019-01-31T11:26:48+08:00&quot;,&quot;@source&quot;:&quot;192.168.12.200&quot;,&quot;hostname&quot;:&quot;proxy.server&quot;,&quot;ip&quot;:&quot;&quot;,&quot;client&quot;:&quot;183.3.239.170&quot;,&quot;request_method&quot;:&quot;GET&quot;,&quot;scheme&quot;:&quot;http&quot;,&quot;domain&quot;:&quot;www.51hangyu.com&quot;,&quot;referer&quot;:&quot;http://www.51hangyu.com/chat&quot;,&quot;request&quot;:&quot;/static/static/css/main.97f6b853.css&quot;,&quot;args&quot;:&quot;&quot;,&quot;size&quot;:460413,&quot;status&quot;: 200,&quot;responsetime&quot;:0.108,&quot;upstreamtime&quot;:&quot;0.002&quot;,&quot;upstreamaddr&quot;:&quot;192.168.12.15:80&quot;,&quot;http_user_agent&quot;:&quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)&quot;&#125;</span><br></pre></td></tr></table></figure><p>这样一来在客户端就自动转换成了json格式,无需logstash通过正则去过滤和匹配日志内容,大大提高logstash的收集性能</p><hr><h3 id="Filebeat配置"><a href="#Filebeat配置" class="headerlink" title="Filebeat配置"></a>Filebeat配置</h3><p>filebeat.yml配置如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">- type: log</span><br><span class="line"></span><br><span class="line">  # Change to true to enable this input configuration.</span><br><span class="line">  enabled: true</span><br><span class="line"></span><br><span class="line">  # Paths that should be crawled and fetched. Glob based paths.</span><br><span class="line">  paths:</span><br><span class="line">    - /data/logs/nginx/hsq_openapi_beta.access.log</span><br><span class="line"></span><br><span class="line">#默认这个值是FALSE的，也就是我们的json日志解析后会被放在json键上。设为TRUE，所有的keys就会被放到根节点</span><br><span class="line">  json.keys_under_root: true</span><br><span class="line"></span><br><span class="line">#是否要覆盖原有的key，这是关键配置，将keys_under_root设为TRUE后，再将overwrite_keys也设为TRUE，就能把filebeat默认的key值给覆盖了</span><br><span class="line">  json.overwrite_keys: true</span><br><span class="line">  exclude_lines: [&apos;HEAD&apos;]    #排除HEAD访问日志,因为HEAD是阿里云SLB的健康检查访问,而且没有外网IP,所有不需要上传</span><br><span class="line">  json.message_key: request_method #HEAD是request_mothod的字段值,所以需要指定字段名</span><br><span class="line">#fields字段用于打标签和索引,在logstash里判断日志来源</span><br><span class="line">  fields:</span><br><span class="line">     type: nginx-openapi</span><br><span class="line">     project: msf</span><br><span class="line">     level: access</span><br><span class="line">     </span><br><span class="line">---output配置,将日志输出到logstash-----</span><br><span class="line">output.logstash:</span><br><span class="line">  # The Logstash hosts</span><br><span class="line">  hosts: [&quot;172.16.20.107:5044&quot;]</span><br></pre></td></tr></table></figure><hr><h3 id="logstash配置"><a href="#logstash配置" class="headerlink" title="logstash配置"></a>logstash配置</h3><p>在logstash的con.d下新建一个Nginx_access的配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk07 elasticsearch]# mkdir /etc/logstash/conf.d/nginx_access.conf</span><br></pre></td></tr></table></figure><p>配置文件内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk07 elasticsearch]# cat /etc/logstash/conf.d/nginx_access.conf</span><br><span class="line">input &#123;</span><br><span class="line">    beats &#123;</span><br><span class="line">      port =&gt; 5044</span><br><span class="line">      codec =&gt; json</span><br><span class="line">      client_inactivity_timeout =&gt; 600</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">filter &#123;</span><br><span class="line">   if  &quot;openapi-nginx-access&quot; in [tags] &#123;</span><br><span class="line">       date &#123;</span><br><span class="line">           match =&gt; [ &quot;timestamp&quot; ,&quot;dd/MMM/yyyy:HH:mm:ss Z&quot; ]</span><br><span class="line">           target =&gt; &quot;@timestamp&quot;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">      mutate &#123;</span><br><span class="line">        remove_field =&gt; &quot;timestamp&quot;</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">      geoip &#123;</span><br><span class="line">        source =&gt; &quot;ip&quot;</span><br><span class="line">        target =&gt; &quot;geoip&quot;</span><br><span class="line">        database =&gt; &quot;/etc/logstash/GeoLite2/GeoLite2-City.mmdb&quot;</span><br><span class="line">        add_field =&gt; [&quot;[geoip][coordinates]&quot;,&quot;%&#123;[geoip][longitude]&#125;&quot;]</span><br><span class="line">        add_field =&gt; [&quot;[geoip][coordinates]&quot;,&quot;%&#123;[geoip][latitude]&#125;&quot;]</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">      mutate &#123;</span><br><span class="line">        remove_field =&gt; &quot;geoip.continent_code&quot;</span><br><span class="line">        remove_field =&gt; &quot;geoip.country_code2&quot;</span><br><span class="line">        remove_field =&gt; &quot;geoip.country_code3&quot;</span><br><span class="line">        convert =&gt; [ &quot;[geoip][coordinates]&quot;,&quot;float&quot; ]</span><br><span class="line">        convert =&gt; [ &quot;status&quot;,&quot;integer&quot; ]</span><br><span class="line">        convert =&gt; [ &quot;size&quot;,&quot;integer&quot; ]</span><br><span class="line">        convert =&gt; [ &quot;upstreatime&quot;,&quot;float&quot; ]</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line">    elasticsearch &#123;</span><br><span class="line">            hosts =&gt; [&quot;172.16.20.101:9200&quot;,&quot;172.16.20.110:9200&quot;,&quot;172.16.20.107:9200&quot;,&quot;172.16.20.108:9200&quot;,&quot;172.16.20.102:9200&quot;,&quot;172.16.20.103:9200&quot;,&quot;172.16.20.104:9200&quot;,&quot;172.16.20.105:9200&quot;,&quot;172.16.20.106:9200&quot;,&quot;172.16.20.109:9200&quot;]</span><br><span class="line">            index =&gt; &quot;logstash-%&#123;[fields][project]&#125;-%&#123;[fields][type]&#125;-%&#123;[fields][level]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">            user =&gt; &quot;elastic&quot;</span><br><span class="line">            password =&gt; &quot;password&quot;</span><br><span class="line">            #flush_size =&gt; 20000</span><br><span class="line">            #idle_flush_time =&gt; 10</span><br><span class="line">            sniffing =&gt; true</span><br><span class="line">            template_overwrite =&gt; true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>建议logstash先将日志输出到控制台测试,确定无误后再输出到es.输出到控制台只需要将output字段修改为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; output &#123;stdout &#123;&#125;&#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><hr><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>打开es集群的http界面,可以看到产生了索引分片</p><p><img src="https://img2.jesse.top/image-20200706112236216.png" alt="image-20200706112236216"></p><p>打开kibana配置了logstash-hsq-nginx-access-*的索引以后,可以看到展示的日志:</p><p><img src="https://img2.jesse.top/image-20200706112436147.png" alt="image-20200706112436147"></p><hr><h3 id="多个http-x-forwarded-fors上游IP的Nginx访问日志"><a href="#多个http-x-forwarded-fors上游IP的Nginx访问日志" class="headerlink" title="多个http_x_forwarded_fors上游IP的Nginx访问日志"></a>多个http_x_forwarded_fors上游IP的Nginx访问日志</h3><p>iqg业务的Nginx经过了2层代理(阿里云SLB—–&gt;Kong—-&gt;本地Nginx).所以上游IP有2个,使用json格式转换后,Nginx的访问日志内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;@timestamp&quot;:&quot;2020-08-14T03:15:38+08:00&quot;,&quot;@source&quot;:&quot;10.111.10.35&quot;,&quot;hostname&quot;:&quot;iqg-new1&quot;,&quot;ip&quot;:&quot;183.192.43.174, 100.117.85.32&quot;,&quot;client&quot;:&quot;10.111.30.194&quot;,&quot;request_method&quot;:&quot;GET&quot;,&quot;scheme&quot;:&quot;http&quot;,&quot;domain&quot;:&quot;api.v4.iqianggou.com&quot;,&quot;client_host&quot;:&quot;v4.api.iqg-new1&quot;,&quot;referer&quot;:&quot;&quot;,&quot;request&quot;:&quot;/open/user/clientupload?......&#125;</span><br></pre></td></tr></table></figure><p>可以看到IP字段包含了用户真实IP和阿里云SLB的IP.这样的访问日志上传到Logstash后,geoip模块无法处理IP字段.所以需要在logstash中解析和处理IP字段,</p><p>但是还有一个更好的办法,不需要logstash去单独处理,减少logstash的负担.可以在nginx本地将处理IP字段.</p><p>在nginx.conf配置文件的http字段中添加以下配置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">......</span><br><span class="line">map $http_x_forwarded_for  $clientRealIp &#123;</span><br><span class="line">                    &quot;&quot;      $remote_addr;</span><br><span class="line">                            ~^(?P&lt;firstAddr&gt;[0-9\.|:|a-f\.|:|A-F\.|:]+),?.*$  $firstAddr;</span><br><span class="line">         &#125;</span><br><span class="line">......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上map函数解析<code>$http_x_forwarded_for</code>字段,当字段为空时,使用<code>$remote_addr</code>映射为<code>$clientRealIp</code>.否则使用<code>$firstAddr</code>映射.</p><p>同时修改json日志格式,使用<code>$clientRealIp</code>替代<code>$http_x_forwarded_for</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">log_format json escape=json  &apos;&#123;&quot;@timestamp&quot;:&quot;$time_iso8601&quot;,&apos;</span><br><span class="line">                    &apos;&quot;@source&quot;:&quot;$server_addr&quot;,&apos;</span><br><span class="line">                    &apos;&quot;hostname&quot;:&quot;$hostname&quot;,&apos;</span><br><span class="line">                    &apos;&quot;ip&quot;:&quot;$clientRealIp&quot;,&apos;</span><br><span class="line">                    &apos;&quot;client&quot;:&quot;$remote_addr&quot;,&apos;</span><br><span class="line">                    &apos;&quot;request_method&quot;:&quot;$request_method&quot;,&apos;</span><br><span class="line">                    &apos;&quot;scheme&quot;:&quot;$scheme&quot;,&apos;</span><br><span class="line">                    &apos;&quot;domain&quot;:&quot;$server_name&quot;,&apos;</span><br><span class="line">                    &apos;&quot;client_host&quot;:&quot;$host&quot;,&apos;</span><br><span class="line">                    &apos;&quot;referer&quot;:&quot;$http_referer&quot;,&apos;</span><br><span class="line">                    &apos;&quot;request&quot;:&quot;$request_uri&quot;,&apos;</span><br><span class="line">                    &apos;&quot;args&quot;:&quot;$args&quot;,&apos;</span><br><span class="line">                    &apos;&quot;size&quot;:$body_bytes_sent,&apos;</span><br><span class="line">                    &apos;&quot;status&quot;: $status,&apos;</span><br><span class="line">                    &apos;&quot;responsetime&quot;:$request_time,&apos;</span><br><span class="line">                    &apos;&quot;upstreamtime&quot;:&quot;$upstream_response_time&quot;,&apos;</span><br><span class="line">                    &apos;&quot;upstreamaddr&quot;:&quot;$upstream_addr&quot;,&apos;</span><br><span class="line">                    &apos;&quot;http_user_agent&quot;:&quot;$http_user_agent&quot;&apos;</span><br><span class="line">                    &apos;&#125;&apos;;</span><br></pre></td></tr></table></figure><p>重启Nginx</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo nginx -t</span><br><span class="line">sudo nginx -s reload</span><br></pre></td></tr></table></figure><p>再次查看日志,发现此时ip字段只保留了公网IP地址,去掉了阿里云的SLB内网地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;@timestamp&quot;:&quot;2020-08-14T09:46:59+08:00&quot;,&quot;@source&quot;:&quot;10.111.10.35&quot;,&quot;hostname&quot;:&quot;iqg-new1&quot;,&quot;ip&quot;:&quot;113.247.47.77&quot;,&quot;client&quot;:&quot;10.111.30.194&quot;,&quot;request_method&quot;:&quot;GET&quot;,&quot;scheme&quot;:&quot;http&quot;,&quot;domain&quot;:&quot;api.v4.iqianggou.com&quot;,&quot;client_host&quot;:&quot;v4.api.iqg-new1&quot;,&quot;referer&quot;:&quot;https://servicewechat.com/wxa36b4671d95ba753/86/page-frame.html&quot;,&quot;request&quot;:&quot;/api/item?......&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Filebeat-logstash收集Nginx访问日志&quot;&gt;&lt;a href=&quot;#Filebeat-logstash收集Nginx访问日志&quot; class=&quot;headerlink&quot; title=&quot;Filebeat+logstash收集Nginx访问日志&quot;&gt;&lt;/a&gt;Filebeat+logstash收集Nginx访问日志&lt;/h2&gt;&lt;h3 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境:&quot;&gt;&lt;/a&gt;环境:&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Filebeat&lt;/strong&gt;: 7.0&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Logstash&lt;/strong&gt;:7.0&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;elasticsearch&lt;/strong&gt;:7.0&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;logstash默认自带了apache标准日志格式的grok正则表达式:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;COMMONAPACHELOG %&amp;#123;IPORHOST:clientip&amp;#125; %&amp;#123;USER:ident&amp;#125; %&amp;#123;NOTSPACE:auth&amp;#125; \[%&amp;#123;HTTPDATE:timestamp&amp;#125;\] &amp;quot;(?:%&amp;#123;WORD:verb&amp;#125; %&amp;#123;NOTSPACE:request&amp;#125;(?: HTTP/%&amp;#123;NUMBER:httpversion&amp;#125;)?|%&amp;#123;DATA:rawrequest&amp;#125;)&amp;quot; %&amp;#123;NUMBER:response&amp;#125; (?:%&amp;#123;NUMBER:bytes&amp;#125;|-)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;COMBINEDAPACHELOG %&amp;#123;COMMONAPACHELOG&amp;#125; %&amp;#123;QS:referrer&amp;#125; %&amp;#123;QS:agent&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;对于 nginx 标准日志格式，可以发现只是最后多了一个 &lt;code&gt;$http_x_forwarded_for&lt;/code&gt; 变量。所以 nginx 标准日志的 grok 正则定义是：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;MAINNGINXLOG %&amp;#123;COMBINEDAPACHELOG&amp;#125; %&amp;#123;QS:x_forwarded_for&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果Nginx日志格式不是标准的日志格式,则需要自行编写grok正则,匹配日志内容.&lt;/p&gt;
&lt;p&gt;但是grok正则表达式不容易上手,非常难写.复杂不说,而且logstash使用正则表达式来处理日志格式,其性能也会受到很大的影响.&lt;/p&gt;
&lt;p&gt;所以这里推荐另外一种收集方式.&lt;/p&gt;
&lt;p&gt;本文档参考博客:&lt;a href=&quot;https://www.popyone.com/post/13.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.popyone.com/post/13.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文档参考书籍:&lt;strong&gt;ELK权威指南中文版第二版&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>使用ElastAlert+ELK实现日志监控告警</title>
    <link href="https://jesse.top/2020/08/25/elk/%E4%BD%BF%E7%94%A8ElastAlert+ELK%E5%AE%9E%E7%8E%B0%E6%97%A5%E5%BF%97%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/"/>
    <id>https://jesse.top/2020/08/25/elk/使用ElastAlert+ELK实现日志监控告警/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用ElastAlert-ELK实现日志监控告警"><a href="#使用ElastAlert-ELK实现日志监控告警" class="headerlink" title="使用ElastAlert+ELK实现日志监控告警"></a>使用ElastAlert+ELK实现日志监控告警</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>目前公司使用ELK做日志收集和展示分析.所以想对一些关键日志进行监控告警.比如Nginx的5xx日志,比如php-fpm的Fatal严重错误日志等.通过监控ES的日志数据,然后使用Python调用钉钉接口来实现日志的告警</p><hr><h3 id="ElastAlert介绍"><a href="#ElastAlert介绍" class="headerlink" title="ElastAlert介绍"></a>ElastAlert介绍</h3><p>ElastAlert是一个开源的工具,用于从Elastisearch中检索数据,并根据匹配模式发出告警.github项目地址如下:<a href="https://github.com/Yelp/elastalert" target="_blank" rel="noopener">https://github.com/Yelp/elastalert</a></p><p>官方文档如下:<a href="https://elastalert.readthedocs.io/en/latest/elastalert.html" target="_blank" rel="noopener">https://elastalert.readthedocs.io/en/latest/elastalert.html</a></p><p>它支持多种监控模式和告警方式,具体可以查阅Github项目介绍.但是自带的ElastAlert并不支持钉钉告警,在github上有第三方的钉钉python项目.地址如下:<a href="https://github.com/xuyaoqiang/elastalert-dingtalk-plugin" target="_blank" rel="noopener">https://github.com/xuyaoqiang/elastalert-dingtalk-plugin</a></p><hr><a id="more"></a><h3 id="ElastAlert安装"><a href="#ElastAlert安装" class="headerlink" title="ElastAlert安装"></a>ElastAlert安装</h3><blockquote><p>新版的ElastAlert不支持python2了.所以需要安装Python3环境</p></blockquote><ul><li>安装依赖</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum -y install python3 python3-devel python3-libs python3-setuptools git gcc</span><br></pre></td></tr></table></figure><p>如果是Ubuntu系统:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt -y upgrade</span><br><span class="line">sudo apt install -y python3.6-dev</span><br><span class="line">sudo apt install -y libffi-dev libssl-dev</span><br><span class="line">sudo apt install -y python3-pip</span><br><span class="line">sudo apt install -y python3-venv</span><br></pre></td></tr></table></figure><ul><li>安装elastalert模块</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install elastalert  -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com</span><br></pre></td></tr></table></figure><ul><li>克隆ElastAlert项目</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/Yelp/elastalert.git</span><br><span class="line">cp -r elastalert /data/</span><br></pre></td></tr></table></figure><ul><li>安装模块</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /data/elastalert/</span><br><span class="line">pip3 install &quot;setuptools&gt;=11.3&quot;</span><br><span class="line"></span><br><span class="line">pip install -r requirements.txt  -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com</span><br></pre></td></tr></table></figure><ul><li>创建ElastAlert的索引</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo elastalert-create-index --index elastalert</span><br></pre></td></tr></table></figure><ul><li>修改ElastAlert的配置文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cp config.yaml.example config.yaml</span><br><span class="line">vim config.yaml</span><br><span class="line"></span><br><span class="line">rules_folder: rule  #rule匹配模式的目录,可以自定义一个/data/elastalert路径下的相对目录</span><br><span class="line">run_every:          #ElastAlert多久向Elasticsearch发送一次请求</span><br><span class="line">  minutes: 1</span><br><span class="line">buffer_time:        #如果某些日志源不是实时的，则ElastAlert将缓冲最近一段时间的结果.这个值默认是15,但是无法触发告警,设置为1正常</span><br><span class="line">  minutes: 1</span><br><span class="line">es_host: localhost      #ES集群节点,随便指定任意一台均可</span><br><span class="line">es_port: 9200           #ES端口号</span><br><span class="line">es_username: elastic    # 如果ES使用了X-pack安全验证,则需要配置此项,否则注释</span><br><span class="line">es_password: password   # 同上</span><br><span class="line">writeback_index: elastalert_status  #ElastAlert索引名</span><br><span class="line">alert_time_limit:       #如果告警发送失败,则会在下面时间范围内尝试重新发送</span><br><span class="line">  days: 2</span><br></pre></td></tr></table></figure><ul><li>配置钉钉报警</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/xuyaoqiang/elastalert-dingtalk-plugin.git</span><br><span class="line">cd elastalert-dingtalk-plugin</span><br><span class="line">pip3 install -r requirements.txt -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com</span><br><span class="line">cp -r elastalert_modules /data/elastalert/</span><br></pre></td></tr></table></figure><hr><h3 id="Rule规则"><a href="#Rule规则" class="headerlink" title="Rule规则"></a>Rule规则</h3><p>官方支持很多Rule模式,在<code>example_rules</code>目录下也有很多参考Rule可以参考.一般常用的是类型(type)是<code>frequence</code></p><p>rule的yaml配置要放在<code>config.yml</code>配置文件中定义的目录下,我这里是rule目录.</p><p>下面这个rule是监控Nginx的5XX状态码,并且调用钉钉告警</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">#rule名字,必须唯一</span><br><span class="line">name:  the count of servnginx log that reponse status code is 5xx and it appears greater than 5 in the period 1 minute</span><br><span class="line"></span><br><span class="line">#类型,官方提供多种类型</span><br><span class="line">type: frequency</span><br><span class="line"></span><br><span class="line">#ES索引,支持通配符</span><br><span class="line">index: logstash-*-nginx-access-*</span><br><span class="line"></span><br><span class="line">#在timeframe时间内,匹配到多少个结果便告警</span><br><span class="line">num_events: 1</span><br><span class="line"></span><br><span class="line">#监控周期.默认是minutes: 1</span><br><span class="line">timeframe:</span><br><span class="line">  seconds: 5  </span><br><span class="line">  </span><br><span class="line">#匹配模式.</span><br><span class="line">filter:</span><br><span class="line">- range:</span><br><span class="line">    status:</span><br><span class="line">      from: 500</span><br><span class="line">      to: 599</span><br><span class="line">      </span><br><span class="line">#告警方式,下面是调用第三方的钉钉告警</span><br><span class="line">alert:</span><br><span class="line">- &quot;elastalert_modules.dingtalk_alert.DingTalkAlerter&quot;</span><br><span class="line"></span><br><span class="line">#钉钉的webhook</span><br><span class="line">dingtalk_webhook: &quot;https://oapi.dingtalk.com/robot/send?access_token=&quot;  #参考地址,需要自行配置</span><br><span class="line">dingtalk_msgtype: text</span><br><span class="line"></span><br><span class="line">#原生的告警信息不友好,自定义告警内容的格式</span><br><span class="line">alert_text: &quot;</span><br><span class="line">域    名: &#123;&#125;\n</span><br><span class="line">调用方式: &#123;&#125;\n</span><br><span class="line">请求链接: &#123;&#125;\n</span><br><span class="line">状 态 码: &#123;&#125;\n</span><br><span class="line">后端服务器: &#123;&#125;\n</span><br><span class="line">数      量: &#123;&#125;</span><br><span class="line">&quot;</span><br><span class="line">alert_text_type: alert_text_only</span><br><span class="line"></span><br><span class="line">#告警内容</span><br><span class="line">alert_text_args:</span><br><span class="line">- domain</span><br><span class="line">- request_method</span><br><span class="line">- request</span><br><span class="line">- status</span><br><span class="line">- upstreamaddr</span><br><span class="line">- num_hits</span><br></pre></td></tr></table></figure><p>测试Rule文件是否正确.在elastalert目录下执行下个命令可以测试某个rule是否正常工作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/bin/elastalert-test-rule --config config.yaml rule/nginx.yaml</span><br></pre></td></tr></table></figure><p>这一步可能会有一些报错情况.一般都是扩展模块版本或者依赖关系的问题.比如下面这个问题,就需要执行<code>pip3 install jira==2.0.0</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/local/bin/elastalert-test-rule&quot;, line 11, in &lt;module&gt;</span><br><span class="line">    load_entry_point(&apos;elastalert==0.1.20&apos;, &apos;console_scripts&apos;, &apos;elastalert-test-rule&apos;)()</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 476, in load_entry_point</span><br><span class="line">    return get_distribution(dist).load_entry_point(group, name)</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 2700, in load_entry_point</span><br><span class="line">    return ep.load()</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 2318, in load</span><br><span class="line">    return self.resolve()</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 2324, in resolve</span><br><span class="line">    module = __import__(self.module_name, fromlist=[&apos;__name__&apos;], level=0)</span><br><span class="line">  File &quot;/usr/local/lib/python3.6/site-packages/elastalert/test_rule.py&quot;, line 20, in &lt;module&gt;</span><br><span class="line">    import elastalert.config</span><br><span class="line">  File &quot;/usr/local/lib/python3.6/site-packages/elastalert/config.py&quot;, line 99</span><br><span class="line">    raise EAException(&quot;Could not import module %s: %s&quot; % (module_name, e)), None, sys.exc_info()[2]</span><br><span class="line">                                                                          ^</span><br><span class="line">SyntaxError: invalid syntax</span><br><span class="line"></span><br><span class="line">或者下面这个错误</span><br><span class="line"></span><br><span class="line">[work@idc-function-elk10 elastalert]$ /usr/local/bin/elastalert-test-rule --config config.yaml rule/nginx.yaml</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 570, in _build_master</span><br><span class="line">    ws.require(__requires__)</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 888, in require</span><br><span class="line">    needed = self.resolve(parse_requirements(requirements))</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 779, in resolve</span><br><span class="line">    raise VersionConflict(dist, req).with_context(dependent_req)</span><br><span class="line">pkg_resources.VersionConflict: (elastalert 0.1.20 (/usr/local/lib/python3.6/site-packages), Requirement.parse(&apos;elastalert==0.2.4&apos;))</span><br><span class="line"></span><br><span class="line">During handling of the above exception, another exception occurred:</span><br><span class="line"></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/local/bin/elastalert-test-rule&quot;, line 6, in &lt;module&gt;</span><br><span class="line">    from pkg_resources import load_entry_point</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 3095, in &lt;module&gt;</span><br><span class="line">    @_call_aside</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 3079, in _call_aside</span><br><span class="line">    f(*args, **kwargs)</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 3108, in _initialize_master_working_set</span><br><span class="line">    working_set = WorkingSet._build_master()</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 572, in _build_master</span><br><span class="line">    return cls._build_from_requirements(__requires__)</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 585, in _build_from_requirements</span><br><span class="line">    dists = ws.resolve(reqs, Environment())</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 774, in resolve</span><br><span class="line">    raise DistributionNotFound(req, requirers)</span><br><span class="line">pkg_resources.DistributionNotFound: The &apos;jira&gt;=2.0.0&apos; distribution was not found and is required by elastalert</span><br></pre></td></tr></table></figure><hr><h3 id="执行ElastAlert"><a href="#执行ElastAlert" class="headerlink" title="执行ElastAlert"></a>执行ElastAlert</h3><p>一切没问题后,就可以执行ElastAlert.如果是针对单个Rule执行就使用下列命令.(在ElastAlert目录下)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk10 elastalert]# python3  -m elastalert.elastalert --verbose --rule /data/elastalert/rule/nginx.yaml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1 rules loaded</span><br><span class="line">INFO:elastalert:Starting up</span><br><span class="line">INFO:elastalert:Disabled rules are: []</span><br><span class="line">INFO:elastalert:Sleeping for 59.999906 seconds</span><br><span class="line">INFO:elastalert:Queried rule the count of servnginx log that reponse status code is 5xx and it appears greater than 5 in the period 1 minute from 2020-08-26 09:11 CST to 2020-08-26 09:26 CST: 10000 / 10000 hits (scrolling..)</span><br><span class="line">INFO:elastalert:Queried rule the count of servnginx log that reponse status code is 5xx and it appears greater than 5 in the period 1 minute from 2020-08-26 09:11 CST to 2020-08-26 09:26 CST: 20000 / 10000 hits (scrolling..)</span><br></pre></td></tr></table></figure><p>等待几秒钟后,钉钉会收到告警(我这里用的是200状态码测试).报警内容是Rule配置文件中自定义的格式和内容</p><p><img src="https://img2.jesse.top/image-20200826094746820.png" alt="image-20200826094746820"></p><hr><h3 id="Rule2-监控php-fpm的Fatal错误信息"><a href="#Rule2-监控php-fpm的Fatal错误信息" class="headerlink" title="Rule2. 监控php-fpm的Fatal错误信息"></a>Rule2. 监控php-fpm的Fatal错误信息</h3><p>fpm的错误日志也收集到了ELK中.我们期望只要pfm日志中出现”Fatal”关键字错误信息就立即告警.最初计划是用ElastAlert的黑名单(blacklist)类型的Rule.但是由于fpm的错误日志没有解析,而是直接保存原始日志,所以不符合要求.</p><p>参考github上我提的ISSUE:<a href="https://github.com/Yelp/elastalert/issues/2937" target="_blank" rel="noopener">balacklist query hits but no matches no alerts</a></p><p>也可以用Any类型的type.Rule文件如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk10 elastalert]# sed &apos;/^#/d&apos; rule/php-fpm.yaml | sed &apos;/^$/d&apos;</span><br><span class="line">name: monitor the fatal,error log in php-fpm log</span><br><span class="line">type: any</span><br><span class="line">index: logstash-*-fpm-error-*</span><br><span class="line">num_events: 1</span><br><span class="line">timeframe:</span><br><span class="line">  seconds: 5</span><br><span class="line">filter:</span><br><span class="line"> - query:</span><br><span class="line">     query_string:</span><br><span class="line">       query: &quot;message: \&quot;PHP Fatal\&quot;&quot;  #匹配Fatal关键字</span><br><span class="line">alert:</span><br><span class="line">- &quot;elastalert_modules.dingtalk_alert.DingTalkAlerter&quot;</span><br><span class="line">dingtalk_webhook: &quot;https://oapi.dingtalk.com/robot/send?access_token=&quot; </span><br><span class="line">dingtalk_msgtype: text</span><br><span class="line">alert_text: &quot;</span><br><span class="line"> 主机: &#123;&#125;\n</span><br><span class="line"> IP地址: &#123;&#125;\n</span><br><span class="line"> 业务线: &#123;&#125;\n</span><br><span class="line"> 日志类型: &#123;&#125;\n</span><br><span class="line"> 完整日志: &#123;&#125;</span><br><span class="line">&quot;</span><br><span class="line">alert_text_type: alert_text_only</span><br><span class="line">alert_text_args:</span><br><span class="line">  - host.name</span><br><span class="line">  - host.ip</span><br><span class="line">  - fields.project</span><br><span class="line">  - fields.type</span><br><span class="line">  - message</span><br></pre></td></tr></table></figure><hr><h3 id="启动ElastAlert"><a href="#启动ElastAlert" class="headerlink" title="启动ElastAlert"></a>启动ElastAlert</h3><p>开启一个Screen然后,使用nohup挂起执行.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python3  -m elastalert.elastalert --verbose &amp;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;使用ElastAlert-ELK实现日志监控告警&quot;&gt;&lt;a href=&quot;#使用ElastAlert-ELK实现日志监控告警&quot; class=&quot;headerlink&quot; title=&quot;使用ElastAlert+ELK实现日志监控告警&quot;&gt;&lt;/a&gt;使用ElastAlert+ELK实现日志监控告警&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;目前公司使用ELK做日志收集和展示分析.所以想对一些关键日志进行监控告警.比如Nginx的5xx日志,比如php-fpm的Fatal严重错误日志等.通过监控ES的日志数据,然后使用Python调用钉钉接口来实现日志的告警&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;ElastAlert介绍&quot;&gt;&lt;a href=&quot;#ElastAlert介绍&quot; class=&quot;headerlink&quot; title=&quot;ElastAlert介绍&quot;&gt;&lt;/a&gt;ElastAlert介绍&lt;/h3&gt;&lt;p&gt;ElastAlert是一个开源的工具,用于从Elastisearch中检索数据,并根据匹配模式发出告警.github项目地址如下:&lt;a href=&quot;https://github.com/Yelp/elastalert&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Yelp/elastalert&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;官方文档如下:&lt;a href=&quot;https://elastalert.readthedocs.io/en/latest/elastalert.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://elastalert.readthedocs.io/en/latest/elastalert.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;它支持多种监控模式和告警方式,具体可以查阅Github项目介绍.但是自带的ElastAlert并不支持钉钉告警,在github上有第三方的钉钉python项目.地址如下:&lt;a href=&quot;https://github.com/xuyaoqiang/elastalert-dingtalk-plugin&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/xuyaoqiang/elastalert-dingtalk-plugin&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>生产环境部署ELK7+ES冷热数据分离</title>
    <link href="https://jesse.top/2020/08/25/elk/%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2ELK7+ES%E5%86%B7%E7%83%AD%E6%95%B0%E6%8D%AE%E5%88%86%E7%A6%BB/"/>
    <id>https://jesse.top/2020/08/25/elk/生产环境部署ELK7+ES冷热数据分离/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.314Z</updated>
    
    <content type="html"><![CDATA[<h2 id="生产环境部署ELK7-ES冷热数据分离"><a href="#生产环境部署ELK7-ES冷热数据分离" class="headerlink" title="生产环境部署ELK7+ES冷热数据分离"></a>生产环境部署ELK7+ES冷热数据分离</h2><h3 id="需求整理"><a href="#需求整理" class="headerlink" title="需求整理"></a>需求整理</h3><p>当前公司还没有日志收集系统,.线上故障排错需要开发和运维人员通过跳板机登陆到业务服务器去查看日志,如果集群下服务器节点较多,可能需要登陆到每一台服务器才能找到准确的故障信息.这种方式会带来许多问题:</p><ul><li>效率低下</li><li>需要给研发人员分配生产服务器账号,有安全隐患</li><li>查看日志的方式极为麻烦,复杂</li><li>每台业务服务器需要大容量磁盘保存日志文件</li></ul><hr><h3 id="Elasticsearch冷热数据分离"><a href="#Elasticsearch冷热数据分离" class="headerlink" title="Elasticsearch冷热数据分离"></a>Elasticsearch冷热数据分离</h3><p>公司目前共有4个APP产品,每天的nginx日志+业务日志+php日志总共预计在1.5T左右.一周的日志数据量在10T左右.Elasticsearch规划了20T的SSD磁盘用来一周的日志数据(7天日志+1个副本).这些数据保存在Elasticsearch的热节点.</p><p>考虑到1周以后的日志数据查询频率非常低,所以可以将这部分的日志迁移和归档到Elasticsearch的冷节点.并且冷节点日志无需索引.所以预留了10T的SAS磁盘保留1周的冷数据.</p><p>总共是20TSSD磁盘的热节点和10TSAS机械磁盘的冷节点</p><a id="more"></a><hr><h3 id="Elasticsearch集群节点规划"><a href="#Elasticsearch集群节点规划" class="headerlink" title="Elasticsearch集群节点规划"></a>Elasticsearch集群节点规划</h3><ul><li><strong>热节点规划</strong></li></ul><p>ES集群的DATA节点负责保存收集到的日志数据,索引,整理和查询日志等工作.所以data节点对硬件资源要求较高.</p><p>计划使用3台物理服务器(VMware虚拟化集群),每台物理机的配置是32核134G,每台物理机有2个VM.一共计划6个elasticsearch的节点.</p><p>每个物理服务器上使用4块1.92T的SSD磁盘.平均每个VM虚拟机可以使用16核67G内存4TSSD磁盘.</p><blockquote><p>后期又增加了一台服务器,配置为16核48G内存2TSSD</p></blockquote><p>7个节点一共有20TSSD磁盘空间,刚好满足7天的日志数据的需求.</p><blockquote><p>为了避免磁盘IO竞争.每个节点使用独立的单块SSD磁盘,</p></blockquote><ul><li><strong>冷节点规划</strong></li></ul><p>集群冷节点可以适当放低硬件资源.计划使用2台VM作为ES冷节点.每台服务器为8核32G.为了更充分利用带宽和CPU资源.以及合理规划ES的节点角色.冷节点同时又作为集群的master节点和logstash的角色.</p><blockquote><p>这样热节点就有充分资源作为data角色</p></blockquote><hr><h3 id="ELK架构规划"><a href="#ELK架构规划" class="headerlink" title="ELK架构规划"></a>ELK架构规划</h3><p>为了ELK的高性能和数据传输安全性,完整的架构如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Filebeat------&gt;logstash-------&gt;Elasticsearch--------&gt;kibana-------&gt;nginx</span><br></pre></td></tr></table></figure><h5 id="以下是各组件作用介绍"><a href="#以下是各组件作用介绍" class="headerlink" title="以下是各组件作用介绍:"></a>以下是各组件作用介绍:</h5><ul><li>Filebeat: 轻量级的日志收集Agent,部署在每台业务服务器</li><li>logstash(5节点): 从filebeat费日志数据,并对数据进行加工处理,传输给ES.由于集群资源本身的稳定和健壮,logstash并不需要使用持久化数据特性.</li><li>Elasticsearch(7节点):保存和索引数据.7个热节点和2个冷节点</li><li>kibana:从Elasticsearch获取数据并在web界面展示</li><li>nginx: kibana的代理服务器</li></ul><p>官网要求ELK组件必须使用相同的版本,这次部署的是ELK的最新版.7.7版本</p><hr><h3 id="ELK服务器信息"><a href="#ELK服务器信息" class="headerlink" title="ELK服务器信息"></a>ELK服务器信息</h3><table><thead><tr><th>主机名</th><th>IP地址</th><th>操作系统</th><th>运行组件</th><th>功能</th><th>配置</th></tr></thead><tbody><tr><td>idc-function-elk01</td><td>172.16.20.101</td><td>CentOS7.7</td><td>logstash,es</td><td>logstash节点,data,master节点</td><td>16核80G3.84TSSD</td></tr><tr><td>idc-function-elk02</td><td>172.16.20.102</td><td>CentOS7.7</td><td>es</td><td>ES data节点</td><td>16核54G3.84TSSD</td></tr><tr><td>idc-function-elk03</td><td>172.16.20.103</td><td>CentOS7.7</td><td>logstash,es</td><td>logstash节点,data节点</td><td>16核80G3.84TSSD</td></tr><tr><td>idc-function-elk04</td><td>172.16.20.104</td><td>CentOS7.7</td><td>es</td><td>ES data节点</td><td>16核54G3.84TSSD</td></tr><tr><td>idc-function-elk05</td><td>172.16.20.105</td><td>CentOS7.7</td><td>logstash,es</td><td>logstash节点,data节点</td><td>16核80G3.84TSSD</td></tr><tr><td>idc-function-elk06</td><td>172.16.20.106</td><td>CentOS7.7</td><td>es</td><td>ES data节点</td><td>16核54G3.84TSSD</td></tr><tr><td>idc-function-elk07</td><td>172.16.20.107</td><td>CentOS7.7</td><td>es</td><td>ES data节点</td><td>16核54G1.92TSSD</td></tr><tr><td>idc-function-elk08</td><td>172.16.20.108</td><td>CentOS7.7</td><td>logstash,es</td><td>ES的master,冷节点,logstash节点</td><td>8核32G9.6TSAS</td></tr><tr><td>idc-function-elk09</td><td>172.16.20.109</td><td>CentOS7.7</td><td>logstash,es</td><td>ES的主master节点,冷节点,logstash节点</td><td>16核32G5TSAS</td></tr><tr><td>idc-function-docker</td><td>172.16.20.30</td><td>CentOS7.7</td><td>nginx</td><td>一台现有的服务器</td><td></td></tr><tr><td>idc-function-elk10</td><td>172.16.20.110</td><td>CentOS7.7</td><td>logstash,es</td><td>仅仅作为ES的master节点,,logstash节点</td><td>8核8G200GSAS</td></tr></tbody></table><hr><h3 id="部署安装"><a href="#部署安装" class="headerlink" title="部署安装"></a>部署安装</h3><h4 id="部署elasticsearch"><a href="#部署elasticsearch" class="headerlink" title="部署elasticsearch"></a>部署elasticsearch</h4><ul><li><strong>部署JAVA环境</strong>.</li></ul><p>官网要求需要JDK8或者11版本.这里部署的是11版本.在Oracle有rpm包.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@idc-function-elk01 ~]$ java -version</span><br><span class="line">java version &quot;11.0.7&quot; 2020-04-14 LTS</span><br><span class="line">Java(TM) SE Runtime Environment 18.9 (build 11.0.7+8-LTS)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.7+8-LTS, mixed mode)</span><br></pre></td></tr></table></figure><ul><li><strong>安装elasticsearch</strong></li></ul><p>官网的yum安装非常慢.先从elasticsearch中文社区下载rpm包.然后使用yum本地安装</p><p>中文社区下载中心:<a href="https://elasticsearch.cn/download/" target="_blank" rel="noopener">https://elasticsearch.cn/download/</a></p><ul><li><strong>配置elasticserch</strong></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#以下是ES的配置文件</span></span><br><span class="line"><span class="string">[root@idc-function-elk01</span> <span class="string">~]#</span> <span class="string">cat</span> <span class="string">/etc/elasticsearch/elasticsearch.yml</span> <span class="string">| sed '/^#/d'</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#ES集群名.每个节点需要定义同一个集群名</span></span><br><span class="line"><span class="string">cluster.name: dwd-elk  </span></span><br><span class="line"><span class="string">#节点名,一般和主机名一致</span></span><br><span class="line"><span class="string">node.name: idc-function-elk01</span></span><br><span class="line"><span class="string">#节点是否为master节点</span></span><br><span class="line"><span class="string">node.master: true</span></span><br><span class="line"><span class="string">#节点是否为data节点</span></span><br><span class="line"><span class="string">node.data: true</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#添加属性,标志位热节点.如果是冷节点,该值为cold</span></span><br><span class="line"><span class="string">node.attr.box_type: hot </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#数据存储路径,指定多块磁盘</span></span><br><span class="line"><span class="string">path.data: /data,/data1</span></span><br><span class="line"><span class="string">#日志存储路径</span></span><br><span class="line"><span class="string">path.logs: /data/logs/elasticsearch</span></span><br><span class="line"><span class="string">#是否需要锁定内存,不适应swap虚拟内存</span></span><br><span class="line"><span class="string">bootstrap.memory_lock: true</span></span><br><span class="line"><span class="string">#绑定IP.本机的内网IP</span></span><br><span class="line"><span class="string">network.host: 172.16.20.101</span></span><br><span class="line"><span class="string">#默认端口</span></span><br><span class="line"><span class="string">http.port: 9200</span></span><br><span class="line"><span class="string">#集群内其他ES节点地址</span></span><br><span class="line"><span class="string">discovery.seed_hosts: ["172.16.20.101","172.16.20.102","172.16.20.103","172.16.20.104","172.16.20.105","172.16.20.106","172.16.20.107","172.16.20.108","172.16.20.109"]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#初始化master节点,#master节点分布在不同的物理服务器</span></span><br><span class="line"><span class="string">cluster.initial_master_nodes: ["172.16.20.101","172.16.20.108","172.16.20.109"] </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#磁盘水位线</span></span><br><span class="line"><span class="string">cluster.routing.allocation.disk.watermark.low: 85%</span></span><br><span class="line"><span class="string">cluster.routing.allocation.disk.watermark.high: 90%</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#用于 fielddata 的最大内存，如果 fielddata 达到该阈值，就会把旧数据交换出去。该参数可以设置百分比或者绝对值。默认设置是不限制，所#以强烈建议设置该值</span></span><br><span class="line"><span class="string">indices.fielddata.cache.size: 10%</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#limit值需要大于indices.fielddata.cache.size的值。否则的话fielddata大小一到limit阈值就报错,就永远道不了size阈值,无法触发对##旧数据的交换任务</span></span><br><span class="line"><span class="string">indices.breaker.fielddata.limit: 30%</span></span><br><span class="line"><span class="string">[root@idc-function-elk01 ~]#</span></span><br></pre></td></tr></table></figure><ul><li><p><strong>优化系统和elasticsearch配置</strong></p><ul><li><strong>优化内核参数</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]<span class="comment"># cat /etc/sysctl.conf</span></span><br><span class="line"></span><br><span class="line">net.ipv4.ip_forward=1</span><br><span class="line">net.ipv6.conf.all.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.lo.disable_ipv6 = 1</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">net.ipv4.neigh.default.gc_stale_time=120</span><br><span class="line">net.ipv4.conf.all.rp_filter=0</span><br><span class="line">net.ipv4.conf.default.rp_filter=0</span><br><span class="line">net.ipv4.conf.default.arp_announce = 2</span><br><span class="line">net.ipv4.conf.lo.arp_announce=2</span><br><span class="line">net.ipv4.conf.all.arp_announce=2</span><br><span class="line">net.ipv4.tcp_max_tw_buckets = 5000</span><br><span class="line">net.ipv4.tcp_syncookies = 1</span><br><span class="line">net.ipv4.tcp_max_syn_backlog = 1024</span><br><span class="line">net.ipv4.tcp_synack_retries = 2</span><br><span class="line">kernel.core_uses_pid = 1 <span class="comment">#追加进程号到core文件名中</span></span><br><span class="line">fs.suid_dumpable = 2 <span class="comment">#确保设置属主的进程也可以生成core文件</span></span><br><span class="line">kernel.core_pattern = /tmp/core-%e-%s-%u-%g-%p-%t <span class="comment">#指定core文件生成的位置和文件名规则。</span></span><br><span class="line">vm.overcommit_memory = 1</span><br><span class="line">net.ipv4.conf.lo.arp_announce=2</span><br><span class="line">kernel.sysrq=1</span><br><span class="line">fs.file-max=6553500 <span class="comment">#最大文件句柄</span></span><br><span class="line">fs.nr_open=6553500  <span class="comment">#最大文件句柄</span></span><br><span class="line">net.core.somaxconn = 65535</span><br><span class="line">vm.max_map_count=262144  <span class="comment">#ES官网推荐</span></span><br><span class="line">net.ipv4.tcp_keepalive_time = 600</span><br><span class="line">net.ipv4.tcp_keepalive_probes = 3</span><br><span class="line">net.ipv4.tcp_keepalive_intvl = 15</span><br><span class="line">net.ipv4.tcp_tw_reuse = 1</span><br><span class="line">net.ipv4.tcp_tw_recycle = 0</span><br><span class="line">net.ipv4.tcp_syn_retries = 1</span><br><span class="line">net.ipv4.tcp_synack_retries = 1</span><br><span class="line">net.ipv4.tcp_timestamps=0</span><br><span class="line">net.ipv4.tcp_fin_timeout=30</span><br><span class="line">net.ipv4.ip_local_port_range = 10000 65000</span><br><span class="line">net.nf_conntrack_max = 655360</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_established = 1200</span><br></pre></td></tr></table></figure><ul><li><strong>设置文件句柄,内存</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]<span class="comment"># cat /etc/security/limits.conf</span></span><br><span class="line">elasticsearch soft memlock unlimited</span><br><span class="line">elasticsearch  hard memlock unlimited</span><br><span class="line">* soft nofile 655360</span><br><span class="line">* hard nofile 655360</span><br></pre></td></tr></table></figure><ul><li><strong>关闭Selinux和防火墙</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line"></span><br><span class="line">setenforce 0</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk01 ~]# cat /etc/selinux/config</span><br><span class="line"></span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure><ul><li><strong>创建目录,并且修改权限</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]# ll /data/elasticsearch/ -d</span><br><span class="line">drwxr-xr-x 3 elasticsearch elasticsearch 27 May 25 23:08 /data/elasticsearch/</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk01 ~]# ll /data/logs/elasticsearch/ -d</span><br><span class="line">drwxr-xr-x 2 elasticsearch elasticsearch 4096 May 26 00:00 /data/logs/elasticsearch/</span><br></pre></td></tr></table></figure><ul><li><strong>修改elasticsearch的systemctl启动文件,加入下面这一行</strong>:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]# cat /usr/lib/systemd/system/elasticsearch.service</span><br><span class="line"></span><br><span class="line">#新增这一行.否则启动的时候会提示在配置文件中开启了bootstrap.memory_lock: true.但是系统中没有检测到配置</span><br><span class="line">#unlimit memory</span><br><span class="line">LimitMEMLOCK=infinity</span><br></pre></td></tr></table></figure><ul><li><strong>修改elasticsearch的JVM内存</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]# cat /etc/elasticsearch/jvm.options</span><br><span class="line"></span><br><span class="line">-Xms32g</span><br><span class="line">-Xmx32g</span><br></pre></td></tr></table></figure><ul><li>启动elasticsearch</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable elasticsearch</span><br><span class="line">  systemctl start elasticsearch</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="Elasticsearch集群监控工具"><a href="#Elasticsearch集群监控工具" class="headerlink" title="Elasticsearch集群监控工具"></a>Elasticsearch集群监控工具</h3><p>这里推荐使用cerebro工具,是一个独立的小工具,无需安装到es插件.具体使用方法请参考:<a href="https://github.com/lmenezes/cerebro" target="_blank" rel="noopener">github项目地址</a></p><p>启动cerebro后.访问cerebro的9000端口.可以看到ES集群状态监控界面,界面还挺不错</p><p><img src="https://img2.jesse.top/image-20200724092916238.png" alt="image-20200724092916238"></p><blockquote><p>从上面的集群截图中可以看到,每个ES节点都有<code>hot</code>或者<code>cold</code>标签以区分节点属性.这就说明冷热节点属性配置正常</p></blockquote><p>但是每次登陆cerebro都需要选择连接的ES节点.无法实现登陆自动连接.这非常不方便,我在github上提交了相关<a href="https://github.com/lmenezes/cerebro/issues/450" target="_blank" rel="noopener">issue</a>.得到回复是使用以下URL:  <a href="http://localhost:9000/#/overview?host=http:%2F%2Flocalhost:9200" target="_blank" rel="noopener">http://localhost:9000/#/overview?host=http:%2F%2Flocalhost:9200</a></p><p>所以.可以配置Nginx反向代理:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line"> listen 80;</span><br><span class="line"> server_name elk.doweidu.com;</span><br><span class="line"></span><br><span class="line"> access_log /data/logs/nginx/elk_access.log main;</span><br><span class="line"> error_log /data/logs/nginx/elk_error.log;</span><br><span class="line"></span><br><span class="line">location / &#123;</span><br><span class="line">        proxy_set_header   Host  $host;</span><br><span class="line">        proxy_set_header   X-Real-IP $remote_addr;</span><br><span class="line">        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">   rewrite ^/(.*)  http://172.16.20.101:9000/#/overview?host=http:%2F%2Flocalhost:9200/$1 permanent;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>172.16.20.101:9000这个为cerebro所在服务器.localhost:9200是同台服务器的ES节点.如果ES节点为其他服务器,使用IP地址指定即可</p></blockquote><hr><h3 id="部署logstash"><a href="#部署logstash" class="headerlink" title="部署logstash"></a>部署logstash</h3><p>logstash也是运行在JVM的java环境中.安装方法和ES一样.</p><ul><li><strong>配置logstash文件</strong></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@idc-function-elk01</span> <span class="string">~]#</span> <span class="string">vim</span> <span class="string">/etc/logstash/logstash.yml</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#这个参数是指单次接收的块大小.可以根据实际场景和性能压力不断的调整和测试</span></span><br><span class="line"><span class="string">pipeline.batch.size:</span> <span class="number">1500</span></span><br><span class="line"><span class="string">pipeline.batch.delay:</span> <span class="number">10</span></span><br><span class="line"><span class="comment">#节点名称</span></span><br><span class="line"><span class="string">node.name:</span> <span class="string">idc-function-elk01</span></span><br><span class="line"><span class="comment">#数据路径</span></span><br><span class="line"><span class="string">path.data:</span> <span class="string">/data/logstash</span></span><br><span class="line"><span class="comment">#日志路径</span></span><br><span class="line"><span class="string">path.logs:</span> <span class="string">/data/logs/logstash</span></span><br><span class="line"><span class="comment">#绑定IP,本机内网IP地址</span></span><br><span class="line"><span class="string">http.host:</span> <span class="string">"172.16.20.101"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#下面这个对logstash性能比较重要的配置保持默认即可.默认就是CPU的内核数</span></span><br><span class="line"><span class="comment"># This defaults to the number of the host's CPU cores.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># pipeline.workers: 2</span></span><br></pre></td></tr></table></figure><ul><li><strong>修改JVM内存</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]# vim /etc/logstash/jvm.options</span><br><span class="line">-Xms16g</span><br><span class="line">-Xmx16g</span><br></pre></td></tr></table></figure><ul><li>systemctl启动logstash</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service logstash start</span><br></pre></td></tr></table></figure><hr><h3 id="部署到其他服务器"><a href="#部署到其他服务器" class="headerlink" title="部署到其他服务器"></a>部署到其他服务器</h3><p>如果通过克隆或者模板的方式部署到其他服务器,那么ES集群应该会报错,提示无法将节点加入到集群中.这是因为集群中所有节点的data数据目录拥有同一个ID.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">can&apos;t add node &#123;idc-function-elk03&#125;&#123;dt57hBjCQ9uCAp8G6sxJYw&#125;&#123;7-s_05ZMSl6CtpmMZGZEUA&#125;&#123;172.16.20.103&#125;&#123;172.16.20.103:9300&#125;&#123;dilmrt&#125;&#123;ml.machine_memory=61044445184, ml.max_open_jobs=20, xpack.installed=true, transform.node=true&#125;, found existing node &#123;idc-function-elk01&#125;&#123;dt57hBjCQ9uCAp8G6sxJYw&#125;&#123;biZPCqjCSx-PX3eU-CquUw&#125;&#123;172.16.20.101&#125;&#123;172.16.20.101:9300&#125;&#123;dilmrt&#125;&#123;ml.machine_memory=61044445184, ml.max_open_jobs=20, xpack.installed=true, transform.node=true&#125; with the same id but is a different node instance</span><br></pre></td></tr></table></figure><p>删除data数据路径下的内容,然后重启ES.等待集群自动同步</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk03 ~]# rm -rf /data/elasticsearch/nodes</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk03 ~]# ll /data/elasticsearch/</span><br><span class="line">total 0</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk03 ~]# systemctl restart elasticsearch</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk03 ~]# ll /data/elasticsearch/</span><br><span class="line">drwxr-xr-x 3 elasticsearch elasticsearch 15 May 26 17:30 nodes</span><br><span class="line">[root@idc-function-elk03 ~]#</span><br></pre></td></tr></table></figure><hr><h3 id="kibana安装"><a href="#kibana安装" class="headerlink" title="kibana安装"></a>kibana安装</h3><p>安装方式非常简单,直接下载rpm包安装即可.安装完成后修改<code>/etc/kibana/kibana.yaml</code>配置文件.新增以下配置.将kibana的web界面改为中文:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">i18n.locale: &quot;zh-CN&quot;</span><br></pre></td></tr></table></figure><blockquote><p>我这里是采用rpm包安装,如果是其他安装方式,请自行寻找配置文件路径</p></blockquote><p><strong>nginx反向代理配置</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">  server_name kibana.doweidu.com;</span><br><span class="line"></span><br><span class="line">  error_log /data/logs/nginx/kibana.error.log;</span><br><span class="line">  access_log /data/logs/nginx/kibana.access.log main;</span><br><span class="line"></span><br><span class="line">listen 443 ssl http2;</span><br><span class="line">  ssl_certificate /data/letsencrypt/kibana.doweidu.com/fullchain.cer;</span><br><span class="line">  ssl_certificate_key /data/letsencrypt/kibana.doweidu.com/kibana.doweidu.com.key;</span><br><span class="line">  include /data/letsencrypt/options-ssl-nginx.conf;</span><br><span class="line">  ssl_dhparam /data/letsencrypt/ssl-dhparams.pem;</span><br><span class="line"></span><br><span class="line">  location / &#123;</span><br><span class="line">    proxy_pass http://172.16.20.105:5601;</span><br><span class="line">    proxy_set_header Host $host;</span><br><span class="line">    proxy_redirect off;</span><br><span class="line">    proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">    proxy_connect_timeout    60;</span><br><span class="line">    proxy_read_timeout       60;</span><br><span class="line">    proxy_send_timeout       60;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">if ($host = kibana.doweidu.com) &#123;</span><br><span class="line">    return 301 https://$host$request_uri;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">listen 80;</span><br><span class="line">server_name kibana.doweidu.com;</span><br><span class="line">return 404;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h4 id="安装filebeat"><a href="#安装filebeat" class="headerlink" title="安装filebeat."></a>安装filebeat.</h4><p>对于有supervisor的服务器,采用二进制安装,否则使用rpm包安装</p><p>二进制包下载地址:<a href="http://repo.doweidu.com/ELK/filebeat-7.7.0-linux-x86_64.tar.gz" target="_blank" rel="noopener">http://repo.doweidu.com/ELK/filebeat-7.7.0-linux-x86_64.tar.gz</a></p><p>rpm包下载地址:<a href="http://repo.doweidu.com/ELK/filebeat-7.7.0-x86_64.rpm" target="_blank" rel="noopener">http://repo.doweidu.com/ELK/filebeat-7.7.0-x86_64.rpm</a></p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;生产环境部署ELK7-ES冷热数据分离&quot;&gt;&lt;a href=&quot;#生产环境部署ELK7-ES冷热数据分离&quot; class=&quot;headerlink&quot; title=&quot;生产环境部署ELK7+ES冷热数据分离&quot;&gt;&lt;/a&gt;生产环境部署ELK7+ES冷热数据分离&lt;/h2&gt;&lt;h3 id=&quot;需求整理&quot;&gt;&lt;a href=&quot;#需求整理&quot; class=&quot;headerlink&quot; title=&quot;需求整理&quot;&gt;&lt;/a&gt;需求整理&lt;/h3&gt;&lt;p&gt;当前公司还没有日志收集系统,.线上故障排错需要开发和运维人员通过跳板机登陆到业务服务器去查看日志,如果集群下服务器节点较多,可能需要登陆到每一台服务器才能找到准确的故障信息.这种方式会带来许多问题:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;效率低下&lt;/li&gt;
&lt;li&gt;需要给研发人员分配生产服务器账号,有安全隐患&lt;/li&gt;
&lt;li&gt;查看日志的方式极为麻烦,复杂&lt;/li&gt;
&lt;li&gt;每台业务服务器需要大容量磁盘保存日志文件&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&quot;Elasticsearch冷热数据分离&quot;&gt;&lt;a href=&quot;#Elasticsearch冷热数据分离&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearch冷热数据分离&quot;&gt;&lt;/a&gt;Elasticsearch冷热数据分离&lt;/h3&gt;&lt;p&gt;公司目前共有4个APP产品,每天的nginx日志+业务日志+php日志总共预计在1.5T左右.一周的日志数据量在10T左右.Elasticsearch规划了20T的SSD磁盘用来一周的日志数据(7天日志+1个副本).这些数据保存在Elasticsearch的热节点.&lt;/p&gt;
&lt;p&gt;考虑到1周以后的日志数据查询频率非常低,所以可以将这部分的日志迁移和归档到Elasticsearch的冷节点.并且冷节点日志无需索引.所以预留了10T的SAS磁盘保留1周的冷数据.&lt;/p&gt;
&lt;p&gt;总共是20TSSD磁盘的热节点和10TSAS机械磁盘的冷节点&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>记一次生产ELK性能优化</title>
    <link href="https://jesse.top/2020/08/25/elk/%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%94%9F%E4%BA%A7ELK%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    <id>https://jesse.top/2020/08/25/elk/记一次生产ELK性能优化/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.314Z</updated>
    
    <content type="html"><![CDATA[<h3 id="记一次生产ELK性能优化"><a href="#记一次生产ELK性能优化" class="headerlink" title="记一次生产ELK性能优化"></a>记一次生产ELK性能优化</h3><p>ES上线后遇到一些问题:</p><h5 id="一-内存压力过高"><a href="#一-内存压力过高" class="headerlink" title="一.内存压力过高"></a>一.内存压力过高</h5><p>目前ES节点的有两种规格内存.</p><p>80GB内存(节点同时运行了Logstash,分配了16G给logstash).</p><p>54GB内存(只运行ES,并且只作为data节点)</p><p>ES的JVM内存从32G下降到28G.建议JVM内存是总物理内存的一半左右,</p><p>节点内存使用情况如下:</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&quot;mem&quot;: &#123; - </span><br><span class="line">          &quot;total&quot;: &quot;78.5gb&quot;,</span><br><span class="line">          &quot;total_in_bytes&quot;: 84296601600,</span><br><span class="line">          &quot;free&quot;: &quot;4.3gb&quot;,</span><br><span class="line">          &quot;free_in_bytes&quot;: 4688633856,</span><br><span class="line">          &quot;used&quot;: &quot;74.1gb&quot;,</span><br><span class="line">          &quot;used_in_bytes&quot;: 79607967744,</span><br><span class="line">          &quot;free_percent&quot;: 6,</span><br><span class="line">          &quot;used_percent&quot;: 94</span><br></pre></td></tr></table></figure><p>建议: ES节点内存建议配置高一点,虽然ES的JVM内存最大不超过32G.但是在其他方面仍然很吃内存</p><hr><h5 id="二-索引字段数"><a href="#二-索引字段数" class="headerlink" title="二.索引字段数"></a>二.索引字段数</h5><p>ES的单个索引默认最大字段数量是1000个.当超过这个字段数量时,会拒绝字段映射.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[2020-08-14T10:44:10,605][INFO ][o.e.a.b.TransportShardBulkAction] [idc-function-elk01] [logstash-mg-tc-netrcd-gateway-2020.08.14][0] mapping update rejected by primary</span><br><span class="line">java.lang.IllegalArgumentException: Limit of total fields [10000] in index [logstash-mg-tc-netrcd-gateway-2020.08.14] has been exceeded</span><br></pre></td></tr></table></figure><p>此时可以在索引模板中,修改默认字段数量.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PUT _template/logstash</span><br><span class="line">&#123;</span><br><span class="line">    &quot;index_patterns&quot;: &quot;logstash-*&quot;,</span><br><span class="line">    &quot;settings&quot;: &#123;</span><br><span class="line">        &quot;mapping.total_fields.limit&quot;:5000</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h5 id="三-ES线程和队列优化"><a href="#三-ES线程和队列优化" class="headerlink" title="三.ES线程和队列优化"></a>三.ES线程和队列优化</h5><p>ES日志有部分警告信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[2020-08-14T10:15:06,151][WARN ][o.e.c.r.a.AllocationService] [idc-function-elk01] failing shard [failed shard, shard [logstash-msf-internal-access-2020.08.14][0], node[3uwnN_KEQGiywi</span><br><span class="line">u9xR5Jng], [R], s[STARTED], a[id=ACE7jGGoS6yhS9T_Sn53UA], message [failed to perform indices:data/write/bulk[s] on replica [logstash-msf-internal-access-2020.08.14][0], node[3uwnN_KEQ</span><br><span class="line">Giywiu9xR5Jng], [R], s[STARTED], a[id=ACE7jGGoS6yhS9T_Sn53UA]], failure [RemoteTransportException[[idc-function-elk08][172.16.20.108:9300][indices:data/write/bulk[s][r]]]; nested: Cir</span><br><span class="line">cuitBreakingException[[parent] Data too large, data for [&lt;transport_request&gt;] would be [20720204958/19.2gb], which is larger than the limit of [20401094656/19gb], real usage: [2068074</span><br><span class="line">9096/19.2gb], new bytes reserved: [39455862/37.6mb], usages [request=197056/192.4kb, fielddata=1310/1.2kb, in_flight_requests=11463397212/10.6gb, accounting=54576960/52mb]]; ], markAs</span><br><span class="line">Stale [true]]</span><br><span class="line">org.elasticsearch.transport.RemoteTransportException: [idc-function-elk08][172.16.20.108:9300][indices:data/write/bulk[s][r]]</span><br><span class="line">Caused by: org.elasticsearch.common.breaker.CircuitBreakingException: [parent] Data too large, data for [&lt;transport_request&gt;] would be [20720204958/19.2gb], which is larger than the l</span><br><span class="line">imit of [20401094656/19gb], real usage: [20680749096/19.2gb], new bytes reserved: [39455862/37.6mb], usages [request=197056/192.4kb, fielddata=1310/1.2kb, in_flight_requests=114633972</span><br><span class="line">12/10.6gb, accounting=54576960/52mb]</span><br><span class="line">        at org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService.checkParentLimit(HierarchyCircuitBreakerService.java:347) ~[elasticsearch-7.7.0.jar:7.7.0]</span><br><span class="line">        at org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.addEstimateBytesAndMaybeBreak(ChildMemoryCircuitBreaker.java:128) ~[elasticsearch-7.7.0.jar:7.7.0]</span><br></pre></td></tr></table></figure><p>编辑<code>/etc/elasticsearch/elasticsearch.yml</code>配置文件.新增如下配置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">thread_pool:</span><br><span class="line">    write:</span><br><span class="line">        size: 32</span><br><span class="line">        queue_size: 10000</span><br><span class="line">processors: 32</span><br></pre></td></tr></table></figure><p>同时,修改<code>/etc/logstash/logstash.yml</code>配置文件.修改如下配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pipeline.batch.size: 10000</span><br><span class="line">pipeline.batch.delay: 100</span><br></pre></td></tr></table></figure><blockquote><p>以上这些配置都需要不断的调试,修改,观察,找到最适合的一个范围和效果</p></blockquote><hr><h5 id="四-集群最大分片-shard-数"><a href="#四-集群最大分片-shard-数" class="headerlink" title="四.集群最大分片(shard)数"></a>四.集群最大分片(shard)数</h5><p>今天收集新的日志报错,logstash日志内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2020-08-24T10:32:41,009][WARN ][logstash.outputs.elasticsearch][main][7ca4981ae091d6b8f604e5695405b2c74a9cb7fc4a72b338be4e2ede66a04d7d] Could not index event to Elasticsearch. &#123;:status=&gt;400, :action=&gt;[&quot;index&quot;, &#123;:_id=&gt;nil, :_index=&gt;&quot;logstash-hsq-search-netrcd-2020.08.24&quot;, :routing=&gt;nil, :_type=&gt;&quot;_doc&quot;&#125;, #&lt;LogStash::Event:0x4a0b491&gt;], :response=&gt;&#123;&quot;index&quot;=&gt;&#123;&quot;_index&quot;=&gt;&quot;logstash-hsq-search-netrcd-2020.08.24&quot;, &quot;_type&quot;=&gt;&quot;_doc&quot;, &quot;_id&quot;=&gt;nil, &quot;status&quot;=&gt;400, &quot;error&quot;=&gt;&#123;&quot;type&quot;=&gt;&quot;illegal_argument_exception&quot;, &quot;reason&quot;=&gt;&quot;Validation Failed: 1: this action would add [14] total shards, but this cluster currently has [8998]/[9000] maximum shards open;&quot;&#125;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure><p>看日志关键字发现当前集群总共分片数是8998个,如果再加上14个分片,那么就超过了最大的9000个.</p><p><strong>14和9000个分片是怎么来的?</strong></p><p>当前ES集群中有7个热节点,每个索引分片数量是7个,副本数是1..也就是说一个索引就要14个分片(包括副本)</p><p>另外,集群中还有2个冷节点,总共是9个节点.从Elasticsearch v7.0.0 开始，集群中的每个节点默认限制 1000 个shard.所以集群所有节点总共是9000个shard(分片)</p><p><strong>解决方案</strong></p><p>1.每个索引(Index)分配多少个分片(shared)合适?</p><p>配置Elasticsearch集群后,对于分片的数量通常比较难确定.分配过小或者过大对性能都不好.实际上每个分片都会消耗硬件资源:</p><ul><li>由于分片本质上是Lucene索引，因此会消耗文件句柄，内存和CPU资源。</li><li>每个搜索请求都将触摸索引中每个分片的副本，当分片分布在多个节点上时，这不是问题。当分片争夺相同的硬件资源时，就会出现争用并且性能会下降。</li></ul><p>我个人理解一般设置分片数量有几个方向可以考虑</p><ul><li>由于Elasticsearch的最大JVM一般在30-32G.所以一个分片的数量不能超过30G大小.如果一个索引最大是200G.那么就需要分片7个分片.</li><li>最好是按照日志创建索引,可以按天,或者周,甚至月来创建索引,如果一个月的日志太大,那么就按天创建索引</li><li>分片数量的配置最好是基于当前的日志量级,集群节点数量评估.或者可以规划为可以预见的增长幅度评估,千万不能盲目的为一年或者2年以后的日志量分配资源</li><li>如果不能确定索引容量,或者对于分片设置完全没有概念和把握,那么建议按照ES集群节点数量设置分片,有多少个节点,就设置多少分片,后期再观察和考虑是否需要增加或删减</li><li><p>生产环境中,最好设置为1个副本.副本有助于加速查询和健壮高可用行,但是也会占用磁盘容量空间,没必要设置2个或以上的副本.如果是做了冷热分离,对于冷数据如果没有太大的安全性要求,也可以不设置副本</p><p>2.每个节点的<code>maximum shards open</code>设置为多大合适</p></li></ul><p><strong>根据以下几个指标来评估:</strong></p><p>1.当前集群的shard分片数</p><p>2.当前集群索引量</p><p>3.索引保存时间</p><p>以我们生产集群为例,当前集群共有7个热节点,每个索引7个分片.每节点是一个分片.每天是55个索引,加上副本是110个索引,也就是每个节点,每天有110个shard分片.</p><p>热节点索引保留14天,单节点总共是110*14=1540个Shard.预留20%的空间,也就是1848个分片.所以给每个节点分配1800-2000个分片比较合适</p><p>另外,集群中还有2个冷节点.超过14天的索引会自动迁移到热节点(没有副本),并且保留14天后删除.所以冷节点14天总共分片数是55*7*14=5390个Shard.分摊到2个节点,平均每个节点的Shard是2695..留出20%的空间,每节点的Shard为3234.</p><p><strong>经过评估下来,ES热节点每节点Shard数量2000,冷节点3500</strong></p><p>编辑ES配置文件,添加以下配置,修改每节点的Shard数量</p><ul><li><p>热节点: <code>cluster.max_shards_per_node: 2000</code></p></li><li><p>冷节点: <code>cluster.max_shards_per_node: 3500</code></p></li></ul><p>以下是Ansible发布模板</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if es_role  is defined and es_role == &apos;hot&apos; %&#125;</span><br><span class="line">cluster.max_shards_per_node: 2000</span><br><span class="line">&#123;% elif es_role  is defined and es_role == &apos;cold&apos; %&#125;</span><br><span class="line">cluster.max_shards_per_node: 3500</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>重启ES后并没有生效.临时在Kibana的dev tool中设置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">PUT /_cluster/settings</span><br><span class="line">&#123;</span><br><span class="line">  &quot;transient&quot;: &#123;</span><br><span class="line">    &quot;cluster&quot;: &#123;</span><br><span class="line">      &quot;max_shards_per_node&quot;:3000</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>设置立即生效,在ELK集群面板中,总的shard分片数超过了9000的最大值,已经达到9,376 shards,</p><p>博客参考:<a href="https://studygolang.com/articles/25396" target="_blank" rel="noopener">https://studygolang.com/articles/25396</a></p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;记一次生产ELK性能优化&quot;&gt;&lt;a href=&quot;#记一次生产ELK性能优化&quot; class=&quot;headerlink&quot; title=&quot;记一次生产ELK性能优化&quot;&gt;&lt;/a&gt;记一次生产ELK性能优化&lt;/h3&gt;&lt;p&gt;ES上线后遇到一些问题:&lt;/p&gt;
&lt;h5 id=&quot;一-内存压力过高&quot;&gt;&lt;a href=&quot;#一-内存压力过高&quot; class=&quot;headerlink&quot; title=&quot;一.内存压力过高&quot;&gt;&lt;/a&gt;一.内存压力过高&lt;/h5&gt;&lt;p&gt;目前ES节点的有两种规格内存.&lt;/p&gt;
&lt;p&gt;80GB内存(节点同时运行了Logstash,分配了16G给logstash).&lt;/p&gt;
&lt;p&gt;54GB内存(只运行ES,并且只作为data节点)&lt;/p&gt;
&lt;p&gt;ES的JVM内存从32G下降到28G.建议JVM内存是总物理内存的一半左右,&lt;/p&gt;
&lt;p&gt;节点内存使用情况如下:&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>kibana查询语法</title>
    <link href="https://jesse.top/2020/08/25/elk/kibana%E6%9F%A5%E8%AF%A2%E8%AF%AD%E6%B3%95/"/>
    <id>https://jesse.top/2020/08/25/elk/kibana查询语法/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kibana查询语法"><a href="#kibana查询语法" class="headerlink" title="kibana查询语法"></a>kibana查询语法</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>在kibana6.3版本开始,kibana引入了Kibana Query Language(KQL)查询语法.并且从7.0版本开始已经是Kibana的默认查询语言.KQL包含脚本化字段查询和简易化的查询.</p><p><img src="https://img2.jesse.top/image-20200724104504825.png" alt="image-20200724104504825"></p><p>kibana支持以下两种查询语法:</p><ul><li><p>Kibana query language (KQL)</p></li><li><p>Lucene query syntax (Lucene)</p></li></ul><hr><a id="more"></a><h3 id="Discover"><a href="#Discover" class="headerlink" title="Discover"></a>Discover</h3><p>kibana的日志展示和搜索查询在discover界面中.在查询之前需要先指定:</p><ul><li>索引名.指定要查询的索引,索引类似于关系型数据库(比如mysql)的库.</li><li>选择一个时间范围,或者时间点</li></ul><p><strong>discovery界面介绍</strong></p><p>下面是一个搜索展示的页面</p><p><img src="https://img2.jesse.top/image-20200724105414932.png" alt="image-20200724105414932"></p><p>在界面中会统计查询结果条数,以及日志时间,在下面会展示查询结果的具体日志内容,以及高亮显示关键字.</p><p>左侧栏中有字段值可以选择,例如在下面的搜索结果中,选择responsetime字段,可以详细看到字段的具体值,并且倒序排序..如果选定了字段后,在右侧就优先展示该字段的内容</p><p><img src="https://img2.jesse.top/image-20200724160502495.png" alt="image-20200724160502495"></p><p>也可以在查询结果中,对其他字段进行排序,统计,例如在responsetime大于2的文档中,对request进行排序,</p><p><img src="https://img2.jesse.top/image-20200724161002418.png" alt="image-20200724161002418"></p><h5 id="保存查询"><a href="#保存查询" class="headerlink" title="保存查询"></a>保存查询</h5><p>对经常需要查询的字段或者语法,可以保存查询,方便下次再次查询,</p><p><img src="https://img2.jesse.top/image-20200724161528813.png" alt="image-20200724161528813"></p><hr><h3 id="查询语法"><a href="#查询语法" class="headerlink" title="查询语法"></a>查询语法</h3><p>以下主要讲解KQL的查询语法.Lucene的查询语法大同小异,有区别之处会另行说明.</p><p>详细的语法可以参考elastic官网:<a href="https://www.elastic.co/guide/en/kibana/current/search.html" target="_blank" rel="noopener">kibana文档</a></p><h4 id="全文搜索"><a href="#全文搜索" class="headerlink" title="全文搜索"></a>全文搜索</h4><p>在搜索栏直接输入要查询的关键词,如果关键词包含空格,特殊字符,中文等,使用双引号起来作为一个短语进行搜索.</p><blockquote><p>当数据量较大时,全文搜索比较缓慢,因为ES需要去日志的所有字段中依次查找关键字</p></blockquote><p><strong>注意</strong></p><p>当需要搜索msf-api1这个关键词时,需要将这个短语用双引号括起来,否则会搜索任何满足msf关键词的数据</p><hr><h4 id="字段匹配"><a href="#字段匹配" class="headerlink" title="字段匹配"></a>字段匹配</h4><p>全文搜索比较缓慢,因为需要从所有字段中查找匹配的关键字,这个时候指定字段名,查询速度就会非常快,.</p><p><code>语法格式: 字段名:值</code> </p><p>查询状态为200的Nginx日志: <code>status:200</code></p><p>查询主机为msf-api1的所有日志: hostname: “msf-api1”</p><hr><h4 id="逻辑操作"><a href="#逻辑操作" class="headerlink" title="逻辑操作"></a>逻辑操作</h4><p>操作符: <code>AND</code> ,<code>OR</code></p><p>例如.我要寻找request请求URL字段中包含关键字632303f301bebf55724ed28eaba6ec6477cb5670,同时来自msf-api1的服务器,并且status状态为200的日志</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">request:632303f301bebf55724ed28eaba6ec6477cb5670 AND status:200 AND hostname:&quot;msf-api1&quot;</span><br></pre></td></tr></table></figure><blockquote><p>在KQL查询语法中,AND操作符不能省略,否则会报错,但是在Lucene中可以省略</p></blockquote><p>默认<code>AND</code>操作符的优先级要高于<code>OR</code>,例如<code>response:200 and extension:php or extension:css</code>会匹配所有response为200并且extension为php,或者extension为CSS但是response为所有的文档.</p><h5 id="单字段逻辑查询"><a href="#单字段逻辑查询" class="headerlink" title="单字段逻辑查询"></a>单字段逻辑查询</h5><p>逻辑操作符还有更方便的做法,可以针对一个单一的字段指定多个值.例如</p><p><code>status:(200 or 499)</code> 表示查询status为200或者499的文档</p><p><code>tags:(success and info and security)</code> 表示同时具有以上tags的文档</p><hr><h4 id="分组查询"><a href="#分组查询" class="headerlink" title="分组查询"></a>分组查询</h4><p>像数学运算一下,可以使用括号来指定优先级,</p><p>比如<code>response:200 and (extension:php or extension:css)</code>会匹配所有response为200并且extension是php或者css的查询结果</p><hr><h4 id="not操作符"><a href="#not操作符" class="headerlink" title="not操作符"></a>not操作符</h4><p>使用not前缀,表示一个否定的查询.</p><p>例如: <code>not response:200</code> 会匹配所有response不等于200的文档</p><p>not还可以结合分组查询使用</p><p><code>response:200 and not (extension:php or extension:css)</code>匹配所有response为200,并且extesnion不等于php或者css的文档</p><hr><h4 id="KQL范围查询"><a href="#KQL范围查询" class="headerlink" title="KQL范围查询"></a>KQL范围查询</h4><p>使用运算符号<code>&gt;</code>,<code>&gt;=</code>,<code>&lt;</code>,<code>&lt;=</code>等可以进行范围查询.</p><p>在使用范围查询的时候可以省略字段后的冒号,比如查询Nginx中的cost值大于1秒的文档.</p><p><code>responsetime&gt;1</code> (在我的nginx日志中定义responsetime字段表示cost值)</p><p>IP地址也可以使用运算符查询,例如查询大于10.111.30.34的主机IP:</p><p><code>host.ip&gt;10.111.30.34</code></p><hr><h4 id="Lucene的范围查询"><a href="#Lucene的范围查询" class="headerlink" title="Lucene的范围查询:"></a>Lucene的范围查询:</h4><p>格式: <code>[START_VALUE TO END_VALUE]</code></p><p>例如查询Nginx400-499的状态:</p><p><code>status: [400 TO 499]</code></p><p>例如查询10个以上的计数:</p><p><code>count:[10 TO *]</code></p><p>2012年之前的日期</p><p><code>date:{* TO 2012-01-01}</code> 或者 <code>date:[* TO 2012-01-01]</code></p><p>查询IP地址范围:<code>host_ip:[10.111.10.34 to 10.111.10.40]</code></p><p>中括号和大括号可以混合使用,例如下面例子中表示从1-4的数字,(不包括5)</p><p><code>count:[1 TO 5}</code></p><blockquote><p>注：<code>[ ]</code>表示端点数值包含在范围内，<code>{ }</code> 表示端点数值不包含在范围内</p></blockquote><p><strong>Lucene范围运算符</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">age:&gt;10</span><br><span class="line">age:&gt;=10</span><br><span class="line">age:&lt;10</span><br><span class="line">age:&lt;=10</span><br></pre></td></tr></table></figure><p>例如:要寻找cost值在1-2秒之间的文档,可以写成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#1.responsetime:(&gt;=1 AND &lt;=2)</span><br><span class="line">#2.responsetime:[1 TO 2]</span><br><span class="line">#3.responsetime:(+&gt;1 +&lt;2)</span><br></pre></td></tr></table></figure><hr><h4 id="Lucene-加减查询"><a href="#Lucene-加减查询" class="headerlink" title="Lucene 加减查询"></a>Lucene 加减查询</h4><p>Lucene的<code>+</code>和<code>-</code>号有特殊涵义:</p><p><code>+</code>: 表示匹配该项</p><p><code>-</code>:表示不匹配该项.和<code>NOT</code>作用类似</p><p>例如下面这个例子匹配responsetime大于2,但是status不为200的文档</p><p><code>+responsetime:&gt;2 -status:200</code></p><p>在下面的例子中表示fox关键字必须匹配,news关键字必须排除,quick和brown则是可选项,有就显示,没有也无所谓</p><p><code>quick brown +fox -news</code></p><hr><h4 id="通配符查询"><a href="#通配符查询" class="headerlink" title="通配符查询"></a>通配符查询</h4><p>KQL也支持通配符字段,使用<code>字段:*</code>表示查询所有存在该字段的文档.在下面的例子中表示查询所有msf-api开头的主机</p><p><code>hostname:&quot;msf-api*&quot;</code></p><p>通配符不仅可以应用在值上,在字段中也可以使用通配符,比如有<code>machine.os</code>和<code>machine.os.keyword</code>这2个字段.如果想查找这两个字段都包含一个<code>windows 10</code>的值,可以使用<code>machine.os*:windows 10</code></p><hr><h4 id="lucene正则"><a href="#lucene正则" class="headerlink" title="lucene正则"></a>lucene正则</h4><p>lucene支持部分正则语法.</p><p><code>?</code>表示匹配一个单一的字符</p><p><code>*</code>表示匹配0到无限个字符</p><p><code>~</code>波浪线表示模糊匹配..例如<code>quikc~ brwn~ fok~</code></p><blockquote><p>官方建议最好不要使用正则查询,因为会消耗大量内存并且查询速度较慢</p></blockquote><p>正则表达式可以使用斜杠包括起来.例如:</p><p><code>name:/joh?n(ath[oa]n)/</code></p><hr><h4 id="特殊字符转换"><a href="#特殊字符转换" class="headerlink" title="特殊字符转换"></a>特殊字符转换</h4><p>如果查询的字段或者内容中包含以下特殊字符,需要转义</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">+ - = &amp;&amp; || &gt; &lt; ! ( ) &#123; &#125; [ ] ^ &quot; ~ * ? : \ /</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kibana查询语法&quot;&gt;&lt;a href=&quot;#kibana查询语法&quot; class=&quot;headerlink&quot; title=&quot;kibana查询语法&quot;&gt;&lt;/a&gt;kibana查询语法&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;在kibana6.3版本开始,kibana引入了Kibana Query Language(KQL)查询语法.并且从7.0版本开始已经是Kibana的默认查询语言.KQL包含脚本化字段查询和简易化的查询.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2.jesse.top/image-20200724104504825.png&quot; alt=&quot;image-20200724104504825&quot;&gt;&lt;/p&gt;
&lt;p&gt;kibana支持以下两种查询语法:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kibana query language (KQL)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lucene query syntax (Lucene)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>ELK使用x-pack插件实现权限控制</title>
    <link href="https://jesse.top/2020/08/25/elk/ELK%E4%BD%BF%E7%94%A8x-pack%E6%8F%92%E4%BB%B6%E5%AE%9E%E7%8E%B0%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6/"/>
    <id>https://jesse.top/2020/08/25/elk/ELK使用x-pack插件实现权限控制/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.312Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ELK使用x-pack插件实现权限控制"><a href="#ELK使用x-pack插件实现权限控制" class="headerlink" title="ELK使用x-pack插件实现权限控制"></a>ELK使用x-pack插件实现权限控制</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>Elsticsearch集群搭建完成,使用Kibana可以直接访问WEB界面,或者使用curl命令行工具也可以直接访问ES的索引,并且可以直接对索引进行增删改查的操作.这在生产中可能就会有严重的安全隐患.</p><p>为了防止Kibana的重要管理功能配置,或者ES的重要索引数据被认为的误删,误配置.就有必要对ES和Kibana进行权限管控.</p><p>而Xpack插件就非常方便,完美的实现了这个功能</p><hr><h3 id="Xpack介绍"><a href="#Xpack介绍" class="headerlink" title="Xpack介绍"></a>Xpack介绍</h3><p>Xpack能够对网络流量进行加密、创建和管理用户、定义能够保护索引和集群级别访问权限的角色，并且使用 Spaces 为 Kibana提供全面保护.在Elastic Stack7.x中已经免费开放基础版本功能.但是更高版本的X-PACK仍然需要付费购买.</p><p>而且在7.x版本中Xpack默认就已经安装了,无需另行安装插件.</p><p>x-pack详细介绍请点击<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.7/setup-xpack.html" target="_blank" rel="noopener">官方文档</a></p><p>x-pack免费版提供一下功能</p><ul><li>TLS 功能。 可对通信进行加密；</li><li>文件和原生 Realm。 可用于创建和管理用户；</li><li>基于角色的访问控制。 可用于控制用户对集群 API 和索引的访问权限；</li><li>通过针对 Kibana Spaces 的安全功能，还可允许在Kibana 中实现多租户</li></ul><a id="more"></a><hr><h3 id="ELK集群中配置Xpack"><a href="#ELK集群中配置Xpack" class="headerlink" title="ELK集群中配置Xpack"></a>ELK集群中配置Xpack</h3><p>主要参考博客:<a href="http://www.eryajf.net/3500.html" target="_blank" rel="noopener">http://www.eryajf.net/3500.html</a></p><p>一.在任意一台ES节点中生成证书</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#直接回车生成证书,无需设置密码</span><br><span class="line">[root@idc-function-elk10 bin]# ./elasticsearch-certutil ca</span><br><span class="line">[root@idc-function-elk10 bin]# ./elasticsearch-certutil cert --ca elastic-stack-ca.p12</span><br></pre></td></tr></table></figure><p>二.将生成的证书拷贝到<code>/etc/elasticsearch/</code>目录下(我这里是拷贝到Ansible的目录下)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk10 bin]# cd ..</span><br><span class="line"></span><br><span class="line">#拷贝elastic-certificates.p12和elastic-stack-ca.p12证书文件到ES的目录下</span><br><span class="line">[root@idc-function-elk10 elasticsearch]# ls</span><br><span class="line">bin  elastic-certificates.p12  elastic-stack-ca.p12  jdk  lib  LICENSE.txt  modules  NOTICE.txt  plugins  README.asciidoc</span><br></pre></td></tr></table></figure><p>我这里使用Ansible来发布</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- name: 拷贝x-pack证书文件</span><br><span class="line">        copy: src=files/&#123;&#123; item &#125;&#125; dest=/etc/elasticsearch/&#123;&#123; item &#125;&#125; owner=elasticsearch  group=elasticsearch</span><br><span class="line">        with_items:</span><br><span class="line">           - elastic-certificates.p12</span><br><span class="line">           - elastic-stack-ca.p12</span><br></pre></td></tr></table></figure><p>三.修改ES配置文件.在所有节点的ES配置文件中新增下面配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/elasticsearch/elasticsearch.yml</span><br><span class="line"></span><br><span class="line">xpack.security.enabled: true</span><br><span class="line">xpack.security.transport.ssl.enabled: true</span><br><span class="line">xpack.security.transport.ssl.verification_mode: certificate</span><br><span class="line">xpack.security.transport.ssl.keystore.path: /etc/elasticsearch/elastic-certificates.p12</span><br><span class="line">xpack.security.transport.ssl.truststore.path: /etc/elasticsearch/elastic-certificates.p12</span><br></pre></td></tr></table></figure><p>我仍然使用Ansible将配置文件同步到所有ES节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- name: 拷贝elasticsearch配置文件</span><br><span class="line">        template: src=templates/elasticsearch.yml.j2 dest=/etc/elasticsearch/elasticsearch.yml owner=elasticsearch  group=elasticsearch</span><br></pre></td></tr></table></figure><blockquote><p>ES配置文件同步完成后,重启ES服务</p></blockquote><p>四.为内置账号添加密码.</p><p>ES中内置了几个管理其他集成组件的账号即：<code>apm_system</code>, <code>beats_system</code>, <code>elastic</code>, <code>kibana</code>, <code>logstash_system</code>, <code>remote_monitoring_user</code>，使用之前，首先需要设置一下密码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive</span><br></pre></td></tr></table></figure><p>为了简便起见,我使用了同一个密码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Enter password for [elastic]: [root@idc-function-elk10 bin]# ./elasticsearch-setup-passwords interactive</span><br><span class="line">Initiating the setup of passwords for reserved users elastic,apm_system,kibana,logstash_system,beats_system,remote_monitoring_user.</span><br><span class="line">You will be prompted to enter passwords as the process progresses.</span><br><span class="line">Please confirm that you would like to continue [y/N]y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Enter password for [elastic]:</span><br><span class="line">Reenter password for [elastic]:</span><br><span class="line">Enter password for [apm_system]:</span><br><span class="line">Reenter password for [apm_system]:</span><br><span class="line">Enter password for [kibana]:</span><br><span class="line">Reenter password for [kibana]:</span><br><span class="line">Enter password for [logstash_system]:</span><br><span class="line">Reenter password for [logstash_system]:</span><br><span class="line">Enter password for [beats_system]:</span><br><span class="line">Reenter password for [beats_system]:</span><br><span class="line">Enter password for [remote_monitoring_user]:</span><br><span class="line">Reenter password for [remote_monitoring_user]:</span><br><span class="line">Changed password for user [apm_system]</span><br><span class="line">Changed password for user [kibana]</span><br><span class="line">Changed password for user [logstash_system]</span><br><span class="line">Changed password for user [beats_system]</span><br><span class="line">Changed password for user [remote_monitoring_user]</span><br><span class="line">Changed password for user [elastic]</span><br></pre></td></tr></table></figure><p>此时,登陆ES的集群管理页面就需要账号密码认证了.需要使用elastic账号和密码进行登录</p><p><img src="https://img2.jesse.top/image-20200812145302139.png" alt="image-20200812145302139"></p><p>五.由于ES开启了身份验证,所以此时Kibana和logstash都无法访问ES了.在logstash配置文件的output中添加ES的账号密码:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">output &#123;</span><br><span class="line">        elasticsearch &#123;</span><br><span class="line">            hosts =&gt; [&quot;172.16.20.101:9200&quot;,&quot;172.16.20.110:9200&quot;,&quot;172.16.20.107:9200&quot;,&quot;172.16.20.108:9200&quot;,&quot;172.16.20.102:9200&quot;,&quot;172.16.20.103:9200&quot;,&quot;172.16.20.104:9200&quot;,&quot;172.16.20.105:9200&quot;,&quot;172.16.20.106:9200&quot;,&quot;172.16.20.109:9200&quot;]</span><br><span class="line">            index =&gt; &quot;logstash-%&#123;[fields][project]&#125;-%&#123;[fields][type]&#125;-%&#123;[fields][level]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">            user =&gt; &quot;elastic&quot;</span><br><span class="line">            password =&gt; &quot;上一步设置的密码&quot;</span><br></pre></td></tr></table></figure><blockquote><p>使用Ansible将配置文件同步到所有logstash节点,然后重启logstash</p></blockquote><p>六. 为Kibana添加ES的elastic账号密码</p><p>在kibana服务器中配置访问ES集群密文账号.在弹出的用户密码中我写的是elastic的用户密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk05 bin]# ./kibana-keystore create --allow-root</span><br><span class="line">Created Kibana keystore in /var/lib/kibana/kibana.keystore</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk05 bin]# ./kibana-keystore add elasticsearch.username --allow-root</span><br><span class="line">Enter value for elasticsearch.username: *******</span><br><span class="line">[root@idc-function-elk05 bin]# ./kibana-keystore add elasticsearch.password --allow-root</span><br><span class="line">Enter value for elasticsearch.password: **********</span><br><span class="line"></span><br><span class="line">#重启kibana</span><br><span class="line">[root@idc-function-elk05 bin]# systemctl restart kibana</span><br></pre></td></tr></table></figure><p>PS: 有些文档中提到还需要在kibana.yml配置文件中新增以下内容,但是我没有添加也没问题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xpack.reporting.encryptionKey: &quot;a_random_string&quot;</span><br><span class="line">xpack.security.encryptionKey: &quot;something_at_least_32_characters&quot;</span><br></pre></td></tr></table></figure><hr><h3 id="Kibana角色权限控制"><a href="#Kibana角色权限控制" class="headerlink" title="Kibana角色权限控制"></a>Kibana角色权限控制</h3><p>安装配置了x-pack后,如果一切正常.此时登陆Kibana就需要用户密码认证.使用elastic管理员账号登陆,给Kibana配置角色和只读用户</p><p><img src="https://img2.jesse.top/image-20200812150302014.png" alt="image-20200812150302014"></p><p>登陆kibana以后,在管理页面中多了个安全性的功能菜单.</p><p><img src="https://img2.jesse.top/image-20200812150507260.png" alt="image-20200812150507260"></p><p>这篇博客详细介绍了Kibana的角色介绍:<a href="https://www.elastic.co/cn/blog/getting-started-with-elasticsearch-security" target="_blank" rel="noopener">https://www.elastic.co/cn/blog/getting-started-with-elasticsearch-security</a></p><p>一般只需要创建一个只读用户即可,防止用户对Kibana以及ES索引进行一些误操作.</p><p>1.创建一个只读的角色.该角色对默认的<code>logstash-*</code>有read权限.(如果有其他索引,可以自行添加).以及对所有工作区有read权限.</p><p><img src="https://img2.jesse.top/image-20200812150646871.png" alt="image-20200812150646871"></p><ol start="2"><li><p>然后添加一个账户,绑定该只读角色即可</p><p><img src="https://img2.jesse.top/image-20200812150801194.png" alt="image-20200812150801194"></p></li></ol><p>使用只读账号登陆Kibana以后,对Kibana只有只读权限,无法访问或者配置任何菜单功能</p><p><img src="https://img2.jesse.top/image-20200812150939231.png" alt="image-20200812150939231"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ELK使用x-pack插件实现权限控制&quot;&gt;&lt;a href=&quot;#ELK使用x-pack插件实现权限控制&quot; class=&quot;headerlink&quot; title=&quot;ELK使用x-pack插件实现权限控制&quot;&gt;&lt;/a&gt;ELK使用x-pack插件实现权限控制&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;Elsticsearch集群搭建完成,使用Kibana可以直接访问WEB界面,或者使用curl命令行工具也可以直接访问ES的索引,并且可以直接对索引进行增删改查的操作.这在生产中可能就会有严重的安全隐患.&lt;/p&gt;
&lt;p&gt;为了防止Kibana的重要管理功能配置,或者ES的重要索引数据被认为的误删,误配置.就有必要对ES和Kibana进行权限管控.&lt;/p&gt;
&lt;p&gt;而Xpack插件就非常方便,完美的实现了这个功能&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;Xpack介绍&quot;&gt;&lt;a href=&quot;#Xpack介绍&quot; class=&quot;headerlink&quot; title=&quot;Xpack介绍&quot;&gt;&lt;/a&gt;Xpack介绍&lt;/h3&gt;&lt;p&gt;Xpack能够对网络流量进行加密、创建和管理用户、定义能够保护索引和集群级别访问权限的角色，并且使用 Spaces 为 Kibana提供全面保护.在Elastic Stack7.x中已经免费开放基础版本功能.但是更高版本的X-PACK仍然需要付费购买.&lt;/p&gt;
&lt;p&gt;而且在7.x版本中Xpack默认就已经安装了,无需另行安装插件.&lt;/p&gt;
&lt;p&gt;x-pack详细介绍请点击&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/7.7/setup-xpack.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方文档&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;x-pack免费版提供一下功能&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TLS 功能。 可对通信进行加密；&lt;/li&gt;
&lt;li&gt;文件和原生 Realm。 可用于创建和管理用户；&lt;/li&gt;
&lt;li&gt;基于角色的访问控制。 可用于控制用户对集群 API 和索引的访问权限；&lt;/li&gt;
&lt;li&gt;通过针对 Kibana Spaces 的安全功能，还可允许在Kibana 中实现多租户&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>Filebeat+logstash收集Nginx错误日志</title>
    <link href="https://jesse.top/2020/08/25/elk/Filebeat+logstash%E6%94%B6%E9%9B%86Nginx%E9%94%99%E8%AF%AF%E6%97%A5%E5%BF%97/"/>
    <id>https://jesse.top/2020/08/25/elk/Filebeat+logstash收集Nginx错误日志/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Filebeat-logstash收集Nginx错误日志"><a href="#Filebeat-logstash收集Nginx错误日志" class="headerlink" title="Filebeat+logstash收集Nginx错误日志"></a>Filebeat+logstash收集Nginx错误日志</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>在上一个笔记&lt;Filebeat+logstash收集Nginx访问日志&gt;中分享了如何收集logstash的访问日志,这篇笔记主要是记录如何收集nginx的错误日志</p><p>Nginx 错误日志是运维人员最常见但又极其容易忽略的日志类型之一。Nginx 错误日志即没有统一明确的分隔符，也没有特别方便的正则模式，但通过 logstash 不同插件的组合，还是可以轻松做到数据处理。</p><hr><a id="more"></a><h3 id="logstash配置"><a href="#logstash配置" class="headerlink" title="logstash配置"></a>logstash配置</h3><p>在/etc/logstash/conf.d目录下编辑配置文件<code>nginx_error.conf</code>内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    beats &#123;</span><br><span class="line">      port =&gt; 5044</span><br><span class="line">      client_inactivity_timeout =&gt; 600</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">filter &#123;</span><br><span class="line">    if [fields][type] == &quot;nginx-error&quot; &#123;</span><br><span class="line">       grok &#123;</span><br><span class="line">      match =&gt; [</span><br><span class="line">          &quot;message&quot;, &quot;(?&lt;time_local&gt;%&#123;YEAR&#125;[./-]%&#123;MONTHNUM&#125;[./-]%&#123;MONTHDAY&#125;[- ]%&#123;TIME&#125;) \[%&#123;LOGLEVEL:log_level&#125;\] %&#123;POSINT:pid&#125;#%&#123;NUMBER&#125;: %&#123;GREEDYDATA:error_message&#125;(?:, client: (?&lt;client&gt;%&#123;IP&#125;|%&#123;HOSTNAME&#125;))(?:, server: %&#123;IPORHOST:server&#125;?)(?:, request: %&#123;QS:request&#125;)?(?:, upstream: (?&lt;upstream&gt;\&quot;%&#123;URI&#125;\&quot;|%&#123;QS&#125;))?(?:, host: %&#123;QS:request_host&#125;)?(?:, referrer: \&quot;%&#123;URI:referrer&#125;\&quot;)?&quot;,</span><br><span class="line">        &quot;message&quot;, &quot;(?&lt;time_local&gt;%&#123;YEAR&#125;[./-]%&#123;MONTHNUM&#125;[./-]%&#123;MONTHDAY&#125;[- ]%&#123;TIME&#125;) \[%&#123;LOGLEVEL:log_level&#125;\]\s&#123;1,&#125;%&#123;GREEDYDATA:error_message&#125;&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  date &#123;</span><br><span class="line">    match =&gt; [ &quot;timestamp&quot; , &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; ]</span><br><span class="line">  &#125;</span><br><span class="line">    mutate &#123;</span><br><span class="line">        convert =&gt; [ &quot;status&quot;, &quot;integer&quot; ]</span><br><span class="line">        convert =&gt; [ &quot;body_bytes&quot;,&quot;integer&quot; ]</span><br><span class="line">        remove_field =&gt; [&quot;message&quot;]</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  ruby &#123;</span><br><span class="line">    code =&gt; &quot;event.set(&apos;log_day&apos;, event.get(&apos;@timestamp&apos;).time.localtime.strftime(&apos;%Y%m%d&apos;))&quot;</span><br><span class="line">  &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">        elasticsearch &#123;</span><br><span class="line">            hosts =&gt; [&quot;172.16.20.107:9200&quot;]</span><br><span class="line">            index =&gt; &quot;logstash-%&#123;[fields][project]&#125;-%&#123;[fields][type]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">            #flush_size =&gt; 20000</span><br><span class="line">            #idle_flush_time =&gt; 10</span><br><span class="line">            #sniffing =&gt; true</span><br><span class="line">            #template_overwrite =&gt; true</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><hr><h3 id="Filebeat配置"><a href="#Filebeat配置" class="headerlink" title="Filebeat配置"></a>Filebeat配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">- type: log</span><br><span class="line"></span><br><span class="line">  # Change to true to enable this input configuration.</span><br><span class="line">  enabled: true</span><br><span class="line"></span><br><span class="line">  # Paths that should be crawled and fetched. Glob based paths.</span><br><span class="line">  paths:</span><br><span class="line">    - /data/logs/nginx/hsq_openapi_beta.error.log</span><br><span class="line"></span><br><span class="line">#fields字段用于打标签和索引,在logstash里判断日志来源</span><br><span class="line">  fields:</span><br><span class="line">     type: nginx-error</span><br><span class="line">     project: hsq</span><br><span class="line">     </span><br><span class="line">---output配置,将日志输出到logstash-----</span><br><span class="line">output.logstash:</span><br><span class="line">  # The Logstash hosts</span><br><span class="line">  hosts: [&quot;172.16.20.107:5044&quot;]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Filebeat-logstash收集Nginx错误日志&quot;&gt;&lt;a href=&quot;#Filebeat-logstash收集Nginx错误日志&quot; class=&quot;headerlink&quot; title=&quot;Filebeat+logstash收集Nginx错误日志&quot;&gt;&lt;/a&gt;Filebeat+logstash收集Nginx错误日志&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;在上一个笔记&amp;lt;Filebeat+logstash收集Nginx访问日志&amp;gt;中分享了如何收集logstash的访问日志,这篇笔记主要是记录如何收集nginx的错误日志&lt;/p&gt;
&lt;p&gt;Nginx 错误日志是运维人员最常见但又极其容易忽略的日志类型之一。Nginx 错误日志即没有统一明确的分隔符，也没有特别方便的正则模式，但通过 logstash 不同插件的组合，还是可以轻松做到数据处理。&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>使用goreplay收集线上真实http流量</title>
    <link href="https://jesse.top/2020/08/25/Linux-Web/%E4%BD%BF%E7%94%A8goreplay%E6%94%B6%E9%9B%86%E7%BA%BF%E4%B8%8A%E7%9C%9F%E5%AE%9Ehttp%E6%B5%81%E9%87%8F/"/>
    <id>https://jesse.top/2020/08/25/Linux-Web/使用goreplay收集线上真实http流量/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.311Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用goreplay收集线上真实http流量"><a href="#使用goreplay收集线上真实http流量" class="headerlink" title="使用goreplay收集线上真实http流量"></a>使用goreplay收集线上真实http流量</h2><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>在很多场景中,我们需要将线上服务器的真实Http请求复制转发到某台服务器中(或者测试环境中),并且前提是不影响线上生产业务进行.</p><p>例如:</p><ol><li>通常可能会通过ab等压测工具来对单一http接口进行压测。但如果是需要http服务整体压测，使用ab来压测工作量大且不方便，通过线上流量复制引流，通过将真实请求流量放大N倍来进行压测，能对服务有一个较为全面的检验.</li><li>将线上流量引入到测试环境中,测试某个中间件或者数据库的压力</li><li>上线前在预发布环境，使用线上真实的请求，检查是否准备发布的版本，是否具备发布标准</li><li>用线上的流量转发到预发，检查相同流量下一些指标的反馈情况，检查核心数据是否有改善、优化.</li></ol><a id="more"></a><hr><h3 id="goreplay介绍"><a href="#goreplay介绍" class="headerlink" title="goreplay介绍"></a>goreplay介绍</h3><p>goreplay项目请参考github:<a href="https://github.com/buger/goreplay" target="_blank" rel="noopener">goreplay介绍</a></p><p>goreplay是一款开源网络监控工具,可以在不影响业务的情况下,记录服务器真实流量,将该流量用来做镜像,压力测试,监控和分析等用途.</p><p>简单来说就是goreplay抓取线上真实的流量，并将捕捉到的流量转发到测试服务器上(或者保存到本地文件中)</p><p>goreplay大致工作流程如下:</p><p><img src="https://img2.jesse.top/20200629110035.png" alt=""></p><hr><h3 id="goreplay常见使用方式"><a href="#goreplay常见使用方式" class="headerlink" title="goreplay常见使用方式"></a>goreplay常见使用方式</h3><p>goreplay使用文档参考:<a href="https://github.com/buger/goreplay/wiki" target="_blank" rel="noopener">goreplay文档</a></p><p>常用的一些命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-input-raw 抓取指定端口的流量 gor --input-raw :8080</span><br><span class="line">-output-stdout 打印到控制台</span><br><span class="line">-output-file 将请求写到文件中 gor --input-raw :80 --output-file ./requests.gor</span><br><span class="line">-input-file 从文件中读取请求，与上一条命令呼应 gor --input-file ./requests.gor</span><br><span class="line">-exit-after 5s 持续时间</span><br><span class="line">-http-allow-url url白名单，其他请求将会被丢弃</span><br><span class="line">-http-allow-method 根据请求方式过滤</span><br><span class="line">-http-disallow-url 遇上一个url相反，黑名单，其他的请求会被捕获到</span><br></pre></td></tr></table></figure><blockquote><p>更多命令可以使用 ./gor –help查看</p></blockquote><hr><h3 id="goreplay安装"><a href="#goreplay安装" class="headerlink" title="goreplay安装"></a>goreplay安装</h3><p>在github上下载Linux的二进制文件: <a href="https://github.com/buger/goreplay/releases" target="_blank" rel="noopener">goreplay安装</a></p><blockquote><p>注意.虽然在github上提供了rpm安装包,但是实际安装发现无法安装:</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# rpm -ivh gor-1.0.0-1.x86_64.rpm</span><br><span class="line">Preparing...                          ################################# [100%]</span><br><span class="line">package goreplay-1.0.0-1.x86_64 is intended for a different operating system</span><br></pre></td></tr></table></figure><p>下载github上的二进制文件,解压后是一个gor的二进制可执行文件.复制到PATH变量路径下即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# wget https://github.com/buger/goreplay/releases/download/v1.0.0/gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# ls</span><br><span class="line"> gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# tar -xf gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# ls</span><br><span class="line">gor </span><br><span class="line">[root@dwd-tongji-3 ~]# ll gor</span><br><span class="line">-rwxr-xr-x 1 501 games 17779040 Mar 30  2019 gor</span><br><span class="line">[root@dwd-tongji-3 ~]# cp gor /usr/local/bin/</span><br></pre></td></tr></table></figure><hr><h3 id="goreplay简单实践"><a href="#goreplay简单实践" class="headerlink" title="goreplay简单实践"></a>goreplay简单实践</h3><h4 id="1-将本地http的流量保存到本地文件中"><a href="#1-将本地http的流量保存到本地文件中" class="headerlink" title="1.将本地http的流量保存到本地文件中."></a>1.将本地http的流量保存到本地文件中.</h4><p>为了简便起见,以下命令都在root用户下执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## 1.开启一个screen窗口</span><br><span class="line">[root@dwd-tongji-3 ~]# screen -S GOR</span><br><span class="line">## 2.将80流量保存到本地的文件</span><br><span class="line">[root@dwd-tongji-3 ~]# gor --input-raw :80 --output-file /data/requests.gor</span><br><span class="line">Version: 1.0.0</span><br></pre></td></tr></table></figure><p>默认情况下goreplay会以块文件存储,将流量保存为多个块文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# ls /data/requests_* | more</span><br><span class="line">/data/requests_0.gor</span><br><span class="line">/data/requests_100.gor</span><br><span class="line">/data/requests_101.gor</span><br><span class="line">/data/requests_102.gor</span><br><span class="line">/data/requests_103.gor</span><br><span class="line">/data/requests_104.gor</span><br><span class="line">/data/requests_105.gor</span><br><span class="line">/data/requests_106.gor</span><br><span class="line">/data/requests_107.gor</span><br><span class="line">/data/requests_108.gor</span><br><span class="line">/data/requests_109.gor</span><br><span class="line">/data/requests_10.gor</span><br></pre></td></tr></table></figure><p>使用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>[root@dwd-tongji-3 ~]#./gor –input-raw :80 –output-file /data/gor.gor –output-file-append</p><p>[root@dwd-tongji-3 ~]# ll /data -h<br>total 1.4M<br>-rw-r—– 1 root root 1.4M Jun 29 15:13 gor.gor<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 2.将http的请求打印到终端</span><br></pre></td></tr></table></figure></p><p>[root@dwd-tongji-3 ~]#gor –input-raw :8000 –output-stdout<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 3.将http的请求转发到测试环境</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-http=”<a href="http://beta:80&quot;" target="_blank" rel="noopener">http://beta:80&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在测试服务器上的nginx查看日志,.发现流量已经进来了</span><br></pre></td></tr></table></figure></p><p>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik.php HTTP/1.1” 200 5 “<a href="https://2021001151691008.hybrid.alipay-eco.com/2021001151691008/0.2.2006111453.18/index.html#pages/index/index?appid=2021001151691008&amp;taskId=415&quot;" target="_blank" rel="noopener">https://2021001151691008.hybrid.alipay-eco.com/2021001151691008/0.2.2006111453.18/index.html#pages/index/index?appid=2021001151691008&amp;taskId=415&quot;</a> “Mozilla/5.0 (iPhone; CPU iPhone OS 13_3_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/17D50 Ariver/1.0.12 AliApp(AP/10.1.95.7030) Nebula WK RVKType(0) AlipayDefined(nt:4G,ws:375|667|2.0) AlipayClient/10.1.95.7030 Language/zh-Hans Region/CN NebulaX/1.0.0” “112.96.179.238”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik.php HTTP/1.1” 200 5 “<a href="https://servicewechat.com/wxa090d3923fde0d4b/132/page-frame.html&quot;" target="_blank" rel="noopener">https://servicewechat.com/wxa090d3923fde0d4b/132/page-frame.html&quot;</a> “Mozilla/5.0 (iPhone; CPU iPhone OS 13_5_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 MicroMessenger/7.0.13(0x17000d29) NetType/4G Language/zh_CN” “14.106.171.11”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">####  也可以将流量输出到多个终端</span><br><span class="line"></span><br><span class="line">* 输出到多个http服务器</span><br></pre></td></tr></table></figure></p><p>gor –input-tcp :28020 –output-http “<a href="http://staging.com&quot;" target="_blank" rel="noopener">http://staging.com&quot;</a>  –output-http “<a href="http://dev.com&quot;" target="_blank" rel="noopener">http://dev.com&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 输出到文件或者Http服务器</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-file requests.log –output-http “<a href="http://staging.com&quot;" target="_blank" rel="noopener">http://staging.com&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">####  4.将流量从文件重放到http服务器</span><br><span class="line"></span><br><span class="line">1.首先将请求流量保存到本地文件</span><br></pre></td></tr></table></figure></p><p>sudo ./gor –input-raw :8000 –output-file=requests.gor<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2.再开一个窗口,运行gor,将请求流量从文件中重放</span><br></pre></td></tr></table></figure></p><p>./gor –input-file requests.gor –output-http=”<a href="http://localhost:8001&quot;" target="_blank" rel="noopener">http://localhost:8001&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 压力测试</span><br><span class="line"></span><br><span class="line">goreplay支持将捕获到的生产实际请求流量减少或者放大重播以用于测试环境的压力测试.压力测试一般针对Input流量减少或者放大.例如下面的例子</span><br></pre></td></tr></table></figure></p><h1 id="Replay-from-file-on-2x-speed"><a href="#Replay-from-file-on-2x-speed" class="headerlink" title="Replay from file on 2x speed"></a>Replay from file on 2x speed</h1><p>#将请求流量以2倍的速度放大重播<br>gor –input-file “requests.gor|200%” –output-http “staging.com”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">当然也也支持10%,20%等缩小请求流量</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 限速</span><br><span class="line"></span><br><span class="line">如果受限于测试环境的服务器资源压力,只想重播一部分流量到测试环境中,而不需要所有的实际生产流量,那么就可以用限速功能.有两种策略可以实现限流</span><br><span class="line"></span><br><span class="line">1.随机丢弃请求流量</span><br><span class="line"></span><br><span class="line">2.基于Header或者URL丢弃一定的流量(百分比)</span><br><span class="line"></span><br><span class="line">#####  随机丢弃请求流量</span><br><span class="line"></span><br><span class="line">input和output两端都支持限速,有两种限速算法:**百分比**或者**绝对值**</span><br><span class="line"></span><br><span class="line">* 百分比: input端支持缩小或者放大请求流量,基于指定的策略随机丢弃请求流量</span><br><span class="line">* 绝对值: 如果单位时间(秒)内达到临界值,则丢弃剩余请求流量,下一秒临界值还原</span><br><span class="line"></span><br><span class="line">**用法**:</span><br><span class="line"></span><br><span class="line">在output终端使用&quot;|&quot;运算符指定限速阈值,例如:</span><br><span class="line"></span><br><span class="line">* 使用绝对值限速</span><br></pre></td></tr></table></figure></p><h1 id="staging-server-will-not-get-more-than-ten-requests-per-second"><a href="#staging-server-will-not-get-more-than-ten-requests-per-second" class="headerlink" title="staging.server will not get more than ten requests per second"></a>staging.server will not get more than ten requests per second</h1><p>#staging服务每秒只接收10个请求<br>gor –input-tcp :28020 –output-http “<a href="http://staging.com|10&quot;" target="_blank" rel="noopener">http://staging.com|10&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 使用百分比限速</span><br></pre></td></tr></table></figure></p><h1 id="replay-server-will-not-get-more-than-10-of-requests"><a href="#replay-server-will-not-get-more-than-10-of-requests" class="headerlink" title="replay server will not get more than 10% of requests"></a>replay server will not get more than 10% of requests</h1><h1 id="useful-for-high-load-environments"><a href="#useful-for-high-load-environments" class="headerlink" title="useful for high-load environments"></a>useful for high-load environments</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##### 基于Header或者URL参数限速</span><br><span class="line"></span><br><span class="line">如果header或者URL参数中有唯一值,例如(API key),则可以转发指定百分比的流量到后端,例如:</span><br></pre></td></tr></table></figure></p><h1 id="Limit-based-on-header-value"><a href="#Limit-based-on-header-value" class="headerlink" title="Limit based on header value"></a>Limit based on header value</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%” –http-header-limiter “X-API-KEY: 10%”</p><h1 id="Limit-based-on-URL-param-value"><a href="#Limit-based-on-URL-param-value" class="headerlink" title="Limit based on URL param value"></a>Limit based on URL param value</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%” –http-param-limiter “api_key: 10%”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">###  过滤</span><br><span class="line"></span><br><span class="line">如果只想捕获指定的URL路径请求,或者http头部,或者Http方法,则可以使用过滤功能</span><br><span class="line"></span><br><span class="line">下面是几个例子</span><br><span class="line"></span><br><span class="line">* 只捕获某个URL</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-being-sent-to-the-api-endpoint"><a href="#only-forward-requests-being-sent-to-the-api-endpoint" class="headerlink" title="only forward requests being sent to the /api endpoint"></a>only forward requests being sent to the /api endpoint</h1><p>gor –input-raw :8080 –output-http staging.com –http-allow-url /api<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 拒绝某个URL</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-NOT-being-sent-to-the-api…-endpoint"><a href="#only-forward-requests-NOT-being-sent-to-the-api…-endpoint" class="headerlink" title="only forward requests NOT being sent to the /api… endpoint"></a>only forward requests NOT being sent to the /api… endpoint</h1><p>gor –input-raw :8080 –output-http staging.com –http-disallow-url /api<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 基于正则表达式过滤头部</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-with-an-api-version-of-1-0x"><a href="#only-forward-requests-with-an-api-version-of-1-0x" class="headerlink" title="only forward requests with an api version of 1.0x"></a>only forward requests with an api version of 1.0x</h1><p>gor –input-raw :8080 –output-http staging.com –http-allow-header api-version:^1.0\d</p><h1 id="only-forward-requests-NOT-containing-User-Agent-header-value-“Replayed-by-Gor”"><a href="#only-forward-requests-NOT-containing-User-Agent-header-value-“Replayed-by-Gor”" class="headerlink" title="only forward requests NOT containing User-Agent header value “Replayed by Gor”"></a>only forward requests NOT containing User-Agent header value “Replayed by Gor”</h1><p>gor –input-raw :8080 –output-http staging.com –http-disallow-header “User-Agent: Replayed by Gor”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 过滤HTTP请求方法</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-http “<a href="http://staging.server&quot;" target="_blank" rel="noopener">http://staging.server&quot;</a> \<br>    –http-allow-method GET \<br>    –http-allow-method OPTIONS<br><code>`</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;使用goreplay收集线上真实http流量&quot;&gt;&lt;a href=&quot;#使用goreplay收集线上真实http流量&quot; class=&quot;headerlink&quot; title=&quot;使用goreplay收集线上真实http流量&quot;&gt;&lt;/a&gt;使用goreplay收集线上真实http流量&lt;/h2&gt;&lt;h3 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;背景介绍&lt;/h3&gt;&lt;p&gt;在很多场景中,我们需要将线上服务器的真实Http请求复制转发到某台服务器中(或者测试环境中),并且前提是不影响线上生产业务进行.&lt;/p&gt;
&lt;p&gt;例如:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通常可能会通过ab等压测工具来对单一http接口进行压测。但如果是需要http服务整体压测，使用ab来压测工作量大且不方便，通过线上流量复制引流，通过将真实请求流量放大N倍来进行压测，能对服务有一个较为全面的检验.&lt;/li&gt;
&lt;li&gt;将线上流量引入到测试环境中,测试某个中间件或者数据库的压力&lt;/li&gt;
&lt;li&gt;上线前在预发布环境，使用线上真实的请求，检查是否准备发布的版本，是否具备发布标准&lt;/li&gt;
&lt;li&gt;用线上的流量转发到预发，检查相同流量下一些指标的反馈情况，检查核心数据是否有改善、优化.&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="goreplay" scheme="https://jesse.top/tags/goreplay/"/>
    
  </entry>
  
  <entry>
    <title>ELK收集mysql5.7慢日志</title>
    <link href="https://jesse.top/2020/08/25/elk/ELK%E6%94%B6%E9%9B%86mysql5.7%E6%85%A2%E6%97%A5%E5%BF%97/"/>
    <id>https://jesse.top/2020/08/25/elk/ELK收集mysql5.7慢日志/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.312Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ELK收集mysql5-7慢日志"><a href="#ELK收集mysql5-7慢日志" class="headerlink" title="ELK收集mysql5.7慢日志"></a>ELK收集mysql5.7慢日志</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>公司ELK平台计划收集生产业务的所有mysql慢日志.由于所有环境中均使用mysql5.7版本,所以其他mysql版本的慢日志格式不在讨论范围之内.</p><p>慢日志的grok正则匹配我折腾了很久,网上的大多文档中给出的logstash的grok正则其实并不能正确的解析到mysql慢日志的字段.</p><p>这个博客的grok正则经过实践可行.而且filebeat,logstash的filter配置也是参考这个博客配置的:<a href="https://www.cnblogs.com/minseo/p/10441913.html" target="_blank" rel="noopener">博客地址</a></p><hr><h3 id="MySQL慢日志"><a href="#MySQL慢日志" class="headerlink" title="MySQL慢日志"></a>MySQL慢日志</h3><p>慢日志格式如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[work@msf-mysql-master log]$ sudo head slow_2019072103.log</span><br><span class="line">/usr/local/mysql5.7/bin/mysqld, Version: 5.7.24-log (MySQL Community Server (GPL)). started with:</span><br><span class="line">Tcp port: 3306  Unix socket: /mysql_log/msf/tmp/mysql.sock</span><br><span class="line">Time                 Id Command    Argument</span><br><span class="line"># Time: 2019-07-21T08:54:04.145255+08:00</span><br><span class="line"># User@Host: u_msf[u_msf] @  [10.111.10.40]  Id: 131421254</span><br><span class="line"># Query_time: 1.595300  Lock_time: 0.000031 Rows_sent: 20  Rows_examined: 809259</span><br><span class="line">use msf_prod;</span><br><span class="line">SET timestamp=1563670444;</span><br><span class="line">SELECT `id`,`type`,`honey`,`remark`,`created_at` FROM `t_log_user_honey` WHERE `user_id` = &apos;1000014423&apos; ORDER BY `created_at` DESC LIMIT 20 OFFSET 0;</span><br><span class="line"># Time: 2019-07-21T10:51:06.184010+08:00</span><br></pre></td></tr></table></figure><a id="more"></a><p>每个日志文件的格式为<code>slow_日期.log</code> .7天切割一次新的日志文件</p><p>每个日志的开头三行是不需要的内容,所以需要filebeat排除</p><p>每一条慢日志有以下几行组成:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Time: 2019-07-21T08:54:04.145255+08:00</span><br><span class="line"># User@Host: u_msf[u_msf] @  [10.111.10.40]  Id: 131421254</span><br><span class="line"># Query_time: 1.595300  Lock_time: 0.000031 Rows_sent: 20  Rows_examined: 809259</span><br><span class="line">use msf_prod;</span><br><span class="line">SET timestamp=1563670444;</span><br><span class="line">SELECT `id`,`type`,`honey`,`remark`,`created_at` FROM `t_log_user_honey` WHERE `user_id` = &apos;1000014423&apos; ORDER BY `created_at` DESC LIMIT 20 OFFSET 0;</span><br></pre></td></tr></table></figure><p>第一行Time时间不需要,所以也需要filebeat排除.</p><p>从第二行开始匹配,有些慢日志可能没有<code>use database;</code>的语句.所以需要分别针对对待</p><hr><h3 id="filebeat配置"><a href="#filebeat配置" class="headerlink" title="filebeat配置"></a>filebeat配置</h3><p>filebeat需要开启多行日志功能.并且排除特定的字段.除此之外,和其他的日志收集配置一样.下面是生产环境中filebeat的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">- type: log</span><br><span class="line"></span><br><span class="line">  # Change to true to enable this input configuration.</span><br><span class="line">  enabled: true</span><br><span class="line"></span><br><span class="line">  # Paths that should be crawled and fetched. Glob based paths.</span><br><span class="line">  paths:</span><br><span class="line">    - /mysql_log/msf/log/slow_*.log</span><br><span class="line">  exclude_lines: [&apos;^\# Time&apos;,&apos;^\/usr&apos;,&apos;^Tcp&apos;,&apos;^Time&apos;]</span><br><span class="line">  multiline.pattern: &apos;^\# Time|^\# User&apos;</span><br><span class="line">  multiline.negate: true</span><br><span class="line">  multiline.match: after</span><br><span class="line">  fields:</span><br><span class="line">    project: msf</span><br><span class="line">    type: mysql</span><br><span class="line">    level: slow</span><br></pre></td></tr></table></figure><hr><h3 id="logstash配置"><a href="#logstash配置" class="headerlink" title="logstash配置"></a>logstash配置</h3><p>logstash需要使用正则匹配2种格式的慢日志.当一种grok匹配到了后,logstash就不会再接着往下匹配了,所以每条日志只会匹配一种grok规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">      if [fields][type] == &quot;mysql&quot; &#123;</span><br><span class="line">          grok &#123;</span><br><span class="line">            #有use database语句</span><br><span class="line">            match =&gt; [ &quot;message&quot; , &quot;^#\s+User@Host:\s+%&#123;USER:user&#125;\[[^\]]+\]\s+@\s+(?:(?&lt;clienthost&gt;\S*) )?\[(?:%&#123;IPV4:clientip&#125;)?\]\s+Id:\s+%&#123;NUMBER:row_id:int&#125;\n#\s+Query_time:\s+%&#123;NUMBER:query_time:float&#125;\s+Lock_time:\s+%&#123;NUMBER:lock_time:float&#125;\s+Rows_sent:\s+%&#123;NUMBER:rows_sent:int&#125;\s+Rows_examined:\s+%&#123;NUMBER:rows_examined:int&#125;\n\s*(?:use %&#123;DATA:database&#125;;\s*\n)?SET\s+timestamp=%&#123;NUMBER:timestamp&#125;;\n\s*(?&lt;sql&gt;(?&lt;action&gt;\w+)\b.*;)\s*(?:\n#\s+Time)?.*$&quot; ]</span><br><span class="line"></span><br><span class="line">            remove_field =&gt; [&quot;message&quot;] #删除原始日志,我试过写在mutate中,发现不起作用</span><br><span class="line">&#125;</span><br><span class="line">            #无use database语句</span><br><span class="line">          grok &#123;</span><br><span class="line">            match =&gt; [ &quot;message&quot; , &quot;^#\s+User@Host:\s+%&#123;USER:user&#125;\[[^\]]+\]\s+@\s+(?:(?&lt;clienthost&gt;\S*) )?\[(?:%&#123;IPV4:clientip&#125;)?\]\s+Id:\s+%&#123;NUMBER:row_id:int&#125;\n#\s+Query_time:\s+%&#123;NUMBER:query_time:float&#125;\s+Lock_time:\s+%&#123;NUMBER:lock_time:float&#125;\s+Rows_sent:\s+%&#123;NUMBER:rows_sent:int&#125;\s+Rows_examined:\s+%&#123;NUMBER:rows_examined:int&#125;\nSET\s+timestamp=%&#123;NUMBER:timestamp&#125;;\n\s*(?&lt;sql&gt;(?&lt;action&gt;\w+)\b.*;)\s*(?:\n#\s+Time)?.*$&quot; ]</span><br><span class="line">           remove_field =&gt; [&quot;message&quot;]  #删除原始日志,我试过写在mutate中,发现不起作用</span><br><span class="line">    &#125;</span><br><span class="line">        date &#123;</span><br><span class="line">            match =&gt; [&quot;timestamp_mysql&quot;, &quot;UNIX&quot;]</span><br><span class="line">            target =&gt; &quot;@timestamp&quot;</span><br><span class="line">        &#125;</span><br><span class="line">       mutate &#123;</span><br><span class="line">            remove_field =&gt; &quot;@version&quot;  #删除filebeat传输过来的无用字段,可以视情况删除</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><blockquote><p>也可以将2个match语句写入到一个grok中,但是我试过好像不行,有些慢日志并不能正常解析</p></blockquote><p>实际上无论是grok debbuger在线工具还是Kibana的dev tool均提供了grok的在线调试工具,可以检测和调试grok的正则匹配.例如使用上述中的grok正则检测一下是否能正常匹配到前文提到的mysql原始日志数据.可以使用kibana的dev tool工具中的Grok Debugger工具来校验:</p><p><img src="https://img2.jesse.top/image-20200729142559717.png" alt="image-20200729142559717"></p><p>上面的结构化数据输出中可以看到grok正则能正常解析原始日志中的数据,并且以json格式将日志内容映射给各字段.</p><hr><h3 id="Kibana展示"><a href="#Kibana展示" class="headerlink" title="Kibana展示"></a>Kibana展示</h3><p>我尝试在kibana中使用图形化展示query_time(也就是SQL执行时间)最长的TOP5的SQL语句,制作成可视化图表,方便动态展示.但是发现可视化的聚合图形并不能满足这个需求.</p><p>但是Kibana的Discover界面通过选定字段,也能对query_time进行排序.例如下面的截图中先选定<code>query_time</code>和<code>sql</code>这2个字段,然后再对<code>query_time</code>进行排序(在右边的query_time字段下有个倒三角形表示倒序排序).</p><p><img src="https://img2.jesse.top/image-20200729143159670.png" alt="image-20200729143159670"></p><p>然后将这个discover的筛选结果保存到Dashboard中,方便以后查看:</p><p><img src="https://img2.jesse.top/image-20200729143309655.png" alt="image-20200729143309655"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ELK收集mysql5-7慢日志&quot;&gt;&lt;a href=&quot;#ELK收集mysql5-7慢日志&quot; class=&quot;headerlink&quot; title=&quot;ELK收集mysql5.7慢日志&quot;&gt;&lt;/a&gt;ELK收集mysql5.7慢日志&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;公司ELK平台计划收集生产业务的所有mysql慢日志.由于所有环境中均使用mysql5.7版本,所以其他mysql版本的慢日志格式不在讨论范围之内.&lt;/p&gt;
&lt;p&gt;慢日志的grok正则匹配我折腾了很久,网上的大多文档中给出的logstash的grok正则其实并不能正确的解析到mysql慢日志的字段.&lt;/p&gt;
&lt;p&gt;这个博客的grok正则经过实践可行.而且filebeat,logstash的filter配置也是参考这个博客配置的:&lt;a href=&quot;https://www.cnblogs.com/minseo/p/10441913.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;博客地址&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;MySQL慢日志&quot;&gt;&lt;a href=&quot;#MySQL慢日志&quot; class=&quot;headerlink&quot; title=&quot;MySQL慢日志&quot;&gt;&lt;/a&gt;MySQL慢日志&lt;/h3&gt;&lt;p&gt;慢日志格式如下:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[work@msf-mysql-master log]$ sudo head slow_2019072103.log&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/usr/local/mysql5.7/bin/mysqld, Version: 5.7.24-log (MySQL Community Server (GPL)). started with:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tcp port: 3306  Unix socket: /mysql_log/msf/tmp/mysql.sock&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Time                 Id Command    Argument&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# Time: 2019-07-21T08:54:04.145255+08:00&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# User@Host: u_msf[u_msf] @  [10.111.10.40]  Id: 131421254&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# Query_time: 1.595300  Lock_time: 0.000031 Rows_sent: 20  Rows_examined: 809259&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;use msf_prod;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;SET timestamp=1563670444;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;SELECT `id`,`type`,`honey`,`remark`,`created_at` FROM `t_log_user_honey` WHERE `user_id` = &amp;apos;1000014423&amp;apos; ORDER BY `created_at` DESC LIMIT 20 OFFSET 0;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# Time: 2019-07-21T10:51:06.184010+08:00&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>Kibana图表制作</title>
    <link href="https://jesse.top/2020/08/25/elk/Kibana%E5%9B%BE%E8%A1%A8%E5%88%B6%E4%BD%9C/"/>
    <id>https://jesse.top/2020/08/25/elk/Kibana图表制作/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kibana图表制作"><a href="#Kibana图表制作" class="headerlink" title="Kibana图表制作"></a>Kibana图表制作</h2><p>kibana的可视化可以制作各种统计分析图表,然后合并展示到dashbord中.下面介绍一些常用的nginx的访问图表.</p><blockquote><p>在kibana7.7版本中可以配置kibana web界面的中文语言.</p></blockquote><p>编辑kibana配置文件,<code>/etc/kibana/kibana.yml</code>,加入以下配置: </p><p><code>i18n.locale: &quot;zh-CN&quot;</code></p><h4 id="一-统计过去XXX时间的访问量"><a href="#一-统计过去XXX时间的访问量" class="headerlink" title="一.统计过去XXX时间的访问量"></a>一.统计过去XXX时间的访问量</h4><p>在可视化界面,添加仪表盘图表.指定nginx的openapi access访问日志索引.</p><p>无需进行任何配置,选择右上角的时间范围,会显示计数,也就是日志量的计数</p><p><img src="https://img2.jesse.top/image-20200723154706247.png" alt="image-20200723154706247"></p><a id="more"></a><h3 id="openapi-nginx流量图"><a href="#openapi-nginx流量图" class="headerlink" title="openapi-nginx流量图"></a>openapi-nginx流量图</h3><p><img src="https://img2.jesse.top/image-20200806155157188.png" alt="image-20200806155157188"></p><h4 id="添加城市访问地图"><a href="#添加城市访问地图" class="headerlink" title="添加城市访问地图"></a>添加城市访问地图</h4><p>在<code>map</code>界面新建一个地图.在<code>road map</code>的地图上添加图层.</p><p><img src="https://img2.jesse.top/image-20200723155049422.png" alt="image-20200723155049422"></p><p>选择数据源.选择文档类型:</p><p><img src="https://img2.jesse.top/image-20200723160002287.png" alt="image-20200723160002287"></p><p>选择索引后,选择<code>geoip.location</code>字段,此时客户端地图分布自动展现出现,而且会自动计数</p><p><img src="https://img2.jesse.top/image-20200723160127185.png" alt="image-20200723160127185"></p><h4 id="Nginx状态码统计图"><a href="#Nginx状态码统计图" class="headerlink" title="Nginx状态码统计图"></a>Nginx状态码统计图</h4><p>添加饼图,使用<code>status</code>指标来统计各状态码的次数.</p><p><img src="https://img2.jesse.top/image-20200723163110993.png" alt="image-20200723163110993"></p><blockquote><p>如果没有status字段,需要去刷新索引</p></blockquote><h4 id="Nginx访问客户端TOP5"><a href="#Nginx访问客户端TOP5" class="headerlink" title="Nginx访问客户端TOP5"></a>Nginx访问客户端TOP5</h4><p>添加垂直条形图.使用geoip关键词统计各IP的访问次数,并且按降序排序</p><p><img src="https://img2.jesse.top/image-20200723163406793.png" alt="image-20200723163406793"></p><h4 id="nginx请求URL的TOP5"><a href="#nginx请求URL的TOP5" class="headerlink" title="nginx请求URL的TOP5"></a>nginx请求URL的TOP5</h4><p>添加数据图表,使用request关键词统计各URL的访问次数,并按降序排序</p><p><img src="https://img2.jesse.top/image-20200723163554578.png" alt="image-20200723163554578"></p><h4 id="nginx的cost请求时间TOP10"><a href="#nginx的cost请求时间TOP10" class="headerlink" title="nginx的cost请求时间TOP10"></a>nginx的cost请求时间TOP10</h4><p>添加垂直条形图.使用responsetime关键词做聚合,统计请求最慢的cost时间</p><p><img src="https://img2.jesse.top/image-20200723163717829.png" alt="image-20200723163717829"></p><hr><h4 id="在Dashboard中将多个图表聚合成一个大盘"><a href="#在Dashboard中将多个图表聚合成一个大盘" class="headerlink" title="在Dashboard中将多个图表聚合成一个大盘"></a>在Dashboard中将多个图表聚合成一个大盘</h4><p><img src="https://img2.jesse.top/image-20200723163906182.png" alt="image-20200723163906182"></p><p>​                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kibana图表制作&quot;&gt;&lt;a href=&quot;#Kibana图表制作&quot; class=&quot;headerlink&quot; title=&quot;Kibana图表制作&quot;&gt;&lt;/a&gt;Kibana图表制作&lt;/h2&gt;&lt;p&gt;kibana的可视化可以制作各种统计分析图表,然后合并展示到dashbord中.下面介绍一些常用的nginx的访问图表.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在kibana7.7版本中可以配置kibana web界面的中文语言.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;编辑kibana配置文件,&lt;code&gt;/etc/kibana/kibana.yml&lt;/code&gt;,加入以下配置: &lt;/p&gt;
&lt;p&gt;&lt;code&gt;i18n.locale: &amp;quot;zh-CN&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&quot;一-统计过去XXX时间的访问量&quot;&gt;&lt;a href=&quot;#一-统计过去XXX时间的访问量&quot; class=&quot;headerlink&quot; title=&quot;一.统计过去XXX时间的访问量&quot;&gt;&lt;/a&gt;一.统计过去XXX时间的访问量&lt;/h4&gt;&lt;p&gt;在可视化界面,添加仪表盘图表.指定nginx的openapi access访问日志索引.&lt;/p&gt;
&lt;p&gt;无需进行任何配置,选择右上角的时间范围,会显示计数,也就是日志量的计数&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2.jesse.top/image-20200723154706247.png&quot; alt=&quot;image-20200723154706247&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>搭建Anaconda和jupyter notebook</title>
    <link href="https://jesse.top/2020/08/25/Linux-Service/%E6%90%AD%E5%BB%BAAnaconda%E5%92%8Cjupyter%20notebook/"/>
    <id>https://jesse.top/2020/08/25/Linux-Service/搭建Anaconda和jupyter notebook/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.309Z</updated>
    
    <content type="html"><![CDATA[<h2 id="搭建Anaconda和jupyter-notebook"><a href="#搭建Anaconda和jupyter-notebook" class="headerlink" title="搭建Anaconda和jupyter notebook"></a>搭建Anaconda和jupyter notebook</h2><h3 id="一-什么是Anaconda"><a href="#一-什么是Anaconda" class="headerlink" title="一.什么是Anaconda"></a>一.什么是Anaconda</h3><p>Anaconda可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。</p><h2 id="2-特点"><a href="#2-特点" class="headerlink" title="2. 特点"></a>2. 特点</h2><p>Anaconda具有如下特点：</p><ul><li>开源</li><li>安装过程简单</li><li>高性能使用Python和R语言</li><li>免费的社区支持</li></ul><p>其特点的实现主要基于Anaconda拥有的：</p><ul><li>conda包</li><li>环境管理器</li><li>1,000+开源库</li></ul><a id="more"></a><hr><h3 id="3-Anaconda安装"><a href="#3-Anaconda安装" class="headerlink" title="3.Anaconda安装"></a>3.Anaconda安装</h3><p><a href="https://www.anaconda.com/" target="_blank" rel="noopener">Anaconda官方</a>提供了三种不同的版本,除了Individual Edition个人版免费以外,其他2种版本都是收费的.所以这里选择Anaconda个人版.</p><h4 id="3-1-Docker安装-有坑-弃用了"><a href="#3-1-Docker安装-有坑-弃用了" class="headerlink" title="3.1 Docker安装(有坑,弃用了)"></a>3.1 Docker安装(有坑,弃用了)</h4><p>我刚开始选择的是用Docker运行,使用的是Anaconda的镜像:<a href="https://hub.docker.com/r/continuumio/anaconda3" target="_blank" rel="noopener">continuumio/anaconda3</a>.用Dockerfile在此基础之上安装了多个python扩展模块自定义了一个镜像.Dockerfile内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">FROM continuumio/anaconda3:latest</span><br><span class="line">LABEL author=jessehuang</span><br><span class="line">LABEL description=&quot;自定义制作annaconda镜像&quot;</span><br><span class="line"></span><br><span class="line">#更新debian源.使用清华大学的debian 10 brust的源</span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y apt-transport-https ca-certificates</span><br><span class="line"></span><br><span class="line">ADD sources.list /etc/apt/sources.list</span><br><span class="line"></span><br><span class="line">#安装python模块的依赖.否则thriftpy的安装会出现问题</span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \</span><br><span class="line">    python-dev \</span><br><span class="line">    vim \</span><br><span class="line">    gcc \</span><br><span class="line">    g++ \</span><br><span class="line">    libsasl2-dev </span><br><span class="line"></span><br><span class="line">#安装python扩展模块</span><br><span class="line"></span><br><span class="line">ADD requirement.txt /tmp/requirement.txt </span><br><span class="line">RUN pip install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r /tmp/requirement.txt</span><br><span class="line"></span><br><span class="line">RUN /opt/conda/bin/conda install jupyter -y --quiet &amp;&amp; mkdir /opt/notebooks </span><br><span class="line">EXPOSE 8888</span><br><span class="line"></span><br><span class="line">#启动命令</span><br><span class="line">CMD [&quot;/opt/conda/bin/jupyter&quot;,&quot;notebook&quot;, &quot;--notebook-dir=/opt/notebooks&quot;,&quot;--ip=&apos;*&apos;&quot;,&quot;--port=8888&quot;,&quot;--no-browser&quot;,&quot;--allow-root&quot;]</span><br></pre></td></tr></table></figure><p>requirement.txt内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pyecharts</span><br><span class="line">pymysql</span><br><span class="line">psycopg2-binary</span><br><span class="line">six</span><br><span class="line">bit_array</span><br><span class="line">thriftpy</span><br><span class="line">thrift_sasl==0.2.1</span><br><span class="line">impyla</span><br><span class="line">jupyter_contrib_nbextensions</span><br></pre></td></tr></table></figure><p>docker部署的anaconda在运行jupyter notebook的时候提示<code>kernel restarting</code>.容器日志内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[I 07:25:59.512 NotebookApp] Restoring connection for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.541 NotebookApp] Starting buffering for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.565 NotebookApp] Restoring connection for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.595 NotebookApp] Starting buffering for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.620 NotebookApp] Restoring connection for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[W 07:26:04.401 NotebookApp] Replacing stale connection: 63b3942c-a225-4692-8989-893af2c992cc:743009a3e3684e64a9b9fcceedc30775</span><br><span class="line">[W 07:26:05.062 NotebookApp] Replacing stale connection: a9543d90-08b5-4395-8a3c-f7bbec9f44a1:0bc5b6496d59406b82ab54a181db7215</span><br></pre></td></tr></table></figure><p>这个故障在搜遍了Google和baidu后也无解,尝试了网上各种解决办法也不行.所以果断弃掉,转而使用Linux虚拟机部署</p><h3 id="3-2-Linux部署安装-成功"><a href="#3-2-Linux部署安装-成功" class="headerlink" title="3.2 Linux部署安装(成功)"></a>3.2 Linux部署安装(成功)</h3><p>Anaconda的<a href="https://docs.anaconda.com/anaconda/install/linux/" target="_blank" rel="noopener">安装官方文档</a>.建议已root用户安装,或者你的普通用户有执行<code>/home/$(USER)/anaconda3/bin/python3</code>的sudo权限</p><p>1.首先下载python的anaconda安装脚本,建议选择python3的版本</p><p><a href="https://www.anaconda.com/products/individual#linux" target="_blank" rel="noopener">https://www.anaconda.com/products/individual#linux</a></p><p>2.安装依赖包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install libXcomposite libXcursor libXi libXtst libXrandr alsa-lib mesa-libEGL libXdamage mesa-libGL libXScrnSaver</span><br></pre></td></tr></table></figure><p>3.执行下载下来的脚本文件.根据提示,一直选择默认就可以了,官方不建议更改安装路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash ~/Anaconda3-2020.02-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><blockquote><p>We recommend you accept the default install location. Do not choose the path as /usr for the Anaconda/Miniconda installation.</p></blockquote><p>4.如果提示<code>Thank you for installing Anaconda&lt;2 or 3&gt;!</code>则说明安装完成,应用环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><p>5.验证安装结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">condal list #显示已经安装的包和版本好</span><br></pre></td></tr></table></figure><p>至此,Anaconda和anaconda自带的jupyter notebook都已经安装完了</p><hr><h3 id="4-jupyter-notebook-扩展模块安装"><a href="#4-jupyter-notebook-扩展模块安装" class="headerlink" title="4.jupyter notebook 扩展模块安装"></a>4.jupyter notebook 扩展模块安装</h3><p>公司的BI团队需要使用jupyter notebook访问MySQL、pgsql、hive,画图等工作,所以需要安装python的部分扩展模块.首先查看服务器上pip版本.需要使用python3的pip来安装模块</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#显示pip版本是python3</span><br><span class="line">(base) [root@anaconda ~]# pip --version</span><br><span class="line">pip 20.0.2 from /root/anaconda3/lib/python3.7/site-packages/pip (python 3.7)</span><br><span class="line">(base) [root@anaconda ~]#</span><br></pre></td></tr></table></figure><ol><li>安装依赖文件,否则安装<code>thriftpy</code>模块会报错</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install gcc-c++ gcc python3-devel python-dev cyrus-sasl cyrus-sasl-devel cyrus-sasl-lib</span><br></pre></td></tr></table></figure><blockquote><p>如果是ubuntu系统,则需要安装如下安装包: apt-get install -y python-dev gcc g++ libsasl2-dev</p></blockquote><ol start="2"><li>编写requirement文件,将所需要安装的python扩展模块加入到文件中</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pyecharts</span><br><span class="line">pymysql</span><br><span class="line">psycopg2-binary</span><br><span class="line">six</span><br><span class="line">bit_array</span><br><span class="line">thriftpy</span><br><span class="line">thrift_sasl==0.2.1</span><br><span class="line">impyla</span><br><span class="line">jupyter_contrib_nbextensions</span><br></pre></td></tr></table></figure><p>3.安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r requirement.txt</span><br></pre></td></tr></table></figure><p>安装完依赖包<code>jupyter_contrib_nbextensions</code>以后,安装一个服务:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter-contrib-nbextension install --user</span><br></pre></td></tr></table></figure><p>4.初始化jupyter配置文件,会在默认路径下初始化一个配置文件<code>/root/.jupyter/jupyter_notebook_config.py</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure><p>5.编辑文件,修改如下配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.ip=&apos;当前服务器IP&apos;</span><br><span class="line">c.NotebookApp.notebook_dir = u&apos;/data/notebooks&apos; #jupyter笔记的工作目录</span><br><span class="line">c.NotebookApp.open_browser = False #服务端无需启动浏览器</span><br><span class="line">c.NotebookApp.port = 80 #监听端口,默认是8888</span><br><span class="line">c.NotebookApp.allow_root = True  #允许root用户运行jupyter notebook</span><br><span class="line">c.NotebookApp.allow_origin = &apos;*&apos; #允许所有来源访问,解决跨域问题</span><br><span class="line">c.NotebookApp.quit_button = False #关闭jupyter notebook浏览器界面的quit按钮功能.因为quit按钮会关闭jupyter服务</span><br></pre></td></tr></table></figure><p>6.启动服务.记下pid号,后面要重启</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup jupyter-notebook --config=/root/.jupyter/jupyter_notebook_config.py &amp;</span><br></pre></td></tr></table></figure><p>7.打开浏览器,输入IP地址,此时会进入jupyter的界面.要求输入Token,或者使用Token设置一个密码</p><p><img src="https://img2.jesse.top/image-20200708105619499.png" alt="image-20200708105619499"></p><p>8.根据提示,使用命令<code>jupyter notebook list</code>查看Token</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">base) root@anaconda:/# jupyter notebook list</span><br><span class="line">Currently running servers:</span><br><span class="line">http://localhost:8888/?token=5469d940ce3a70950299e5c907e1d47b09acc61457348cd9 :: /data/notebooks</span><br></pre></td></tr></table></figure><p>9.拿到Token后,在浏览器中下方位置设置一个新密码.</p><p><img src="https://img2.jesse.top/image-20200708105756748.png" alt="image-20200708105756748"></p><p>10.设置完密码后,成功登陆了jupyter的工作界面.此时kill掉jupyter进程,重启</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#关闭进程</span><br><span class="line">kill $PID</span><br><span class="line"></span><br><span class="line">#重新启动</span><br><span class="line">nohup jupyter-notebook --config=/root/.jupyter/jupyter_notebook_config.py &amp;</span><br></pre></td></tr></table></figure><p>11.此时再次打开浏览器,就提示输入密码</p><p><img src="https://img2.jesse.top/image-20200708110014017.png" alt="image-20200708110014017"></p><p>12.输入密码后,成功登陆</p><p><img src="https://img2.jesse.top/image-20200708110048546.png" alt="image-20200708110048546"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;搭建Anaconda和jupyter-notebook&quot;&gt;&lt;a href=&quot;#搭建Anaconda和jupyter-notebook&quot; class=&quot;headerlink&quot; title=&quot;搭建Anaconda和jupyter notebook&quot;&gt;&lt;/a&gt;搭建Anaconda和jupyter notebook&lt;/h2&gt;&lt;h3 id=&quot;一-什么是Anaconda&quot;&gt;&lt;a href=&quot;#一-什么是Anaconda&quot; class=&quot;headerlink&quot; title=&quot;一.什么是Anaconda&quot;&gt;&lt;/a&gt;一.什么是Anaconda&lt;/h3&gt;&lt;p&gt;Anaconda可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。&lt;/p&gt;
&lt;h2 id=&quot;2-特点&quot;&gt;&lt;a href=&quot;#2-特点&quot; class=&quot;headerlink&quot; title=&quot;2. 特点&quot;&gt;&lt;/a&gt;2. 特点&lt;/h2&gt;&lt;p&gt;Anaconda具有如下特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;开源&lt;/li&gt;
&lt;li&gt;安装过程简单&lt;/li&gt;
&lt;li&gt;高性能使用Python和R语言&lt;/li&gt;
&lt;li&gt;免费的社区支持&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其特点的实现主要基于Anaconda拥有的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;conda包&lt;/li&gt;
&lt;li&gt;环境管理器&lt;/li&gt;
&lt;li&gt;1,000+开源库&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Linux-Service" scheme="https://jesse.top/categories/Linux-Service/"/>
    
    
      <category term="Anaconda" scheme="https://jesse.top/tags/Anaconda/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---Docker卷挂载</title>
    <link href="https://jesse.top/2020/07/04/docker/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0---%E5%8D%B7%E6%8C%82%E8%BD%BD/"/>
    <id>https://jesse.top/2020/07/04/docker/docker学习笔记---卷挂载/</id>
    <published>2020-07-04T13:59:58.000Z</published>
    <updated>2020-07-03T23:31:58.131Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker学习笔记—卷挂载"><a href="#docker学习笔记—卷挂载" class="headerlink" title="docker学习笔记—卷挂载"></a>docker学习笔记—卷挂载</h2><h3 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h3><p>回顾一下docker网络的相关知识:</p><ul><li>docker默认所有容器连接到docker0这个bridge虚拟交换机,连接到同一个虚拟交换机的所有容器网络可以互访</li><li>docker可以指定一个bridge虚拟交换机,只需要在<code>docker run</code>命令中,使用<code>--network</code>参数</li><li>连接到指定bridge虚拟机交换机的所有容器之间不仅仅可以互访,而且还可以直接访问对方容器的主机名</li><li>在<code>docker run</code>命令中使用<code>-p</code>参数可以将容器内部端口映射给外部宿主机,使外部客户端可以访问容器内提供的服务</li><li>可以使用多个<code>-p</code>选项暴露出多个端口,<code>-p</code>暴露端口的格式如下:<ul><li><code>-p Container_Port</code> (将宿主机上随机的一个端口,映射到后端指定的端口.例如: -p 80)</li><li><code>-p Host_Port:Container_Port</code>(这是使用最多的方式,将宿主机上指定的一个端口,映射给容器内某个端口.例如: -p 80:80)</li><li><code>-p range_Port:range_Port</code>(映射一组连续的端口.例如: -p 1100-1110:1100-1110)</li></ul></li><li>无法给运行中的容器,增加端口映射.所以想给当前运行中的容器增加映射端口,只能删除容器,重新run一个</li><li>端口映射的本质是添加nat转发记录</li><li>不是所有的容器都需要映射端口.</li></ul><a id="more"></a><hr><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Docker容器是有生命周期的.一个容器就是一个应用,应用始终有停止和删除的时候.一旦容器被删除,则容器内产生的任何数据都会被删除.</p><p>大部分应用都是有状态的服务,比如mysql,redis,nginx(只作为反向代理的Nginx除外),这类容器一旦被删除,新创建的容器无法继续工作.所以任何有状态的应用都需要将数据持久存储.</p><p>另外,容器内部都是微型的OS系统,只提供应用程序所必须的依赖环境和进程文件.所以缺少各种调试工具(vim.telnet,ping)等.所以Docker的初衷是把容器看做是一个单一的应用,无需进入容器做任何操作.但是应用的配置文件可能需要经常修改,(比如nginx虚拟主机,php-fpm配置文件等).进入容器内部操作并不是提倡的办法.</p><hr><h3 id="Docker-卷介绍"><a href="#Docker-卷介绍" class="headerlink" title="Docker 卷介绍"></a>Docker 卷介绍</h3><p>基于以上难题,为了实现有状态应用的持久化数据存储和频繁修改容器内部文件.Docker提供了数据卷(Volume)实现.</p><p>卷(Volume)是一个特殊的目录,或者一个特殊的文件系统,将宿主机本地的文件系统(或者目录)映射到一个或者多个容器内部的某个目录.将两者进行绑定,从而实现容器数据持久化.</p><p><strong>Docker卷特性</strong></p><ul><li><p>卷是独立的,不在容器生命周期当中.这意味着即使容器删除,映射给容器的卷和卷当中的数据仍然会存在.</p></li><li><p>卷可以同时映射(挂载)到多个容器,多个容器共享该卷的数据.</p></li><li>卷实现了容器的迁移和移植</li><li>对数据卷的修改会立即在容器中生效</li><li>对数据卷的更新和修改,不会影响Docker镜像</li></ul><p><img src="https://img2.jesse.top/docker-volume2.png" alt="image-20200703104635678"></p><p><strong>Docker 卷作用</strong></p><p><img src="https://img2.jesse.top/docker-volume1.png" alt="image-20200703104433942"></p><p><img src="https://img2.jesse.top/docker-volume3.png" alt="image-20200703104851073"></p><hr><h3 id="Docker-卷类型"><a href="#Docker-卷类型" class="headerlink" title="Docker 卷类型"></a>Docker 卷类型</h3><p>Docker有两种卷类型:</p><p>1.指定绑定挂载卷</p><ul><li>用户指定宿主机上的目录,映射到容器某个目录下</li></ul><p>2.Docker自行管理的挂载卷</p><ul><li>不需要指定苏追究上的目录,Docker会自行在默认的<code>/var/lib/docker/volumes/container_ID/</code>下创建一个目录映射给容器</li></ul><p><img src="https://img2.jesse.top/docker-volume4.png" alt="image-20200703105105712"></p><hr><h3 id="在容器中使用挂载卷"><a href="#在容器中使用挂载卷" class="headerlink" title="在容器中使用挂载卷"></a>在容器中使用挂载卷</h3><p><img src="https://img2.jesse.top/docker-volume5.png" alt="image-20200703105412500"></p><hr><h3 id="实验演示"><a href="#实验演示" class="headerlink" title="实验演示"></a>实验演示</h3><ul><li><p><strong>使用docker自行管理的挂载卷映射</strong></p><ul><li>将本地目录映射到容器的/data目录.使用<code>-v /data</code>命令.在挂载卷的时候,并没有指定宿主机上的路径</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[work@docker-demon ~]$ docker run -d --name b1  -v /data busybox sleep 1d</span><br><span class="line">57bb4beeed7e0eb090043c396066be204f5ed93ec031ea132aa983b9c45ad9b2</span><br></pre></td></tr></table></figure><ul><li>通过<code>docker inspect b1</code>命令可以看到映射关系.Docker在<code>/var/lib/docker/volumes/Volume_ID</code>目录下创建了一个_data目录,并且挂载到b1容器的/data目录下</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;Mounts&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;Type&quot;: &quot;volume&quot;,</span><br><span class="line">                &quot;Name&quot;: &quot;ca2bd3d8ac222f2eb95e63cf98a961b47aa517063386a4e4cc4431dc0b2c43c8&quot;,</span><br><span class="line">                &quot;Source&quot;: &quot;/var/lib/docker/volumes/ca2bd3d8ac222f2eb95e63cf98a961b47aa517063386a4e4cc4431dc0b2c43c8/_data&quot;,</span><br><span class="line">                &quot;Destination&quot;: &quot;/data&quot;,</span><br></pre></td></tr></table></figure><ul><li>在宿主机的目录_data目录下新创建一个<code>test.txt</code>文件,可以看到立即同步到b1容器内的/data目录下</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#在宿主机</span><br><span class="line">[root@docker-demon ~]# touch /var/lib/docker/volumes/ca2bd3d8ac222f2eb95e63cf98a961b47aa517063386a4e4cc4431dc0b2c43c8/_data/test.txt</span><br><span class="line"></span><br><span class="line">#在b1容器内部</span><br><span class="line">[work@docker-demon ~]$ docker exec -it b1 sh</span><br><span class="line">/ # ls /data</span><br><span class="line">test.txt</span><br><span class="line">/ #</span><br></pre></td></tr></table></figure><blockquote><p>反过来,在容器内部对该目录下进行任何数据修改,也会同步到宿主机相关目录下</p></blockquote><h5 id="容器被删除-数据还会继续保留吗"><a href="#容器被删除-数据还会继续保留吗" class="headerlink" title="容器被删除,数据还会继续保留吗?"></a>容器被删除,数据还会继续保留吗?</h5><ul><li>删除容器测试一下</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@docker-demon ~]$ docker stop b1</span><br><span class="line">b1</span><br><span class="line">[work@docker-demon ~]$ docker rm b1</span><br><span class="line">b1</span><br></pre></td></tr></table></figure><p>宿主机本地目录依然存在,数据并没有被随之删除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-demon ~]# ll /var/lib/docker/volumes/ca2bd3d8ac222f2eb95e63cf98a961b47aa517063386a4e4cc4431dc0b2c43c8/_data</span><br><span class="line">total 0</span><br><span class="line">-rw-r--r-- 1 root root 0 Jul  3 11:03 test.txt</span><br></pre></td></tr></table></figure><h5 id="虽然数据依然存在-但是也可以看出来这种挂载方式非常不友好之处在于-时间一长很难找到当时映射的目录在宿主机的具体路径-特别是容器删除以后-也就是说宿主机的挂载卷很难维护"><a href="#虽然数据依然存在-但是也可以看出来这种挂载方式非常不友好之处在于-时间一长很难找到当时映射的目录在宿主机的具体路径-特别是容器删除以后-也就是说宿主机的挂载卷很难维护" class="headerlink" title="虽然数据依然存在,但是也可以看出来这种挂载方式非常不友好之处在于,时间一长很难找到当时映射的目录在宿主机的具体路径.特别是容器删除以后..也就是说宿主机的挂载卷很难维护"></a>虽然数据依然存在,但是也可以看出来这种挂载方式非常不友好之处在于,时间一长很难找到当时映射的目录在宿主机的具体路径.特别是容器删除以后..也就是说宿主机的挂载卷很难维护</h5><hr></li><li><p><strong>指定目录映射</strong></p></li></ul><p>第二种方式是将宿主机上指定的一个目录(这个目录可以事先并不存在,Docker会自动创建)映射到容器内部</p><p>映射方式: <code>-v HOST_DIR:CONTAINER_DIR</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[work@docker-demon ~]$ docker run -d --name b1  -v/data/b1:/data busybox sleep 1d</span><br><span class="line">57bb4beeed7e0eb090043c396066be204f5ed93ec031ea132aa983b9c45ad9b2</span><br></pre></td></tr></table></figure><p>此时,宿主机会自动创建一个<code>/data/b1</code>的目录,并且映射给容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-demon ~]# docker inspect b1</span><br><span class="line">&quot;Mounts&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;Type&quot;: &quot;bind&quot;,</span><br><span class="line">                &quot;Source&quot;: &quot;/data/b1&quot;,</span><br><span class="line">                &quot;Destination&quot;: &quot;/data&quot;,</span><br></pre></td></tr></table></figure><p>在宿主机上的<code>/data/b1</code>目录下创建一个文件,会同步到容器的<code>/data/</code>目录.反之,在容器内部创建文件,也会同步到宿主机</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-demon ~]# cat /data/b1/hello.txt</span><br><span class="line">hello world</span><br><span class="line"></span><br><span class="line">#容器内部会产生一个hello.txt文件</span><br><span class="line">[root@docker-demon _data]# docker exec -it b1 sh</span><br><span class="line">/ # ls /data</span><br><span class="line">hello.txt   hello1.txt  opsdir</span><br><span class="line">/ # cat /data/hello.txt</span><br><span class="line">hello world</span><br></pre></td></tr></table></figure><ul><li><strong>volume卷映射</strong></li></ul><p>这种方法也可以映射卷,具体操作如下</p><p>1.先创建一个volume卷.命令:<code>docker volume creante VOLUME_NAME</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-demon ~]# docker volume create jesse</span><br></pre></td></tr></table></figure><p>2.然后将新创建的jesse卷映射到容器,映射方式大体一样</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[work@docker-demon ~]$ docker run -d --name b3  -v jesse:/data busybox sleep 1d</span><br><span class="line">[root@docker-demon ~]# docker inspect b3</span><br><span class="line">&quot;Mounts&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;Type&quot;: &quot;volume&quot;,</span><br><span class="line">                &quot;Name&quot;: &quot;jesse&quot;,</span><br><span class="line">                &quot;Source&quot;: &quot;/var/lib/docker/volumes/jesse/_data&quot;,</span><br><span class="line">                &quot;Destination&quot;: &quot;/data&quot;,</span><br><span class="line">                &quot;Driver&quot;: &quot;local&quot;,</span><br><span class="line">                &quot;Mode&quot;: &quot;z&quot;,</span><br><span class="line">                &quot;RW&quot;: true,</span><br><span class="line">                &quot;Propagation&quot;: &quot;&quot;</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure><blockquote><p>还可以通过这种方式映射: docker run -d –name b1  –mount source=jesse,targe=/data busybox sleep 1d</p></blockquote><p>这种方式使用的也比较少,因为<code>/var/lib/docker</code>的docker默认存储路径一般在Linux系统的/根目录下,非数据目录.所以可能根目录的容量并不大,而且扩展相对来说不便</p><hr><h3 id="卷映射注意事项"><a href="#卷映射注意事项" class="headerlink" title="卷映射注意事项"></a>卷映射注意事项</h3><p>以上三种卷映射方式均能实现容器数据持久化,也可以将宿主机上的同一个目录映射给多个容器,实现容器间数据共享.但是使用最多的还是第二种指定目录映射方式.具体指定宿主机上的一个目录映射给容器,而不是Docker自行管理卷.</p><p>以下是卷映射具体工作中的注意事项:</p><h5 id="1-不能映射到docker容器的相对目录"><a href="#1-不能映射到docker容器的相对目录" class="headerlink" title="1.不能映射到docker容器的相对目录."></a>1.不能映射到docker容器的相对目录.</h5><p>对于宿主机来说,并不知道docker容器的当前工作目录,所以挂载到docker容器的相对目录是没有意义的.</p><h5 id="2-注意docker容器的目标路径是否已经有数据"><a href="#2-注意docker容器的目标路径是否已经有数据" class="headerlink" title="2.注意docker容器的目标路径是否已经有数据."></a>2.注意docker容器的目标路径是否已经有数据.</h5><p>这里要特别注意一下.docker映射的时候尽量映射到容器的空目录下.或者确定映射后不影响容器启动.</p><p><strong>不同的卷挂载方式对容器已有数据的目录挂载效果不一样:</strong></p><p>这里使用nginx的容器来演示,nginx镜像的<code>/etc/nginx</code>目录下有<code>nginx.conf</code>等文件.</p><ul><li>实验一. 宿主机上存在数据的目录挂载到容器里存在数据的目录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#将宿主机上的/data/b1目录映射到nginx容器的/etc/nginx目录,</span><br><span class="line">#/data/b1目录存在数据:</span><br><span class="line">[work@docker-demon ~]$ ll /data/b1</span><br><span class="line">total 8</span><br><span class="line">-rw-r--r-- 1 root root    0 Jul  3 17:00 hello1.txt</span><br><span class="line">-rwxrwxrwx 1 root root   12 Jul  3 16:57 hello.txt</span><br><span class="line">drwxr-xr-x 2 1000 1000 4096 Jul  3 17:22 opsdir</span><br><span class="line"></span><br><span class="line">#映射给nginx容器</span><br><span class="line">[work@docker-demon ~]$ docker run --name nginx -d -p 80:80 -v /data/b1:/etc/nginx nginx:alpine</span><br><span class="line">2966b897332673a0429614428fb62638a273318d1e29dbe7bda9761002d9b4f4</span><br><span class="line"></span><br><span class="line">#此时容器无法正常启动,因为没有找到nginx.conf配置文件</span><br><span class="line">[work@docker-demon ~]$ docker logs nginx</span><br><span class="line">2020/07/03 11:13:40 [emerg] 1#1: open() &quot;/etc/nginx/nginx.conf&quot; failed (2: No such file or directory)</span><br><span class="line">nginx: [emerg] open() &quot;/etc/nginx/nginx.conf&quot; failed (2: No such file or directory)</span><br><span class="line"></span><br><span class="line">#删除容器,重新映射.这次不后台运行,而是使用-it交互式界面,运行sh命令,进入sh环境.</span><br><span class="line">[work@docker-demon ~]$ docker run --name nginx -it -p 80:80 -v /data/b1:/etc/nginx nginx:alpine sh</span><br><span class="line">/ #</span><br><span class="line"></span><br><span class="line">#查看/etc/nginx目录</span><br><span class="line">/ # ls /etc/nginx</span><br><span class="line">hello.txt   hello1.txt  opsdir</span><br><span class="line">/ #</span><br></pre></td></tr></table></figure><p>经过上次测试,发现指定目录挂载方式,会将宿主机上的目录数据覆盖容器下的目录的数据</p><ul><li>实验二.宿主机空目录挂载到容器里存在数据的目录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#宿主机的/data/b2为空目录</span><br><span class="line">[work@docker-demon ~]$ ls /data/b2</span><br><span class="line">ls: cannot access /data/b2: No such file or directory</span><br><span class="line"></span><br><span class="line">#映射到容器的/etc/nginx目录下,发现该数据已经被宿主机的空目录覆盖</span><br><span class="line">[work@docker-demon ~]$ docker run -it --name nginx -v /data/b2:/etc/nginx nginx:alpine sh</span><br><span class="line">/ # ls /etc/nginx</span><br><span class="line">/ #</span><br></pre></td></tr></table></figure><p>通过以上实验,发现对于指定卷映射方式来说,无论宿主机上的目录是否为空,挂载到容器的目录后,都会覆盖容器目录下的数据.</p><hr><ul><li>实验三.宿主机上存在数据的Volume卷挂载到容器里存在数据的目录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#创建一个卷.并且往卷里写入数据</span><br><span class="line">[work@docker-demon ~]$ docker volume create jesse_nginx</span><br><span class="line">jesse_nginx</span><br><span class="line"></span><br><span class="line">#查看jesse_nginx卷的路径</span><br><span class="line">[work@docker-demon ~]$ docker inspect jesse_nginx</span><br><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;CreatedAt&quot;: &quot;2020-07-03T22:36:23+08:00&quot;,</span><br><span class="line">        &quot;Driver&quot;: &quot;local&quot;,</span><br><span class="line">        &quot;Labels&quot;: &#123;&#125;,</span><br><span class="line">        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/jesse_nginx/_data&quot;,</span><br><span class="line">        &quot;Name&quot;: &quot;jesse_nginx&quot;,</span><br><span class="line">        &quot;Options&quot;: &#123;&#125;,</span><br><span class="line">        &quot;Scope&quot;: &quot;local&quot;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">#往卷的目录/var/lib/docker/volumes/jesse_nginx/_data写入数据</span><br><span class="line">[root@docker-demon ~]# sudo echo &quot;hello world&quot; &gt; /var/lib/docker/volumes/jesse_nginx/_data/test.txt</span><br><span class="line"></span><br><span class="line">#新建一个测试文件</span><br><span class="line">[root@docker-demon ~]# ls /var/lib/docker/volumes/jesse_nginx/_data</span><br><span class="line">test.txt</span><br></pre></td></tr></table></figure><p>将该卷映射给nginx容器的/etc/nginx目录.发现容器内目录已经被卷下的数据覆盖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-demon ~]# docker run -it  -v jesse_nginx:/etc/nginx nginx:alpine sh</span><br><span class="line">/ # ls /etc/nginx</span><br><span class="line">test.txt</span><br><span class="line">/ #</span><br></pre></td></tr></table></figure><ul><li>实验4.将空目录的卷挂载到容器里存在数据的目录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#创建一个新的卷.此时卷路径为空</span><br><span class="line">[root@docker-demon ~]# docker volume create jesse_nginx2</span><br><span class="line">jesse_nginx2</span><br><span class="line"></span><br><span class="line">#挂载到容器的/etc/nginx目录下.发现并没有覆盖容器内的数据</span><br><span class="line">[root@docker-demon ~]# docker run -it  -v jesse_nginx2:/etc/nginx nginx:alpine sh</span><br><span class="line">/ # ls /etc/nginx</span><br><span class="line">conf.d          fastcgi_params  koi-win         modules         scgi_params     win-utf</span><br><span class="line">fastcgi.conf    koi-utf         mime.types      nginx.conf      uwsgi_params</span><br><span class="line">/ #</span><br><span class="line"></span><br><span class="line">#查看宿主机jesse_nginx2卷下的数据,数据确实存在</span><br><span class="line">[root@docker-demon ~]# ls /var/lib/docker/volumes/jesse_nginx2/_data/</span><br><span class="line">conf.d  fastcgi.conf  fastcgi_params  koi-utf  koi-win  mime.types  modules  nginx.conf  scgi_params  uwsgi_params  win-utf</span><br><span class="line">[root@docker-demon ~]#</span><br></pre></td></tr></table></figure><p>通过以上2个实验得出.当挂载一个空目录的Volume卷到容器时,并不会覆盖容器里原本已存在的数据,但是如果是非空目录,仍然会覆盖</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker学习笔记—卷挂载&quot;&gt;&lt;a href=&quot;#docker学习笔记—卷挂载&quot; class=&quot;headerlink&quot; title=&quot;docker学习笔记—卷挂载&quot;&gt;&lt;/a&gt;docker学习笔记—卷挂载&lt;/h2&gt;&lt;h3 id=&quot;回顾&quot;&gt;&lt;a href=&quot;#回顾&quot; class=&quot;headerlink&quot; title=&quot;回顾&quot;&gt;&lt;/a&gt;回顾&lt;/h3&gt;&lt;p&gt;回顾一下docker网络的相关知识:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;docker默认所有容器连接到docker0这个bridge虚拟交换机,连接到同一个虚拟交换机的所有容器网络可以互访&lt;/li&gt;
&lt;li&gt;docker可以指定一个bridge虚拟交换机,只需要在&lt;code&gt;docker run&lt;/code&gt;命令中,使用&lt;code&gt;--network&lt;/code&gt;参数&lt;/li&gt;
&lt;li&gt;连接到指定bridge虚拟机交换机的所有容器之间不仅仅可以互访,而且还可以直接访问对方容器的主机名&lt;/li&gt;
&lt;li&gt;在&lt;code&gt;docker run&lt;/code&gt;命令中使用&lt;code&gt;-p&lt;/code&gt;参数可以将容器内部端口映射给外部宿主机,使外部客户端可以访问容器内提供的服务&lt;/li&gt;
&lt;li&gt;可以使用多个&lt;code&gt;-p&lt;/code&gt;选项暴露出多个端口,&lt;code&gt;-p&lt;/code&gt;暴露端口的格式如下:&lt;ul&gt;
&lt;li&gt;&lt;code&gt;-p Container_Port&lt;/code&gt; (将宿主机上随机的一个端口,映射到后端指定的端口.例如: -p 80)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-p Host_Port:Container_Port&lt;/code&gt;(这是使用最多的方式,将宿主机上指定的一个端口,映射给容器内某个端口.例如: -p 80:80)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-p range_Port:range_Port&lt;/code&gt;(映射一组连续的端口.例如: -p 1100-1110:1100-1110)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;无法给运行中的容器,增加端口映射.所以想给当前运行中的容器增加映射端口,只能删除容器,重新run一个&lt;/li&gt;
&lt;li&gt;端口映射的本质是添加nat转发记录&lt;/li&gt;
&lt;li&gt;不是所有的容器都需要映射端口.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Letsencrypt证书OCSP域名国内无法访问问题</title>
    <link href="https://jesse.top/2020/07/01/Linux-Basic/SSL/letsencrypt%E8%AF%81%E4%B9%A6OCSP%E5%9F%9F%E5%90%8D%E5%9B%BD%E5%86%85%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE%E9%97%AE%E9%A2%98/"/>
    <id>https://jesse.top/2020/07/01/Linux-Basic/SSL/letsencrypt证书OCSP域名国内无法访问问题/</id>
    <published>2020-07-01T03:59:58.000Z</published>
    <updated>2020-07-03T23:31:58.130Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Letsencrypt证书OCSP域名国内无法访问问题"><a href="#Letsencrypt证书OCSP域名国内无法访问问题" class="headerlink" title="Letsencrypt证书OCSP域名国内无法访问问题"></a>Letsencrypt证书OCSP域名国内无法访问问题</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>近期公司线上APP,小程序的h5页面在IOS移动设备上加载速度非常慢.安卓设备不受影响.调试显示 TCP SSL 相关时间在3s以上.</p><hr><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>经排查发现是国内Let’s Encrypt免费SSL证书机构的OSCP域名 <a href="http://ocsp.int-x3.letsencrypt.org的CNAME解析在国内被DNS污染..安卓和chrome浏览器默认不在客户端检测OCSP,所以没有影响.但是移动IOS设备和Safari浏览器则会出现这种故障" target="_blank" rel="noopener">http://ocsp.int-x3.letsencrypt.org的CNAME解析在国内被DNS污染..安卓和chrome浏览器默认不在客户端检测OCSP,所以没有影响.但是移动IOS设备和Safari浏览器则会出现这种故障</a>.</p><p>参考V2EX的论坛:<a href="https://www.v2ex.com/t/658605" target="_blank" rel="noopener">https://www.v2ex.com/t/658605</a></p><p>使用国内的公众DNS服务器解析该域名结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  dig @114.114.114.114 ocsp.int-x3.letsencrypt.org</span><br><span class="line"></span><br><span class="line">....略....</span><br><span class="line"></span><br><span class="line">;; ANSWER SECTION:</span><br><span class="line">ocsp.int-x3.letsencrypt.org. 31INCNAMEocsp.int-x3.letsencrypt.org.edgesuite.net.</span><br><span class="line">ocsp.int-x3.letsencrypt.org.edgesuite.net. 31 IN CNAME a771.dscq.akamai.net.</span><br><span class="line">a771.dscq.akamai.net.31INA31.13.69.86</span><br><span class="line"></span><br><span class="line">;; Query time: 15 msec</span><br><span class="line">;; SERVER: 114.114.114.114#53(114.114.114.114)</span><br><span class="line">;; WHEN: Wed Jul 01 16:40:14 CST 2020</span><br><span class="line">;; MSG SIZE  rcvd: 158</span><br></pre></td></tr></table></figure><p>这个<code>a771.dscq.akamai.net</code> 的CNAME域名解析的IP地址并不是akamai的IP地址.也无法访问这个iP</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  ping 31.13.69.86</span><br><span class="line">PING 31.13.69.86 (31.13.69.86): 56 data bytes</span><br><span class="line">Request timeout for icmp_seq 0</span><br><span class="line">Request timeout for icmp_seq 1</span><br></pre></td></tr></table></figure><a id="more"></a><p>正确的IP地址应该是:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  dig @8.8.8.8 ocsp.int-x3.letsencrypt.org</span><br><span class="line"></span><br><span class="line">;; ANSWER SECTION:</span><br><span class="line">ocsp.int-x3.letsencrypt.org. 4085 INCNAMEocsp.int-x3.letsencrypt.org.edgesuite.net.</span><br><span class="line">ocsp.int-x3.letsencrypt.org.edgesuite.net. 1783IN CNAME a771.dscq.akamai.net.</span><br><span class="line">a771.dscq.akamai.net.19INA23.210.215.97</span><br><span class="line">a771.dscq.akamai.net.19INA23.210.215.80</span><br></pre></td></tr></table></figure><hr><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><ul><li>更换证书—-使用阿里云的免费DV域名证书(赛门铁克)没有这个问题</li><li>服务器做ocsp stapling.让ocscp请求通过自己的服务器. (这个办法我尝试过很久都没有成功)</li><li>由于只是DNS被污染,.所以可以在nginx服务器绑定hosts.解析到正确的IP地址上.(我暂时使用这个办法)</li></ul><hr><h3 id="服务器做ocsp-stapling"><a href="#服务器做ocsp-stapling" class="headerlink" title="服务器做ocsp stapling"></a>服务器做ocsp stapling</h3><p>百度上有大量的相关配置文档,这里列举3种:</p><p><strong>第一种</strong>: 获取OCSP响应,配置ssl_stapling_file.点击查看<a href="https://holmesian.org/letsencrypt-ocsp-fix" target="_blank" rel="noopener">文档地址</a></p><p>我在这一步的时候报错:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">openssl ocsp -no_nonce \</span><br><span class="line">                -respout /root/.acme.sh/meta.bi.doweidu.com/ocsp_res.der \</span><br><span class="line">                -issuer /root/.acme.sh/meta.bi.doweidu.com/ca.cer \</span><br><span class="line">                -cert /root/.acme.sh/meta.bi.doweidu.com/meta.bi.doweidu.com.cer \</span><br><span class="line">                -url http://ocsp.int-x3.letsencrypt.org/ \</span><br><span class="line">                -header &quot;HOST&quot; &quot;ocsp.int-x3.letsencrypt.org&quot;</span><br></pre></td></tr></table></figure><p>报错内容如下,去掉<code>-header &quot;HOST&quot; &quot;ocsp.int-x3.letsencrypt.org</code> 仍然会报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Response Verify Failure</span><br><span class="line">139961608943504:error:27069076:OCSP routines:OCSP_basic_verify:signer certificate not found:ocsp_vfy.c:92:</span><br><span class="line">/root/.acme.sh/meta.bi.doweidu.com/meta.bi.doweidu.com.cer: good</span><br><span class="line">This Update: Jun 28 06:00:00 2020 GMT</span><br><span class="line">Next Update: Jul  5 06:00:00 2020 GMT</span><br></pre></td></tr></table></figure><p><strong>第二种</strong>: 配置根证书,中间证书文件.配置<code>ssl_trusted_certificate</code>: <a href="https://www.songhaifeng.com/razt/193.html" target="_blank" rel="noopener">文档地址</a></p><p>但是我检测OCSP的时候,仍然提示<code>OCSP response: no response sent</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openssl s_client -connect meta.bi.doweidu.com:443 -status -tlsextdebug &lt; /dev/null 2&gt;&amp;1 | grep -i &quot;OCSP response&quot;</span><br><span class="line">OCSP response: no response sent</span><br></pre></td></tr></table></figure><p><strong>第三种</strong>:直接配置nginx.文档地址: <a href="https://juejin.im/post/5ea12732f265da47d12933c5" target="_blank" rel="noopener">文档地址</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 在Nginx的虚拟主机配置文件中加上以下几行</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">ssl_stapling on;</span><br><span class="line">ssl_stapling_verify on;</span><br><span class="line">resolver 172.16.20.30;  #内部DNS服务器,配置了正确的解析</span><br><span class="line">......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>经过测试,这种方法也不起作用</p><p>我在Let’s Encrypt官方论坛提交了一个topic,也没有收到有用的帮助信息:</p><p><a href="https://community.letsencrypt.org/t/how-to-enable-ocsp-on-nginx-please/127288" target="_blank" rel="noopener">https://community.letsencrypt.org/t/how-to-enable-ocsp-on-nginx-please/127288</a></p><hr><h3 id="临时解决方案"><a href="#临时解决方案" class="headerlink" title="临时解决方案"></a>临时解决方案</h3><p>由于只是DNS收到污染,TCP链接和网络层面并没有被彻底封掉.所以只是简单在nginx的服务器上绑定hosts:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">23.192.45.96 ocsp.int-x3.letsencrypt.org</span><br><span class="line">23.192.45.96 a771.dscq.akamai.net</span><br></pre></td></tr></table></figure><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>还需要再观察一下该问题.目前尚不清楚是Akamai的整个CDN域名被污染,还是仅仅是Let’s Entrypt的域名被污染.甚至或者是伟大的GFW原因.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Letsencrypt证书OCSP域名国内无法访问问题&quot;&gt;&lt;a href=&quot;#Letsencrypt证书OCSP域名国内无法访问问题&quot; class=&quot;headerlink&quot; title=&quot;Letsencrypt证书OCSP域名国内无法访问问题&quot;&gt;&lt;/a&gt;Letsencrypt证书OCSP域名国内无法访问问题&lt;/h2&gt;&lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;近期公司线上APP,小程序的h5页面在IOS移动设备上加载速度非常慢.安卓设备不受影响.调试显示 TCP SSL 相关时间在3s以上.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;原因&quot;&gt;&lt;a href=&quot;#原因&quot; class=&quot;headerlink&quot; title=&quot;原因&quot;&gt;&lt;/a&gt;原因&lt;/h3&gt;&lt;p&gt;经排查发现是国内Let’s Encrypt免费SSL证书机构的OSCP域名 &lt;a href=&quot;http://ocsp.int-x3.letsencrypt.org的CNAME解析在国内被DNS污染..安卓和chrome浏览器默认不在客户端检测OCSP,所以没有影响.但是移动IOS设备和Safari浏览器则会出现这种故障&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://ocsp.int-x3.letsencrypt.org的CNAME解析在国内被DNS污染..安卓和chrome浏览器默认不在客户端检测OCSP,所以没有影响.但是移动IOS设备和Safari浏览器则会出现这种故障&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;参考V2EX的论坛:&lt;a href=&quot;https://www.v2ex.com/t/658605&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.v2ex.com/t/658605&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用国内的公众DNS服务器解析该域名结果:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt; huangyong@huangyong-Macbook-Pro  ~  dig @114.114.114.114 ocsp.int-x3.letsencrypt.org&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;....略....&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;;; ANSWER SECTION:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ocsp.int-x3.letsencrypt.org. 31	IN	CNAME	ocsp.int-x3.letsencrypt.org.edgesuite.net.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ocsp.int-x3.letsencrypt.org.edgesuite.net. 31 IN CNAME a771.dscq.akamai.net.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a771.dscq.akamai.net.	31	IN	A	31.13.69.86&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;;; Query time: 15 msec&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;;; SERVER: 114.114.114.114#53(114.114.114.114)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;;; WHEN: Wed Jul 01 16:40:14 CST 2020&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;;; MSG SIZE  rcvd: 158&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;这个&lt;code&gt;a771.dscq.akamai.net&lt;/code&gt; 的CNAME域名解析的IP地址并不是akamai的IP地址.也无法访问这个iP&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt; huangyong@huangyong-Macbook-Pro  ~  ping 31.13.69.86&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;PING 31.13.69.86 (31.13.69.86): 56 data bytes&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Request timeout for icmp_seq 0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Request timeout for icmp_seq 1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Linux-Basic" scheme="https://jesse.top/categories/Linux-Basic/"/>
    
      <category term="SSL" scheme="https://jesse.top/categories/Linux-Basic/SSL/"/>
    
    
      <category term="SSL" scheme="https://jesse.top/tags/SSL/"/>
    
  </entry>
  
  <entry>
    <title>将服务器日志归档到阿里云OSS的脚本</title>
    <link href="https://jesse.top/2020/06/30/Linux-Basic/shell&amp;shell%E8%84%9A%E6%9C%AC/%E5%B0%86%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%97%A5%E5%BF%97%E5%BD%92%E6%A1%A3%E5%88%B0%E9%98%BF%E9%87%8C%E4%BA%91OSS%E7%9A%84%E8%84%9A%E6%9C%AC/"/>
    <id>https://jesse.top/2020/06/30/Linux-Basic/shell&amp;shell脚本/将服务器日志归档到阿里云OSS的脚本/</id>
    <published>2020-06-29T16:44:48.000Z</published>
    <updated>2020-06-30T13:02:31.723Z</updated>
    
    <content type="html"><![CDATA[<h2 id="将服务器日志归档到阿里云OSS的脚本"><a href="#将服务器日志归档到阿里云OSS的脚本" class="headerlink" title="将服务器日志归档到阿里云OSS的脚本"></a>将服务器日志归档到阿里云OSS的脚本</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">#Descripion: upload trade center logs file to Aliyun OSS</span><br><span class="line">#Author: HuangYong</span><br><span class="line">#date: 2019-04-29 </span><br><span class="line"></span><br><span class="line">buket=mg-tradecenter-log-archived #OSS Buket name</span><br><span class="line">year=$(date +%Y) #年份</span><br><span class="line">year_dir=log-archived-$&#123;year&#125; #OSS年份目录</span><br><span class="line">month=$(date +%m) #月份</span><br><span class="line">ossutil64_dir=/home/work</span><br><span class="line"></span><br><span class="line">yesterday_logtime=$(date +%Y%m%d --date=&quot;-1 day&quot;) #upload yeasterday logfile</span><br><span class="line">log_dir=/data/logs/apps/trade-center/trade-center # tradecenter log file parent dir</span><br><span class="line">log_prefix=&quot;trade-center.log&quot; #logfile prefix</span><br><span class="line">hostname=api1 #tradecenter server</span><br><span class="line"></span><br><span class="line">#判断是否安装ossutil64工具</span><br><span class="line">[ ! -f &quot;/home/work/ossutil64&quot; ] &amp;&amp; echo &quot;请安装ossutil64软件&quot; &amp;&amp; exit 1</span><br><span class="line"></span><br><span class="line">#判断年份目录是否存在,不存在则创建</span><br><span class="line">if ! `$&#123;ossutil64_dir&#125;/ossutil64 ls oss://$&#123;buket&#125;/$&#123;year_dir&#125;/ &gt; /dev/null 2&gt;&amp;1`;then</span><br><span class="line">   $&#123;ossutil64_dir&#125;/ossutil64 mkdir oss://$&#123;buket&#125;/$&#123;year_dir&#125;/</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#判断月份目录是否存在,不存在则创建</span><br><span class="line">if ! `$&#123;ossutil64_dir&#125;/ossutil64 ls oss://$&#123;buket&#125;/$&#123;year_dir&#125;/$&#123;month&#125; &gt; /dev/null 2&gt;&amp;1`;then</span><br><span class="line">   $&#123;ossutil64_dir&#125;/ossutil64 mkdir oss://$&#123;buket&#125;/$&#123;year_dir&#125;/$&#123;month&#125;/</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 打包昨天的日志文件</span><br><span class="line"></span><br><span class="line">cd $log_dir</span><br><span class="line"></span><br><span class="line">for log_type in &quot;debug&quot; &quot;error&quot; &quot;netrcd-admin&quot; &quot;netrcd-callback&quot; &quot;netrcd-gateway&quot; &quot;netrcd-notify&quot; &quot;script&quot;;do</span><br><span class="line"></span><br><span class="line">    log_name=&quot;$&#123;log_prefix&#125;.$&#123;log_type&#125;.$yesterday_logtime&quot;</span><br><span class="line">    if [ -f $log_name ];then</span><br><span class="line">        tar -zc -f $&#123;hostname&#125;.$&#123;log_name&#125;.tar.gz  $log_name</span><br><span class="line">    fi</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line"># 上传文件到OSS</span><br><span class="line"></span><br><span class="line">$&#123;ossutil64_dir&#125;/ossutil64 cp $log_dir --include=&quot;$&#123;hostname&#125;*.tar.gz&quot; -r -f oss://$&#123;buket&#125;/$&#123;year_dir&#125;/$&#123;month&#125;/</span><br><span class="line"></span><br><span class="line">#上传完成后,删除打包的日志</span><br><span class="line">[ $? == 0 ] &amp;&amp; rm -f $log_dir/$&#123;hostname&#125;*</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;将服务器日志归档到阿里云OSS的脚本&quot;&gt;&lt;a href=&quot;#将服务器日志归档到阿里云OSS的脚本&quot; class=&quot;headerlink&quot; title=&quot;将服务器日志归档到阿里云OSS的脚本&quot;&gt;&lt;/a&gt;将服务器日志归档到阿里云OSS的脚本&lt;/h2&gt;&lt;figure c
      
    
    </summary>
    
      <category term="Linux-Basic" scheme="https://jesse.top/categories/Linux-Basic/"/>
    
      <category term="shell&amp;shell脚本" scheme="https://jesse.top/categories/Linux-Basic/shell-shell%E8%84%9A%E6%9C%AC/"/>
    
    
      <category term="scripts" scheme="https://jesse.top/tags/scripts/"/>
    
  </entry>
  
  <entry>
    <title>kvm自动创建虚拟机,自定义IP地址</title>
    <link href="https://jesse.top/2020/06/30/Linux-Basic/shell&amp;shell%E8%84%9A%E6%9C%AC/kvm%E9%80%9A%E8%BF%87%E6%A8%A1%E6%9D%BF%E8%87%AA%E5%8A%A8%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA,%E9%85%8D%E7%BD%AEIP%E7%BD%91%E7%BB%9C/"/>
    <id>https://jesse.top/2020/06/30/Linux-Basic/shell&amp;shell脚本/kvm通过模板自动创建虚拟机,配置IP网络/</id>
    <published>2020-06-29T16:44:48.000Z</published>
    <updated>2020-06-30T13:00:33.613Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">通过KVM模板自动化创建虚拟机（需要电脑中存在模板）.以及创建完虚拟机后,自动修改IP地址</span></span><br><span class="line"><span class="meta">#</span><span class="bash">该脚本需要以root身份执行</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta"> #</span><span class="bash">镜像和模板源文件</span></span><br><span class="line">src_img_path=/opt/vmx/linux/hadoop.dev.base.img</span><br><span class="line">src_xml_path=/etc/libvirt/qemu/hadoop.dev.base.xml</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">镜像和模板源文件父目录</span></span><br><span class="line">prefix_img_path=/opt/vmx/linux</span><br><span class="line">prefix_xml_path=/etc/libvirt/qemu</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">获取新虚拟机名称</span></span><br><span class="line">get_newname()&#123;</span><br><span class="line">while true</span><br><span class="line">        do</span><br><span class="line">                read -p "请输入新虚拟机名称(主机名)：" newname</span><br><span class="line">                if [ $newname ];then</span><br><span class="line">                    if `virsh list --all | grep "\b$&#123;newname&#125;\b"`;then</span><br><span class="line">                        echo "该虚拟机已经存在.请检查系统当前虚拟机"</span><br><span class="line">                    else</span><br><span class="line"></span><br><span class="line">                        #设置新虚拟机镜像和模板路径</span><br><span class="line">                        new_img_path=$&#123;prefix_img_path&#125;/$&#123;newname&#125;.qcow2</span><br><span class="line">                        new_xml_path=$&#123;prefix_xml_path&#125;/$&#123;newname&#125;.xml</span><br><span class="line">                        break</span><br><span class="line">                    fi</span><br><span class="line">                else</span><br><span class="line">                        echo "************"</span><br><span class="line">                        echo "请输入虚拟机主机名！"</span><br><span class="line">                        echo "************"</span><br><span class="line">                fi</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">设置新虚拟机内存</span></span><br><span class="line">get_newmemary()&#123;</span><br><span class="line">while true</span><br><span class="line">        do</span><br><span class="line">                current_free_mem=`free -g|awk '/^Mem/&#123;print $4&#125;'`</span><br><span class="line">                mem_total=`free -g|awk '/^Mem/&#123;print $2&#125;'`</span><br><span class="line">                echo "目前本机内存总大小：$&#123;mem_total&#125;g"</span><br><span class="line">                echo "当前空闲内存大小为：$&#123;current_free_mem&#125;g"</span><br><span class="line">                read -p "请输入新虚拟机内存大小(单位G)：" newmemary</span><br><span class="line">                if [ $newmemary ];then</span><br><span class="line">                        if [[ $newmemary &lt; $mem_total ]];then</span><br><span class="line">                                break</span><br><span class="line">                        else</span><br><span class="line">                                echo "**********************************"</span><br><span class="line">                                echo "输入的数值必须小于当前内存总大小！"</span><br><span class="line">                                echo "**********************************"</span><br><span class="line">                        fi</span><br><span class="line">                else</span><br><span class="line">                        echo "********************"</span><br><span class="line">                        echo "请输入新虚拟机内存！"</span><br><span class="line">                        echo "********************"</span><br><span class="line">                fi</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">设置新虚拟机CPU</span></span><br><span class="line">get_newcpu()&#123;</span><br><span class="line">while true</span><br><span class="line">        do</span><br><span class="line">                core=`cat /proc/cpuinfo| grep "processor"| wc -l`</span><br><span class="line">                echo "可用core个数：$&#123;core&#125;"</span><br><span class="line">                read -p "请输入新虚拟机处理器核数：" newcpu</span><br><span class="line">                if [ $newcpu ];then</span><br><span class="line">                        if [ $newcpu -le $core ];then</span><br><span class="line">                                break</span><br><span class="line">                        else</span><br><span class="line">                                echo "******************************"</span><br><span class="line">                                echo "不能超过可用个数或者输入错误！"</span><br><span class="line">                                echo "******************************"</span><br><span class="line">                        fi</span><br><span class="line">                else</span><br><span class="line">                        echo "**************"</span><br><span class="line">                        echo "输入不能为空！"</span><br><span class="line">                        echo "**************"</span><br><span class="line">                fi</span><br><span class="line">        done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">设置IP</span></span><br><span class="line">get_ip()&#123;</span><br><span class="line"></span><br><span class="line">    while true</span><br><span class="line">        do </span><br><span class="line">            read -p "请输入新虚拟机的IP地址(格式:172.16.10.[2-240]):" ip</span><br><span class="line">            if `echo $ip| grep -E "172.16.10.[0-9]&#123;1,3&#125;$" &gt; /dev/null 2&gt;&amp;1`;then</span><br><span class="line">                ipaddr=$(echo $ip | awk -F "." '&#123;print $4&#125;')</span><br><span class="line">                #valid_check=$(echo $ip | awk -F "." '2&lt;$4 &amp;&amp; $4&lt;241 &#123;print "yes"&#125;')</span><br><span class="line">                #if [ "$valid_check" == "yes" ];then</span><br><span class="line">                if [ $ipaddr -gt 2 -a $ipaddr -lt 241 ];then</span><br><span class="line">                    if ! `ping -W 3 -c 1 $ip &gt; /dev/null 2&gt;&amp;1`;then</span><br><span class="line">                       break</span><br><span class="line">                    </span><br><span class="line">                    else</span><br><span class="line">                       echo "当前IP已经被占用,请重新输入"</span><br><span class="line">                    </span><br><span class="line">                    fi</span><br><span class="line">                </span><br><span class="line">                else</span><br><span class="line">                    echo "IP地址必须要在2-240之间"</span><br><span class="line"></span><br><span class="line">                fi</span><br><span class="line">            </span><br><span class="line">            else</span><br><span class="line">                echo "IP地址格式不对,请重新输入"</span><br><span class="line">            </span><br><span class="line">            fi</span><br><span class="line">    done</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">复制模板、xml</span></span><br><span class="line">copy_model_xml()&#123;</span><br><span class="line">cp $src_img_path $new_img_path</span><br><span class="line">        cp $src_xml_path $new_xml_path</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">修改xml文件</span></span><br><span class="line">modification_xml()&#123;</span><br><span class="line">    uuid=`uuidgen`</span><br><span class="line">sed -ri "s/(&lt;name&gt;).*(&lt;\/name&gt;)/\1$&#123;newname&#125;\2/" $new_xml_path  </span><br><span class="line">        sed -ri "s/(&lt;uuid&gt;).*(&lt;\/uuid&gt;)/\1$&#123;uuid&#125;\2/" $new_xml_path </span><br><span class="line">        mem_kb=$(($&#123;newmemary&#125;*1024*1024)) </span><br><span class="line"> </span><br><span class="line">        sed -ri "s/(&lt;memory.*&gt;).*(&lt;\/memory&gt;)/\1$&#123;mem_kb&#125;\2/" $new_xml_path </span><br><span class="line">        sed -ri "s/(&lt;currentMemory.*&gt;).*(&lt;\/currentMemory&gt;)/\1$&#123;mem_kb&#125;\2/" $new_xml_path</span><br><span class="line">        sed -ri "s/(&lt;vcpu.*&gt;).*(&lt;\/vcpu&gt;)/\1$&#123;newcpu&#125;\2/" $new_xml_path</span><br><span class="line">        sed -ri "s@(&lt;source file=').*('\/&gt;)@\1$&#123;new_img_path&#125;\2@" $new_xml_path  #定义新虚拟机的xml文件路径</span><br><span class="line"></span><br><span class="line">        sed -i "s@port='5930'@port='"59$&#123;ipaddr&#125;"'@" $new_xml_path #配置VNC端口号.格式59+ip地址最后一位</span><br><span class="line">        mac_addr=`openssl rand -hex 3 | sed -r 's/..\B/&amp;:/g'` #生成一个随机的MAC地址</span><br><span class="line"> </span><br><span class="line">        sed -ri "s/(&lt;mac address='..:..:..:).*('\/&gt;)/\1$&#123;mac_addr&#125;\2/" $new_xml_path #配置MAC地址</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">define</span></span><br><span class="line">define_vm()&#123;</span><br><span class="line">virsh define $new_xml_path #从xml文件中生成新虚拟机</span><br><span class="line">echo "**********"</span><br><span class="line">echo "$&#123;newname&#125;建完成！"</span><br><span class="line">echo "**********"</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">network_setting()&#123;</span><br><span class="line"></span><br><span class="line">    #该脚本使用guestmount工具，Centos7中安装libguestfs-tools-c可以获得guestmount工具</span><br><span class="line">    #脚本在不登陆虚拟机的情况下，修改虚拟机的IP地址信息</span><br><span class="line"></span><br><span class="line">    mountpath="/tmp/$&#123;newname&#125;"</span><br><span class="line">    [ ! -d $mountpath ] &amp;&amp; mkdir $mountpath</span><br><span class="line"></span><br><span class="line">    guestmount  -d $&#123;newname&#125; -i $mountpath</span><br><span class="line"></span><br><span class="line">    #修改IP地址: 注意这里要通过这种方式才能修改IP..不能使用sed命令直接修改原文件,否则虚拟机启动后无法识别ifcfg-eth0文件</span><br><span class="line">    cat &gt; $&#123;mountpath&#125;/etc/sysconfig/network-scripts/ifcfg-eth0 &lt;&lt; EOF</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">PROXY_METHOD=none</span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">DEFROUTE=yes</span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line">NAME=eth0</span><br><span class="line">DEVICE=eth0</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=$ip</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">GATEWAY=172.16.10.254</span><br><span class="line">DNS1=114.114.114.114</span><br><span class="line">EOF</span><br><span class="line">    #sed -i "/IPADDR/s@.*@IPADDR=$&#123;ip&#125;@" $&#123;mountpath&#125;/etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line">    #修改主机名</span><br><span class="line">    echo "$&#123;newname&#125;" &gt; $&#123;mountpath&#125;/etc/hostname</span><br><span class="line"></span><br><span class="line">    echo "虚拟主机IP地址和主机名配置完成"</span><br><span class="line"></span><br><span class="line">    #卸载临时挂载目录</span><br><span class="line">    umount /tmp/$&#123;newname&#125;</span><br><span class="line">    </span><br><span class="line">    #删除临时挂载目录</span><br><span class="line">    rm -rf /tmp/$&#123;newname&#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash">------------运行分界线------------------------------------</span></span><br><span class="line"></span><br><span class="line">get_newname</span><br><span class="line">get_newmemary</span><br><span class="line">get_newcpu</span><br><span class="line">get_ip</span><br><span class="line">copy_model_xml</span><br><span class="line">modification_xml</span><br><span class="line">define_vm</span><br><span class="line">network_setting</span><br></pre></td></tr></table></figure><p>参考 <a href="https://blog.csdn.net/qq_41814635/article/details/82256970" target="_blank" rel="noopener">https://blog.csdn.net/qq_41814635/article/details/82256970</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class
      
    
    </summary>
    
      <category term="Linux-Basic" scheme="https://jesse.top/categories/Linux-Basic/"/>
    
      <category term="shell&amp;shell脚本" scheme="https://jesse.top/categories/Linux-Basic/shell-shell%E8%84%9A%E6%9C%AC/"/>
    
    
      <category term="scripts" scheme="https://jesse.top/tags/scripts/"/>
    
  </entry>
  
  <entry>
    <title>mysql数据库使用xtrabackup全量,增量备份</title>
    <link href="https://jesse.top/2020/06/30/Linux-Basic/shell&amp;shell%E8%84%9A%E6%9C%AC/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BD%BF%E7%94%A8xtrabackup%E5%85%A8%E9%87%8F,%E5%A2%9E%E9%87%8F%E5%A4%87%E4%BB%BD/"/>
    <id>https://jesse.top/2020/06/30/Linux-Basic/shell&amp;shell脚本/mysql数据库使用xtrabackup全量,增量备份/</id>
    <published>2020-06-29T16:44:48.000Z</published>
    <updated>2020-06-30T12:59:00.486Z</updated>
    
    <content type="html"><![CDATA[<h2 id="mysql数据库使用xtrabackup全量-增量备份"><a href="#mysql数据库使用xtrabackup全量-增量备份" class="headerlink" title="mysql数据库使用xtrabackup全量,增量备份"></a>mysql数据库使用xtrabackup全量,增量备份</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">##############################################################</span><br><span class="line"># File Name: backup.sh</span><br><span class="line"># Version: V1.0</span><br><span class="line"># Author: huangyong</span><br><span class="line"># Created Time : 2018-3-1 18:42:00</span><br><span class="line"># Description: 数据库全量,增量备份脚本</span><br><span class="line"></span><br><span class="line">#备份策略:每周一进行全备,其他日期备份当周的增量备份.每次全备前删除2周前的备份</span><br><span class="line">#可扩展功能:打包备份文件.备份文件传输到远程服务器</span><br><span class="line"></span><br><span class="line">#date:2018-04-15</span><br><span class="line">#update:由于数据库/data磁盘已快满.所以备份只保留一周.</span><br><span class="line"></span><br><span class="line">#date:2018-04-23</span><br><span class="line">#update:增加如果备份失败则发邮件通知功能</span><br><span class="line">#       增加自动删除备份日志功能</span><br><span class="line"></span><br><span class="line">#data:2018-04-24</span><br><span class="line">#update:1.增加xtrabackup自带的的备份压缩功能,且压缩线程数4.</span><br><span class="line">#       2.全备完成后,打包整个全备的备份文件(暂时先不打包)</span><br><span class="line">#       3.全备完成后,同步备份文件到BETA服务器</span><br><span class="line">#       4.保留2份备份文件,也就是保留2周</span><br><span class="line">#       5.将脚本的执行用户从root改到work.</span><br><span class="line"></span><br><span class="line">#date:2018-05-03</span><br><span class="line">#update:修改N_变量的抓取inc增量备份目录的命令.之前用的是sort命令,经常会抓取到错误的inc增量备份目录</span><br><span class="line">#        脚本执行用户改成root,因为work用户没有权限访问mysql的数据文件目录</span><br><span class="line">##############################################################</span><br><span class="line"></span><br><span class="line">#获取脚本所存放目录</span><br><span class="line">cd `dirname $0`</span><br><span class="line">bash_path=`pwd`</span><br><span class="line">#脚本名</span><br><span class="line">me=$(basename $0)</span><br><span class="line"></span><br><span class="line">#设置要备份的innodb数据库，用空格格开，空为备份所有库</span><br><span class="line">databases=&apos;&apos;</span><br><span class="line"></span><br><span class="line">#定义变量</span><br><span class="line">DATE=$(date +%W) #全年的第几周,一个星期为一个备份周期.备份根目录，其子目录：base为全量，inc1、inc2...为增量</span><br><span class="line">TWO_WEEKS_AGO=$(echo $&#123;DATE&#125;-2|bc) #前两周前的备份</span><br><span class="line">FULL_DATE=$(date +%F) #存储日志日期</span><br><span class="line">DAY_DATE=$(date +%w) #判断一周的第几天</span><br><span class="line">#MYSQL=&quot;mysql&quot;  # mysql命令绝对路径或在PATH中</span><br><span class="line">MYSQL_DATA_DIR=&quot;/data/mysql/data&quot;  # 数据库目录</span><br><span class="line">BACKUP_USER=&quot;tongji&quot;  # 备份用户</span><br><span class="line">PASSWD=$(cat /data/xtrabackup/password)  # 备份密码保存文件</span><br><span class="line">BACK_FILE_DIR=&quot;/data/backups/$&#123;DATE&#125;&quot;  # 备份频率目录，此目录变化频率为备份一周期</span><br><span class="line">LOG_P_DIR=&quot;/data/backup_logs&quot; #备份日志根目录</span><br><span class="line">LOG_DIR=&quot;/data/backup_logs/$&#123;FULL_DATE&#125;&quot;  # 备份过程日志目录</span><br><span class="line">#LOG_ERR=&quot;$&#123;LOG_DIR&#125;/mysql_backup_fail.log&quot; #备份错误日志文件</span><br><span class="line">LOG_FILE=&quot;$&#123;LOG_DIR&#125;/mysql_backup.log&quot;  #备份过程日志文件</span><br><span class="line">email_user=&quot;huangyong@doweidu.com&quot;</span><br><span class="line">ssh_server=&quot;10.25.2.85&quot;  # 远程备份服务器IP</span><br><span class="line">ssh_server_dir=&quot;/data/tongjidb-mysqlbackup&quot;  # 远程备份服务器目录</span><br><span class="line">ssh_port=&quot;5822&quot;  # ssh端口</span><br><span class="line">ssh_parameters=&quot;-o StrictHostKeyChecking=no -o ConnectTimeout=60&quot;</span><br><span class="line">ssh_user=&quot;work&quot;</span><br><span class="line">ssh_command=&quot;ssh $&#123;ssh_parameters&#125; -p $&#123;ssh_port&#125;&quot;</span><br><span class="line">#scp_command=&quot;scp $&#123;ssh_parameters&#125; -P $&#123;ssh_port&#125;&quot;</span><br><span class="line"></span><br><span class="line">#定义保存日志函数</span><br><span class="line">function save_log () &#123;</span><br><span class="line">        </span><br><span class="line">echo -e &quot;#################[`date +%F\ %T`]$* ####################&quot; &gt;&gt; $LOG_FILE</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#定义发送邮件函数</span><br><span class="line">function send_mail () &#123;</span><br><span class="line">        echo $1 | mail -s $1  $email_user</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#创建目录</span><br><span class="line">[ ! -d &quot;$&#123;BACK_FILE_DIR&#125;&quot; ] &amp;&amp; mkdir -p $&#123;BACK_FILE_DIR&#125;</span><br><span class="line">[ ! -d &quot;$&#123;LOG_DIR&#125;&quot; ] &amp;&amp; mkdir -p $&#123;LOG_DIR&#125;</span><br><span class="line"></span><br><span class="line">function full_backup () &#123;</span><br><span class="line"># 全量备份函数</span><br><span class="line">[ ! -z &quot;$databases&quot; ] &amp;&amp; option=&quot;--databases=$&#123;databases&#125;&quot; || option=&quot;&quot; </span><br><span class="line"></span><br><span class="line">##############################MYSQL全库备份#########################</span><br><span class="line">/usr/bin/xtrabackup  --user=$BACKUP_USER --password=$PASSWD --compress --compress-threads=4 --backup --target-dir=$&#123;BACK_FILE_DIR&#125;/base --datadir=$&#123;MYSQL_DATA_DIR&#125; $option &gt; $LOG_FILE 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line">        if [ $? -eq 0 ];then</span><br><span class="line">             save_log &quot;mysql full_backup succeed&quot;</span><br><span class="line">             chown -R mysql:mysql $&#123;BACK_FILE_DIR&#125;/base</span><br><span class="line">        </span><br><span class="line">        else</span><br><span class="line">             save_log &quot;mysql full_backup failed&quot;</span><br><span class="line">             #send_mail &quot;mysql full_backup failed&quot;</span><br><span class="line">             exit 1   </span><br><span class="line">        </span><br><span class="line">        fi                </span><br><span class="line">###################################################################</span><br><span class="line"></span><br><span class="line">          </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function incremental_backup () &#123;</span><br><span class="line">    [ ! -z &quot;$databases&quot; ] &amp;&amp; option=&quot;--databases=$&#123;databases&#125;&quot; || option=&quot;&quot;</span><br><span class="line"></span><br><span class="line">    cd  $BACK_FILE_DIR</span><br><span class="line">    # 判断是否存在第一次增量备份目录inc1</span><br><span class="line">    # 存在则获取最后一次增量备份目录incN，然后基于最后一次增量备份，做增量备份</span><br><span class="line">    # 不存在则基于全量备份目录base做增量备份</span><br><span class="line">    if [ -d &quot;inc1&quot; ];then</span><br><span class="line">        N_=$(ls -l | awk -F &apos;inc&apos; &apos;/^d+.+inc[0-9]+$/&#123;a[NR]=$NF;len=asort(a,sa)&#125;END&#123;print sa[len]&#125;&apos;)</span><br><span class="line">        N=$(echo $N_+1|bc)</span><br><span class="line">        #增量备份 </span><br><span class="line">        /usr/bin/xtrabackup --user=$BACKUP_USER --password=$PASSWD --backup --compress --target-dir=$BACK_FILE_DIR/inc$N \</span><br><span class="line">        --incremental-basedir=$BACK_FILE_DIR/inc$N_ --datadir=$MYSQL_DATA_DIR $option &gt; $LOG_FILE 2&gt;&amp;1</span><br><span class="line">    else</span><br><span class="line">        N=&quot;1&quot;</span><br><span class="line">        #增量备份 </span><br><span class="line">        [ ! -d $BACK_FILE_DIR/base ] &amp;&amp; save_log &quot;incremental backup failed,no full_backup&quot; &amp;&amp; exit 1</span><br><span class="line">        /usr/bin/xtrabackup --user=$BACKUP_USER --password=$PASSWD --backup --compress --target-dir=$BACK_FILE_DIR/inc$N \</span><br><span class="line">        --incremental-basedir=$BACK_FILE_DIR/base --datadir=$MYSQL_DATA_DIR $option &gt; $LOG_FILE 2&gt;&amp;1</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    if [ $? -eq 0 ];then</span><br><span class="line">             save_log &quot;mysql inc$&#123;N&#125;-backup successed&quot;</span><br><span class="line">             chown -R mysql:mysql $&#123;BACK_FILE_DIR&#125;/inc$N</span><br><span class="line">        </span><br><span class="line">        else</span><br><span class="line">             save_log &quot;mysql inc$&#123;N&#125;-backup failed&quot; </span><br><span class="line">            #send_mail &quot;mysql inc$&#123;N&#125;-backup failed&quot;</span><br><span class="line">             exit 1   </span><br><span class="line">                        </span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    return 0</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function rsync_backup_files () &#123;</span><br><span class="line">#传输到远程服务器备份, 需要配置免密ssh认证</span><br><span class="line">        </span><br><span class="line">        #使用rsync将本地的/data/backups目录同步到BETA服务器.同时删除BETA服务器上2周前的备份目录</span><br><span class="line">rsync -az --delete /data/backups -e &quot;$&#123;ssh_command&#125;&quot; $ssh_user@$&#123;ssh_server&#125;:$ssh_server_dir</span><br><span class="line">[ $? -eq 0 ] &amp;&amp; save_log &quot;full-backuped rsync successed&quot; || \</span><br><span class="line">&#123; save_log &quot;backup rsync failed&quot; ; send_mail &quot;mysql backup rsync failed&quot; ; &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#每周1进行全备.其他日期对本周一的全备做增量备份</span><br><span class="line">if [ $DAY_DATE -eq 1 ];then</span><br><span class="line">    </span><br><span class="line">   </span><br><span class="line">   #删除2周前的备份文件</span><br><span class="line">   if [ 1 -le $TWO_WEEKS_AGO -a $TWO_WEEKS_AGO -lt 10 ];then #如果本周和2周前的数相减小于10,并且大于等于1,则相差的结果前加个0.比如07</span><br><span class="line">        FILE_NAME=$(dirname $BACK_FILE_DIR)/0$TWO_WEEKS_AGO</span><br><span class="line">        [ -d $FILE_NAME ] &amp;&amp; rm -rf $FILE_NAME</span><br><span class="line"></span><br><span class="line">   elif [ $TWO_WEEKS_AGO -ge 10 ];then  #如果两数相减等于两位数,直接删除文件</span><br><span class="line">           FILE_NAME=$(dirname $BACK_FILE_DIR)/$TWO_WEEKS_AGO</span><br><span class="line">           [ -d $FILE_NAME ] &amp;&amp; rm -rf $FILE_NAME</span><br><span class="line"></span><br><span class="line">   fi</span><br><span class="line"></span><br><span class="line">   full_backup #调用全备</span><br><span class="line">  </span><br><span class="line">else</span><br><span class="line">     incremental_backup #调用增备</span><br><span class="line"></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#删除7天前日志文件</span><br><span class="line">find $LOG_P_DIR -type d -mtime +7 -exec rm -rf &#123;&#125; \;</span><br><span class="line"></span><br><span class="line">rsync_backup_files</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;mysql数据库使用xtrabackup全量-增量备份&quot;&gt;&lt;a href=&quot;#mysql数据库使用xtrabackup全量-增量备份&quot; class=&quot;headerlink&quot; title=&quot;mysql数据库使用xtrabackup全量,增量备份&quot;&gt;&lt;/a&gt;mysql
      
    
    </summary>
    
      <category term="Linux-Basic" scheme="https://jesse.top/categories/Linux-Basic/"/>
    
      <category term="shell&amp;shell脚本" scheme="https://jesse.top/categories/Linux-Basic/shell-shell%E8%84%9A%E6%9C%AC/"/>
    
    
      <category term="scripts" scheme="https://jesse.top/tags/scripts/"/>
    
  </entry>
  
  <entry>
    <title>acme.sh自动申请SSL证书脚本</title>
    <link href="https://jesse.top/2020/06/30/Linux-Basic/shell&amp;shell%E8%84%9A%E6%9C%AC/acme.sh%E8%87%AA%E5%8A%A8%E7%94%B3%E8%AF%B7SSL%E8%AF%81%E4%B9%A6%E8%84%9A%E6%9C%AC/"/>
    <id>https://jesse.top/2020/06/30/Linux-Basic/shell&amp;shell脚本/acme.sh自动申请SSL证书脚本/</id>
    <published>2020-06-29T16:44:48.000Z</published>
    <updated>2020-06-30T13:01:53.371Z</updated>
    
    <content type="html"><![CDATA[<h2 id="acme-sh自动申请SSL证书脚本"><a href="#acme-sh自动申请SSL证书脚本" class="headerlink" title="acme.sh自动申请SSL证书脚本"></a>acme.sh自动申请SSL证书脚本</h2><p>acme.sh是GitHub上的一个项目.有关这个工具的介绍可以参考github,或者查看Linux-证书目录下的相关笔记</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">#description: 使用acme.sh工具通过自动DNS验证方式申请SSL证书.</span><br><span class="line">#date: 20190620</span><br><span class="line"></span><br><span class="line">#判断是否有指定域名,以及域名</span><br><span class="line">if [ $# != 2 ];then</span><br><span class="line">   echo &quot;usage: $0 domain_name ssl_install_dir&quot;</span><br><span class="line">   exit 1</span><br><span class="line">fi</span><br><span class="line">#接收要申请SSL证书的域名参数</span><br><span class="line">ssl_domain=$1</span><br><span class="line">#接收证书安装目标路径的参数</span><br><span class="line">ssl_install_dir=$2</span><br><span class="line"></span><br><span class="line">#判断申请的是否是通配符证书</span><br><span class="line">if `echo $ssl_domain | grep &quot;^\*&quot; &gt; /dev/null 2&gt;&amp;1`;then</span><br><span class="line"></span><br><span class="line">    #如果是通配符证书,那么替换到域名前面的&quot;*.&quot;</span><br><span class="line">    ssl_name=$(echo $ssl_domain | sed &apos;s@\*\.@@&apos;)</span><br><span class="line"></span><br><span class="line">else</span><br><span class="line"></span><br><span class="line">    ssl_name=$ssl_domain</span><br><span class="line"></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#检查证书安装目录,判断该域名是否已经申请过证书.如果已经申请过,则直接退出</span><br><span class="line"></span><br><span class="line">#新建ssl_dir证书路径变量</span><br><span class="line">ssl_dir=$&#123;ssl_install_dir&#125;/$&#123;ssl_name&#125;</span><br><span class="line"></span><br><span class="line">if [ -d $ssl_dir ];then</span><br><span class="line"></span><br><span class="line">    echo &quot;$ssl_domain certificate has already installed&quot; &amp;&amp; exit 0</span><br><span class="line"></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#导入环境变量</span><br><span class="line">export DP_Id=DNS服务器的AccessKeyID</span><br><span class="line">export DP_Key=DNS服务器的Secret</span><br><span class="line"></span><br><span class="line">#申请证书</span><br><span class="line">cd /home/work/.acme.sh/</span><br><span class="line"></span><br><span class="line"># 判断申请的是通配符证书,还是单域名证书</span><br><span class="line">if [ $ssl_domain == $ssl_name ];then</span><br><span class="line"></span><br><span class="line">./acme.sh --issue --dns dns_dp  -d $ssl_name</span><br><span class="line"></span><br><span class="line">else</span><br><span class="line">./acme.sh --issue --dns dns_dp  -d $ssl_name -d $ssl_domain</span><br><span class="line"></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#判断证书申请是否成功</span><br><span class="line">if [ $? != 0 ];then</span><br><span class="line">     echo &quot;$ssl_domain certificate applied failed&quot; &amp;&amp; exit 1</span><br><span class="line">else</span><br><span class="line">    echo &quot;$ssl_domain certificate applied successfully&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#新建证书目标路径</span><br><span class="line">mkdir -p $ssl_dir</span><br><span class="line"></span><br><span class="line">#安装证书</span><br><span class="line">./acme.sh  --installcert  -d  $ssl_name  \</span><br><span class="line">       --key-file $&#123;ssl_dir&#125;/$&#123;ssl_name&#125;.key  \</span><br><span class="line">       --fullchain-file $&#123;ssl_dir&#125;/fullchain.cer     \</span><br><span class="line">       --reloadcmd  &quot;supervisorctl -c /etc/supervisord/supervisord.conf restart nginx&quot;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;acme-sh自动申请SSL证书脚本&quot;&gt;&lt;a href=&quot;#acme-sh自动申请SSL证书脚本&quot; class=&quot;headerlink&quot; title=&quot;acme.sh自动申请SSL证书脚本&quot;&gt;&lt;/a&gt;acme.sh自动申请SSL证书脚本&lt;/h2&gt;&lt;p&gt;acme.s
      
    
    </summary>
    
      <category term="Linux-Basic" scheme="https://jesse.top/categories/Linux-Basic/"/>
    
      <category term="shell&amp;shell脚本" scheme="https://jesse.top/categories/Linux-Basic/shell-shell%E8%84%9A%E6%9C%AC/"/>
    
    
      <category term="scripts" scheme="https://jesse.top/tags/scripts/"/>
    
  </entry>
  
  <entry>
    <title>分析统计Nginx日志的响应时间</title>
    <link href="https://jesse.top/2020/06/30/Linux-Basic/shell&amp;shell%E8%84%9A%E6%9C%AC/%E5%88%86%E6%9E%90Nginx%E6%97%A5%E5%BF%97%E7%9A%84%E5%93%8D%E5%BA%94%E6%97%B6%E9%97%B4/"/>
    <id>https://jesse.top/2020/06/30/Linux-Basic/shell&amp;shell脚本/分析Nginx日志的响应时间/</id>
    <published>2020-06-29T16:44:48.000Z</published>
    <updated>2020-06-30T13:03:22.173Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分析统计Nginx日志的响应时间"><a href="#分析统计Nginx日志的响应时间" class="headerlink" title="分析统计Nginx日志的响应时间"></a>分析统计Nginx日志的响应时间</h2><p>Nginx日志格式如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos;</span><br><span class="line">                &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos;</span><br><span class="line">                &apos;&quot;$http_user_agent&quot; $http_x_forwarded_for &quot;$request_time&quot;&apos;;</span><br></pre></td></tr></table></figure><p>实际日志如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[work@iqg-yyq2 ~]$ head  /data/logs/nginx/iqg_api_v5.access.log</span><br><span class="line">100.97.74.45 - - [11/Mar/2019:03:46:01 +0800] &quot;HEAD / HTTP/1.0&quot; 200 0 &quot;-&quot; &quot;-&quot; - &quot;0.001&quot;</span><br><span class="line">100.117.85.154 - - [11/Mar/2019:03:46:01 +0800] &quot;HEAD / HTTP/1.0&quot; 200 0 &quot;-&quot; &quot;-&quot; - &quot;0.001&quot;</span><br><span class="line">100.97.74.22 - - [11/Mar/2019:03:46:02 +0800] &quot;HEAD / HTTP/1.0&quot; 200 0 &quot;-&quot; &quot;-&quot; - &quot;0.002&quot;</span><br><span class="line">100.117.85.96 - - [11/Mar/2019:03:46:02 +0800] &quot;HEAD / HTTP/1.0&quot; 200 0 &quot;-&quot; &quot;-&quot; - &quot;0.001&quot;</span><br><span class="line">100.117.85.133 - - [11/Mar/2019:03:46:02 +0800] &quot;HEAD / HTTP/1.0&quot; 200 0 &quot;-&quot; &quot;-&quot; - &quot;0.001&quot;</span><br><span class="line">100.117.85.172 - - [11/Mar/2019:03:46:02 +0800] &quot;HEAD / HTTP/1.0&quot; 200 0 &quot;-&quot; &quot;-&quot; - &quot;0.002&quot;</span><br><span class="line">100.97.74.0 - - [11/Mar/2019:03:46:02 +0800] &quot;HEAD / HTTP/1.0&quot; 200 0 &quot;-&quot; &quot;-&quot; - &quot;0.001&quot;</span><br><span class="line">100.97.73.184 - - [11/Mar/2019:03:46:02 +0800] &quot;HEAD / HTTP/1.0&quot; 200 0 &quot;-&quot; &quot;-&quot; - &quot;0.001&quot;</span><br><span class="line">100.117.85.85 - - [11/Mar/2019:03:46:02 +0800] &quot;HEAD / HTTP/1.0&quot; 200 0 &quot;-&quot; &quot;-&quot; - &quot;0.001&quot;</span><br><span class="line">100.97.74.62 - - [11/Mar/2019:03:46:02 +0800] &quot;HEAD / HTTP/1.0&quot; 200 0 &quot;-&quot; &quot;-&quot; - &quot;0.001&quot;</span><br></pre></td></tr></table></figure><p>最后一行为响应时间.但是是个字符串,还不能直接用awk来统计</p><p>下面这个脚本用来统计响应时间:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">[ $# -ne 2 ] &amp;&amp; echo &quot;Usage: ./loganalysis.sh two parameters: logfile path  and cost time&quot; &amp;&amp; exit 0</span><br><span class="line"></span><br><span class="line">[ ! -f &quot;$1&quot; ] &amp;&amp; echo &quot;the file doesn&apos;t exsit,please check again&quot; &amp;&amp; exit 0</span><br><span class="line">logfile=$(basename $1)</span><br><span class="line"></span><br><span class="line">[ &quot;$2&quot; -lt 0 ] &amp;&amp; echo &quot; the second parameter is not a digit&quot; &amp;&amp; exit 0</span><br><span class="line">cost_time=$2</span><br><span class="line"></span><br><span class="line">cat $1 | awk &apos;&#123;print $NF&#125;&apos;  | awk -F &quot;\&quot;&quot; &apos;&#123;print $2&#125;&apos;  &gt;  time.txt</span><br><span class="line">echo &quot;split request_time over!!!&quot;</span><br><span class="line"></span><br><span class="line">paste  -d &quot; &quot; $1 time.txt &gt; new.log</span><br><span class="line">echo &quot;build new logfile over!!!&quot;</span><br><span class="line"></span><br><span class="line">awk &apos;($NF&gt;&apos;$cost_time&apos;)&#123;print $0&#125;&apos; new.log &gt; slow-$&#123;logfile&#125;</span><br><span class="line">echo &quot;please see slowtime in slow-$&#123;logfile&#125;&quot;</span><br><span class="line"></span><br><span class="line">rm -f time.txt</span><br><span class="line">rm -f new.log</span><br><span class="line"></span><br><span class="line"># analyze the access frequence of API</span><br><span class="line"></span><br><span class="line">echo &quot;#############the access frequence of API ##################&quot;</span><br><span class="line">awk &apos;&#123;++S[$4]&#125; END &#123; for (i in S) print &quot;URL:&quot;i &quot;\t&quot; &quot;access times:&quot;S[i]&#125;&apos; slow-$&#123;logfile&#125;</span><br></pre></td></tr></table></figure><p>下面是统计iqg_api_v5.access.log这个日志响应时间超过1秒的记录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[work@iqg-yyq2 ~]$ ./loganalysis.sh /data/logs/nginx/iqg_api_v5.access.log 1</span><br><span class="line">split request_time over!!!</span><br><span class="line">build new logfile over!!!</span><br><span class="line">please see slowtime in slowtime.txt!!!</span><br><span class="line">[work@iqg-yyq2 ~]$</span><br></pre></td></tr></table></figure><p>查看最终结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">work@iqg-yyq2 ~]$ head slowtime.txt</span><br><span class="line">100.117.85.64 [11/Mar/2019:14:14:15 &quot;HEAD / 1.223</span><br><span class="line">100.97.74.41 [11/Mar/2019:14:14:16 &quot;HEAD / 1.984</span><br><span class="line">100.97.74.104 [11/Mar/2019:14:14:16 &quot;HEAD / 1.880</span><br><span class="line">100.97.74.5 [11/Mar/2019:14:14:16 &quot;HEAD / 1.758</span><br><span class="line">100.117.85.100 [11/Mar/2019:14:14:16 &quot;HEAD / 1.757</span><br><span class="line">100.97.73.213 [11/Mar/2019:14:14:16 &quot;HEAD / 1.767</span><br><span class="line">100.117.85.160 [11/Mar/2019:14:14:16 &quot;HEAD / 1.662</span><br><span class="line">100.97.74.118 [11/Mar/2019:14:14:16 &quot;HEAD / 1.566</span><br><span class="line">100.97.73.141 [11/Mar/2019:14:14:16 &quot;HEAD / 1.215</span><br><span class="line">100.117.56.238 [11/Mar/2019:14:14:16 &quot;HEAD / 1.132</span><br><span class="line">[work@iqg-yyq2 ~]$</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;分析统计Nginx日志的响应时间&quot;&gt;&lt;a href=&quot;#分析统计Nginx日志的响应时间&quot; class=&quot;headerlink&quot; title=&quot;分析统计Nginx日志的响应时间&quot;&gt;&lt;/a&gt;分析统计Nginx日志的响应时间&lt;/h2&gt;&lt;p&gt;Nginx日志格式如下:&lt;/
      
    
    </summary>
    
      <category term="Linux-Basic" scheme="https://jesse.top/categories/Linux-Basic/"/>
    
      <category term="shell&amp;shell脚本" scheme="https://jesse.top/categories/Linux-Basic/shell-shell%E8%84%9A%E6%9C%AC/"/>
    
    
      <category term="scripts" scheme="https://jesse.top/tags/scripts/"/>
    
  </entry>
  
</feed>

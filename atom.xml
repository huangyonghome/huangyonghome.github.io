<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jesse&#39;s home</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jesse.top/"/>
  <updated>2020-06-29T13:39:20.018Z</updated>
  <id>https://jesse.top/</id>
  
  <author>
    <name>Jesse</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E5%AE%B9%E5%99%A8%E7%AF%87/"/>
    <id>https://jesse.top/2020/06/29/docker/docker学习笔记—容器篇/</id>
    <published>2020-06-29T13:51:55.189Z</published>
    <updated>2020-06-29T13:39:20.018Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker学习笔记—-docker容器篇"><a href="#docker学习笔记—-docker容器篇" class="headerlink" title="docker学习笔记— docker容器篇"></a>docker学习笔记— docker容器篇</h2><p><strong>一.创建并启动容器</strong></p><ul><li><strong>创建容器</strong></li></ul><figure class="highlight docker"><figcaption><span>create -it 镜像名:tag```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* **查看所有容器**(包括运行中,已退出,错误容器)</span><br><span class="line"></span><br><span class="line">```docker ps -a</span><br></pre></td></tr></table></figure><ul><li>查看所有容器的ID</li></ul><figure class="highlight docker"><figcaption><span>ps -qa```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* **启动容器**</span><br><span class="line"></span><br><span class="line">```docker start container_id``` </span><br><span class="line"></span><br><span class="line">* **使用指定的镜像直接创建并启动容器**</span><br><span class="line"></span><br><span class="line">```docker <span class="keyword">run</span> -it 镜像名:标签 COMMAND</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">jesse@jesse-virtual-machine:~$ docker run -it ubuntu:16.04 /bin/bash</span><br><span class="line">root@66a29f973548:/#   ----------此时已经进入到容器的bash环境</span><br></pre></td></tr></table></figure><p>当利用 docker run 来创建容器时，Docker 在后台运行的标准操作包括：</p><p>1.检查本地是否存在指定的镜像，不存在就从公有仓库下载</p><p>2.利用镜像创建并启动一个容器</p><p>3.分配一个文件系统，并在只读的镜像层外面挂载一层可读写层</p><p>4.从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去</p><p>5.从地址池配置一个 ip 地址给容器</p><p>6.执行用户指定的应用程序</p><p>7.执行完毕后容器被终止</p><ul><li><strong>以守护状态运行:</strong></li></ul><p>以守护态运行（加参数-d):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d registry.intra.weibo.com/yushuang3/centos:v1 /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;</span><br></pre></td></tr></table></figure><ul><li><strong>容器终止:</strong></li></ul><p><strong>获取容器输出的信息</strong></p><figure class="highlight docker"><figcaption><span>logs container_id```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">**停止容器**</span><br><span class="line"></span><br><span class="line">```docker stop container_Id ``` </span><br><span class="line"></span><br><span class="line">&gt; 终止一个容器  加入-t=<span class="number">10</span> 表示等待<span class="number">10</span>秒(不加-t选项则默认就是<span class="number">10</span>秒)再次发送SIGKILL信号终止容器</span><br><span class="line"></span><br><span class="line">**重启容器**</span><br><span class="line"></span><br><span class="line">```docker restart container_id</span><br></pre></td></tr></table></figure><p><strong>容器状态:</strong></p><ul><li><strong>status表示容器的状态..</strong></li></ul><ul><li><p><strong>exited 表示容器已经退出</strong></p></li><li><p><strong>up 表示容器正在运行</strong></p></li></ul><figure class="highlight docker"><figcaption><span>run```启动容器时还可以指定其他的配置参数:</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* -h HOSTNAME 或者 --hostname=HOSTNAME.设置容器的主机名</span><br><span class="line">* --dns=IP_ADDRESS:设置容器的DNS.写在容器的/etc/resolv.conf文件中.</span><br></pre></td></tr></table></figure><p>[root@localhost test]$docker run -itd –name busybox1 -h dwd-busybox –dns=8.8.8.8 busybox<br>c25dac9c641705e00f02aefe302987f39f853a1feb8c0d3f32dc1675747edd84</p><p>[root@localhost test]$docker exec -it busybox1 hostname<br>dwd-busybox</p><p>[root@localhost test]$docker exec -it busybox1 cat /etc/resolv.conf<br>nameserver 8.8.8.8<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">但是需要注意的是.这些修改不会被```docker commit```保存,也就是不会保存在镜像中</span><br></pre></td></tr></table></figure></p><p>#将busybox1容器保存为busybox1:test镜像<br>[root@localhost test]$docker commit -m ‘test’ -a ‘jesse’ busybox1 busybox1:test<br>sha256:3c8faee532f177cf8bb8736db89694c2c3ff5be1a30a15d604e450130909d123</p><p>#用这个镜像,启动一个新的busybox1-test的容器<br>[root@localhost test]$docker run –name busybox1-test -itd busybox1:test<br>9326b615e9e3af64336683f7f82e048929de560d4ad0a5caf2485bbc4a62e18c</p><p>#可以看到hostname和dns信息没有被保留<br>[root@localhost test]$docker exec -it busybox1-test hostname<br>9326b615e9e3</p><p>[root@localhost test]$docker exec -it busybox1-test cat /etc/resolv.conf<br>nameserver 114.114.114.114<br>nameserver 114.114.115.115<br><code>`</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;docker学习笔记—-docker容器篇&quot;&gt;&lt;a href=&quot;#docker学习笔记—-docker容器篇&quot; class=&quot;headerlink&quot; title=&quot;docker学习笔记— docker容器篇&quot;&gt;&lt;/a&gt;docker学习笔记— docker容器篇&lt;/
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>使用goreplay收集线上真实http流量</title>
    <link href="https://jesse.top/2020/06/29/Linux-Web/%E4%BD%BF%E7%94%A8goreplay%E6%94%B6%E9%9B%86%E7%BA%BF%E4%B8%8A%E7%9C%9F%E5%AE%9Ehttp%E6%B5%81%E9%87%8F/"/>
    <id>https://jesse.top/2020/06/29/Linux-Web/使用goreplay收集线上真实http流量/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:45:33.890Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用goreplay收集线上真实http流量"><a href="#使用goreplay收集线上真实http流量" class="headerlink" title="使用goreplay收集线上真实http流量"></a>使用goreplay收集线上真实http流量</h2><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>在很多场景中,我们需要将线上服务器的真实Http请求复制转发到某台服务器中(或者测试环境中),并且前提是不影响线上生产业务进行.</p><p>例如:</p><ol><li>通常可能会通过ab等压测工具来对单一http接口进行压测。但如果是需要http服务整体压测，使用ab来压测工作量大且不方便，通过线上流量复制引流，通过将真实请求流量放大N倍来进行压测，能对服务有一个较为全面的检验.</li><li>将线上流量引入到测试环境中,测试某个中间件或者数据库的压力</li><li>上线前在预发布环境，使用线上真实的请求，检查是否准备发布的版本，是否具备发布标准</li><li>用线上的流量转发到预发，检查相同流量下一些指标的反馈情况，检查核心数据是否有改善、优化.</li></ol><a id="more"></a><hr><h3 id="goreplay介绍"><a href="#goreplay介绍" class="headerlink" title="goreplay介绍"></a>goreplay介绍</h3><p>goreplay项目请参考github:<a href="https://github.com/buger/goreplay" target="_blank" rel="noopener">goreplay介绍</a></p><p>goreplay是一款开源网络监控工具,可以在不影响业务的情况下,记录服务器真实流量,将该流量用来做镜像,压力测试,监控和分析等用途.</p><p>简单来说就是goreplay抓取线上真实的流量，并将捕捉到的流量转发到测试服务器上(或者保存到本地文件中)</p><p>goreplay大致工作流程如下:</p><p><img src="https://img2.jesse.top/20200629110035.png" alt=""></p><hr><h3 id="goreplay常见使用方式"><a href="#goreplay常见使用方式" class="headerlink" title="goreplay常见使用方式"></a>goreplay常见使用方式</h3><p>goreplay使用文档参考:<a href="https://github.com/buger/goreplay/wiki" target="_blank" rel="noopener">goreplay文档</a></p><p>常用的一些命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-input-raw 抓取指定端口的流量 gor --input-raw :8080</span><br><span class="line">-output-stdout 打印到控制台</span><br><span class="line">-output-file 将请求写到文件中 gor --input-raw :80 --output-file ./requests.gor</span><br><span class="line">-input-file 从文件中读取请求，与上一条命令呼应 gor --input-file ./requests.gor</span><br><span class="line">-exit-after 5s 持续时间</span><br><span class="line">-http-allow-url url白名单，其他请求将会被丢弃</span><br><span class="line">-http-allow-method 根据请求方式过滤</span><br><span class="line">-http-disallow-url 遇上一个url相反，黑名单，其他的请求会被捕获到</span><br></pre></td></tr></table></figure><blockquote><p>更多命令可以使用 ./gor –help查看</p></blockquote><hr><h3 id="goreplay安装"><a href="#goreplay安装" class="headerlink" title="goreplay安装"></a>goreplay安装</h3><p>在github上下载Linux的二进制文件: <a href="https://github.com/buger/goreplay/releases" target="_blank" rel="noopener">goreplay安装</a></p><blockquote><p>注意.虽然在github上提供了rpm安装包,但是实际安装发现无法安装:</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# rpm -ivh gor-1.0.0-1.x86_64.rpm</span><br><span class="line">Preparing...                          ################################# [100%]</span><br><span class="line">package goreplay-1.0.0-1.x86_64 is intended for a different operating system</span><br></pre></td></tr></table></figure><p>下载github上的二进制文件,解压后是一个gor的二进制可执行文件.复制到PATH变量路径下即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# wget https://github.com/buger/goreplay/releases/download/v1.0.0/gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# ls</span><br><span class="line"> gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# tar -xf gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# ls</span><br><span class="line">gor </span><br><span class="line">[root@dwd-tongji-3 ~]# ll gor</span><br><span class="line">-rwxr-xr-x 1 501 games 17779040 Mar 30  2019 gor</span><br><span class="line">[root@dwd-tongji-3 ~]# cp gor /usr/local/bin/</span><br></pre></td></tr></table></figure><hr><h3 id="goreplay简单实践"><a href="#goreplay简单实践" class="headerlink" title="goreplay简单实践"></a>goreplay简单实践</h3><h4 id="1-将本地http的流量保存到本地文件中"><a href="#1-将本地http的流量保存到本地文件中" class="headerlink" title="1.将本地http的流量保存到本地文件中."></a>1.将本地http的流量保存到本地文件中.</h4><p>为了简便起见,以下命令都在root用户下执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## 1.开启一个screen窗口</span><br><span class="line">[root@dwd-tongji-3 ~]# screen -S GOR</span><br><span class="line">## 2.将80流量保存到本地的文件</span><br><span class="line">[root@dwd-tongji-3 ~]# gor --input-raw :80 --output-file /data/requests.gor</span><br><span class="line">Version: 1.0.0</span><br></pre></td></tr></table></figure><p>默认情况下goreplay会以块文件存储,将流量保存为多个块文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# ls /data/requests_* | more</span><br><span class="line">/data/requests_0.gor</span><br><span class="line">/data/requests_100.gor</span><br><span class="line">/data/requests_101.gor</span><br><span class="line">/data/requests_102.gor</span><br><span class="line">/data/requests_103.gor</span><br><span class="line">/data/requests_104.gor</span><br><span class="line">/data/requests_105.gor</span><br><span class="line">/data/requests_106.gor</span><br><span class="line">/data/requests_107.gor</span><br><span class="line">/data/requests_108.gor</span><br><span class="line">/data/requests_109.gor</span><br><span class="line">/data/requests_10.gor</span><br></pre></td></tr></table></figure><p>使用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>[root@dwd-tongji-3 ~]#./gor –input-raw :80 –output-file /data/gor.gor –output-file-append</p><p>[root@dwd-tongji-3 ~]# ll /data -h<br>total 1.4M<br>-rw-r—– 1 root root 1.4M Jun 29 15:13 gor.gor<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 2.将http的请求打印到终端</span><br></pre></td></tr></table></figure></p><p>[root@dwd-tongji-3 ~]#gor –input-raw :8000 –output-stdout<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 3.将http的请求转发到测试环境</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-http=”<a href="http://beta:80&quot;" target="_blank" rel="noopener">http://beta:80&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在测试服务器上的nginx查看日志,.发现流量已经进来了</span><br></pre></td></tr></table></figure></p><p>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik.php HTTP/1.1” 200 5 “<a href="https://2021001151691008.hybrid.alipay-eco.com/2021001151691008/0.2.2006111453.18/index.html#pages/index/index?appid=2021001151691008&amp;taskId=415&quot;" target="_blank" rel="noopener">https://2021001151691008.hybrid.alipay-eco.com/2021001151691008/0.2.2006111453.18/index.html#pages/index/index?appid=2021001151691008&amp;taskId=415&quot;</a> “Mozilla/5.0 (iPhone; CPU iPhone OS 13_3_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/17D50 Ariver/1.0.12 AliApp(AP/10.1.95.7030) Nebula WK RVKType(0) AlipayDefined(nt:4G,ws:375|667|2.0) AlipayClient/10.1.95.7030 Language/zh-Hans Region/CN NebulaX/1.0.0” “112.96.179.238”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik.php HTTP/1.1” 200 5 “<a href="https://servicewechat.com/wxa090d3923fde0d4b/132/page-frame.html&quot;" target="_blank" rel="noopener">https://servicewechat.com/wxa090d3923fde0d4b/132/page-frame.html&quot;</a> “Mozilla/5.0 (iPhone; CPU iPhone OS 13_5_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 MicroMessenger/7.0.13(0x17000d29) NetType/4G Language/zh_CN” “14.106.171.11”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">####  也可以将流量输出到多个终端</span><br><span class="line"></span><br><span class="line">* 输出到多个http服务器</span><br></pre></td></tr></table></figure></p><p>gor –input-tcp :28020 –output-http “<a href="http://staging.com&quot;" target="_blank" rel="noopener">http://staging.com&quot;</a>  –output-http “<a href="http://dev.com&quot;" target="_blank" rel="noopener">http://dev.com&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 输出到文件或者Http服务器</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-file requests.log –output-http “<a href="http://staging.com&quot;" target="_blank" rel="noopener">http://staging.com&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">####  4.将流量从文件重放到http服务器</span><br><span class="line"></span><br><span class="line">1.首先将请求流量保存到本地文件</span><br></pre></td></tr></table></figure></p><p>sudo ./gor –input-raw :8000 –output-file=requests.gor<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2.再开一个窗口,运行gor,将请求流量从文件中重放</span><br></pre></td></tr></table></figure></p><p>./gor –input-file requests.gor –output-http=”<a href="http://localhost:8001&quot;" target="_blank" rel="noopener">http://localhost:8001&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 压力测试</span><br><span class="line"></span><br><span class="line">goreplay支持将捕获到的生产实际请求流量减少或者放大重播以用于测试环境的压力测试.压力测试一般针对Input流量减少或者放大.例如下面的例子</span><br></pre></td></tr></table></figure></p><h1 id="Replay-from-file-on-2x-speed"><a href="#Replay-from-file-on-2x-speed" class="headerlink" title="Replay from file on 2x speed"></a>Replay from file on 2x speed</h1><p>#将请求流量以2倍的速度放大重播<br>gor –input-file “requests.gor|200%” –output-http “staging.com”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">当然也也支持10%,20%等缩小请求流量</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 限速</span><br><span class="line"></span><br><span class="line">如果受限于测试环境的服务器资源压力,只想重播一部分流量到测试环境中,而不需要所有的实际生产流量,那么就可以用限速功能.有两种策略可以实现限流</span><br><span class="line"></span><br><span class="line">1.随机丢弃请求流量</span><br><span class="line"></span><br><span class="line">2.基于Header或者URL丢弃一定的流量(百分比)</span><br><span class="line"></span><br><span class="line">#####  随机丢弃请求流量</span><br><span class="line"></span><br><span class="line">input和output两端都支持限速,有两种限速算法:**百分比**或者**绝对值**</span><br><span class="line"></span><br><span class="line">* 百分比: input端支持缩小或者放大请求流量,基于指定的策略随机丢弃请求流量</span><br><span class="line">* 绝对值: 如果单位时间(秒)内达到临界值,则丢弃剩余请求流量,下一秒临界值还原</span><br><span class="line"></span><br><span class="line">**用法**:</span><br><span class="line"></span><br><span class="line">在output终端使用&quot;|&quot;运算符指定限速阈值,例如:</span><br><span class="line"></span><br><span class="line">* 使用绝对值限速</span><br></pre></td></tr></table></figure></p><h1 id="staging-server-will-not-get-more-than-ten-requests-per-second"><a href="#staging-server-will-not-get-more-than-ten-requests-per-second" class="headerlink" title="staging.server will not get more than ten requests per second"></a>staging.server will not get more than ten requests per second</h1><p>#staging服务每秒只接收10个请求<br>gor –input-tcp :28020 –output-http “<a href="http://staging.com|10&quot;" target="_blank" rel="noopener">http://staging.com|10&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 使用百分比限速</span><br></pre></td></tr></table></figure></p><h1 id="replay-server-will-not-get-more-than-10-of-requests"><a href="#replay-server-will-not-get-more-than-10-of-requests" class="headerlink" title="replay server will not get more than 10% of requests"></a>replay server will not get more than 10% of requests</h1><h1 id="useful-for-high-load-environments"><a href="#useful-for-high-load-environments" class="headerlink" title="useful for high-load environments"></a>useful for high-load environments</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##### 基于Header或者URL参数限速</span><br><span class="line"></span><br><span class="line">如果header或者URL参数中有唯一值,例如(API key),则可以转发指定百分比的流量到后端,例如:</span><br></pre></td></tr></table></figure></p><h1 id="Limit-based-on-header-value"><a href="#Limit-based-on-header-value" class="headerlink" title="Limit based on header value"></a>Limit based on header value</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%” –http-header-limiter “X-API-KEY: 10%”</p><h1 id="Limit-based-on-URL-param-value"><a href="#Limit-based-on-URL-param-value" class="headerlink" title="Limit based on URL param value"></a>Limit based on URL param value</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%” –http-param-limiter “api_key: 10%”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">###  过滤</span><br><span class="line"></span><br><span class="line">如果只想捕获指定的URL路径请求,或者http头部,或者Http方法,则可以使用过滤功能</span><br><span class="line"></span><br><span class="line">下面是几个例子</span><br><span class="line"></span><br><span class="line">* 只捕获某个URL</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-being-sent-to-the-api-endpoint"><a href="#only-forward-requests-being-sent-to-the-api-endpoint" class="headerlink" title="only forward requests being sent to the /api endpoint"></a>only forward requests being sent to the /api endpoint</h1><p>gor –input-raw :8080 –output-http staging.com –http-allow-url /api<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 拒绝某个URL</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-NOT-being-sent-to-the-api…-endpoint"><a href="#only-forward-requests-NOT-being-sent-to-the-api…-endpoint" class="headerlink" title="only forward requests NOT being sent to the /api… endpoint"></a>only forward requests NOT being sent to the /api… endpoint</h1><p>gor –input-raw :8080 –output-http staging.com –http-disallow-url /api<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 基于正则表达式过滤头部</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-with-an-api-version-of-1-0x"><a href="#only-forward-requests-with-an-api-version-of-1-0x" class="headerlink" title="only forward requests with an api version of 1.0x"></a>only forward requests with an api version of 1.0x</h1><p>gor –input-raw :8080 –output-http staging.com –http-allow-header api-version:^1.0\d</p><h1 id="only-forward-requests-NOT-containing-User-Agent-header-value-“Replayed-by-Gor”"><a href="#only-forward-requests-NOT-containing-User-Agent-header-value-“Replayed-by-Gor”" class="headerlink" title="only forward requests NOT containing User-Agent header value “Replayed by Gor”"></a>only forward requests NOT containing User-Agent header value “Replayed by Gor”</h1><p>gor –input-raw :8080 –output-http staging.com –http-disallow-header “User-Agent: Replayed by Gor”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 过滤HTTP请求方法</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-http “<a href="http://staging.server&quot;" target="_blank" rel="noopener">http://staging.server&quot;</a> \<br>    –http-allow-method GET \<br>    –http-allow-method OPTIONS<br><code>`</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;使用goreplay收集线上真实http流量&quot;&gt;&lt;a href=&quot;#使用goreplay收集线上真实http流量&quot; class=&quot;headerlink&quot; title=&quot;使用goreplay收集线上真实http流量&quot;&gt;&lt;/a&gt;使用goreplay收集线上真实http流量&lt;/h2&gt;&lt;h3 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;背景介绍&lt;/h3&gt;&lt;p&gt;在很多场景中,我们需要将线上服务器的真实Http请求复制转发到某台服务器中(或者测试环境中),并且前提是不影响线上生产业务进行.&lt;/p&gt;
&lt;p&gt;例如:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通常可能会通过ab等压测工具来对单一http接口进行压测。但如果是需要http服务整体压测，使用ab来压测工作量大且不方便，通过线上流量复制引流，通过将真实请求流量放大N倍来进行压测，能对服务有一个较为全面的检验.&lt;/li&gt;
&lt;li&gt;将线上流量引入到测试环境中,测试某个中间件或者数据库的压力&lt;/li&gt;
&lt;li&gt;上线前在预发布环境，使用线上真实的请求，检查是否准备发布的版本，是否具备发布标准&lt;/li&gt;
&lt;li&gt;用线上的流量转发到预发，检查相同流量下一些指标的反馈情况，检查核心数据是否有改善、优化.&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
    
      <category term="goreplay" scheme="https://jesse.top/tags/goreplay/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker网络之overlay</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0--docker%E7%BD%91%E7%BB%9C%E4%B9%8Boverlay/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习--docker网络之overlay/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:14:41.575Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker官网学习-7-docker网络之overlay"><a href="#docker官网学习-7-docker网络之overlay" class="headerlink" title="docker官网学习-7.docker网络之overlay"></a>docker官网学习-7.docker网络之overlay</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>overlay网卡在多个docker宿主机之间创建一个分布式的网络,允许多个容器安全通信.</p><p>当初始化一个swarm集群,或者加入docker宿主机到一个swarm集群中.Docker会在该宿主机上创建2个网络:</p><ul><li>ingress: 负责swarm集群的控制以及数据流量</li><li>docker_gwbridge:一个Bridge网络,负责连接swarm集群中的每个docker节点</li></ul><p>overlay网络的创建方式和bridge一样.也是<figure class="highlight docker"><figcaption><span>network create```命令</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"><span class="comment">### 创建overlay网络</span></span><br><span class="line"></span><br><span class="line">&lt;!--more--&gt;</span><br><span class="line"></span><br><span class="line">**创建overlay网络的前提条件**</span><br><span class="line"></span><br><span class="line"><span class="number">1</span>.防火墙开通以下端口</span><br><span class="line"></span><br><span class="line">* TCP <span class="number">2377</span>——集群管理节点通信</span><br><span class="line">* TCP,UPD <span class="number">7946</span>—节点间通信</span><br><span class="line">* UDP <span class="number">4789</span>—overlay网络流量</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>.初始化docker宿主机为swarm集群的manager角色.命令为```docker swarm init```.或者使用```docker swarm join```命令加入到一个现有的swarm集群.</span><br><span class="line"></span><br><span class="line">这两种方式都会创建默认的ingress overlay网络.</span><br><span class="line"></span><br><span class="line">&gt; 即使你不打算使用swarm服务,但是也要这样做.然后才能创建自定义的overlay网络</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**创建overlay网络命令**</span><br></pre></td></tr></table></figure></p><p>#创建个overlay网络.名字为my-overlay<br>docker network create -d overlay my-overlay<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">如果swarm服务或者独立容器需要和其他docker宿主机上的独立容器通信.需要加上```—attachable```参数</span><br></pre></td></tr></table></figure></p><p>#创建个overlay网络,名字为my-overlay.并且和其他docker宿主机的standalone容器通信.<br>docker network create -d overlay –attachable my-overlay<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 可以自定义地址段,掩码,网关等信息.具体方法见docker network create --help</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 自定义默认Ingress网络</span><br><span class="line"></span><br><span class="line">大部分用户不需要配置ingress网络.但是docker17.05以上版本可以自定义ingress网络.如果默认的Ingress网络iP地址段和已经存在的网络有冲突,则自定义的配置可很有用.</span><br><span class="line"></span><br><span class="line">配置Ingress网络需要先删除ingress,然后重新创建.所以最好是在创建容器服务之前先定义好ingress.如果已经有暴露出端口的服务.则需要先删除服务.</span><br><span class="line"></span><br><span class="line">**自定义默认ingress网络步骤如下:**</span><br><span class="line"></span><br><span class="line">1.查看当前ingress网络.```docker network inspect ingress```.删除所有连接到ingress的容器的服务.</span><br><span class="line"></span><br><span class="line">2.移除现有的ingress网络.```docker network rm ingress</span><br></pre></td></tr></table></figure></p><p>3.使用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p> docker network create \<br>  –driver overlay \<br>  –ingress \<br>  –subnet=10.11.0.0/16 \<br>  –gateway=10.11.0.2 \<br>  –opt com.docker.network.driver.mtu=1200 \<br>  my-ingress<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 可以用自定义名称(my-ingress)来定义ingress网络.但是只允许存在一个自定义Ingress.</span><br><span class="line"></span><br><span class="line">4.重启步骤1中的服务</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 自定义默认docker_gwbridge 接口</span><br><span class="line"></span><br><span class="line">docker_gwbridge是连接overlay网络和docker宿主机物理网卡之间的虚拟网桥.当初始化一个swarm集群,或者将docker宿主机加入到一个swarm集群时,docker会自动创建docker_gwbridge.但是docker_gwbridge不是一个docker服务,而是存在于docker宿主机的内核当中.</span><br><span class="line"></span><br><span class="line">所以需要在加入到swarm集群前先配置好docker_gwbridge.</span><br><span class="line"></span><br><span class="line">**自定义默认docker_gwbridge网络步骤如下:**</span><br><span class="line"></span><br><span class="line">1.停止docker</span><br><span class="line"></span><br><span class="line">2.删除当前```docker_gwbridge</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip link set docker_gwbridge down</span><br><span class="line"></span><br><span class="line">$ sudo ip link del dev docker_gwbridge</span><br></pre></td></tr></table></figure><p>3.开启docker.不要加入或者初始化swarm</p><p>4.手动创建<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 关于docker_gwbridge的更多参数请参考[Bridg dirver options](&lt;https://docs.docker.com/engine/reference/commandline/network_create/#bridge-driver-options&gt;)</span><br></pre></td></tr></table></figure></p><p>$ docker network create \<br>–subnet 10.11.0.0/16 \<br>–opt com.docker.network.bridge.name=docker_gwbridge \<br>–opt com.docker.network.bridge.enable_icc=false \<br>–opt com.docker.network.bridge.enable_ip_masquerade=true \<br>docker_gwbridge<br><code>`</code></p><p>5.初始化或者加入到swarm集群</p><p>### </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;docker官网学习-7-docker网络之overlay&quot;&gt;&lt;a href=&quot;#docker官网学习-7-docker网络之overlay&quot; class=&quot;headerlink&quot; title=&quot;docker官网学习-7.docker网络之overlay&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---Docker镜像篇</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%95%9C%E5%83%8F%E7%AF%87/"/>
    <id>https://jesse.top/2020/06/29/docker/docker学习笔记——镜像篇/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:28:47.322Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker笔记——镜像篇"><a href="#docker笔记——镜像篇" class="headerlink" title="docker笔记——镜像篇"></a>docker笔记——镜像篇</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>docker镜像是一个只读的Docker容器模板.含有启动docker容器所需的文件系统结构以及内容.因此是启动一个容器的基础.docker镜像的文件内容以及一些运行docker容器的配置文件组成了docker容器的静态文件运行环境—rootfs.</p><p>可以这么理解,docker镜像是docker容器的静态视角.docker容器是docker镜像的运行状态</p><hr><p><strong>1.rootfs</strong></p><p>rootfs是docker容器的根目录.如:/dev,/proc,/bin,/etc …….传统的Linux容器操作系统内核启动时,首先挂载一个只读(read-only)的rootfs.当系统检测到完整性后,再将其切换到读写(read-write)模式.而在docker架构中.也沿用了Linux内核的启动方法.在docker为容器挂载rootfs时,将rootfs设置为只读模式,挂载完毕后,在已有的只读rootfs上再挂载一个读写层.</p><p>读写层位于docker容器文件系统的最顶层.下面可能挂载了多个只读层.</p><a id="more"></a><p><strong>2.docker镜像的特点</strong></p><ul><li><strong>分层</strong></li></ul><p>每个镜像都由一系列的”镜像层”组成.当需要修改容器镜像内的某个文件时,只对最上方的读写层进行修改,不覆盖下面的只读层文件系统.例如删除一个只读文件系统中的文件时,只会在读写层标记这个文件”已经被删除”,但是这个文件在只读层中仍然存在.只不过不被用户感知.</p><ul><li><strong>写时复制(copy-on-write)</strong></li></ul><p>每个容器在启动的时候并不需要单独复制一份镜像文件,而是将所有镜像层以只读的方式挂载到一个挂载点,在多个容器之间共享.在未更改镜像文件内容时,所有容器共享一份数据,只有在docker容器运行过程中修改过文件时,才会把变化的文件内容写到读写层.并隐藏只读层中的老版本文件.</p><p>写时复制机制减少了镜像对磁盘空间的占用和容器的启动时间</p><ul><li><strong>联合挂载</strong></li></ul><p>联合挂载技术可以在一个挂载点同时挂载多个文件系统.实现这种联合挂载技术的文件系统被称为联合文件系统(union filesystem).从内核的角度来看,docker容器的文件系统分为只读rootfs层和读写层.但是在用户的视角看来,整个文件系统都是rootfs底层.</p><p>下面这个图可以理解,镜像是由一堆的只读层堆叠起来的统一视角:</p><p><img src="![Docker镜像](https://img1.jesse.top/docker-image1.gif" alt=""></p><p>下面这个图理解了docker镜像和docker容器的区别</p><p><img src="https://img1.jesse.top/docker-container1.png" alt=""></p><hr><h3 id="docker镜像的相关概念"><a href="#docker镜像的相关概念" class="headerlink" title="docker镜像的相关概念"></a>docker镜像的相关概念</h3><p>1.<strong>registry</strong></p><p>registry用来保存docker镜像.可以将registry简单的想象成类似于git仓库之类的实体.当<figure class="highlight docker"><figcaption><span>run```命令启动一个容器时,如果宿主机上并不存在该镜像,那么docker将从registry中下载镜像并保存到宿主机</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">可以使用docker官方的公共registry服务(docker hub),可以可以使用阿里云私有的registry,甚至还可以自己搭建私有的registry</span><br><span class="line"></span><br><span class="line">**<span class="number">2</span>.repository**</span><br><span class="line"></span><br><span class="line">repository是由具有某个功能的docker镜像的所有迭代版本构成的镜像组.repository通常表示镜像所具有的功能,例如ansible/ubunbu14.<span class="number">4</span>-ansible.而顶层仓库则只包含repository名.例如,Ubuntu</span><br><span class="line"></span><br><span class="line">repository是一个镜像集合,包含了多个不同版本的镜像.使用标签进行版本区分,例如ubuntu:<span class="number">14.04</span>,ubuntu12.<span class="number">04</span>.他们均属于ubuntu这个repository</span><br><span class="line"></span><br><span class="line">**总而言之,registry是repository的集合,repository是镜像的集合**</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"><span class="comment">### docker镜像相关的命令</span></span><br><span class="line"></span><br><span class="line">* **拉取镜像**</span><br><span class="line"></span><br><span class="line">```docker pull [OPTIONS] NAME[:TAG|@DIGEST]</span><br></pre></td></tr></table></figure></p><p>如果只指定了镜像名,则默认从docker hub官方拉取该镜像的最新latest版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker pull nginx</span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library/nginx</span><br><span class="line">743f2d6c1f65: Pull complete</span><br><span class="line">6bfc4ec4420a: Pull complete</span><br><span class="line">688a776db95f: Pull complete</span><br><span class="line">Digest: sha256:23b4dcdf0d34d4a129755fc6f52e1c6e23bb34ea011b315d87e193033bcd1b68</span><br><span class="line">Status: Downloaded newer image for nginx:latest</span><br></pre></td></tr></table></figure><p>如果指定了tag,则拉取指定的版本镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker pull nginx:1.15</span><br><span class="line">1.15: Pulling from library/nginx</span><br><span class="line">Digest: sha256:23b4dcdf0d34d4a129755fc6f52e1c6e23bb34ea011b315d87e193033bcd1b68</span><br><span class="line">Status: Downloaded newer image for nginx:1.15</span><br></pre></td></tr></table></figure><p>拉取我阿里云的私人registry下的镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#registry.cn-hangzhou.aliyuncs.com/jesse_images为仓库地址</span><br><span class="line">#php7.1.9为镜像版本</span><br><span class="line">[root@localhost ~]$docker pull registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:php7.1.9</span><br><span class="line"></span><br><span class="line">php7.1.9: Pulling from jesse_images/jesse_images</span><br><span class="line">Digest: sha256:ed9b7326b539f47a81697e51ed8ec698bec49fb62959990c1277d068fc55ff94</span><br><span class="line">Status: Downloaded newer image for registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:php7.1.9</span><br></pre></td></tr></table></figure><hr><ul><li><strong>删除镜像</strong></li></ul><p>命令格式:</p><figure class="highlight docker"><figcaption><span>rmi [OPTIONS] IMAGE [IMAGE…]```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">可以是docker rmi 镜像ID 或者 docker rmi 镜像名:tag</span><br></pre></td></tr></table></figure><p>docker images #查看当前宿主机上的镜像<br>[root@localhost ~]$docker images<br>REPOSITORY                                                    TAG                    IMAGE ID            CREATED             SIZE<br>busybox                                                       latest                 64f5d945efcc        6 days ago          1.2MB<br>nginx                                                         1.15                   53f3fd8007f7        7 days ago          109MB<br>nginx                                                         latest                 53f3fd8007f7        7 days ago          109MB<br>php-swoole                                                    7.1                    aa71c42a22ca        9 days ago          588MB</p><p><none>                                                        <none>                 01f5d7914e61        9 days ago          585MB</none></none></p><p>#删除镜像ID为01f5d7914e61的镜像</p><p>[root@localhost ~]$docker rmi 01f5d7914e61<br>Deleted: sha256:01f5d7914e615b0e2f7cc36a494c876dfc0c678963898374d9ef512d7a762aac<br>Deleted: sha256:b4dd4a057d2561647ff7bf6b299a143c99f66831c129618f49bca5e6ac82f99e<br>Deleted: sha256:c37c880338efd3d340bfa71b35b7653b6cec8eb4f5dfcfab8c7ad0045fef3ce6<br>Deleted: sha256:fb7d015f8921c1244134730b6c21f0bda6c7156ccd421d9e0069d5a1074b48dd<br>Deleted: sha256:ab74760ab0af7680fa9338100c92306392ffeb384b8976045a11dab9a4ebbc57<br>Deleted: sha256:8544a2552375c861955db9034e9c3c5a3e83530b84de9b9bb6d4a7d0d5e5b8ac<br>Deleted: sha256:4eebc2d39a0733b28992a064fc71852297927a3994b01a9d1123d71b042ab729</p><p>#删除nginx.tag为1.15的镜像<br>[root@localhost ~]$docker rmi nginx:1.15<br>Untagged: nginx:1.15<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">* **查看镜像**</span><br><span class="line"></span><br><span class="line">```docker history 镜像名```可以看到镜像的构建分层</span><br></pre></td></tr></table></figure></p><p>[root@localhost ~]$docker history nginx<br>IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT<br>53f3fd8007f7        7 days ago          /bin/sh -c #(nop)  CMD [“nginx” “-g” “daemon…   0B</p><p><missing>           7 days ago          /bin/sh -c #(nop)  STOPSIGNAL SIGTERM           0B</missing></p><p><missing>           7 days ago          /bin/sh -c #(nop)  EXPOSE 80                    0B</missing></p><p><missing>           7 days ago          /bin/sh -c ln -sf /dev/stdout /var/log/nginx…   0B</missing></p><p><missing>           7 days ago          /bin/sh -c set -x  &amp;&amp; apt-get update  &amp;&amp; apt…   54.1MB</missing></p><p><missing>           7 days ago          /bin/sh -c #(nop)  ENV NJS_VERSION=1.15.12.0…   0B</missing></p><p><missing>           7 days ago          /bin/sh -c #(nop)  ENV NGINX_VERSION=1.15.12…   0B</missing></p><p><missing>           7 days ago          /bin/sh -c #(nop)  LABEL maintainer=NGINX Do…   0B</missing></p><p><missing>           8 days ago          /bin/sh -c #(nop)  CMD [“bash”]                 0B</missing></p><p><missing>           8 days ago          /bin/sh -c #(nop) ADD file:fcb9328ea4c115670…   55.3MB<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```docker inspect 镜像名``` 可以看到镜像的具体信息</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">* **创建镜像**</span><br><span class="line"></span><br><span class="line">```docker commit```命令可以基于现有的容器创建出一个镜像</span><br></pre></td></tr></table></figure></missing></p><p>#用法格式:<br>docker commit -m ‘镜像说明信息’   -a  作者  容器ID 镜像名:版本</p><p>[root@localhost ~]$docker commit -h</p><p>Usage:    docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]</p><p>Create a new image from a container’s changes</p><p>Options:<br>  -a, –author string    Author (e.g., “John Hannibal Smith <a href="mailto:&#x68;&#x61;&#110;&#x6e;&#x69;&#x62;&#x61;&#x6c;&#64;&#97;&#x2d;&#116;&#x65;&#x61;&#x6d;&#x2e;&#99;&#x6f;&#x6d;" target="_blank" rel="noopener">&#x68;&#x61;&#110;&#x6e;&#x69;&#x62;&#x61;&#x6c;&#64;&#97;&#x2d;&#116;&#x65;&#x61;&#x6d;&#x2e;&#99;&#x6f;&#x6d;</a>“)<br>  -c, –change list      Apply Dockerfile instruction to the created image<br>  -m, –message string   Commit message<br>  -p, –pause            Pause container during commit (default true)</p><p>  #例如.将正在运行中的Nginx容器提交为一个新的nginx:test镜像<br>[root@localhost ~]$docker commit -m ‘test’ -a ‘jesse’ nginx nginx:test<br>sha256:028f5e2b21a66a1bf5f70727f20cac04e8918f57d5584cc2aeb09f18791d9680<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">* 导入导出镜像</span><br><span class="line"></span><br><span class="line">命令: </span><br><span class="line"></span><br><span class="line">```docker save -o 保存文件名 镜像名:tag```  ————将某个镜像保存为一个文件</span><br><span class="line"></span><br><span class="line">```docker load &lt; 文件名``` or ```docker load —input 文件名``` ——将某个文件导入到本地镜像</span><br><span class="line"></span><br><span class="line">例如</span><br></pre></td></tr></table></figure></p><p>#将nginx:test这个镜像保存为Nginx_test.tar文件<br>[root@localhost ~]$docker save -o nginx_test.tar nginx:test<br>[root@localhost ~]$ll nginx_test.tar<br>-rw——- 1 root root 113036800 5月  16 10:29 nginx_test.tar</p><p>#删除ningx:test这个镜像.然后再从该文件恢复<br>[root@localhost ~]$docker load &lt; nginx_test.tar<br>67392954caf5: Loading layer [==================================================&gt;]  8.192kB/8.192kB<br>Loaded image: nginx:test</p><p>#镜像已经被导入<br>[root@localhost ~]$docker images | grep nginx<br>nginx                                                         test                   028f5e2b21a6        4 minutes ago       109MB<br><code>`</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker笔记——镜像篇&quot;&gt;&lt;a href=&quot;#docker笔记——镜像篇&quot; class=&quot;headerlink&quot; title=&quot;docker笔记——镜像篇&quot;&gt;&lt;/a&gt;docker笔记——镜像篇&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;docker镜像是一个只读的Docker容器模板.含有启动docker容器所需的文件系统结构以及内容.因此是启动一个容器的基础.docker镜像的文件内容以及一些运行docker容器的配置文件组成了docker容器的静态文件运行环境—rootfs.&lt;/p&gt;
&lt;p&gt;可以这么理解,docker镜像是docker容器的静态视角.docker容器是docker镜像的运行状态&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;1.rootfs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;rootfs是docker容器的根目录.如:/dev,/proc,/bin,/etc …….传统的Linux容器操作系统内核启动时,首先挂载一个只读(read-only)的rootfs.当系统检测到完整性后,再将其切换到读写(read-write)模式.而在docker架构中.也沿用了Linux内核的启动方法.在docker为容器挂载rootfs时,将rootfs设置为只读模式,挂载完毕后,在已有的只读rootfs上再挂载一个读写层.&lt;/p&gt;
&lt;p&gt;读写层位于docker容器文件系统的最顶层.下面可能挂载了多个只读层.&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker-compose</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94docker-compose/"/>
    <id>https://jesse.top/2020/06/29/docker/docker学习笔记—docker-compose/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:15:08.679Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker学习笔记——docker-compose"><a href="#docker学习笔记——docker-compose" class="headerlink" title="docker学习笔记——docker-compose"></a>docker学习笔记——docker-compose</h2><p>docker compose 定义并且运行多个docker容器.使用YAML风格文件定义一个compose文件.利用compose文件创建和启动所有服务.</p><p>使用docker compose基本只需要3个步骤</p><ul><li>在Dockerfile文件定义app环境</li><li>在docker-compose.yml文件中定义组成app的各个服务</li><li>run docker-compose up 和compose 启动和运行app</li></ul><p>下面文档均可以在docker-compose官方找到详细资料:<a href="https://docs.docker.com/compose/" target="_blank" rel="noopener">docker-compose</a></p><h3 id="docker-compose安装"><a href="#docker-compose安装" class="headerlink" title="docker-compose安装"></a>docker-compose安装</h3><a id="more"></a><p>docker-compose的安装非常简单.下面是Linux上的安装方法.其他平台请自行参考官网</p><p>1.下载最近的1.24版本的二进制文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose</span><br></pre></td></tr></table></figure><p>2.给予执行权限.加入环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod +x /usr/local/bin/docker-compose</span><br><span class="line">sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose</span><br></pre></td></tr></table></figure><p>3.安装完成.查看是否安装成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose --version</span><br><span class="line">docker-compose version 1.24.0, build 1110ad01</span><br></pre></td></tr></table></figure><hr><h2 id="compose例子"><a href="#compose例子" class="headerlink" title="compose例子"></a>compose例子</h2><p>在官网上,或者去github上下载一个例子.这里我参考<docker 深入浅出="">这本书的例子</docker></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir /data/counter-app</span><br><span class="line">cd /data/counter-app</span><br><span class="line">git clone https://github.com/nigelpoulton/counter-app.git</span><br></pre></td></tr></table></figure><h3 id="docker-compose文件"><a href="#docker-compose文件" class="headerlink" title="docker-compose文件"></a>docker-compose文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost counter-app]$cat docker-compose.yml</span><br><span class="line"></span><br><span class="line">version: &quot; 3.5&quot;</span><br><span class="line">services:</span><br><span class="line">   web-fe:</span><br><span class="line">      build:</span><br><span class="line">         command: python app.py</span><br><span class="line">         ports:</span><br><span class="line">            - target: 5000</span><br><span class="line">              published: 5000</span><br><span class="line">         </span><br><span class="line">         networks:</span><br><span class="line">            - counter-net</span><br><span class="line">         </span><br><span class="line">         volumes:</span><br><span class="line">            - type: volume</span><br><span class="line">              source: counter-vol</span><br><span class="line">              target: /code</span><br><span class="line">    </span><br><span class="line">   redis:</span><br><span class="line">      image: &quot;redis:alpine&quot;</span><br><span class="line">      networks:</span><br><span class="line">         counter-net</span><br><span class="line">   </span><br><span class="line">networks:</span><br><span class="line">        counter-net:</span><br><span class="line">   </span><br><span class="line">volumes:</span><br><span class="line">       counter-vol:</span><br></pre></td></tr></table></figure><p><strong>compose文件结构</strong></p><p>包含4个一级key: version.services.network.volumes</p><ul><li>version: 必须指定,定义了compose文件格式版本.这里是3.5最新版</li><li>services: 用于定义不同的应用服务.这个例子中定义了2个服务.一个是web-fe的web前端.一个是redis的内存数据库.docker compose会将每个服务部署在各自的容器中</li><li>networks用于创建新的网络.默认情况下会创建bridge网络</li><li>volume用于创建新的卷</li></ul><p>上面的docker compose文件定义了2个服务.在web-fe的服务定义中.包含如下指令:</p><ul><li>build:  指定docker基于当前目录下的Dockerfile文件构建一个新镜像</li><li>command: 指定在容器中执行app.py脚本作为主程序 (这个指令可以忽略,因为dockerfile镜像中已经配置了CMD指令)</li><li>ports: 将容器(target)的5000端口映射到宿主机(published)5000端口</li><li>networks: docker将此容器连接到指定的网络上</li><li>volumes: 指定docker将宿主机counter-vol卷(source)挂载到容器内的/code(target)上.counter-vol卷是已经存在的,或者是在文件下方的volumes一级key中定义的</li></ul><p>redis服务比较简单,就不再赘述..</p><hr><h2 id="部署docker-compose"><a href="#部署docker-compose" class="headerlink" title="部署docker-compose"></a>部署docker-compose</h2><p>简要介绍counter-app目录内的几个文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost counter-app]$ll</span><br><span class="line">总用量 20</span><br><span class="line">-rw-r--r-- 1 root root 599 6月  18 17:34 app.py    #应用程序代码</span><br><span class="line">-rw-r--r-- 1 root root 475 6月  17 18:46 docker-compose.ymal  #compose文件,定义了如何部署容器</span><br><span class="line">-rw-r--r-- 1 root root 109 6月  18 17:34 Dockerfile  #构建web-fe服务镜像的dockerfile</span><br><span class="line">-rw-r--r-- 1 root root 128 6月  18 17:34 README.md   </span><br><span class="line">-rw-r--r-- 1 root root  11 6月  18 17:34 requirements.txt #列出app.py代码文件中python的依赖包</span><br></pre></td></tr></table></figure><h4 id="启动docker-compose"><a href="#启动docker-compose" class="headerlink" title="启动docker-compose"></a>启动docker-compose</h4><p>在当前目录下执行下列路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose up -d #后台启动</span><br></pre></td></tr></table></figure><p>默认情况下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>docker-compose -f compose_file up -d<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">如果找不到文件则会报错:</span><br></pre></td></tr></table></figure></p><p>ERROR:<br>        Can’t find a suitable configuration file in this directory or any<br>        parent. Are you in the right directory?</p><pre><code>Supported filenames: docker-compose.yml, docker-compose.yaml</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">部署过程中创建或者拉取了3个镜像: counterapp_web-fe,python,redis</span><br><span class="line"></span><br><span class="line">部署完成后,启动了如下2个容器:</span><br></pre></td></tr></table></figure><p>[root@localhost counter-app]$docker ps<br>CONTAINER ID        IMAGE                             COMMAND                  CREATED             STATUS              PORTS                    NAMES<br>474301996ccc        redis:alpine                      “docker-entrypoint.s…”   21 hours ago        Up 21 hours         6379/tcp                 counter-app_redis_1<br>c7a1e28b5e28        counter-app_web-fe                “python app.py”          21 hours ago        Up 21 hours         0.0.0.0:5000-&gt;5000/tcp   counter-app_web-fe_1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">每个容器都以项目名为前缀(所在目录名称).此外,还用一个数字为后缀用于表示容器序列(因为docker-compose允许扩容和缩减服务器数量)</span><br><span class="line"></span><br><span class="line">同时,docker-compose还创建了counter-app_counter-net网络:</span><br></pre></td></tr></table></figure></p><p>[root@localhost counter-app]$docker network ls<br>NETWORK ID          NAME                      DRIVER              SCOPE<br>6d40a81d76e7        bridge                    bridge              local<br>ef71284e9acc        counter-app_counter-net   bridge              local<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">应用部署成功后,可以查看容器的运行效果.每次访问,计数器就+1</span><br></pre></td></tr></table></figure></p><p>[root@localhost counter-app]$curl <a href="http://localhost:5000" target="_blank" rel="noopener">http://localhost:5000</a><br>What’s up Docker Deep Divers! You’ve visited me 1 times.<br>[root@localhost counter-app]$curl <a href="http://localhost:5000" target="_blank" rel="noopener">http://localhost:5000</a><br>What’s up Docker Deep Divers! You’ve visited me 2 times.<br>[root@localhost counter-app]$curl <a href="http://localhost:5000" target="_blank" rel="noopener">http://localhost:5000</a><br>What’s up Docker Deep Divers! You’ve visited me 3 times.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## docker Compose管理</span><br><span class="line"></span><br><span class="line">上面讲到如何部署一个compose应用..接下来讲解一下compose的管理命令.需要注意的是所有的docker-compose命令都需要在相关目录下执行.不然仍然会提示找不到docker-compose.yml(yaml)文件</span><br><span class="line"></span><br><span class="line">如果是停止应用.只需将up换成down即可.</span><br></pre></td></tr></table></figure></p><p>[root@localhost counter-app]$docker-compose down<br>Stopping counter-app_redis_1  … done<br>Stopping counter-app_web-fe_1 … done<br>Removing counter-app_redis_1  … done<br>Removing counter-app_web-fe_1 … done<br>Removing network counter-app_counter-net<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">停止compose经历了如下的过程:</span><br><span class="line"></span><br><span class="line">* 停止所有容器</span><br><span class="line">* 移除容器</span><br><span class="line">* 移除docker网络</span><br><span class="line"></span><br><span class="line">此时,无论是执行```docker ps ```还是```docker ps -a```都看不到容器</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 查看compose各个服务容器的运行的进程</span><br></pre></td></tr></table></figure></p><p>[root@localhost counter-app]$docker-compose top<br>counter-app_redis_1</p><h2 id="UID-PID-PPID-C-STIME-TTY-TIME-CMD"><a href="#UID-PID-PPID-C-STIME-TTY-TIME-CMD" class="headerlink" title="UID    PID    PPID    C   STIME   TTY     TIME         CMD"></a>UID    PID    PPID    C   STIME   TTY     TIME         CMD</h2><p>100   32558   32542   0   13:21   ?     00:00:00   redis-server</p><p>counter-app_web-fe_1</p><h2 id="UID-PID-PPID-C-STIME-TTY-TIME-CMD-1"><a href="#UID-PID-PPID-C-STIME-TTY-TIME-CMD-1" class="headerlink" title="UID     PID    PPID    C   STIME   TTY     TIME                    CMD"></a>UID     PID    PPID    C   STIME   TTY     TIME                    CMD</h2><p>root   32582   32564   6   13:21   ?     00:00:00   python app.py<br>root   32703   32582   4   13:21   ?     00:00:00   /usr/local/bin/python /code/app.py<br>[root@localhost counter-app]$<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; PID是docker宿主机的进程ID</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 停止应用容器.但是并不删除资源</span><br><span class="line"></span><br><span class="line">执行完```docker-compose stop```命令后,容器还存在</span><br></pre></td></tr></table></figure></p><p>[root@localhost counter-app]$docker-compose stop<br>Stopping counter-app_web-fe_1 … done<br>Stopping counter-app_redis_1  … done</p><p>[root@localhost counter-app]$docker-compose ps</p><pre><code>Name                      Command               State    Ports</code></pre><hr><p>counter-app_redis_1    docker-entrypoint.sh redis …   Exit 0<br>counter-app_web-fe_1   python app.py                    Exit 0<br>[root@localhost counter-app]$<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 删除,重启已停止的compose应用容器</span><br></pre></td></tr></table></figure></p><p>docker-compose rm #删除.删除应用相关的容器,但是不会删除卷和镜像和网络.<br>docker-compose restart #重启<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### </span><br><span class="line"></span><br><span class="line">#### 拉取服务镜像</span><br><span class="line"></span><br><span class="line">```docker-compose pull server_name ```这个命令会先拉取服务镜像到本地.例如在本文的docker compose例子中有2个服务:web-fe和redis.如果执行下列命令,会仅仅拉取redis镜像到本地</span><br></pre></td></tr></table></figure></p><p>docker-compose pull redis<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 其他命令</span><br><span class="line"></span><br><span class="line">日常docker管理容器的命令都可以使用```docker-compose```替代.例如:</span><br></pre></td></tr></table></figure></p><p>docker-compose logs service_name #查看服务容器日志<br>docker-compose exec service_name #开启终端登陆容器<br>docker-compose kill -s SIGINT    #杀死docker-compose服务容器<br>docker-compose ps                #列出容器<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### docker-compose配置文件指令解析</span><br><span class="line"></span><br><span class="line">以下配置文件以版本3.x为例.官网参考:&lt;https://docs.docker.com/compose/compose-file/&gt;</span><br><span class="line"></span><br><span class="line">下面是个包含完整指令的样例:</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:</p><p>  redis:<br>    image: redis:alpine<br>    ports:</p><pre><code>  - &quot;6379&quot;networks:  - frontenddeploy:  replicas: 2  update_config:    parallelism: 2    delay: 10s  restart_policy:    condition: on-failure</code></pre><p>  db:<br>    image: postgres:9.4<br>    volumes:</p><pre><code>  - db-data:/var/lib/postgresql/datanetworks:  - backenddeploy:  placement:    constraints: [node.role == manager]</code></pre><p>  vote:<br>    image: dockersamples/examplevotingapp_vote:before<br>    ports:</p><pre><code>  - &quot;5000:80&quot;networks:  - frontenddepends_on:  - redisdeploy:  replicas: 2  update_config:    parallelism: 2  restart_policy:    condition: on-failure</code></pre><p>  result:<br>    image: dockersamples/examplevotingapp_result:before<br>    ports:</p><pre><code>  - &quot;5001:80&quot;networks:  - backenddepends_on:  - dbdeploy:  replicas: 1  update_config:    parallelism: 2    delay: 10s  restart_policy:    condition: on-failure</code></pre><p>  worker:<br>    image: dockersamples/examplevotingapp_worker<br>    networks:</p><pre><code>  - frontend  - backenddeploy:  mode: replicated  replicas: 1  labels: [APP=VOTING]  restart_policy:    condition: on-failure    delay: 10s    max_attempts: 3    window: 120s  placement:    constraints: [node.role == manager]</code></pre><p>  visualizer:<br>    image: dockersamples/visualizer:stable<br>    ports:</p><pre><code>  - &quot;8080:8080&quot;stop_grace_period: 1m30svolumes:  - &quot;/var/run/docker.sock:/var/run/docker.sock&quot;deploy:  placement:    constraints: [node.role == manager]</code></pre><p>networks:<br>  frontend:<br>  backend:</p><p>volumes:<br>  db-data:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Service块级别的配置文件指令</span><br><span class="line"></span><br><span class="line">#### build </span><br><span class="line"></span><br><span class="line">build可以指定一个目录或者在build下还可以指定context上下文环境和docker-file文件名</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  webapp:<br>    build: ./dir</p><p>或者<br>webapp:<br>    build:<br>      context: ./dir<br>      dockerfile: Dockerfile-alternate #如果指定dockerfile,则必须要指定一个build路径,也就是context<br>      args:<br>        buildno: 1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">如果同时指定了Image关键字,那么会构建一个指定的镜像名:tag</span><br></pre></td></tr></table></figure></p><p>build: ./dir<br>image: webapp:tag  #构建webapp:tag的镜像名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; build选项在swarm中部署stack时是无效的,因为docker stack命令只接受已经build好的镜像</span><br><span class="line"></span><br><span class="line">#### CONTEXT</span><br><span class="line"></span><br><span class="line">定义上下文目录.如果是一个相对目录,那么是相对Compose file文件的目录.</span><br><span class="line"></span><br><span class="line">#### ARGS</span><br><span class="line"></span><br><span class="line">在build过程中可以允许使用ARGS变量传递给dockerfile.具体用法参考官网</span><br><span class="line"></span><br><span class="line">#### COMMAND</span><br><span class="line"></span><br><span class="line">重写dockerfile或者镜像中的默认命令.和dockerfile一样可以是shell方式也可以是exec方式执行</span><br></pre></td></tr></table></figure></p><p>command: bundle exec thin -p 3000<br>command: [“bundle”, “exec”, “thin”, “-p”, “3000”]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### configs</span><br><span class="line"></span><br><span class="line">授予每个service的配置文件访问.具体用法参考官网</span><br><span class="line"></span><br><span class="line">#### container_name</span><br><span class="line"></span><br><span class="line">指定一个容器名,而不是使用默认名字</span><br></pre></td></tr></table></figure></p><p>container_name: my-web-container<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 需要注意的是由于容器名必须唯一,所以当扩展多个容器副本时,指定一个具体的容器名会报错,所以这个指令在swarm模式下部署stack时会被忽略</span><br><span class="line"></span><br><span class="line">#### depends_on</span><br><span class="line"></span><br><span class="line">用于在多个services之间指定依赖性.service dependencies会导致以下行为</span><br><span class="line"></span><br><span class="line">* ```docker-compose up``` 启动时会参考depndency顺序.在下面这个例子中.db和redis服务会先于web服务启动</span><br><span class="line">* ```docker-compose up SERVICE``` 会自动启动该SERVICE的依赖服务.在下面例子中```docker-compose up web```命令会自动创建和启动db和redis</span><br><span class="line">* ```docker-compose stop```会参考依赖顺序而停止服务.在下面例子中,web服务会先于db和redis服务停止</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  web:<br>    build: .<br>    depends_on:</p><pre><code>- db- redis</code></pre><p>  redis:<br>    image: redis<br>  db:<br>    image: postgres<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 使用depens_on需要注意以下几点:</span><br><span class="line">&gt;</span><br><span class="line">&gt; 1.denpds_on只会在web依赖的服务启动后就启动web服务,而不是等待db和redis服务启动并且处于ready状态才启动web.这有可能会带来一些问题,比如mysql启动较慢,数据库还没准备好等.如果你需要确定后端的db,redis数据库启动成功,并且可以连接时才启动web服务,可以参考https://docs.docker.com/compose/startup-order/</span><br><span class="line">&gt;</span><br><span class="line">&gt; 2.version3版本不再支持depends_on下的condition指令</span><br><span class="line">&gt;</span><br><span class="line">&gt; 3.version3版本的depends_on选项在swarm模式下部署stack时会被忽略</span><br><span class="line"></span><br><span class="line">depends_on选项的控制启动顺序参考:</span><br><span class="line"></span><br><span class="line">编写一个shell脚本循环判断后端的数据库是否ready.如果ready则执行CMD命令.然后在command指令中指定脚本的后端db数据库服务名,以及CMD命令参数</span><br></pre></td></tr></table></figure></p><p>#!/bin/sh</p><h1 id="wait-for-postgres-sh"><a href="#wait-for-postgres-sh" class="headerlink" title="wait-for-postgres.sh"></a>wait-for-postgres.sh</h1><p>#循环测试db服务($1参数)的状态.一旦可以连接了,执行cmd命令(python app.py)<br>set -e</p><p>host=”$1”<br>shift<br>cmd=”$@”</p><p>until PGPASSWORD=$POSTGRES_PASSWORD psql -h “$host” -U “postgres” -c ‘\q’; do</p><blockquote><p>&amp;2 echo “Postgres is unavailable - sleeping”<br>  sleep 1<br>done</p></blockquote><blockquote><p>&amp;2 echo “Postgres is up - executing command”<br>exec $cmd<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p></blockquote><p>command: [“./wait-for-postgres.sh”, “db”, “python”, “app.py”]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### deploy指令</span><br><span class="line"></span><br><span class="line">该指令用于配置服务相关的配置和部署方式.这个指令只在version3版本支持,而且只在swarm模式下才生效.单机```docker-compose up```方式执行会被忽略</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  redis:<br>    image: redis:alpine<br>    deploy:<br>      replicas: 6<br>      update_config:<br>        parallelism: 2<br>        delay: 10s<br>      restart_policy:<br>        condition: on-failure<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">下面是有关deploy的几个子指令介绍</span><br><span class="line"></span><br><span class="line">* ENDPOINT_MODE</span><br><span class="line"></span><br><span class="line">&gt; version 3.3 only</span><br><span class="line"></span><br><span class="line">```endpoint_mode: VIP```  Docker为service分配一个虚拟IP.作为用户的前端入口.docker路由用户请求到所有可用的worker节点..这也是默认模式</span><br><span class="line"></span><br><span class="line">```endpoint_mode: dnsrr``` DNS轮询服务,Docker发起一个service name的DNS查询,并且返回一个包含多个IP地址的列表.客户端通过轮询方式链接其中一个IP地址.</span><br><span class="line"></span><br><span class="line">* LABLES</span><br><span class="line"></span><br><span class="line">为service指定一个标签.只对service生效,无法为service的具体某个容器生效</span><br><span class="line"></span><br><span class="line">* MODE</span><br><span class="line"></span><br><span class="line">mode定义了在swarm节点上的副本部署模式,有global和replicated两种模式,默认是replicated</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  worker:<br>    image: dockersamples/examplevotingapp_worker<br>    deploy:<br>      mode: global<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">关于两种模式的区别参考:&lt;https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/#replicated-and-global-services&gt;</span><br><span class="line"></span><br><span class="line">* REPLACEMENT</span><br><span class="line"></span><br><span class="line">定义了constrants(约束条件)和preferences.的参数.</span><br><span class="line"></span><br><span class="line">下面是constrants指令的用法:</span><br><span class="line"></span><br><span class="line">constrants指令可以限制某个task在哪些swarm节点上运行.多个constrants指令是逻辑AND的关系来匹配满足条件的nodes.constrants可以匹配swarm节点或者Docker引擎标签:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">| node attribute  | matches                  | example                                       |</span><br><span class="line">| :-------------- | :----------------------- | :-------------------------------------------- |</span><br><span class="line">| `node.id`       | Node ID                  | `node.id==2ivku8v2gvtg4`                      |</span><br><span class="line">| `node.hostname` | Node hostname            | `node.hostname!=node-2`                       |</span><br><span class="line">| `node.role`     | Node role                | `node.role==manager`                          |</span><br><span class="line">| `node.labels`   | user defined node labels | `node.labels.security==high`                  |</span><br><span class="line">| `engine.labels` | Docker Engine&apos;s labels   | `engine.labels.operatingsystem==ubuntu 14.04` |</span><br><span class="line"></span><br><span class="line">例如下面这个例子中限制redis service的task运行在lable标签等于queue的swarm节点</span><br></pre></td></tr></table></figure></p><p>$ docker service create \<br>  –name redis_2 \<br>  –constraint ‘node.labels.type == queue’ \<br>  redis:3.0.6<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">回到刚才REPLACEMENT的例子,下面的例子中表示db service只运行在swarm manager节点,而且docker node节点的操作系统是Ubuntu 14.04</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  db:<br>    image: postgres<br>    deploy:<br>      placement:<br>        constraints:</p><pre><code>  - node.role == manager  - engine.labels.operatingsystem == ubuntu 14.04preferences:  - spread: node.labels.zone</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* REPLICAS</span><br><span class="line"></span><br><span class="line">如果service 是replicated模式(默认模式),定义容器的启动数量.在下面的例子中启动6个worker容器</span><br></pre></td></tr></table></figure><p>version: “3.7”<br>services:<br>  worker:<br>    image: dockersamples/examplevotingapp_worker<br>    networks:</p><pre><code>  - frontend  - backenddeploy:  mode: replicated  replicas: 6</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* RESOURCES</span><br><span class="line"></span><br><span class="line">配置容器限定的使用资源</span><br><span class="line"></span><br><span class="line">在下面这个例子中.redis service被限制只允许使用不超过50M内存,以及0.5的CPU处理器时间(单个CPU内核的50%).并且有20M内存和0.25的CPU处理器时间预留(也就是永远为redis service保留)</span><br></pre></td></tr></table></figure><p>version: “3.7”<br>services:<br>  redis:<br>    image: redis:alpine<br>    deploy:<br>      resources:<br>        limits:<br>          cpus: ‘0.50’<br>          memory: 50M<br>        reservations:<br>          cpus: ‘0.25’<br>          memory: 20M<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">**Out Of Memory Exceptions[OOME]**</span><br><span class="line"></span><br><span class="line">如果service 或者容器使用了超过限制的资源.容器或者docker引擎就会出现OOME错误.docker进程可能会被内核OOM killer给kill掉.关于如何规避这种问题,请参考[understand the risks of running out of memory](&lt;https://docs.docker.com/config/containers/resource_constraints/&gt;)</span><br><span class="line"></span><br><span class="line">* RESTART_POLICY</span><br><span class="line"></span><br><span class="line">配置当容器停止时如何重新启动容器的策略.有以下几种子指令</span><br><span class="line"></span><br><span class="line">1.```condition``` 重启容器的约束条件.有: ```none```,```on-failure```,和```any```(default:any)</span><br><span class="line"></span><br><span class="line">2.```delay```: 尝试重启容器的时间间隔.默认是0</span><br><span class="line"></span><br><span class="line">3.```max_attempts```:如果容器重启失败,重启最大尝试次数,默认是一直尝试</span><br><span class="line"></span><br><span class="line">4.```window```:重启后等待多久认定重启成功.默认是immediately</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  redis:<br>    image: redis:alpine<br>    deploy:<br>      restart_policy:<br>        condition: on-failure<br>        delay: 5s<br>        max_attempts: 3<br>        window: 120s<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### ROLLBACK_CONFIG</span><br><span class="line"></span><br><span class="line">更新失败的回滚指令.</span><br><span class="line"></span><br><span class="line">#### UPDATE_CONFIG</span><br><span class="line"></span><br><span class="line">配置service如何进行滚动更新.</span><br><span class="line"></span><br><span class="line">#### DNS</span><br><span class="line"></span><br><span class="line">自定义DNS地址,可以是单个值或者一个列表</span><br></pre></td></tr></table></figure></p><p>dns: 8.8.8.8</p><p>dns:</p><ul><li>8.8.8.8</li><li>9.9.9.9<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### entrypoint</span><br><span class="line"></span><br><span class="line">重写dockerfile或者镜像中的entrypoint指令</span><br></pre></td></tr></table></figure></li></ul><p>entrypoint: /code/entrypoint.sh</p><p>#也可以是一个列表格式:</p><p>entrypoint:</p><pre><code>- php- -d- zend_extension=/usr/local/lib/php/extensions/no-debug-non-zts-20100525/xdebug.so- -d- memory_limit=-1- vendor/bin/phpunit</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### env_file</span><br><span class="line"></span><br><span class="line">添加环境变量文件,可以是单个值,也可以是个列表.该文件最好是在当前docker-compose文件目录或者子目录下.</span><br><span class="line"></span><br><span class="line">如果是指定多个变量文件,而且有重复的变量且赋值不同,那么以最后一个变量文件的变量为准</span><br></pre></td></tr></table></figure><p>services:<br>  some-service:<br>    env_file:</p><pre><code>- a.env- b.env</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### environment</span><br><span class="line"></span><br><span class="line">添加环境变量.可以使用列表格式,或者字典格式</span><br></pre></td></tr></table></figure><p>environment:<br>  RACK_ENV: development<br>  SHOW: ‘true’<br>  SESSION_SECRET:</p><p>environment:</p><ul><li>RACK_ENV=development</li><li>SHOW=true</li><li>SESSION_SECRET<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### extra_hosts</span><br><span class="line"></span><br><span class="line">添加hostname和IP地址的绑定映射到hosts文件</span><br></pre></td></tr></table></figure></li></ul><p>extra_hosts:</p><ul><li>“somehost:162.242.195.82”</li><li><p>“otherhost:50.31.209.229”</p><p>#会在容器内的/etc/hosts文件生成如何内容<br>162.242.195.82  somehost<br>50.31.209.229   otherhost</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### healthcheck</span><br><span class="line"></span><br><span class="line">检查service的各个容器是否处于&quot;healthy&quot;状态.例如下面的例子</span><br></pre></td></tr></table></figure></li></ul><p>healthcheck:<br>  test: [“CMD”, “curl”, “-f”, “<a href="http://localhost&quot;]" target="_blank" rel="noopener">http://localhost&quot;]</a><br>  interval: 1m30s<br>  timeout: 10s<br>  retries: 3<br>  start_period: 40s<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```test```指令必须为一个字符串或者一个列表.如果是个上例子中的列表格式.则第一个参数必须为```NONE```,```CMD```,或者```CMD-SHELL```.如果是字符串相当于指定了```CMD-SHELL```参数</span><br><span class="line"></span><br><span class="line">下面2个写法和上文的例子效果一样</span><br></pre></td></tr></table></figure></p><p>test: [“CMD-SHELL”, “curl -f <a href="http://localhost" target="_blank" rel="noopener">http://localhost</a> || exit 1”]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>test: curl -f <a href="https://localhost" target="_blank" rel="noopener">https://localhost</a> || exit 1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">要关闭健康检查,可以使用```disable:true```.等同于```test:[&quot;NONE&quot;]</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">healthcheck:</span><br><span class="line">  disable: true</span><br></pre></td></tr></table></figure><h4 id="image"><a href="#image" class="headerlink" title="image"></a>image</h4><p>指定容器的启动镜像.可以是指定的repository/tag 或者一个镜像ID.如果本地不存在该镜像,会尝试去pull镜像到本地.如果指定了<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">下面这几种写法均正确</span><br></pre></td></tr></table></figure></p><p>image: redis<br>image: ubuntu:14.04<br>image: tutum/influxdb<br>image: example-registry.com:4000/postgresql<br>image: a4bc65fd<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### logging</span><br><span class="line"></span><br><span class="line">service的log配置.下面的例子中指定了一个syslog服务器的地址</span><br></pre></td></tr></table></figure></p><p>logging:<br>  driver: syslog<br>  options:<br>    syslog-address: “tcp://192.168.0.42:123”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```driver```为容器指定logging的驱动,一共以下3种驱动方式,默认是json-file</span><br></pre></td></tr></table></figure></p><p>driver: “json-file”<br>driver: “syslog”<br>driver: “none”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">也可以限定json-file驱动的日志转出.例如下列指定了最大的日志文件大小和日志保留份数</span><br></pre></td></tr></table></figure></p><p>options:<br>  max-size: “200k”<br>  max-file: “10”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### network_mode</span><br><span class="line"></span><br><span class="line">指定网络模式,有以下几种网络模式</span><br></pre></td></tr></table></figure></p><p>network_mode: “bridge”<br>network_mode: “host”<br>network_mode: “none”<br>network_mode: “service:[service name]”<br>network_mode: “container:[container name/id]”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### networks</span><br><span class="line"></span><br><span class="line">services加入的网络.这些网络名在顶级```network```指令中有指定</span><br></pre></td></tr></table></figure></p><p>services:<br>  some-service:<br>    networks:</p><pre><code>- some-network- other-network</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### IPV4_ADDRESS,IPV6_ADDRESS</span><br><span class="line"></span><br><span class="line">为容器指定一个静态的IP地址.但是对应的Network顶级指令中必须指定一个ipam块,定义该网络的IP子网范围.例如</span><br><span class="line"></span><br><span class="line">app_net网络的ipam快中指定了172.16.238.0/24的子网.然后为app service指定一个静态IP</span><br></pre></td></tr></table></figure><p>version: “3.7”</p><p>services:<br>  app:<br>    image: nginx:alpine<br>    networks:<br>      app_net:<br>        ipv4_address: 172.16.238.10<br>        ipv6_address: 2001:3984:3989::10</p><p>networks:<br>  app_net:<br>    ipam:<br>      driver: default<br>      config:</p><pre><code>- subnet: &quot;172.16.238.0/24&quot;- subnet: &quot;2001:3984:3989::/64&quot;</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### ports</span><br><span class="line"></span><br><span class="line">暴露端口.下面是几种短格式写法.推荐将端口用双引号括起来</span><br></pre></td></tr></table></figure><p>ports:</p><ul><li>“3000”</li><li>“3000-3005”</li><li>“8000:8000”</li><li>“9090-9091:8080-8081”</li><li>“49100:22”</li><li>“127.0.0.1:8001:8001”</li><li>“127.0.0.1:5000-5010:5000-5010”</li><li>“6060:6060/udp”<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 此外还有完整格式的写法.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### restart</span><br><span class="line"></span><br><span class="line">默认的restart策略是```no```有以下四种重启策略</span><br></pre></td></tr></table></figure></li></ul><p>restart: “no”<br>restart: always<br>restart: on-failure<br>restart: unless-stopped<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### sysctls</span><br><span class="line"></span><br><span class="line">配置容器的内核参数.可以是数组或者字典类型</span><br></pre></td></tr></table></figure></p><p>sysctls:<br>  net.core.somaxconn: 1024<br>  net.ipv4.tcp_syncookies: 0</p><p>sysctls:</p><ul><li>net.core.somaxconn=1024</li><li>net.ipv4.tcp_syncookies=0<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### ulimits</span><br><span class="line"></span><br><span class="line">配置ulimits</span><br></pre></td></tr></table></figure></li></ul><p>ulimits:<br>  nproc: 65535<br>  nofile:<br>    soft: 20000<br>    hard: 40000<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### volumes</span><br><span class="line"></span><br><span class="line">挂载一个路径或者一个卷.</span><br><span class="line"></span><br><span class="line">如果是在service层级挂载宿主机上的路径到容器,那么不需要在顶级指令中定义```volumes```key.但是如果是挂载一个卷到多个service,可以在顶级指令中定义个卷名.然后使用这个卷名去挂载</span><br><span class="line"></span><br><span class="line">下面这个例子在顶级```volumes```指令中定义了2个卷名:mydata和dbdata. mydata被web service挂载.dbdata被db service挂载.下面2个挂载格式都可以.</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  web:<br>    image: nginx:alpine<br>    volumes:</p><pre><code>- type: volume  source: mydata  target: /data  volume:    nocopy: true- type: bind  source: ./static  target: /opt/app/static</code></pre><p>  db:<br>    image: postgres:latest<br>    volumes:</p><pre><code>- &quot;/var/run/postgres/postgres.sock:/var/run/postgres/postgres.sock&quot;- &quot;dbdata:/var/lib/postgresql/data&quot;</code></pre><p>volumes:<br>  mydata:<br>  dbdata:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">简短格式写法</span><br></pre></td></tr></table></figure></p><p>volumes:</p><h1 id="Just-specify-a-path-and-let-the-Engine-create-a-volume"><a href="#Just-specify-a-path-and-let-the-Engine-create-a-volume" class="headerlink" title="Just specify a path and let the Engine create a volume"></a>Just specify a path and let the Engine create a volume</h1><ul><li><p>/var/lib/mysql</p><h1 id="Specify-an-absolute-path-mapping"><a href="#Specify-an-absolute-path-mapping" class="headerlink" title="Specify an absolute path mapping"></a>Specify an absolute path mapping</h1></li><li><p>/opt/data:/var/lib/mysql</p><h1 id="Path-on-the-host-relative-to-the-Compose-file"><a href="#Path-on-the-host-relative-to-the-Compose-file" class="headerlink" title="Path on the host, relative to the Compose file"></a>Path on the host, relative to the Compose file</h1></li><li><p>./cache:/tmp/cache</p><h1 id="User-relative-path"><a href="#User-relative-path" class="headerlink" title="User-relative path"></a>User-relative path</h1></li><li><p>~/configs:/etc/configs/:ro</p><h1 id="Named-volume"><a href="#Named-volume" class="headerlink" title="Named volume"></a>Named volume</h1></li><li>datavolume:/var/lib/mysql<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">完整格式写法</span><br></pre></td></tr></table></figure></li></ul><p>version: “3.7”<br>services:<br>  web:<br>    image: nginx:alpine<br>    ports:</p><pre><code>  - &quot;80:80&quot;volumes:  - type: volume    source: mydata    target: /data    volume:      nocopy: true  - type: bind    source: ./static    target: /opt/app/static</code></pre><p>networks:<br>  webnet:</p><p>volumes:<br>  mydata:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## Volume块级别配置文件指令</span><br><span class="line"></span><br><span class="line">大部分volume的用法在上面都已经解释过了,Volume的顶级指令配置不多.</span><br><span class="line"></span><br><span class="line">#### driver</span><br><span class="line"></span><br><span class="line">指定volume卷的驱动,docker引擎默认指定的驱动是```local```.</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## Network块级别配置文件指令</span><br><span class="line"></span><br><span class="line">详细的docker network特性以及所有network驱动请见:[Network guide](&lt;https://docs.docker.com/compose/networking/&gt;)</span><br><span class="line"></span><br><span class="line">默认情况下Compose启动单一网络,一个services的每个容器加入到默认的网络,并且该网络下的所有容器之间都能互相访问</span><br><span class="line"></span><br><span class="line">假如下面的compose文件在```myapp```目录下.</span><br></pre></td></tr></table></figure></p><p>version: “3”<br>services:<br>  web:<br>    build: .<br>    ports:</p><pre><code>- &quot;8000:8000&quot;</code></pre><p>  db:<br>    image: postgres<br>    ports:</p><pre><code>- &quot;8001:5432&quot;</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">当执行```docker-compose up```命令启动时,会执行以下几个步骤</span><br><span class="line"></span><br><span class="line">1.创建一个```myapp_default```的网络</span><br><span class="line"></span><br><span class="line">2.使用web的配置文件启动一个容器,加入到```myapp_default```网络中</span><br><span class="line"></span><br><span class="line">3.使用db的配置文件启动一个容器.加入到```myapp_default```的网络中</span><br><span class="line"></span><br><span class="line">所有容器成功启动后,每个容器都能访问对方的```hostname```和对方的IP地址.</span><br><span class="line"></span><br><span class="line">另外,需要理解```HOST_PORT```和```COMTAINER_PORT```的区别.在上面这个例子中,db的```host_port```是8001,容器的端口是5432,services之间的容器都是通过```CONTAINER_PORT```也就是容器的IP进行通信的.例如访问数据库地址应该是:```postgres://db:5432</span><br></pre></td></tr></table></figure><h4 id="driver"><a href="#driver" class="headerlink" title="driver"></a>driver</h4><p>指定网络驱动.在单主机下Docker引擎默认使用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>driver: overlay<br><code>`</code></p><hr><h2 id="configs和secrets块级别配置文件指令"><a href="#configs和secrets块级别配置文件指令" class="headerlink" title="configs和secrets块级别配置文件指令"></a>configs和secrets块级别配置文件指令</h2><p>还不是很懂这个怎么用的,以后有空再研究</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker学习笔记——docker-compose&quot;&gt;&lt;a href=&quot;#docker学习笔记——docker-compose&quot; class=&quot;headerlink&quot; title=&quot;docker学习笔记——docker-compose&quot;&gt;&lt;/a&gt;docker学习笔记——docker-compose&lt;/h2&gt;&lt;p&gt;docker compose 定义并且运行多个docker容器.使用YAML风格文件定义一个compose文件.利用compose文件创建和启动所有服务.&lt;/p&gt;
&lt;p&gt;使用docker compose基本只需要3个步骤&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在Dockerfile文件定义app环境&lt;/li&gt;
&lt;li&gt;在docker-compose.yml文件中定义组成app的各个服务&lt;/li&gt;
&lt;li&gt;run docker-compose up 和compose 启动和运行app&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面文档均可以在docker-compose官方找到详细资料:&lt;a href=&quot;https://docs.docker.com/compose/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;docker-compose&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;docker-compose安装&quot;&gt;&lt;a href=&quot;#docker-compose安装&quot; class=&quot;headerlink&quot; title=&quot;docker-compose安装&quot;&gt;&lt;/a&gt;docker-compose安装&lt;/h3&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker网络之bridge</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0-7.docker%E7%BD%91%E7%BB%9C%E4%B9%8Bbridge/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习-7.docker网络之bridge/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T14:08:42.118Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker官网学习–docker网络之bridge"><a href="#docker官网学习–docker网络之bridge" class="headerlink" title="docker官网学习–docker网络之bridge"></a>docker官网学习–docker网络之bridge</h2><p>本节介绍docker基础网络概念.以便能认识和利用各种不同的网络类型功能.</p><p>docker的网络支持插件化,驱动化定制.有一些网络驱动已经默认集成到docker中.docker网络主要有以下类型</p><ul><li>bridge</li><li>host</li><li>overlay</li><li>macvlan</li><li>none</li><li>其他网络插件</li></ul><hr><a id="more"></a><h4 id="bridge"><a href="#bridge" class="headerlink" title="bridge"></a>bridge</h4><p>​    bridge是docker默认的网络驱动.如果在<figure class="highlight docker"><figcaption><span>run```启动一个容器时没有指定任何网络驱动.那么默认就是bridge桥接网络.桥接网络通常适用于应用进程部署在多个独立的容器中,并且容器之间需要互相通信的场景中.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">   在docker环境中.bridge使用软件桥接允许容器之间通过同一个bridge互联.隔离没有连到这个bridge网络的其他容器网络.docker网络自动创建iptables规则阻止其他网络的容器访问.</span><br><span class="line"></span><br><span class="line">当Docker进程启动时，会在主机上创建一个名为docker0的虚拟网桥，此主机上启动的Docker容器会连接到这个虚拟网桥上。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。</span><br><span class="line"></span><br><span class="line">从docker0子网中分配一个IP给容器使用，并设置docker0的IP地址为容器的默认网关。在主机上创建一对虚拟网卡veth pair设备，Docker将veth pair设备的一端放在新创建的容器中，并命名为eth0（容器的网卡），另一端放在主机中，以vethxxx这样类似的名字命名，并将这个网络设备加入到docker0网桥中。可以通过brctl show命令查看。</span><br><span class="line"></span><br><span class="line">在Linux中.可以使用brctl命令查看和管理网桥(需要先安装bridge-utils软件包).例如查看本机上的网桥及其端口</span><br></pre></td></tr></table></figure></p><p>[work@docker conf.d]$sudo brctl show<br>bridge name    bridge id        STP enabled    interfaces<br>br-17ace6d9d81a        8000.024236cdd4d7    no        veth82ed0e5<br>br-d9897c225d25        8000.024237d1c9f6    no        veth6981090<br>                            veth8e29dbf<br>                            vethd511728<br>docker0        8000.0242322e2e42    no        veth0a0e27d<br>                            veth0c50104<br>                            veth3fe7f4d<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">docker 0网桥下关联了很多vethxxxxx规范命名的interfaces.每一个vethxxxx接口对应一个docker容器.在docker容器中一般是eth0的网卡</span><br></pre></td></tr></table></figure></p><p>[work@docker conf.d]$docker exec -it nginx ifconfig<br>eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 172.17.0.8  netmask 255.255.0.0  broadcast 0.0.0.0<br>        inet6 fe80::42:acff:fe11:8  prefixlen 64  scopeid 0x20<link><br>        ether 02:42:ac:11:00:08  txqueuelen 0  (Ethernet)<br>        RX packets 1428365  bytes 189142687 (180.3 MiB)<br>        RX errors 0  dropped 0  overruns 0  frame 0<br>        TX packets 1403620  bytes 287806317 (274.4 MiB)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">bridge网桥就是这样通信的,docker服务端通过vethxxxx端口和容器的eth0虚拟网卡进行通信.docker容器将宿主机的docker 0虚拟网卡的IP作为它的网关:</span><br></pre></td></tr></table></figure></p><p>[work@docker conf.d]$docker exec -it nginx route -n<br>Kernel IP routing table<br>Destination     Gateway         Genmask         Flags Metric Ref    Use Iface<br>0.0.0.0         172.17.0.1      0.0.0.0         UG    0      0        0 eth0<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bridge模式是docker的默认网络模式，不写--net参数，就是bridge模式。使用docker run -p时，docker实际是在iptables做了DNAT规则，实现端口转发功能。可以使用iptables -t nat -vnL查看。</span><br><span class="line"></span><br><span class="line">bridge网络模式如下所示:</span><br><span class="line"></span><br><span class="line">![](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUViTWFab1lmbU9pYk56NDZwc0FMcDk0bHR1MllTNVZHMmZtNGUxTTNwM0tOUmVQN04xZVh2OHlBLzA/d3hfZm10PXBuZw==)</span><br><span class="line"></span><br><span class="line">​    bridge网桥是docker的默认网络驱动.如果用户在创建容器时自定义了Bridge网络.那么自定义Bridge要优于docker默认的Bridge</span><br><span class="line"></span><br><span class="line"> **用户定义的bridge和默认bridge的区别**</span><br><span class="line"></span><br><span class="line">* 用户定义的bridge在多个容器之间提供更好的隔离性和协调性.</span><br><span class="line"></span><br><span class="line">  连到同一个自定义的bridge的容器之间的所有端口互通.而无需通过-p参数暴露到宿主机.这让容器之间的通信更简单,而且提供更好的安全性.例如:</span><br><span class="line"></span><br><span class="line">  连到同一个自定义的bridge网络的Nginx容器和mysql容器.及时mysql容器没有暴露任何端口.nginx也可以访问mysql容器的3306端口.</span><br><span class="line"></span><br><span class="line">  而默认的Bridge网络,则需要将mysql容器通过```-p```参数暴露3306端口给宿主机.</span><br><span class="line"></span><br><span class="line">* 自定义bridge网络提供容器的主机名DNS解析</span><br><span class="line"></span><br><span class="line">​        默认的bridge网络下的容器间不能通过主机名互相访问,只能通过IP地址.(除非使用—link参数,但是这个参数已经废弃).而用户自定义的bridge网络则可以直接访问对方的主机名.</span><br><span class="line"></span><br><span class="line">* 自定义bridge网络配置更方便</span><br><span class="line"></span><br><span class="line">​        配置一个默认bridge网络,会影响到全局所有容器.而且需要重启docker服务.使用```docker network create```可以创建一个自定义bridge网络.,而且可以分别配置</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 创建和管理自定义bridge网络</span><br><span class="line"></span><br><span class="line">创建命令:</span><br></pre></td></tr></table></figure></p><p>#创建自定义网络名称为my-net<br>docker network create my-net</p><p>#还可以指定网络号,子网掩码等信息<br>[root@localhost ~]$docker network create –help</p><p>Usage:    docker network create [OPTIONS] NETWORK</p><p>Create a network</p><p>Options:<br>      –attachable           Enable manual container attachment<br>      –aux-address map      Auxiliary IPv4 or IPv6 addresses used by Network driver (default map)<br>      –config-from string   The network from which copying the configuration<br>      –config-only          Create a configuration only network<br>  -d, –driver string        Driver to manage the Network (default “bridge”)<br>      –gateway strings      IPv4 or IPv6 Gateway for the master subnet<br>      –ingress              Create swarm routing-mesh network<br>      –internal             Restrict external access to the network<br>      –ip-range strings     Allocate container ip from a sub-range<br>      –ipam-driver string   IP Address Management Driver (default “default”)<br>      –ipam-opt map         Set IPAM driver specific options (default map)<br>      –ipv6                 Enable IPv6 networking<br>      –label list           Set metadata on a network<br>  -o, –opt map              Set driver specific options (default map)<br>      –scope string         Control the network’s scope<br>      –subnet strings       Subnet in CIDR format that represents a network segment<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">删除自定义网络:</span><br></pre></td></tr></table></figure></p><p>#如果有容器正在使用该网络,需要先断开容器<br>docker network rm my-net<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 管理自定义网络下的容器</span><br><span class="line"></span><br><span class="line">创建一个自定义网络下的容器</span><br></pre></td></tr></table></figure></p><p>#在创建容器的时候指定自定义网络名</p><p>docker create –name my-nginx \<br>  –network my-net \<br>  –publish 8080:80 \<br>  nginx:latest<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">将一个正在运行的容器关联(移除)自定义网络</span><br></pre></td></tr></table></figure></p><p>#关联容器和自定义网络命令格式:<br>docker network connect</p><p>#例如,将一个正在运行的mysql容器关联到my-net网络下<br>docker network connect my-net mysql</p><p>#相反从自定义网络下移除一个容器命令:<br>docker network disconnect</p><p>#例如,将一个正在运行的mysql容器从my-net网络下移除<br>docker network disconnect my-net mysql<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 实验:</span><br><span class="line"></span><br><span class="line">* **默认的bridge网络,容器之间无法互相访问对方的主机名.只能通过iP地址通信**</span><br></pre></td></tr></table></figure></p><p>[root@localhost ~]$docker run -itd –rm –name=busybox busybox<br>b3c8be3b3b716579caf11d3852f6c6e04a41b4dc020d9478be1b1f3a4d76cf1f</p><p>[root@localhost ~]$docker run -itd –rm –name=busybox1 busybox<br>a54d962f6a692d96f0bc2fbca37e1b47e59e6b61541f42b8d6872a5008a46b87</p><p>[root@localhost ~]$docker exec busybox ping busybox1<br>ping: bad address ‘busybox1’<br>[root@localhost ~]$</p><p>#通过IP地址可以通信</p><p>[root@localhost ~]$docker exec busybox ping 172.17.0.8<br>PING 172.17.0.8 (172.17.0.8): 56 data bytes<br>64 bytes from 172.17.0.8: seq=0 ttl=64 time=0.136 ms<br>64 bytes from 172.17.0.8: seq=1 ttl=64 time=0.079 ms<br>64 bytes from 172.17.0.8: seq=2 ttl=64 time=0.131 ms<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* **自定义的Bridge网络,可以在容器之间互相访问主机名**</span><br></pre></td></tr></table></figure></p><p>#创建一个自定义网络<br>[root@localhost ~]$docker network create jesse<br>e10a936177681cbfc321f67f961f2a717079ef1790c50e82381296fa77bd7d5f</p><p>[root@localhost ~]$docker network ls | grep jesse<br>e10a93617768        jesse                  bridge              local</p><p>#创建容器,使用network参数指定网络<br>[root@localhost ~]$docker run –name busybox1 -itd –network jesse –rm busybox<br>aedf164ea1769741ae6480583abdc838022f49506761d4054daabdb7fffcd852</p><p>[root@localhost ~]$docker run –name busybox2 -itd –network jesse –rm busybox<br>523bdd54fa12423e25a8ac84f9faef1679ee6031f2d5d2a87c0cacd64f8650ad</p><p>[root@localhost ~]$docker exec busybox1 ping busybox2<br>PING busybox2 (172.20.0.3): 56 data bytes<br>64 bytes from 172.20.0.3: seq=0 ttl=64 time=0.116 ms<br>64 bytes from 172.20.0.3: seq=1 ttl=64 time=0.078 ms</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 连接到不同的bridge网络下的容器互相之间网络隔离</span><br></pre></td></tr></table></figure><p>docker network create bridge<br>docker network create kong-net</p><p>[work@docker conf.d]$docker run –name busybox1 –network bridge -itd busybox<br>f95a229aa7eb5d7022bef4441a075b0ad37ecc50e2a02015f09790d23b28dc33</p><p>[work@docker conf.d]$docker run –name busybox2 –network kong-net -itd busybox<br>a9236a25199cc43a899285462afa851a52cdaf871776a93829897676fc7dd82c</p><p>#busybox1和busybox2不在同一个IP网段</p><p>#busybox1的IP<br>[work@docker conf.d]$docker inspect busybox1<br>172.17.0.11</p><p>#busybox2的IP<br>[work@docker conf.d]$docker exec -it busybox2 ifconfig eth0<br>eth0      Link encap:Ethernet  HWaddr 02:42:AC:12:00:05<br>          inet addr:172.18.0.5  Bcast:0.0.0.0  Mask:255.255.0.0</p><p>#主机名无法访问<br>[work@docker conf.d]$docker exec -it busybox1 ping busybox2<br>ping: bad address ‘busybox2’</p><p>#busybox1容器也无法ping busybox2容器的IP<br>[work@docker conf.d]$docker exec -it busybox1 ping 172.18.0.5<br>PING 172.18.0.5 (172.18.0.5): 56 data bytes<br>^C<br>— 172.18.0.5 ping statistics —<br>32 packets transmitted, 0 packets received, 100% packet loss<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 由于博客不能识别go模板语法,所以省去了go模板,直接用docker inspect busybox1命令来代替.实际场景中该命令无法直接获取容器IP</span><br><span class="line"></span><br><span class="line">#### 将容器从自定义bridge网络中移除</span><br></pre></td></tr></table></figure></p><p>#将jesse从jesse网络移除<br>[root@localhost ~]$docker network disconnect jesse busybox1</p><p>#此时jesse网络下只有busybox2容器<br>[root@localhost ~]$docker network inspect jesse</p><p>#此时busybox1容器的网卡被移除了<br>[root@localhost ~]$docker inspect busybox1 </p><no value=""><p>root@localhost ~]$docker exec -it busybox1 ifconfig<br>lo        Link encap:Local Loopback<br>          inet addr:127.0.0.1  Mask:255.0.0.0<br>          UP LOOPBACK RUNNING  MTU:65536  Metric:1<br>          RX packets:11 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:11 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:1000<br>          RX bytes:618 (618.0 B)  TX bytes:618 (618.0 B)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 将一个正在运行的容器加入到自定义bridge网络</span><br></pre></td></tr></table></figure></p><p>#将jesse加回到jesse网络<br>[root@localhost ~]$docker network connect jesse busybox1</p><p>#获得新的IP地址<br>[root@localhost ~]$docker inspect busybox1<br>172.20.0.2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 宿主机转发容器端口</span><br><span class="line"></span><br><span class="line">默认情况下,bridge网络不会转发外部的请求到容器.开启转发需要更改2个设置:</span><br><span class="line"></span><br><span class="line">1.修改内容,开启转发</span><br></pre></td></tr></table></figure></p><p>sysctl net.ipv4.conf.all.forwarding=1</p><p>sysctl -p<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2.更改iptables的转发的默认规则</span><br></pre></td></tr></table></figure></p><p>sudo iptables -P FORWARD ACCEPT<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 更改默认Bridge网络配置</span><br><span class="line"></span><br><span class="line">docker0网桥是在docker daemon启动时自动创建的.IP默认为172.17.0.1/16.所有连到docker 0网桥的docker容器都会在这个IP范围内选取一个未占用的IP使用.并连接到docker 0网桥上</span><br><span class="line"></span><br><span class="line">docker提供了一些参数帮助用户自定义docker0网桥的设置</span><br><span class="line"></span><br><span class="line">* —bip=CIDR: 设置Docker0的IP地址和子网范围.使用CIDR格式.例如192.168.0.1/24.需要注意的是这个参数仅仅是配置docker0的,对其他自定义的网桥无效.</span><br><span class="line">* —fixed-cidr=CIDR:限制docker容器获取iP的范围.默认情况下docker容器获取的IP范围为整个docker0网桥的IP地址段,也就是—bip指定的地址范围.此参数可以将docker容器缩小到某个子网范围.</span><br><span class="line">* —mtu=BYTES: 指定docker0的最大传输单元(MTU)</span><br></pre></td></tr></table></figure></p><p>#更改daemon.json配置文件.下面这个例子修改了docker0网络的网段地址<br>vim /etc/docker/daemon.json</p><p>{<br>  “registry-mirrors”: [“<a href="https://registry.docker-cn.com&quot;]" target="_blank" rel="noopener">https://registry.docker-cn.com&quot;]</a>,<br>  “bip”: “192.168.1.5/24”,<br>  “mtu”: 1500,<br>  “dns”: [“114.114.114.114”,”114.114.115.115”]</p><p>}</p><p>#重启docker服务<br>[root@localhost ~]$systemctl restart docker</p><p>[root@localhost ~]$ifconfig | grep -A 5 docker0<br>docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 192.168.1.5  netmask 255.255.255.0  broadcast 192.168.1.255<br>        ether 02:42:89:26:d1:c9  txqueuelen 0  (Ethernet)<br>        RX packets 1072032  bytes 61915874 (59.0 MiB)<br>        RX errors 0  dropped 0  overruns 0  frame 0<br>        TX packets 1997027  bytes 1934238839 (1.8 GiB)<br><code>`</code></p><hr></no>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker官网学习–docker网络之bridge&quot;&gt;&lt;a href=&quot;#docker官网学习–docker网络之bridge&quot; class=&quot;headerlink&quot; title=&quot;docker官网学习–docker网络之bridge&quot;&gt;&lt;/a&gt;docker官网学习–docker网络之bridge&lt;/h2&gt;&lt;p&gt;本节介绍docker基础网络概念.以便能认识和利用各种不同的网络类型功能.&lt;/p&gt;
&lt;p&gt;docker的网络支持插件化,驱动化定制.有一些网络驱动已经默认集成到docker中.docker网络主要有以下类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bridge&lt;/li&gt;
&lt;li&gt;host&lt;/li&gt;
&lt;li&gt;overlay&lt;/li&gt;
&lt;li&gt;macvlan&lt;/li&gt;
&lt;li&gt;none&lt;/li&gt;
&lt;li&gt;其他网络插件&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker网络之host,Container,None网络</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0-7.docker%E7%BD%91%E7%BB%9C%E4%B9%8Bhost,Container,None%E7%BD%91%E7%BB%9C/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习-7.docker网络之host,Container,None网络/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:14:06.747Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker官网学习-7-docker网络之host-Container-None网络"><a href="#docker官网学习-7-docker网络之host-Container-None网络" class="headerlink" title="docker官网学习-7.docker网络之host,Container,None网络"></a>docker官网学习-7.docker网络之host,Container,None网络</h2><h3 id="host网络介绍"><a href="#host网络介绍" class="headerlink" title="host网络介绍"></a>host网络介绍</h3><p>如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。</p><hr><h3 id="创建host网络"><a href="#创建host网络" class="headerlink" title="创建host网络"></a>创建host网络</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">docker run -tid --net=host --name busybox busybox1</span><br><span class="line"></span><br><span class="line">#host网络下的容器没有虚拟网卡,而是和宿主机共享网络</span><br><span class="line">[root@localhost ~]$docker exec  -it busybox1 ifconfig</span><br><span class="line">docker0   Link encap:Ethernet  HWaddr 02:42:89:26:D1:C9</span><br><span class="line">          inet addr:192.168.1.5  Bcast:192.168.1.255  Mask:255.255.255.0</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</span><br><span class="line">          RX packets:1072032 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:1997027 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0</span><br><span class="line">          RX bytes:61915874 (59.0 MiB)  TX bytes:1934238839 (1.8 GiB)</span><br></pre></td></tr></table></figure><blockquote><p>注:host网络只能工作在Linux主机上.</p><p>如果容器没有暴露任何端口,那host网络没有任何效果</p></blockquote><a id="more"></a><p>host网络示意图</p><p><img src="http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVObjRZOVg0Q2tRRVZMV2dFZGt4MWljeThKY3VERXFhTGFZZUhiaDB1TWJZeVVtTjhQQ1l0bDl3LzA/d3hfZm10PXBuZw==" alt=""></p><hr><h3 id="Container网络介绍"><a href="#Container网络介绍" class="headerlink" title="Container网络介绍"></a>Container网络介绍</h3><p>这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备通信。</p><p>Container网络示意图</p><p><img src="http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVtaWM5elRoU1d1UmdSc2xVT2oyeHpmeUljZXdpY2E3VkpibE03Nnc5N01PZFRLVEl2TkdpYTBPd2cvMD93eF9mbXQ9cG5n" alt=""></p><hr><h3 id="None网络"><a href="#None网络" class="headerlink" title="None网络"></a>None网络</h3><p>使用none模式，Docker容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置。也就是说，这个Docker容器没有网卡、IP、路由等信息。需要我们自己为Docker容器添加网卡、配置IP等。</p><p>Node模式示意图:</p><p><img src="http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVMeFREaWNxUVFYQ0dObVlUNFlRdVdQYkxBRk1TVmhvRFlrcUtENFVLczVXbWtqbTM1THNpY1FZUS8wP3d4X2ZtdD1wbmc=" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker官网学习-7-docker网络之host-Container-None网络&quot;&gt;&lt;a href=&quot;#docker官网学习-7-docker网络之host-Container-None网络&quot; class=&quot;headerlink&quot; title=&quot;docker官网学习-7.docker网络之host,Container,None网络&quot;&gt;&lt;/a&gt;docker官网学习-7.docker网络之host,Container,None网络&lt;/h2&gt;&lt;h3 id=&quot;host网络介绍&quot;&gt;&lt;a href=&quot;#host网络介绍&quot; class=&quot;headerlink&quot; title=&quot;host网络介绍&quot;&gt;&lt;/a&gt;host网络介绍&lt;/h3&gt;&lt;p&gt;如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;创建host网络&quot;&gt;&lt;a href=&quot;#创建host网络&quot; class=&quot;headerlink&quot; title=&quot;创建host网络&quot;&gt;&lt;/a&gt;创建host网络&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker run -tid --net=host --name busybox busybox1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#host网络下的容器没有虚拟网卡,而是和宿主机共享网络&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[root@localhost ~]$docker exec  -it busybox1 ifconfig&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;docker0   Link encap:Ethernet  HWaddr 02:42:89:26:D1:C9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          inet addr:192.168.1.5  Bcast:192.168.1.255  Mask:255.255.255.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          RX packets:1072032 errors:0 dropped:0 overruns:0 frame:0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          TX packets:1997027 errors:0 dropped:0 overruns:0 carrier:0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          collisions:0 txqueuelen:0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          RX bytes:61915874 (59.0 MiB)  TX bytes:1934238839 (1.8 GiB)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;注:host网络只能工作在Linux主机上.&lt;/p&gt;
&lt;p&gt;如果容器没有暴露任何端口,那host网络没有任何效果&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---学习Docker Stack</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0-6.%E5%AD%A6%E4%B9%A0stack/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习-6.学习stack/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:12:58.803Z</updated>
    
    <content type="html"><![CDATA[<h2 id="学习Docker-Stack"><a href="#学习Docker-Stack" class="headerlink" title="学习Docker Stack"></a>学习Docker Stack</h2><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>一个stack是一组共享依赖包的多个相关的services,并且可以编排和扩展.其实从第4小节开始,在利用compose文件部署app时,就已经开始一直使用stack.但是还只是运行在一个单一服务器的单一service.<br>现在,你可以学习在多个服务器上,运行多个相关的services.</p><hr><ul><li>使用下面的docker-compose.yml文件替换第4小节中的docker-compose.yml</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line">services:</span><br><span class="line">  web:</span><br><span class="line">    # replace username/repo:tag with your name and image details</span><br><span class="line">    image: ianch/friendlyhello:v1</span><br><span class="line">    deploy:</span><br><span class="line">      replicas: 5</span><br><span class="line">      resources:</span><br><span class="line">        limits:</span><br><span class="line">          cpus: &quot;0.1&quot;</span><br><span class="line">          memory: 50M</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;4000:80&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - webnet</span><br><span class="line">  Visualizer:</span><br><span class="line">    image: dockersamples/visualizer:stable</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8080:8080&quot;</span><br><span class="line">    volumes:</span><br><span class="line">      - &quot;/var/run/docker.sock:/var/run/docker.sock&quot;</span><br><span class="line">    deploy:</span><br><span class="line">      placement:</span><br><span class="line">        constraints: [node.role == manager]</span><br><span class="line">    networks:</span><br><span class="line">      - webnet</span><br><span class="line">networks:</span><br><span class="line">  webnet:</span><br></pre></td></tr></table></figure><a id="more"></a><p>docker-compose文件稍微做了点改动.添加一个Visualizer服务,placement指令确保这个Visualizer服务仅仅运行在swarm manager节点.</p><hr><h4 id="部署compose文件"><a href="#部署compose文件" class="headerlink" title="部署compose文件"></a>部署compose文件</h4><ul><li>初始化swarm</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker swarm init</span><br></pre></td></tr></table></figure><ul><li>第二台服务器加入swarm集群</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@php compose]$docker swarm join --token SWMTKN-1-5qr6e90o52h5licxatuvmft65kji5qf1roujebf16auoe5xgam-3d0fuzr8818r6330n88dm1fcu 10.0.0.50:2377</span><br><span class="line">This node joined a swarm as a worker.</span><br></pre></td></tr></table></figure><ul><li>部署app</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack deploy -c docker-compose.yml getstartedlab</span><br><span class="line">Creating network getstartedlab_webnet</span><br><span class="line">Creating service getstartedlab_web</span><br><span class="line">Creating service getstartedlab_Visualizer</span><br><span class="line">[root@localhost compose]$</span><br></pre></td></tr></table></figure><blockquote><p>添加了2个服务.web和Visualizer</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack ps getstartedlab</span><br><span class="line">ID                  NAME                         IMAGE                             NODE                    DESIRED STATE       CURRENT STATE                ERROR               PORTS</span><br><span class="line">ds77cn4hgd9w        getstartedlab_web.1          friendlyhello:latest              php                     Running             Running about a minute ago</span><br><span class="line">agzj3veqno63        getstartedlab_Visualizer.1   dockersamples/visualizer:stable   localhost.localdomain   Running             Running about a minute ago</span><br><span class="line">3hq0if79g3gk        getstartedlab_web.2          friendlyhello:latest              php                     Running             Running about a minute ago</span><br><span class="line">iuxh7qjfpikw        getstartedlab_web.3          friendlyhello:latest              localhost.localdomain   Running             Running 53 seconds ago</span><br><span class="line">ba9hcq5zmlbd        getstartedlab_web.4          friendlyhello:latest              php                     Running             Running about a minute ago</span><br><span class="line">o600yqmqets7        getstartedlab_web.5          friendlyhello:latest              localhost.localdomain   Running             Running 55 seconds ago</span><br><span class="line">[root@localhost compose]$</span><br></pre></td></tr></table></figure><p>访问任意一台服务器的8080端口,可以看到Visualizer服务正在运行</p><p><img src="https://docs.docker.com/get-started/images/get-started-visualizer1.png" alt=""></p><blockquote><p>这是我借用的官网的图片.</p></blockquote><p>可以看到,visualizer运行在swarm manager节点上,5个web服务运行在swarm集群上.visualizer是一个不需要任何依赖,而可以运行在任何app的独立服务.现在尝试一下创建一个具有依赖项的服务:提供访问计数器的Redis服务</p><hr><h3 id="编辑docker-compose文件"><a href="#编辑docker-compose文件" class="headerlink" title="编辑docker-compose文件"></a>编辑docker-compose文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line">services:</span><br><span class="line">  web:</span><br><span class="line">    # replace username/repo:tag with your name and image details</span><br><span class="line">    image: ianch/friendlyhello:v1</span><br><span class="line">    deploy:</span><br><span class="line">      replicas: 5</span><br><span class="line">      resources:</span><br><span class="line">        limits:</span><br><span class="line">          cpus: &quot;0.1&quot;</span><br><span class="line">          memory: 50M</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;4000:80&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - webnet</span><br><span class="line">  Visualizer:</span><br><span class="line">    image: dockersamples/visualizer:stable</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8080:8080&quot;</span><br><span class="line">    volumes:</span><br><span class="line">      - &quot;/var/run/docker.sock:/var/run/docker.sock&quot;</span><br><span class="line">    deploy:</span><br><span class="line">      placement:</span><br><span class="line">        constraints: [node.role == manager]</span><br><span class="line">    networks:</span><br><span class="line">      - webnet</span><br><span class="line"></span><br><span class="line">  redis:</span><br><span class="line">    image: redis</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;6379:6379&quot;</span><br><span class="line">    volumes:</span><br><span class="line">      - &quot;/home/docker/data:/data&quot;</span><br><span class="line">    deploy:</span><br><span class="line">      placement:</span><br><span class="line">        constraints: [node.role == manager]</span><br><span class="line">    command: redis-server --appendonly yes</span><br><span class="line">    networks:</span><br><span class="line">      - webnet</span><br><span class="line">networks:</span><br><span class="line">  webnet:</span><br></pre></td></tr></table></figure><p> 这里我们添加了一个redis服务.在Docker HUB上有redis官方镜像,并且已经暴露了6379端口.所以这里只需要指定redis镜像即可..同样redis也只运行在manager节点服务器.</p><p> 这里为了持久化数据,在启动redis容器的时候指定了appendonly参数,并且挂载了本机的/home/docker/data目录映射到容器的/data.(redis容器默认保存数据路径)</p><ul><li>在manager节点创建/home/docker/data目录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$mkdir -pv /home/docker/data</span><br><span class="line">mkdir: 已创建目录 &quot;/home/docker&quot;</span><br><span class="line">mkdir: 已创建目录 &quot;/home/docker/data&quot;</span><br></pre></td></tr></table></figure><ul><li>部署compose</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack deploy -c docker-compose.yml getstartedlab</span><br><span class="line">Updating service getstartedlab_web (id: mtgafxttekwfh0tkhkaespa1v)</span><br><span class="line">image friendlyhello:latest could not be accessed on a registry to record</span><br><span class="line">its digest. Each node will access friendlyhello:latest independently,</span><br><span class="line">possibly leading to different nodes running different</span><br><span class="line">versions of the image.</span><br><span class="line"></span><br><span class="line">Updating service getstartedlab_Visualizer (id: zmim1kj44afsr9xay8ppxker6)</span><br><span class="line">Creating service getstartedlab_redis</span><br></pre></td></tr></table></figure><p>可以看到3个services都启动起来了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker service ls</span><br><span class="line">ID                  NAME                       MODE                REPLICAS            IMAGE                             PORTS</span><br><span class="line">zmim1kj44afs        getstartedlab_Visualizer   replicated          1/1                 dockersamples/visualizer:stable   *:8080-&gt;8080/tcp</span><br><span class="line">elomaiu5go9p        getstartedlab_redis        replicated          1/1                 redis:latest                      *:6379-&gt;6379/tcp</span><br><span class="line">mtgafxttekwf        getstartedlab_web          replicated          5/5                 friendlyhello:latest              *:4000-&gt;80/tcp</span><br></pre></td></tr></table></figure><p>在浏览器访问服务器的4000端口可以看到有一个访问计数器在增加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; eed56ca4164b&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 5%                                                                                                           huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 777d2cab6468&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 6%                                                                                                           huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 213e6a729c6a&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 7%                                                                                                           huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 85ccc6b1cb18&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 8%                                                                                                           huangyong@huangyong-Macbook-Pro  ~ </span><br></pre></td></tr></table></figure><p>访问另外一台服务器也可以看到同样结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.12:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; e77e36db18be&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 10%                                                                                                          huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; eed56ca4164b&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 11%                                                                                                          huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.12:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; eed56ca4164b&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 12%</span><br></pre></td></tr></table></figure><p>访问visulizer容器的8080端口,可以看到redis服务运行</p><p><img src="https://docs.docker.com/get-started/images/visualizer-with-redis.png" alt=""></p><hr><h3 id="管理命令"><a href="#管理命令" class="headerlink" title="管理命令"></a>管理命令</h3><p>使用docker node ls 列出swarm集群的所有节点<br>使用docker service ls 列出所有服务<br>docker service ps &lt;service_name&gt; 列出某个服务的所有tasks</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker node ls</span><br><span class="line">ID                            HOSTNAME                STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class="line">056r5hr2xb6jjzmbw64m3btd2 *   localhost.localdomain   Ready               Active              Leader              18.09.2</span><br><span class="line">r58bkpsxgi4mjaxrn8octuxw1     php                     Ready               Active                                  18.09.3</span><br><span class="line">[root@localhost ~]$docker service ls</span><br><span class="line">ID                  NAME                       MODE                REPLICAS            IMAGE                             PORTS</span><br><span class="line">zmim1kj44afs        getstartedlab_Visualizer   replicated          1/1                 dockersamples/visualizer:stable   *:8080-&gt;8080/tcp</span><br><span class="line">elomaiu5go9p        getstartedlab_redis        replicated          1/1                 redis:latest                      *:6379-&gt;6379/tcp</span><br><span class="line">mtgafxttekwf        getstartedlab_web          replicated          5/5                 friendlyhello:latest              *:4000-&gt;80/tcp</span><br><span class="line"></span><br><span class="line">[root@localhost ~]$docker service ps getstartedlab_web</span><br><span class="line">ID                  NAME                  IMAGE                  NODE                    DESIRED STATE       CURRENT STATE          ERROR               PORTS</span><br><span class="line">ds77cn4hgd9w        getstartedlab_web.1   friendlyhello:latest   php                     Running             Running 14 hours ago</span><br><span class="line">3hq0if79g3gk        getstartedlab_web.2   friendlyhello:latest   php                     Running             Running 14 hours ago</span><br><span class="line">iuxh7qjfpikw        getstartedlab_web.3   friendlyhello:latest   localhost.localdomain   Running             Running 14 hours ago</span><br><span class="line">ba9hcq5zmlbd        getstartedlab_web.4   friendlyhello:latest   php                     Running             Running 14 hours ago</span><br><span class="line">o600yqmqets7        getstartedlab_web.5   friendlyhello:latest   localhost.localdomain   Running             Running 14 hours ago</span><br><span class="line">[root@localhost ~]$</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;学习Docker-Stack&quot;&gt;&lt;a href=&quot;#学习Docker-Stack&quot; class=&quot;headerlink&quot; title=&quot;学习Docker Stack&quot;&gt;&lt;/a&gt;学习Docker Stack&lt;/h2&gt;&lt;h4 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h4&gt;&lt;p&gt;一个stack是一组共享依赖包的多个相关的services,并且可以编排和扩展.其实从第4小节开始,在利用compose文件部署app时,就已经开始一直使用stack.但是还只是运行在一个单一服务器的单一service.&lt;br&gt;现在,你可以学习在多个服务器上,运行多个相关的services.&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;使用下面的docker-compose.yml文件替换第4小节中的docker-compose.yml&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;version: &amp;quot;3&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;services:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  web:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    # replace username/repo:tag with your name and image details&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    image: ianch/friendlyhello:v1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    deploy:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      replicas: 5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      resources:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        limits:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          cpus: &amp;quot;0.1&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          memory: 50M&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      restart_policy:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        condition: on-failure&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ports:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      - &amp;quot;4000:80&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    networks:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      - webnet&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  Visualizer:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    image: dockersamples/visualizer:stable&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ports:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      - &amp;quot;8080:8080&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    volumes:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      - &amp;quot;/var/run/docker.sock:/var/run/docker.sock&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    deploy:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      placement:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        constraints: [node.role == manager]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    networks:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      - webnet&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;networks:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  webnet:&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---理解swarm集群</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0-5.%E5%AD%A6%E4%B9%A0swarm%E9%9B%86%E7%BE%A4/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习-5.学习swarm集群/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:11:55.850Z</updated>
    
    <content type="html"><![CDATA[<h3 id="理解swarm集群"><a href="#理解swarm集群" class="headerlink" title="理解swarm集群"></a>理解swarm集群</h3><p>一个swarm是一组运行docker服务器的的集群,docker服务器可以是物理机也可以是虚拟机.</p><p>swarm manager可以使用多种策略来运行容器.比如”emptiest node”—部署容器到压力最小的服务器上,或者”global”—确保每台服务器都只允许一个容器实例.你可以在Compose文件中指示swarm manager去选择何种策略</p><p>swarm managers是swarm进群中唯一可以执行命令,或者授权其他服务器以”workers”身份加入swarm集群的服务器.</p><hr><h4 id="初始化swarm-加入节点"><a href="#初始化swarm-加入节点" class="headerlink" title="初始化swarm,加入节点"></a>初始化swarm,加入节点</h4><p>试验环境:</p><p>1.10.0.0.50 —swarm manager<br>2.10.0.0.12 —worker 节点</p><ul><li>初始化swarm,并且指定通告的IP</li></ul><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker swarm init --advertise-addr 10.0.0.50</span><br><span class="line">Swarm initialized: current node (zlvl9l94blu3rfcaaptdvo9u1) is now a manager.</span><br><span class="line"></span><br><span class="line">To add a worker to this swarm, run the following command:</span><br><span class="line"></span><br><span class="line">    docker swarm join --token SWMTKN-1-2i5fyjf2niw81tudcvpw33yuni277vz45lt6tyi5bvcnhvuwea-bj091dpe6e69ph9kt3lmsthgp 10.0.0.50:2377</span><br><span class="line"></span><br><span class="line">To add a manager to this swarm, run &apos;docker swarm join-token manager&apos; and follow the instructions.</span><br></pre></td></tr></table></figure><p>根据上面提示,在第二台服务器上以worker身份加入swarm集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@php ~]$docker swarm join --token SWMTKN-1-2i5fyjf2niw81tudcvpw33yuni277vz45lt6tyi5bvcnhvuwea-bj091dpe6e69ph9kt3lmsthgp 10.0.0.50:2377</span><br><span class="line">This node joined a swarm as a worker.</span><br><span class="line">[root@php ~]$</span><br></pre></td></tr></table></figure><p>执行docker node ls命令可以管理和查看swarm集群的所有节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker node ls</span><br><span class="line">ID                            HOSTNAME                STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class="line">zlvl9l94blu3rfcaaptdvo9u1 *   localhost.localdomain   Ready               Active              Leader              18.09.2</span><br><span class="line">ud5ztqzvvfwg3d3hwmts5y9ct     php                     Ready               Active                                  18.09.3</span><br><span class="line">[root@localhost ~]$</span><br></pre></td></tr></table></figure><p>执行docker swarm leave命令将某个节点退出swarm集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@php ~]$docker swarm leave</span><br><span class="line">Node left the swarm.</span><br><span class="line">[root@php ~]$</span><br><span class="line"></span><br><span class="line">此时这个节点在swarm集群中状态为down</span><br><span class="line"></span><br><span class="line">[root@localhost ~]$docker node ls</span><br><span class="line">ID                            HOSTNAME                STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class="line">zlvl9l94blu3rfcaaptdvo9u1 *   localhost.localdomain   Ready               Active              Leader              18.09.2</span><br><span class="line">ud5ztqzvvfwg3d3hwmts5y9ct     php                     Down                Active                                  18.09.3</span><br><span class="line">[root@localhost ~]$</span><br></pre></td></tr></table></figure><hr><h4 id="在swarm集群部署app"><a href="#在swarm集群部署app" class="headerlink" title="在swarm集群部署app"></a>在swarm集群部署app</h4><p>现在可以把上一小节的docker compose部署在swarm集群上了.执行命令和上一小节一样.但是需要注意的是只能在swarm manager节点服务器上执行命令.</p><p>在第一台服务器上执行如下命令:(确保docker compose文件和镜像文件在这台服务器上)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@localhost ~]$cd /data/compose</span><br><span class="line">[root@localhost compose]$ls</span><br><span class="line">docker-compose.yml</span><br><span class="line">[root@localhost compose]$docker stack deploy -c docker-compose.yml getstartedlab</span><br><span class="line">Creating network getstartedlab_webnet</span><br><span class="line">Creating service getstartedlab_web</span><br><span class="line">[root@localhost compose]$</span><br></pre></td></tr></table></figure><p>APP已经成功部署到swarm集群上,现在可以使用上一小节中的同样的命令来管理app集群,只不过这次services和容器已经部署到两台服务器上:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack ps getstartedlab</span><br><span class="line">ID                  NAME                      IMAGE                  NODE                    DESIRED STATE       CURRENT STATE              ERROR                              PORTS</span><br><span class="line">4borwslue8k5        getstartedlab_web.1       friendlyhello:latest   localhost.localdomain   Running             Preparing 6 seconds ago</span><br><span class="line">ixmkldh16otx        getstartedlab_web.2       friendlyhello:latest   php                     Ready               Preparing 22 seconds ago</span><br><span class="line">i2yhimkst4iq         \_ getstartedlab_web.2   friendlyhello:latest   php                     Shutdown            Rejected 23 seconds ago    &quot;No such image: friendlyhello:…&quot;</span><br><span class="line">gytqpcwnzvrm        getstartedlab_web.3       friendlyhello:latest   localhost.localdomain   Running             Preparing 6 seconds ago</span><br><span class="line">j94tblo4qjwa        getstartedlab_web.4       friendlyhello:latest   php                     Ready               Preparing 22 seconds ago</span><br><span class="line">b7r9xkf4glh6         \_ getstartedlab_web.4   friendlyhello:latest   php                     Shutdown            Rejected 22 seconds ago    &quot;No such image: friendlyhello:…&quot;</span><br><span class="line">i8sv4c293ata        getstartedlab_web.5       friendlyhello:latest   localhost.localdomain   Running             Preparing 6 seconds ago</span><br></pre></td></tr></table></figure><blockquote><p>需要在另外一台服务器上pull同样的镜像,否则容器无法启动</p></blockquote><ul><li><p>在第二台服务器上下载我阿里云私有仓库的镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">docker login --username=jessehuang408 registry.cn-hangzhou.aliyuncs.com</span><br><span class="line">Password:</span><br><span class="line"></span><br><span class="line">[root@php ~]$docker pull registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:frendlyhello-v1.0</span><br><span class="line">frendlyhello-v1.0: Pulling from jesse_images/jesse_images</span><br><span class="line">f7e2b70d04ae: Pull complete</span><br><span class="line">1e9214730e83: Pull complete</span><br><span class="line">5bd4ec081f7b: Pull complete</span><br><span class="line">be26b369a1e7: Pull complete</span><br><span class="line">236be9d80905: Pull complete</span><br><span class="line">1bf8a3675b0b: Pull complete</span><br><span class="line">5752f9477f0c: Pull complete</span><br><span class="line">Digest: sha256:8e8b57ef6e22c8c04c1c80cfab9f336928cffabacaa4ae4e74ec57e54bcffdb2</span><br><span class="line">Status: Downloaded newer image for registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:frendlyhello-v1.0</span><br><span class="line"></span><br><span class="line">[root@php ~]$docker images</span><br><span class="line">REPOSITORY                                                    TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images   frendlyhello-v1.0   f091d1bb803c        2 days ago          131MB</span><br><span class="line">[root@php ~]$*</span><br></pre></td></tr></table></figure></li><li><p>将镜像修改成和第一台服务器一样:frendlyhello:latest</p></li></ul><p>命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker tag 镜像ID REPOSITORY:TAG</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@php ~]$docker tag f091d1bb803c frendlyhello:latest</span><br><span class="line">[root@php ~]$docker images</span><br><span class="line">REPOSITORY                                                    TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">frendlyhello                                                  latest              f091d1bb803c        2 days ago          131MB</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images   frendlyhello-v1.0   f091d1bb803c        2 days ago          131MB</span><br><span class="line">[root@php ~]$</span><br></pre></td></tr></table></figure><ul><li>将第一台服务器的docker-compose文件拷贝到同样的目录下</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@php ~]$mkdir /data/compose</span><br><span class="line">[root@php ~]$scp root@10.0.0.50:/data/compose/docker-compose.yml /data/compose/</span><br></pre></td></tr></table></figure><ul><li>回到第一台服务器上删除刚才创建的getstartedlab</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack rm getstartedlab</span><br><span class="line">Removing service getstartedlab_web</span><br><span class="line">Removing network getstartedlab_webnet</span><br><span class="line"></span><br><span class="line">[root@localhost compose]$docker stack ps getstartedlab</span><br><span class="line">nothing found in stack: getstartedlab</span><br></pre></td></tr></table></figure><ul><li>重新部署docker compose</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack deploy -c docker-compose.yml getstartedlab</span><br><span class="line">Creating network getstartedlab_webnet</span><br><span class="line">Creating service getstartedlab_web</span><br><span class="line">[root@localhost compose]$</span><br></pre></td></tr></table></figure><ul><li>成功部署</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack ps getstartedlab</span><br><span class="line">ID                  NAME                  IMAGE                  NODE                    DESIRED STATE       CURRENT STATE              ERROR               PORTS</span><br><span class="line">uqsj8mim0sac        getstartedlab_web.1   friendlyhello:latest   localhost.localdomain   Running             Preparing 3 seconds ago</span><br><span class="line">shjiwlnj12sp        getstartedlab_web.2   friendlyhello:latest   php                     Running             Preparing 24 seconds ago</span><br><span class="line">8sqllvgid8jp        getstartedlab_web.3   friendlyhello:latest   php                     Running             Preparing 24 seconds ago</span><br><span class="line">v7fsecgcg504        getstartedlab_web.4   friendlyhello:latest   php                     Running             Preparing 24 seconds ago</span><br><span class="line">np8utmyvk5px        getstartedlab_web.5   friendlyhello:latest   localhost.localdomain   Running             Preparing 3 seconds ago</span><br><span class="line">[root@localhost compose]$</span><br></pre></td></tr></table></figure><p>在第二台的worker节点上执行命令会提示失败:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@php compose]$docker stack ps getstartedlab</span><br><span class="line">Error response from daemon: This node is not a swarm manager. Worker nodes can&apos;t be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager.</span><br><span class="line">[root@php compose]$</span><br></pre></td></tr></table></figure><p>现在,在两台服务器上都能访问刚才部署的app</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.12:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 926f433b3896&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; &lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;% </span><br><span class="line"></span><br><span class="line">huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 1535f17586ea&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; &lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;%                                                            huangyong@huangyong-Macbook-Pro  ~ </span><br></pre></td></tr></table></figure><h4 id="扩展app"><a href="#扩展app" class="headerlink" title="扩展app"></a>扩展app</h4><p>扩展app还是直接编辑docker-compose.yml文件.然后重新docker stack deploy部署即可.</p><p>如果是需要将其他虚拟机或者物理服务器加入进swarm集群,就像第二台服务器一样使用docker swarm join命令加入即可,</p><h4 id="停止swarm"><a href="#停止swarm" class="headerlink" title="停止swarm"></a>停止swarm</h4><p>命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker stack rm getstartedlab</span><br></pre></td></tr></table></figure><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 初始化一个swarm集群</span><br><span class="line"></span><br><span class="line">docker swarm init --advertise-addr IP</span><br><span class="line"></span><br><span class="line">#加入到swarm集群</span><br><span class="line">docker swarm join --token &lt;token&gt; &lt;swarm manager IP&gt;:&lt;port&gt;</span><br><span class="line"></span><br><span class="line">#部署app</span><br><span class="line">docker stack deploy -c docker-compose.yml &lt;services name&gt;</span><br><span class="line">&gt; note:在所有docker服务器节点上都需要有docker-compose.yml文件和相关镜像</span><br><span class="line"></span><br><span class="line"># 查看services </span><br><span class="line">docker stack ps &lt;services name&gt;</span><br><span class="line">docker services ls</span><br><span class="line">docker stack ls</span><br><span class="line"></span><br><span class="line"># 从swarm集群中删除 services</span><br><span class="line"></span><br><span class="line">docker stack rm &lt;service name&gt;</span><br><span class="line"></span><br><span class="line"># 删除swarm集群节点</span><br><span class="line"></span><br><span class="line">docker swarm leave #worker节点</span><br><span class="line">docker swarm leave --force #manager节点</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;理解swarm集群&quot;&gt;&lt;a href=&quot;#理解swarm集群&quot; class=&quot;headerlink&quot; title=&quot;理解swarm集群&quot;&gt;&lt;/a&gt;理解swarm集群&lt;/h3&gt;&lt;p&gt;一个swarm是一组运行docker服务器的的集群,docker服务器可以是物理机也可以是虚拟机.&lt;/p&gt;
&lt;p&gt;swarm manager可以使用多种策略来运行容器.比如”emptiest node”—部署容器到压力最小的服务器上,或者”global”—确保每台服务器都只允许一个容器实例.你可以在Compose文件中指示swarm manager去选择何种策略&lt;/p&gt;
&lt;p&gt;swarm managers是swarm进群中唯一可以执行命令,或者授权其他服务器以”workers”身份加入swarm集群的服务器.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&quot;初始化swarm-加入节点&quot;&gt;&lt;a href=&quot;#初始化swarm-加入节点&quot; class=&quot;headerlink&quot; title=&quot;初始化swarm,加入节点&quot;&gt;&lt;/a&gt;初始化swarm,加入节点&lt;/h4&gt;&lt;p&gt;试验环境:&lt;/p&gt;
&lt;p&gt;1.10.0.0.50 —swarm manager&lt;br&gt;2.10.0.0.12 —worker 节点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始化swarm,并且指定通告的IP&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---Dockerfile</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0%E2%80%942.dockerfile/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习—2.dockerfile/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:51:11.208Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h3><p>Dockerfile可以用来编译一个docker镜像.Dockerfile是一个包含一系列指令的文本文档,使用<figure class="highlight docker"><figcaption><span>build```命令,用户可以依据dockerfile和上下文编译一个镜像.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">使用dockerfile需要注意一些事项</span><br><span class="line"></span><br><span class="line">**<span class="number">1</span>.上下文**</span><br><span class="line"></span><br><span class="line">```docker build```编译镜像时,会将当前目录下的Dockerfile和所有文件打包添加发送到docker daemon服务端.所以一般情况下创建一个空目录编辑dockerfile文件.然后将需要<span class="keyword">copy</span>和add的文件放进和dockerfile同一目录下.</span><br><span class="line"></span><br><span class="line">dockerfile中的copy以及add命令,添加文件到docker镜像中时.不要使用绝对路径.例如/home/work/a.txt..docker deamon只能识别到当前上下文环境,无法识别到其他目录.但是可以使用当前上下文的相对路径.</span><br><span class="line"></span><br><span class="line">**2.分层**</span><br><span class="line"></span><br><span class="line">dockerfile编译镜像时,每条指令都是一个镜像层.除了From指令外,每一行指令都是基于上一行生成的临时镜像运行一个容器.执行一条指令就类似于```docker commit```命令生成一个新的镜像.所以两条指令之间互不关联.</span><br><span class="line"></span><br><span class="line">&lt;!--more--&gt;</span><br><span class="line"></span><br><span class="line">例如,下列的dockerfile并不能在/data/目录下创建files文件.</span><br></pre></td></tr></table></figure></p><p>FROM ubuntu<br>RUN mkdir /data<br>RUN cd /data<br>RUN touch files<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">下列的dockerfile甚至不会创建/data/file文件,也不会修改/data/目录权限</span><br></pre></td></tr></table></figure></p><p>FROM ubuntu<br>RUN useradd foo<br>VOLUME /data<br>RUN touch /data/file<br>RUN chown -R foo:foo /data<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">想要实现这个需求,可以这样写:</span><br></pre></td></tr></table></figure></p><p>FROM ubuntu<br>RUN useradd foo<br>RUN mkdir /data &amp;&amp; touch /data/file<br>RUN chown -R foo:foo /data<br>VOLUME /data<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"></span><br><span class="line">**3.精简**</span><br><span class="line"></span><br><span class="line">由于dockerfile在构建镜像时,dockerfile文本中每一行语句会产生每一层镜像.</span><br><span class="line"></span><br><span class="line">例如下面这个dockerfile:</span><br></pre></td></tr></table></figure></p><p>From ubuntu<br>RUN apt-get update<br>RUN apt-get -y install vim git wget net-tools<br>RUN useradd foo<br>RUN mkdir /data<br>RUN touch /data/file<br>RUN chown -R foo:foo /data<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在编译时,每一个RUN语句都会构建一层镜像.(实际上所有指令都是这样,不仅仅是RUN)</span><br></pre></td></tr></table></figure></p><p>[root@localhost test]$docker build -t test:v1 .<br>Sending build context to Docker daemon  2.048kB<br>Step 1/7 : From ubuntu<br> —&gt; 94e814e2efa8<br>Step 2/7 : RUN apt-get update<br> —&gt; Using cache<br> —&gt; 5520126e7fcc<br>Step 3/7 : RUN apt-get -y install vim git wget net-tools<br> —&gt; Using cache<br> —&gt; cb24e170539c<br>Step 4/7 : RUN useradd foo<br> —&gt; Using cache<br> —&gt; ca31aeba0309<br>Step 5/7 : RUN mkdir /data<br> —&gt; Using cache<br> —&gt; d5c6e0f32f6b<br>Step 6/7 : RUN touch /data/file<br> —&gt; Using cache<br> —&gt; 9c4b06e9b25d<br>Step 7/7 : RUN chown -R foo:foo /data<br> —&gt; Using cache<br> —&gt; 75ecea0b0795<br>Successfully built 75ecea0b0795<br>Successfully tagged test:v1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">这种写法会导致镜像层非常多,镜像文件也会相对较大.所以一般推荐更精简的语法,每一条功能相同的语句,尽量写在一行.上面的dockerfile可以优化成:</span><br></pre></td></tr></table></figure></p><p>From ubuntu<br>RUN apt-get update \<br>    &amp;&amp; apt-get -y install \<br>       vim \<br>       git \<br>       wget \<br>       net-tools</p><p>RUN useradd foo<br>RUN mkdir /data &amp;&amp;  touch /data/file &amp;&amp;  chown -R foo:foo /data<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这次编译只需构建4层镜像</span><br></pre></td></tr></table></figure></p><p>[root@localhost test]$docker build -t test:v1 .<br>Sending build context to Docker daemon  2.048kB<br>Step 1/4 : From ubuntu<br> —&gt; 94e814e2efa8<br>Step 2/4 : RUN apt-get update     &amp;&amp; apt-get -y install        vim        git        wget        net-tools<br> —&gt; Using cache<br> —&gt; 7aa2bc9041e0<br>Step 3/4 : RUN useradd foo<br> —&gt; Using cache<br> —&gt; 5a13764414e6<br>Step 4/4 : RUN mkdir /data &amp;&amp;  touch /data/file &amp;&amp;  chown -R foo:foo /data<br> —&gt; Using cache<br> —&gt; bd61817d7526<br>Successfully built bd61817d7526<br>Successfully tagged test:v1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">4.使用no-install-recommends</span><br><span class="line"></span><br><span class="line">如果是使用APT包管理器,则应该在执行apt-get install 命令时加上no-install-recommends参数.这样ATP就仅安装核心依赖.而不安装其他推荐和建议的包,这会显著减少不必要包的下载数量</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### Dockerfile指令介绍</span><br><span class="line"></span><br><span class="line">介绍完Dockerfile的概念和特点后,接下来了解一下Dockerfile语法中的具体指令的介绍和用法</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">下面是一个例子:</span><br></pre></td></tr></table></figure></p><h1 id="Use-an-official-Python-runtime-as-a-parent-image"><a href="#Use-an-official-Python-runtime-as-a-parent-image" class="headerlink" title="Use an official Python runtime as a parent image"></a>Use an official Python runtime as a parent image</h1><p>FROM python:2.7-slim</p><h1 id="Set-the-working-directory-to-app"><a href="#Set-the-working-directory-to-app" class="headerlink" title="Set the working directory to /app"></a>Set the working directory to /app</h1><p>WORKDIR /app</p><h1 id="Copy-the-current-directory-contents-into-the-container-at-app"><a href="#Copy-the-current-directory-contents-into-the-container-at-app" class="headerlink" title="Copy the current directory contents into the container at /app"></a>Copy the current directory contents into the container at /app</h1><p>COPY . /app</p><h1 id="Install-any-needed-packages-specified-in-requirements-txt"><a href="#Install-any-needed-packages-specified-in-requirements-txt" class="headerlink" title="Install any needed packages specified in requirements.txt"></a>Install any needed packages specified in requirements.txt</h1><p>RUN pip install –trusted-host pypi.python.org -r requirements.txt</p><h1 id="Make-port-80-available-to-the-world-outside-this-container"><a href="#Make-port-80-available-to-the-world-outside-this-container" class="headerlink" title="Make port 80 available to the world outside this container"></a>Make port 80 available to the world outside this container</h1><p>EXPOSE 80</p><h1 id="Define-environment-variable"><a href="#Define-environment-variable" class="headerlink" title="Define environment variable"></a>Define environment variable</h1><p>ENV NAME World</p><h1 id="Run-app-py-when-the-container-launches"><a href="#Run-app-py-when-the-container-launches" class="headerlink" title="Run app.py when the container launches"></a>Run app.py when the container launches</h1><p>CMD [“python”, “app.py”]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">* FROM </span><br><span class="line"></span><br><span class="line">格式: ```FROM &lt;image&gt;``` 或者 ```FROM &lt;image&gt;:tag</span><br></pre></td></tr></table></figure></p><p>表示从一个基础镜像构建.Dockerfile必须以FROM语句作为第一条非注释语句.</p><ul><li><p>WORKDIR: 表示工作目录,后续的相对路径也是基于这个目录</p></li><li><p>COPY</p></li></ul><p>格式: <figure class="highlight plain"><figcaption><span>src dest```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">复制宿主机上的文件到镜像中.```src```是当前上下文中的文件或者目录.```dest```是容器中的目标文件或者目录.```src```指定的源可以有多个.此外```src``` 还支持通配符.例如: COPY hom* /mydir/ 表示添加所有当前目录下的hom开头的文件到目录/mydir/下</span><br><span class="line"></span><br><span class="line">&lt;dest&gt;可以是文件或者目录.但是必须是镜像中的绝对路径,或者是WORKDIR的相对路径.若&lt;dest&gt;以反斜杠```/```结尾,则指向的是目录,否则指向文件.当```src```有多个源时,```dest```必须是目录.如果```dest```目录不存在,则会自动被创建</span><br><span class="line"></span><br><span class="line">* ADD</span><br><span class="line"></span><br><span class="line">格式: ```ADD src dest</span><br></pre></td></tr></table></figure></p><p> ADD和COPY命令有相同功能,都支持复制本地文件到镜像里.但ADD能从互联网的URL下载文件到镜像..<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> 但是如果```src```是一个URL的归档格式文件,则不会自动解压.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">* RUN</span><br><span class="line"></span><br><span class="line">RUN命令有两种格式:</span><br><span class="line"></span><br><span class="line">```RUN &lt;command&gt;``` (shell格式)  </span><br><span class="line">```RUN [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]``` (exec格式)</span><br><span class="line"></span><br><span class="line">RUN指令的两种格式表示命令在容器中的两种运行方式.当使用shell格式时,命令通过```/bin/sh -c```运行.当使用exec格式时.命令直接运行,不调用shell程序.exec格式中的参数会被当成JSON数组被Docker解析.所以必须使用双引号,不能使用单引号. </span><br><span class="line"></span><br><span class="line">另外由于exec格式不会在shell中运行.所以无法识别ENV环境变量.例如当执行```CMD [&quot;echo&quot;,&quot;$HOME&quot;]```时,```$HOME```不会被变量替换.如果希望运行shell程序.可以写成```CMD [&quot;sh&quot;,&quot;-c&quot;,&quot;echo&quot;,&quot;$HOME&quot;]```.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">* EXPOSE: 镜像需要暴露出来的端口. </span><br><span class="line"></span><br><span class="line">&gt; 要注意的是,这里只是说明镜像需要暴露哪些端口,在镜像构建完毕,启动容器时,仍然需要-p参数来映射端口,否则端口不会自动映射</span><br><span class="line"></span><br><span class="line">* ENV</span><br><span class="line"></span><br><span class="line">  格式: ENV &lt;key&gt; &lt;value&gt; 或者 ENV &lt;key&gt;=&lt;value&gt;</span><br><span class="line">  </span><br><span class="line">  ENV指令用来声明环境变量,并且可以被(ADD,COPY,WORKDIR等)指令调用.调用ENV环境变量的格式和shell一样:```$variable_name```或者 ```$&#123;variable_name&#125;</span><br></pre></td></tr></table></figure></p><ul><li>CMD </li></ul><p>CMD命令有3种格式:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CMD &lt;command&gt; (shell格式)  </span><br><span class="line">CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] (exec格式)  </span><br><span class="line">CMD [&quot;param1&quot;,&quot;param2&quot;] (为ENTRYPOINT命令提供参数)</span><br></pre></td></tr></table></figure><p>CMD提供容器启动后执行的命令.或者是为ENTRYPOINT传递一些参数.一个dockerfile文件只允许存在一条CMD指令.如果存在多条CMD指令,以最后一条为准.但是如果用户在<figure class="highlight docker"><figcaption><span>run```时指定了命令,则会覆盖CMD中的指令</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* <span class="keyword">ENTRYPOINT</span></span><br><span class="line"></span><br><span class="line">ENTRYPOINT有两种格式.和上文CMD一样分为shell格式和exec格式.</span><br><span class="line"></span><br><span class="line">ENTRYPOINT和CMD类似,指定容器启动时执行的命令.和CMD一样一个Dockerfile文件中可以有多个ENTRYPOINT命令.但只有最后一条生效.但是又有一些区别.当使用shell格式时,ENTRYPOINT会忽略任何CMD指令和```docker run```启动容器时手动输入的指令.并且会运行在```/bin/sh -c```环境中,成为它的子进程.进程在容器中PID不是1,也不能接收UNIX信号.(也就是在执行```docker stop &lt;container&gt;```时,进程接收不到SIGTERM指令)</span><br><span class="line"></span><br><span class="line">当使用exec格式时,```docker run```手动指定的命令,将作为参数覆盖CMD指定的参数传递到ENTRYPOINT.(也就是说```docker run```启动容器时指定的不再是具体命令,而是命令的参数).</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">创建上面dockerfile中所需要的app.py和requirements.txt文件,并且将他们和Dockerfile文件放在同一目录下:</span><br><span class="line"></span><br><span class="line">requirements.txt:</span><br></pre></td></tr></table></figure></p><p>[root@localhost docker_python]$cat requirements.txt<br>Flask<br>Redis</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">app.py:</span><br></pre></td></tr></table></figure><p>root@localhost docker_python]$cat app.py<br>from flask import Flask<br>from redis import Redis, RedisError<br>import os<br>import socket</p><h1 id="Connect-to-Redis"><a href="#Connect-to-Redis" class="headerlink" title="Connect to Redis"></a>Connect to Redis</h1><p>redis = Redis(host=”redis”, db=0, socket_connect_timeout=2, socket_timeout=2)</p><p>app = Flask(<strong>name</strong>)</p><p>@app.route(“/“)<br>def hello():<br>    try:<br>        visits = redis.incr(“counter”)<br>    except RedisError:<br>        visits = “<i>cannot connect to Redis, counter disabled</i>“</p><pre><code>html = &quot;&lt;h3&gt;Hello {name}!&lt;/h3&gt;&quot; \       &quot;&lt;b&gt;Hostname:&lt;/b&gt; {hostname}&lt;br/&gt;&quot; \       &quot;&lt;b&gt;Visits:&lt;/b&gt; {visits}&quot;return html.format(name=os.getenv(&quot;NAME&quot;, &quot;world&quot;), hostname=socket.gethostname(), visits=visits)</code></pre><p>if <strong>name</strong> == “<strong>main</strong>“:<br>    app.run(host=’0.0.0.0’, port=80)<br>[root@localhost docker_python]$<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 开始构建镜像</span><br><span class="line"></span><br><span class="line">首先确保Dockerfile里所需的文件,以及Dockerfile都在同一目录下:</span><br></pre></td></tr></table></figure></p><p>[root@localhost docker_python]$ls<br>app.py  Dockerfile  requirements.txt<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">运行以下命令来构建一个镜像.使用--tag参数(或者-t),可以为镜像打个标签:</span><br></pre></td></tr></table></figure></p><p>docker build –tag=friendlyhello .<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">构建过程略..在构建过程中,注意以下现象:</span><br><span class="line"></span><br><span class="line">1.构建镜像层:</span><br><span class="line"></span><br><span class="line">部分指令会创建一个新的镜像层,而有些指令则不会.关于如何区分命令是否会新建镜像层,一个基本的原则是:</span><br><span class="line"></span><br><span class="line">如果指令的作用是像镜像中添加新的文件或者程序,那么就会新建镜像层.(例如:RUN,COPY,ADD,FROM等)</span><br><span class="line"></span><br><span class="line">如果只是告诉docker如何构建或者运行应用程序,增加或者修改容器的元数据,那么不会构建新的镜像层.(例如:WORKDIR,EXPOSE,ENV,ENTERPOINT等)</span><br><span class="line"></span><br><span class="line">2.构建步骤:</span><br><span class="line"></span><br><span class="line">基本等过程大致为:</span><br><span class="line"></span><br><span class="line">运行临时容器----&gt;在该容器中运行Dockerfile指令----&gt;将运行结果保存为一个新等镜像层------&gt; 删除临时容器</span><br><span class="line"></span><br><span class="line">构建完成后,通过以下命令可以看到构建的镜像</span><br></pre></td></tr></table></figure></p><p>[root@localhost docker_python]$docker images<br>REPOSITORY                 TAG                 IMAGE ID            CREATED              SIZE<br>friendlyhello              latest              f091d1bb803c        About a minute ago   131MB</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt; 或者也可以是输入以下命令 docker image ls</span><br><span class="line"></span><br><span class="line">&gt; 可能会疑惑,为什么tag标签是latest..镜像的完整标签格式应该是:friendlyhello:lastest.</span><br><span class="line">如果需要在构建镜像时指定版本.可以使用: --tag=friendlyhello:v0.0.1</span><br><span class="line"></span><br><span class="line">----</span><br><span class="line"></span><br><span class="line">#### 使用构建的镜像启动一个容器</span><br><span class="line"></span><br><span class="line">输入以下命令,利用刚才的镜像启动一个容器:</span><br></pre></td></tr></table></figure><p>docker run -p 4000:80 friendlyhello<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">执行结果:</span><br></pre></td></tr></table></figure></p><p>[root@localhost docker_python]$docker run -p 4000:80 friendlyhello</p><ul><li>Serving Flask app “app” (lazy loading)</li><li>Environment: production<br>WARNING: Do not use the development server in a production environment.<br>Use a production WSGI server instead.</li><li>Debug mode: off</li><li><p>Running on <a href="http://0.0.0.0:80/" target="_blank" rel="noopener">http://0.0.0.0:80/</a> (Press CTRL+C to quit)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> * docker run 表示启动一个容器</span><br><span class="line"> * -p 宿主机端口:容器端口  表示将宿主机的端口映射给容器.如果是-P 80 表示随机映射一个宿主机的端口给容器</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">此时可以在其他电脑上访问这个容器的80端口,下面是在我的PC上访问宿主机的4000端口,也就是刚才启动的容器</span><br></pre></td></tr></table></figure><p>✘ huangyong@huangyong-Macbook-Pro  ~  curl <a href="http://10.0.0.50:4000" target="_blank" rel="noopener">http://10.0.0.50:4000</a><br></p><h3>Hello World!</h3><b>Hostname:</b> f9b1b804404f<br><b>Visits:</b> <i>cannot connect to Redis, counter disabled</i>%<p></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">容器默认是在前台执行,加上-d参数可以时容器运行在后台:</span><br></pre></td></tr></table></figure></li></ul><p>docker run -d -p 4000:80 friendlyhello<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">docker ps命令可以显示正在运行中的容器</span><br></pre></td></tr></table></figure></p><p>[root@localhost docker_python]$docker ps<br>CONTAINER ID        IMAGE                      COMMAND             CREATED             STATUS              PORTS                    NAMES<br>fcf7d29ac627        friendlyhello              “python app.py”     5 seconds ago       Up 1 second         0.0.0.0:4000-&gt;80/tcp     stoic_colden<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; docker container ls命令也有同样的效果</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">这一节(包括第3小节)涉及到的基础命令如下:</span><br></pre></td></tr></table></figure></p><p>docker build -t friendlyhello .  # Create image using this directory’s Dockerfile<br>docker run -p 4000:80 friendlyhello  # Run “friendlyname” mapping port 4000 to 80<br>docker run -d -p 4000:80 friendlyhello         # Same thing, but in detached mode<br>docker container ls                                # List all running containers<br>docker container ls -a             # List all containers, even those not running<br>docker container stop <hash>           # Gracefully stop the specified container<br>docker container kill <hash>         # Force shutdown of the specified container<br>docker container rm <hash>        # Remove specified container from this machine<br>docker container rm $(docker container ls -a -q)         # Remove all containers<br>docker image ls -a                             # List all images on this machine<br>docker image rm <image id="">            # Remove specified image from this machine<br>docker image rm $(docker image ls -a -q)   # Remove all images from this machine<br>docker login             # Log in this CLI session using your Docker credentials<br>docker tag <image> username/repository:tag  # Tag <image> for upload to registry<br>docker push username/repository:tag            # Upload tagged image to registry<br>docker run username/repository:tag                   # Run image from a registry<br><code>`</code></image></image></image></hash></hash></hash></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Dockerfile&quot;&gt;&lt;a href=&quot;#Dockerfile&quot; class=&quot;headerlink&quot; title=&quot;Dockerfile&quot;&gt;&lt;/a&gt;Dockerfile&lt;/h3&gt;&lt;p&gt;Dockerfile可以用来编译一个docker镜像.Dockerfile是
      
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker使用阿里云私有仓库</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0%E2%80%943.%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F%E5%92%8C%E4%BB%93%E5%BA%93/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习—3.使用阿里云国内镜像和仓库/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:11:08.185Z</updated>
    
    <content type="html"><![CDATA[<h3 id="docker使用阿里云私有仓库"><a href="#docker使用阿里云私有仓库" class="headerlink" title="docker使用阿里云私有仓库"></a>docker使用阿里云私有仓库</h3><p>注册阿里云镜像服务:</p><p>以下是我的阿里云镜像仓库链接:<br><a href="https://cr.console.aliyun.com/cn-hangzhou/repositories" target="_blank" rel="noopener">https://cr.console.aliyun.com/cn-hangzhou/repositories</a></p><p>一.使用阿里云镜像加速器<br><a href="https://cr.console.aliyun.com/cn-hangzhou/mirrors" target="_blank" rel="noopener">https://cr.console.aliyun.com/cn-hangzhou/mirrors</a></p><p>镜像加速地址:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://0w5ygvsg.mirror.aliyuncs.com</span><br></pre></td></tr></table></figure><p>如果是Centos系统,可以通过修改daemon配置文件/etc/docker/daemon.json来使用加速器:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /etc/docker</span><br><span class="line">sudo tee /etc/docker/daemon.json &lt;&lt;-&apos;EOF&apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https://0w5ygvsg.mirror.aliyuncs.com&quot;]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure><hr><a id="more"></a><h4 id="使用阿里云的镜像仓库"><a href="#使用阿里云的镜像仓库" class="headerlink" title="使用阿里云的镜像仓库"></a>使用阿里云的镜像仓库</h4><p>首先在阿里云镜像服务控制台创建镜像仓库和命令空间:</p><p><a href="https://cr.console.aliyun.com/cn-hangzhou/repositories" target="_blank" rel="noopener">https://cr.console.aliyun.com/cn-hangzhou/repositories</a></p><p>我的镜像仓库和命名空间都是:jesse_images<br>这是我的镜像仓库地址:registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images</p><p>下面演示,如何推送本地镜像到阿里云仓库</p><p>1.在本地docker服务器登陆阿里云镜像仓库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost docker_python]$docker login --username=jessehuang408 registry.cn-hangzhou.aliyuncs.com</span><br></pre></td></tr></table></figure><p>2.将本地镜像推送到仓库执行以下两条命令</p><ul><li>为本地镜像打个标签</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker tag [ImageId] registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:[镜像版本号]</span><br></pre></td></tr></table></figure><ul><li>将镜像推送到仓库</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:[镜像版本号]</span><br></pre></td></tr></table></figure><p>下面演示将friendlyhello这个镜像推送到阿里云远程仓库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@localhost docker_python]$docker images</span><br><span class="line">REPOSITORY                                                    TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">friendlyhello                                                 latest              f091d1bb803c        43 minutes ago      131MB</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker tag f091d1bb803c registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:v2.0</span><br><span class="line"></span><br><span class="line">docker push registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:v2.0</span><br></pre></td></tr></table></figure><p>在本机可以看到镜像:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@localhost docker_python]$docker images</span><br><span class="line">REPOSITORY                                                    TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">friendlyhello                                                 latest              f091d1bb803c        43 minutes ago      131MB</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images   v2.0                f091d1bb803c        43 minutes ago      131MB</span><br></pre></td></tr></table></figure><p>登陆阿里云的镜像服务控制台,在镜像仓库的管理界面可以看到上传上去的镜像</p><p>如果是从阿里云镜像仓库拉取镜像,执行以下命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker pull registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:[镜像版本号]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;docker使用阿里云私有仓库&quot;&gt;&lt;a href=&quot;#docker使用阿里云私有仓库&quot; class=&quot;headerlink&quot; title=&quot;docker使用阿里云私有仓库&quot;&gt;&lt;/a&gt;docker使用阿里云私有仓库&lt;/h3&gt;&lt;p&gt;注册阿里云镜像服务:&lt;/p&gt;
&lt;p&gt;以下是我的阿里云镜像仓库链接:&lt;br&gt;&lt;a href=&quot;https://cr.console.aliyun.com/cn-hangzhou/repositories&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://cr.console.aliyun.com/cn-hangzhou/repositories&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一.使用阿里云镜像加速器&lt;br&gt;&lt;a href=&quot;https://cr.console.aliyun.com/cn-hangzhou/mirrors&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://cr.console.aliyun.com/cn-hangzhou/mirrors&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;镜像加速地址:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;https://0w5ygvsg.mirror.aliyuncs.com&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果是Centos系统,可以通过修改daemon配置文件/etc/docker/daemon.json来使用加速器:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sudo mkdir -p /etc/docker&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&amp;apos;EOF&amp;apos;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;quot;registry-mirrors&amp;quot;: [&amp;quot;https://0w5ygvsg.mirror.aliyuncs.com&amp;quot;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;EOF&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo systemctl daemon-reload&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo systemctl restart docker&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---Docker存储驱动篇</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E7%AC%94%E8%AE%B0%E2%80%94%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8%E7%AF%87/"/>
    <id>https://jesse.top/2020/06/29/docker/docker笔记—存储驱动篇/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:42:46.456Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker笔记——存储驱动篇"><a href="#docker笔记——存储驱动篇" class="headerlink" title="docker笔记——存储驱动篇"></a>docker笔记——存储驱动篇</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>通过前一篇的镜像笔记,我们知道docker的镜像是只读的,而且通过同一个镜像启动的docker容器,他们共享同一份底层镜像文件.</p><p>这里主要说一说.这些分层的多个只读Image镜像是如何在磁盘中存储的.</p><hr><h3 id="docker存储驱动"><a href="#docker存储驱动" class="headerlink" title="docker存储驱动"></a>docker存储驱动</h3><p>docker提供了多种存储驱动来实现不同的方式存储镜像，下面是常用的几种存储驱动：</p><ul><li>AUFS</li><li>OverlayFS</li><li>Devicemapper</li><li>Btrfs</li><li>ZFS</li></ul><p>下面说一说AUFS、OberlayFS及Devicemapper，更多的存储驱动说明可参考：<a href="http://dockone.io/article/1513" target="_blank" rel="noopener">http://dockone.io/article/1513</a></p><a id="more"></a><h4 id="AUFS"><a href="#AUFS" class="headerlink" title="AUFS"></a>AUFS</h4><p>AUFS（AnotherUnionFS）是一种Union FS，是文件级的存储驱动。AUFS是一个能透明覆盖一个或多个现有文件系统的层状文件系统，把多层合并成文件系统的单层表示。简单来说就是支持将不同目录挂载到同一个虚拟文件系统下的文件系统。这种文件系统可以一层一层地叠加修改文件。无论底下有多少层都是只读的，只有最上层的文件系统是可写的。当需要修改一个文件时，AUFS创建该文件的一个副本，使用CoW(写时复制)将文件从只读层复制到可写层进行修改，结果也保存在可写层。</p><p>通常来说最上层是可读写层,下层是只读层.当需要读取一个文件A时,会从最顶层的读写层开始向下寻找.本层没有则根据层关系到下一层开始找.直到找到第一个文件A</p><p>当需要写入一个文件A时,如果这个文件不存在,则在读写层新建一个.否则会像上面的步骤一样从顶层开始寻找,找到A文件后,复制到读写层进行修改</p><p>当需要删除一个文件A时,如果这个文件仅仅存在读写层,则直接删除.否则就需要先在读写层删除,然后再在读写层创建一个whiteout文件来标志这个文件不存在,而不是真正删除底层文件.</p><p>当新建一个文件A.如果这个文件在读写层存在对应的whiteout文件,则先将whiteout文件删除再新建.否则直接读写层新建</p><p>在Docker中，底下的只读层就是image，可写层就是Container。结构如下图所示：</p><p><img src="https://img1.jesse.top/docker-aufs.jpg" alt=""></p><p>如果你正在使用aufs作为存储驱动,那么在Docker的工作目录(/var/lib/docker)和image下发现aufs目录:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">root@docker:~# tree /var/lib/docker -d -L 1</span><br><span class="line">/var/lib/docker</span><br><span class="line">├── aufs</span><br><span class="line">├── containers</span><br><span class="line">├── image</span><br><span class="line">├── network</span><br><span class="line">├── plugins</span><br><span class="line">├── swarm</span><br><span class="line">├── tmp</span><br><span class="line">├── trust</span><br><span class="line">└── volumes</span><br><span class="line"></span><br><span class="line">root@docker:~# tree /var/lib/docker/image -d -L 1</span><br><span class="line">/var/lib/docker/image</span><br><span class="line">└── aufs</span><br><span class="line"></span><br><span class="line">root@docker:~# tree /var/lib/docker/aufs/ -d -L 1</span><br><span class="line">/var/lib/docker/aufs/</span><br><span class="line">├── diff</span><br><span class="line">├── layers</span><br><span class="line">└── mnt</span><br></pre></td></tr></table></figure><p>在docker工作目录的aufs目录下有3个目录.其中mnt为aufs的挂载目录,diff为实际数据,包括只读层和读写层.layers为每层依赖有关的层描述文件</p><hr><h3 id="Device-mapper"><a href="#Device-mapper" class="headerlink" title="Device mapper"></a>Device mapper</h3><p>Device mapper是Linux内核2.6.9后支持的，提供的一种从逻辑设备到物理设备的映射框架机制，在该机制下，用户可以很方便的根据自己的需要制定实现存储资源的管理策略。前面讲的AUFS和OverlayFS都是文件级存储，而Device mapper是块级存储，所有的操作都是直接对块进行操作，而不是文件。</p><p>Device mapper驱动会先在块设备上创建一个资源池，然后在资源池上创建一个带有文件系统的基本设备，所有镜像都是这个基本设备的快照，而容器则是镜像的快照。所以在容器里看到文件系统是资源池上基本设备的文件系统的快照，并不有为容器分配空间。当要写入一个新文件时，在容器的镜像内为其分配新的块并写入数据，这个叫用时分配。</p><p>当要修改已有文件时，再使用CoW为容器快照分配块空间，将要修改的数据复制到在容器快照中新的块里再进行修改。Device mapper 驱动默认会创建一个100G的文件包含镜像和容器。每一个容器被限制在10G大小的卷内，可以自己配置调整。结构如下图所示：</p><p><img src="https://img1.jesse.top/docker-devicemapper.jpg" alt=""></p><p>在Centos 7发行版上最新版的docker中,默认的存储驱动就是device mapper.但是提示已经被弃用了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker info</span><br><span class="line">Containers: 3</span><br><span class="line"> Running: 3</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 0</span><br><span class="line">Images: 138</span><br><span class="line">Server Version: 18.09.2</span><br><span class="line">Storage Driver: devicemapper #这一行</span><br><span class="line">......略......</span><br><span class="line">WARNING: the devicemapper storage-driver is deprecated, and will be removed in a future release. #最后这一行提示devicemapper已经被弃用</span><br></pre></td></tr></table></figure><p>和aufs一样,在docker的工作目录下也能看到device mapper目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$ll /var/lib/docker/devicemapper/</span><br><span class="line">总用量 32</span><br><span class="line">drwx------ 2 root root    32 2月  23 16:25 devicemapper</span><br><span class="line">drwx------ 2 root root 24576 5月  16 10:30 metadata</span><br><span class="line">drwxr-xr-x 5 root root  4096 5月  16 10:30 mnt</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker笔记——存储驱动篇&quot;&gt;&lt;a href=&quot;#docker笔记——存储驱动篇&quot; class=&quot;headerlink&quot; title=&quot;docker笔记——存储驱动篇&quot;&gt;&lt;/a&gt;docker笔记——存储驱动篇&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;通过前一篇的镜像笔记,我们知道docker的镜像是只读的,而且通过同一个镜像启动的docker容器,他们共享同一份底层镜像文件.&lt;/p&gt;
&lt;p&gt;这里主要说一说.这些分层的多个只读Image镜像是如何在磁盘中存储的.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;docker存储驱动&quot;&gt;&lt;a href=&quot;#docker存储驱动&quot; class=&quot;headerlink&quot; title=&quot;docker存储驱动&quot;&gt;&lt;/a&gt;docker存储驱动&lt;/h3&gt;&lt;p&gt;docker提供了多种存储驱动来实现不同的方式存储镜像，下面是常用的几种存储驱动：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AUFS&lt;/li&gt;
&lt;li&gt;OverlayFS&lt;/li&gt;
&lt;li&gt;Devicemapper&lt;/li&gt;
&lt;li&gt;Btrfs&lt;/li&gt;
&lt;li&gt;ZFS&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面说一说AUFS、OberlayFS及Devicemapper，更多的存储驱动说明可参考：&lt;a href=&quot;http://dockone.io/article/1513&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://dockone.io/article/1513&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>kong+postgresql+konga集群环境部署</title>
    <link href="https://jesse.top/2020/06/26/Linux-Web/kong%20postgresql%20konga/"/>
    <id>https://jesse.top/2020/06/26/Linux-Web/kong postgresql konga/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T03:09:18.121Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kong-postgresql-konga集群环境部署"><a href="#kong-postgresql-konga集群环境部署" class="headerlink" title="kong+postgresql+konga集群环境部署"></a>kong+postgresql+konga集群环境部署</h2><h3 id="kong简介"><a href="#kong简介" class="headerlink" title="kong简介"></a>kong简介</h3><p>Kong是Mashape开源的一款API网关，起初是用来管理 Mashape 公司15000个微服务的，后来在2015年开源,现在已经在很多创业公司、大型企业和政府机构中广泛使用。基于nginx,Lua和Cassandra或PostgreSQL，支持分布式操作，有很强的可移植性和可扩展性。可以在任何一种基础设施上运行,作为应用和API之间的中间层，加上众多功能强大的插件，可以实现认证授权、访问控制等功能。并且提供易于使用的RESTful API来操作和配置系统。</p><p>有关kong的详细介绍请参考官网.</p><p>–</p><h3 id="postgreSQL简介"><a href="#postgreSQL简介" class="headerlink" title="postgreSQL简介"></a>postgreSQL简介</h3><p><a href="https://baike.baidu.com/item/PostgreSQL/530240" target="_blank" rel="noopener">PostgreSQL</a> 是一个免费的对象-关系数据库服务器(数据库管理系统)，它在灵活的 BSD-风格许可证下发行。它提供了相对其他开放源代码数据库系统(比如 MySQL 和 Firebird)，和专有系统(比如 Oracle、Sybase、IBM 的 DB2 和 Microsoft SQL Server)之外的另一种选择。</p><p>–</p><h3 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h3><ul><li><strong>kong cluster</strong></li></ul><p>kong 集群并不意味着客户端请求将会负载均衡到kong集群中的每个节点上，kong集群并不是开箱即用，仍然需要在kong集群多节点上层搭建负载均衡，以便分发请求。 一个kong集群只是意味着集群内的节点，都共享同样的配置。</p><p>有关Kong cluster集群的详细介绍请参考官网:<a href="https://docs.konghq.com/0.14.x/clustering/" target="_blank" rel="noopener">Kong cluser document</a></p><p>为了提高冗余性和健壮性.我们对kong的每个环节都进行了冗余设计.一个基本的kong集群架构大概如下图所示:</p><p><img src="https://img1.jesse.top/kong-flow.png" alt=""></p><a id="more"></a><p>–</p><h3 id="部署步骤"><a href="#部署步骤" class="headerlink" title="部署步骤"></a>部署步骤</h3><p><strong>环境</strong>: </p><p>阿里云ECS Centos7.4操作系统<br>Kong: 1.0最新版<br>postgresql 9.6</p><p>konga 最新版</p><p><strong>集群架构说明:</strong></p><p>dwd-kong-node1节点服务器部署postgresql master主库</p><p>dwd-kong-node2节点服务器部署postgresql slave从库</p><p>在两个Kong节点服务器上都部署konga,但是konga指向postgresql master主库</p><p>–</p><h4 id="安装postgresql"><a href="#安装postgresql" class="headerlink" title="安装postgresql"></a>安装postgresql</h4><p>安装方式官网参考: <a href="https://www.postgresql.org/" target="_blank" rel="noopener">Installing Postgresql</a></p><p><strong>安装步骤</strong></p><p>1.安装Postgre9.6版本的Yum源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-centos96-9.6-3.noarch.rpm</span><br></pre></td></tr></table></figure><p>2.安装postgresql</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install postgresql96 postgresql96-server -y</span><br></pre></td></tr></table></figure><p>3.切换到root用户下,初始化数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /usr/pgsql-9.6/bin/postgresql96-setup initdb</span><br></pre></td></tr></table></figure><p>4.修改数据库的配置文件,监听所有接口</p><p>postgresql的配置文件默认路径在:/var/lib/pgsql/9.6/data/postgresql.conf</p><p>修改下列配置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">listen_addresses = &apos;*&apos; #监听所有接口.由于我服务器只有内网地址,所有可以侦听在所有接口,(如果有公网地址,最好不要只样做)</span><br><span class="line">log_directory = &apos;/data/logs/postgre&apos; #指定日志文件的父目录</span><br></pre></td></tr></table></figure><p>5.修改数据库远程访问配置文件.开启远程访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim /var/lib/pgsql/9.6/data/postgresql.conf/pg_hba.conf</span><br><span class="line"></span><br><span class="line">修改下列两项:</span><br><span class="line">#修改为md5认证,下列10.0.0.0/8是内网地址段</span><br><span class="line">host    all             all             127.0.0.1/32            md5</span><br><span class="line">host    all             all             10.0.0.0/8           md5</span><br></pre></td></tr></table></figure><p>6.创建日志目录,并且赋权给postgres用户(安装postgresql后默认会创建这个用户)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-kong-node1 data]# mkdir /data/logs/postgre</span><br><span class="line">[root@dwd-kong-node1 data]# chown -R postgre.postgre /data/logs/postgre</span><br></pre></td></tr></table></figure><p>7.启动postgresql数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable postgresql-9.6</span><br><span class="line">systemctl start postgresql-9.6</span><br></pre></td></tr></table></figure><hr><h3 id="创建数据库用户密码"><a href="#创建数据库用户密码" class="headerlink" title="创建数据库用户密码"></a>创建数据库用户密码</h3><p>1.切换到postgres用户,输入psql登陆数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-kong-node1 data]# su - postgres</span><br><span class="line">Last login: Mon Apr  8 09:27:12 CST 2019 on pts/0</span><br><span class="line">-bash-4.2$</span><br><span class="line">-bash-4.2$ psql</span><br><span class="line">psql (11.2, server 9.6.12)</span><br><span class="line">Type &quot;help&quot; for help.</span><br><span class="line"></span><br><span class="line">postgres=#</span><br></pre></td></tr></table></figure><p>2.创建kong用户和kong数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CREATE USER kong; </span><br><span class="line">CREATE DATABASE kong OWNER kong;</span><br></pre></td></tr></table></figure><p>3.创建konga的数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE konga OWNER kong;</span><br></pre></td></tr></table></figure><p>4.为kong用户创建一个密码,密码也是kong</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter user kong with password &apos;kong&apos;;</span><br></pre></td></tr></table></figure><p>5.输入\l,可以看到当前的数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">postgres=# \l</span><br><span class="line">                                  List of databases</span><br><span class="line">   Name    |  Owner   | Encoding |   Collate   |    Ctype    |   Access privileges</span><br><span class="line">-----------+----------+----------+-------------+-------------+-----------------------</span><br><span class="line"> kong      | kong     | UTF8     | en_US.UTF-8 | en_US.UTF-8 |</span><br><span class="line"> konga     | kong     | UTF8     | en_US.UTF-8 | en_US.UTF-8 |</span><br><span class="line"> postgres  | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 |</span><br><span class="line"> template0 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +</span><br><span class="line">           |          |          |             |             | postgres=CTc/postgres</span><br><span class="line"> template1 | postgres | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/postgres          +</span><br><span class="line">           |          |          |             |             | postgres=CTc/postgres</span><br><span class="line">(5 rows)</span><br></pre></td></tr></table></figure><p>6.查看当前数据库有哪些用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">postgres=# select * from pg_roles;</span><br><span class="line">      rolname      | rolsuper | rolinherit | rolcreaterole | rolcreatedb | rolcanlogin | rolreplication | rolconnlimit | rolpassword | rolvaliduntil | rolbypassrls | rolconfig |  oid</span><br><span class="line">-------------------+----------+------------+---------------+-------------+-------------+----------------+--------------+-------------+---------------+--------------+-----------+-------</span><br><span class="line"> postgres          | t        | t          | t             | t           | t           | t              |           -1 | ********    |               | t            |           |    10</span><br><span class="line"> pg_signal_backend | f        | t          | f             | f           | f           | f              |           -1 | ********    |               | f            |           |  4200</span><br><span class="line"> kong              | f        | t          | f             | f           | t           | f              |           -1 | ********    |               | f            |           | 16384</span><br><span class="line">(3 rows)</span><br></pre></td></tr></table></figure><hr><h4 id="安装Kong"><a href="#安装Kong" class="headerlink" title="安装Kong"></a>安装Kong</h4><p>安装方法可以参考官网:<a href="https://docs.konghq.com/install/centos/?_ga=2.110797315.728319704.1539597667-917309945.1539077269#packages" target="_blank" rel="noopener">Install Kong</a></p><p>1.下载,安装rpm安装包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget -O kong-community-edition-1.0.0.el7.noarch.rpm  https://kong.bintray.com/kong-community-edition-rpm/centos/7/:kong-community-edition-1.0.0.el7.noarch.rpm</span><br><span class="line"></span><br><span class="line">sudo yum install epel-release</span><br><span class="line">sudo yum install kong-community-edition-1.0.0.el7.noarch.rpm</span><br></pre></td></tr></table></figure><p>2.dwd-kong-node1修改kong配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kong</span><br><span class="line">sudo cp kong.conf.default kong.conf</span><br><span class="line"></span><br><span class="line">[root@dwd-kong-node1 data]#  sed -r &apos;/^[[:space:]]+#*/d&apos; /etc/kong/kong.conf | sed &apos;/^#/d&apos; | sed &apos;/^$/d&apos;</span><br><span class="line"></span><br><span class="line">prefix = /data/logs/kong/       # Working directory. Equivalent to Nginx&apos;s</span><br><span class="line">proxy_access_log = access.log       # Path for proxy port request access</span><br><span class="line">proxy_error_log = error.log         # Path for proxy port request error</span><br><span class="line">admin_listen = 0.0.0.0:8001     # Address and port on which Kong will expose</span><br><span class="line">database = postgres             # Determines which of PostgreSQL or Cassandra</span><br><span class="line">pg_host = 127.0.0.1             # The PostgreSQL host to connect to.</span><br><span class="line">pg_port = 5432                  # The port to connect to.</span><br><span class="line">pg_user = kong                  # The username to authenticate if required.</span><br><span class="line">pg_password = kong                 # The password to authenticate if required.</span><br><span class="line">pg_database = kong              # The database name to connect to.</span><br><span class="line">cluster_listen = 0.0.0.0:7946   # Address and port used to communicate with</span><br><span class="line">cluster_listen_rpc = 127.0.0.1:7373  # Address and port used to communicate</span><br></pre></td></tr></table></figure><p>3.dwd-kong-node2修改kong配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-kong-node2 system]# clear</span><br><span class="line">[root@dwd-kong-node2 system]# sed -r &apos;/^[[:space:]]+#*/d&apos; /etc/kong/kong.conf | sed &apos;/^#/d&apos; | sed &apos;/^$/d&apos;</span><br><span class="line">prefix = /data/logs/kong/       # Working directory. Equivalent to Nginx&apos;s</span><br><span class="line">proxy_access_log = access.log       # Path for proxy port request access</span><br><span class="line">proxy_error_log = error.log         # Path for proxy port request error</span><br><span class="line">database = postgres             # Determines which of PostgreSQL or Cassandra</span><br><span class="line">pg_host = 10.111.30.174             # The PostgreSQL host to connect to.</span><br><span class="line">pg_port = 5432                  # The port to connect to.</span><br><span class="line">pg_user = kong                  # The username to authenticate if required.</span><br><span class="line">pg_password = kong                 # The password to authenticate if required.</span><br><span class="line">pg_database = kong              # The database name to connect to.</span><br><span class="line">pg_ssl = off                    # Toggles client-server TLS connections</span><br><span class="line">pg_ssl_verify = off             # Toggles server certificate verification if</span><br><span class="line">cluster_listen = 0.0.0.0:7946   # Address and port used to communicate with</span><br><span class="line">cluster_listen_rpc = 127.0.0.1:7373  # Address and port used to communicate</span><br></pre></td></tr></table></figure><blockquote><p>唯一区别就是这里的数据库指向dwd-kong-node1上的postgresql(IP:10.111.30.174),而非本机.</p></blockquote><p>3.创建kong目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /data/logs/kong/</span><br></pre></td></tr></table></figure><p>4.在dwd-kong-node1上准备数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kong migrations bootstrap -c /etc/kong/kong.conf</span><br></pre></td></tr></table></figure><blockquote><p>由于dwd-kong-node2上指向了node1的数据库,所以在node2上不需要执行这个命令</p></blockquote><p>5.启动kong</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kong start -c /etc/kong/kong.conf</span><br></pre></td></tr></table></figure><p>查看端口.可以看到postgresql和kong的侦听端口都已经成功启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-kong-node1 data]# netstat -tulpn</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp        0      0 0.0.0.0:5432            0.0.0.0:*               LISTEN      26580/postmaster</span><br><span class="line">tcp        0      0 0.0.0.0:8443            0.0.0.0:*               LISTEN      24746/nginx: master</span><br><span class="line">tcp        0      0 0.0.0.0:5822            0.0.0.0:*               LISTEN      29688/sshd</span><br><span class="line">tcp        0      0 0.0.0.0:8000            0.0.0.0:*               LISTEN      24746/nginx: master</span><br><span class="line">tcp        0      0 0.0.0.0:8001            0.0.0.0:*               LISTEN      24746/nginx: master</span><br><span class="line">tcp6       0      0 :::5432                 :::*                    LISTEN      26580/postmaster</span><br><span class="line">udp        0      0 0.0.0.0:68              0.0.0.0:*                           748/dhclient</span><br><span class="line">udp        0      0 10.111.30.174:123       0.0.0.0:*                           6290/ntpd</span><br><span class="line">udp        0      0 127.0.0.1:123           0.0.0.0:*                           6290/ntpd</span><br><span class="line">udp        0      0 0.0.0.0:123             0.0.0.0:*                           6290/ntpd</span><br><span class="line">udp        0      0 0.0.0.0:34019           0.0.0.0:*                           748/dhclient</span><br><span class="line">udp6       0      0 :::31421                :::*                                748/dhclient</span><br><span class="line">udp6       0      0 :::123                  :::*                                6290/ntpd</span><br></pre></td></tr></table></figure><hr><h3 id="搭建konga"><a href="#搭建konga" class="headerlink" title="搭建konga"></a>搭建konga</h3><p>konga是管理kong各个组件(serveice,route,plugin,upstream,consumer)的可视化UI管理工具.在增删改查各个组件的配置时非常方便.</p><p>个人觉得UI界面比kong-dashboard要漂亮</p><p>konga的github参考:<a href="https://github.com/pantsel/konga" target="_blank" rel="noopener">konga</a></p><hr><p>1.确保需要有node.js环境.如果没有npm工具,必须先安装nodejs</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@DWD-BETA kong]$ npm -v</span><br><span class="line">6.4.1</span><br><span class="line">[work@DWD-BETA kong]$ node -v</span><br><span class="line">v10.15.3</span><br></pre></td></tr></table></figure><p>2.安装bower,gulp包.安装git软件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm install bower</span><br><span class="line">npm install gulp</span><br><span class="line">yum install git</span><br></pre></td></tr></table></figure><p>3.work用户下安装konga</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/pantsel/konga.git</span><br><span class="line">$ cd konga</span><br><span class="line">$ npm i</span><br></pre></td></tr></table></figure><p>4.编辑.env环境文件(dwd-kong-node2的文件内容中将下列的localhost修改为node1服务器的IP:10.111.30.174)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">work@dwd-kong-node1 konga]$ cat .env_example</span><br><span class="line">PORT=1337</span><br><span class="line">NODE_ENV=production</span><br><span class="line">KONGA_HOOK_TIMEOUT=120000</span><br><span class="line">DB_ADAPTER=postgres</span><br><span class="line">DB_URI=postgresql://localhost:5432/konga</span><br><span class="line">KONGA_LOG_LEVEL=warn</span><br><span class="line">TOKEN_SECRET=some_secret_token</span><br><span class="line">DB_USER=kong</span><br><span class="line">DB_PASSWORD=kong</span><br><span class="line">DB_DATABASE=konga</span><br></pre></td></tr></table></figure><p>5.启动konga</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">work@dwd-kong-node1 konga]$ npm start</span><br></pre></td></tr></table></figure><p>6.如果启动报错,则安装依赖</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">work@dwd-kong-node1 konga]$ npm run bower-deps</span><br></pre></td></tr></table></figure><p>这个程序默认是前台启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[work@dwd-kong-node1 konga]$ npm start</span><br><span class="line"></span><br><span class="line">&gt; kongadmin@0.14.3 start /home/work/konga</span><br><span class="line">&gt; node --harmony app.js</span><br><span class="line"></span><br><span class="line">No DB Adapter defined. Using localDB...</span><br><span class="line">debug: Hook:api_health_checks:process() called</span><br><span class="line">debug: Hook:health_checks:process() called</span><br><span class="line">debug: Hook:start-scheduled-snapshots:process() called</span><br><span class="line">debug: Hook:upstream_health_checks:process() called</span><br><span class="line">debug: Hook:user_events_hook:process() called</span><br><span class="line">debug: User had models, so no seed needed</span><br><span class="line">debug: Kongnode had models, so no seed needed</span><br><span class="line">debug: Emailtransport seeds updated</span><br><span class="line">debug: -------------------------------------------------------</span><br><span class="line">debug: :: Mon Apr 08 2019 18:34:31 GMT+0800 (China Standard Time)</span><br><span class="line">debug: Environment : development</span><br><span class="line">debug: Port        : 1337</span><br><span class="line">debug: -------------------------------------------------------</span><br></pre></td></tr></table></figure><p>此时在浏览器输入:<a href="http://10.111.30.174:1337" target="_blank" rel="noopener">http://10.111.30.174:1337</a> 就能访问konga了.</p><hr><h3 id="systemctl管理kong和konga进程"><a href="#systemctl管理kong和konga进程" class="headerlink" title="systemctl管理kong和konga进程"></a>systemctl管理kong和konga进程</h3><ul><li>kong</li></ul><p>在/usr/lib/systemd/system路径下编辑kong.service文件.内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">work@dwd-kong-node1 system]$ pwd</span><br><span class="line">/usr/lib/systemd/system</span><br><span class="line">[work@dwd-kong-node1 system]$ cat kong.service</span><br><span class="line">[Unit]</span><br><span class="line">Description= kong service</span><br><span class="line">After=syslog.target network.target postgresql-9.6.target</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">User=work</span><br><span class="line">Group=work</span><br><span class="line">Type=forking</span><br><span class="line">ExecStart=/usr/local/bin/kong start -c /etc/kong/kong.conf</span><br><span class="line">ExecReload=/usr/local/bin/kong reload -c /etc/kong/kong.conf</span><br><span class="line">ExecStop=/usr/local/bin/kong stop</span><br><span class="line">Restart=always</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><ul><li>konga</li></ul><p>在/usr/lib/systemd/system路径下编辑konga.service文件.内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[work@dwd-kong-node1 system]$ cat konga.service</span><br><span class="line">[Unit]</span><br><span class="line">Description= konga service</span><br><span class="line">After=syslog.target network.target postgresql-9.6.target  kong.target</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">User=work</span><br><span class="line">Group=work</span><br><span class="line">Type=forking</span><br><span class="line">#需要指定工作目录,因为npm命令要在konga的目录下执行</span><br><span class="line">WorkingDirectory=/home/work/konga</span><br><span class="line">ExecStart=/usr/local/bin/npm start</span><br><span class="line">ExecStop=kill $(netstat -tlnp |grep 1337|  awk &apos;&#123;print $NF&#125;&apos; | awk -F &quot;/&quot; &apos;&#123;print $1&#125;&apos;)</span><br><span class="line">Restart=always</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><ul><li>启动进程,且设置开机启动</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[work@dwd-kong-node2 ~]$ sudo systemctl enable kong</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/kong.service to /usr/lib/systemd/system/kong.service.</span><br><span class="line">[work@dwd-kong-node2 ~]$ sudo systemctl enable konga.service</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/konga.service to /usr/lib/systemd/system/konga.service.</span><br><span class="line">[work@dwd-kong-node2 ~]$ sudo systemctl start kong.service</span><br><span class="line">[work@dwd-kong-node2 ~]$ sudo systemctl start konga.service</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kong-postgresql-konga集群环境部署&quot;&gt;&lt;a href=&quot;#kong-postgresql-konga集群环境部署&quot; class=&quot;headerlink&quot; title=&quot;kong+postgresql+konga集群环境部署&quot;&gt;&lt;/a&gt;kong+postgresql+konga集群环境部署&lt;/h2&gt;&lt;h3 id=&quot;kong简介&quot;&gt;&lt;a href=&quot;#kong简介&quot; class=&quot;headerlink&quot; title=&quot;kong简介&quot;&gt;&lt;/a&gt;kong简介&lt;/h3&gt;&lt;p&gt;Kong是Mashape开源的一款API网关，起初是用来管理 Mashape 公司15000个微服务的，后来在2015年开源,现在已经在很多创业公司、大型企业和政府机构中广泛使用。基于nginx,Lua和Cassandra或PostgreSQL，支持分布式操作，有很强的可移植性和可扩展性。可以在任何一种基础设施上运行,作为应用和API之间的中间层，加上众多功能强大的插件，可以实现认证授权、访问控制等功能。并且提供易于使用的RESTful API来操作和配置系统。&lt;/p&gt;
&lt;p&gt;有关kong的详细介绍请参考官网.&lt;/p&gt;
&lt;p&gt;–&lt;/p&gt;
&lt;h3 id=&quot;postgreSQL简介&quot;&gt;&lt;a href=&quot;#postgreSQL简介&quot; class=&quot;headerlink&quot; title=&quot;postgreSQL简介&quot;&gt;&lt;/a&gt;postgreSQL简介&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://baike.baidu.com/item/PostgreSQL/530240&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;PostgreSQL&lt;/a&gt; 是一个免费的对象-关系数据库服务器(数据库管理系统)，它在灵活的 BSD-风格许可证下发行。它提供了相对其他开放源代码数据库系统(比如 MySQL 和 Firebird)，和专有系统(比如 Oracle、Sybase、IBM 的 DB2 和 Microsoft SQL Server)之外的另一种选择。&lt;/p&gt;
&lt;p&gt;–&lt;/p&gt;
&lt;h3 id=&quot;集群架构&quot;&gt;&lt;a href=&quot;#集群架构&quot; class=&quot;headerlink&quot; title=&quot;集群架构&quot;&gt;&lt;/a&gt;集群架构&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;kong cluster&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;kong 集群并不意味着客户端请求将会负载均衡到kong集群中的每个节点上，kong集群并不是开箱即用，仍然需要在kong集群多节点上层搭建负载均衡，以便分发请求。 一个kong集群只是意味着集群内的节点，都共享同样的配置。&lt;/p&gt;
&lt;p&gt;有关Kong cluster集群的详细介绍请参考官网:&lt;a href=&quot;https://docs.konghq.com/0.14.x/clustering/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kong cluser document&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;为了提高冗余性和健壮性.我们对kong的每个环节都进行了冗余设计.一个基本的kong集群架构大概如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img1.jesse.top/kong-flow.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes volume</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/kubernetes%20volume%E5%AD%98%E5%82%A8%E5%8D%B7/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/kubernetes volume存储卷/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T10:05:07.431Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kubernetes-volume"><a href="#kubernetes-volume" class="headerlink" title="kubernetes volume"></a>kubernetes volume</h2><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在学习docker时,学习了如何将宿主机上的文件系统挂载到容器当中,实现持久化存储,以及多个容器共享同一个文件目录.</p><p>在k8s中,我们也希望pod容器中的数据能够持久化存储.当一个容器因为故障或者其他原因需要删除时,我们希望新的容器能够在上一个容器结束的位置继续运行.</p><p>但是和docker一样,容器默认情况下删除后,所有该容器产生的数据都会消失.并且,虽然容器和宿主机共享CPU,网络,内存等资源,但是并不会共享文件存储.甚至同一个Pod中每个容器都有自己独立的文件系统,彼此互相隔离.</p><p>和docker容器一样,这就需要有一种存储卷能够挂载到Pod容器中,并脱离Pod的生命周期之外,将容器运行产生的数据保存在宿主机或者独立的外部存储卷中</p><a id="more"></a><hr><h2 id="存储卷类型"><a href="#存储卷类型" class="headerlink" title="存储卷类型"></a>存储卷类型</h2><ul><li>emptyDir——-用于存储容器临时数据的空目录</li></ul><p>emptyDir类型的存储卷和pod容器的生命周期相关联当pod容器删除时,emtpyDir卷的数据就会丢失</p><ul><li>hostPath——–和docker类似,将宿主机的某个目录挂载到pod容器.</li></ul><p>但是本地节点运行的Pod容器和其他节点服务器上的同一组pod容器之间无法共享数据.只能本地持久化,无法实现集群存储</p><ul><li>nfs,iscsi,FC SAN——-网络存储</li></ul><p>网络存储设备,挂载一个独立的第三方存储设备</p><ul><li><p>AWS EBK,Azure Disk等—— 云厂商虚拟存储</p></li><li><p>cephfs,glusterfs等—– 分布式集群存储</p></li></ul><blockquote><p>使用<figure class="highlight plain"><figcaption><span>explain pods.spec.volumes```命令可以查看k8s支持的许多存储卷类型</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">----</span><br><span class="line"></span><br><span class="line">## 一. emptyDir卷</span><br><span class="line"></span><br><span class="line">为了演示emtpyDir卷,现在创建2个简单的pod容器:</span><br><span class="line"></span><br><span class="line">nginx-alpine容器.</span><br><span class="line"></span><br><span class="line">fortune容器定期输出字符串到/var/hotdocs/index.html文件下.然后Nginx容器展示.</span><br><span class="line"></span><br><span class="line">这2个容器需要共享同一个目录.</span><br></pre></td></tr></table></figure></p></blockquote><p>#furtune容器镜像主要是运行一个while循环脚本,该脚本每隔10秒随机想/var/htdocs/index.html文件写入一段名人名言</p><p>#!/bin/bash<br>trap ”exit” SIGINT<br>mkdir /var/htdocs<br>while :<br>do<br>echo $(date) Writing fortune to /var/htdocs/index.html<br>/usr/games/fortune &gt; /var/htdocs/index.html<br>sleep 10<br>done</p><p>然后编译Dockerfile的文件，其中包含以下内容:</p><p>FROM ubuntu:latest<br>RUN apt-get update;apt-get -y install fortune<br>ADD fortuneloop.sh /bin/fortuneloop.sh<br>ENTRYPOINT /bin/fortuneloop.sh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">使用```kubectl explain pod.volumes```命令可以查看k8s支持哪些存储卷类型</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">在pod定义的sepc.containers.volumeMounts字段中详细介绍了挂载磁盘卷的配置参数.</span><br><span class="line"></span><br><span class="line">使用命令 ``` kubectl explian pod.spec.containers.volumeMounts```可以看到支持以下主要挂载参数</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl explain pod.spec.containers.volumeMounts<br>KIND:     Pod<br>VERSION:  v1  #pod对象属于哪个版本</p><p>#volumeMounts参数的值是一个列表对象<br>RESOURCE: volumeMounts &lt;[]Object&gt;</p><p>FIELDS:</p><p>#挂载路径.必要字段<br>   mountPath    <string> -required-<br>     Path within the container at which the volume should be mounted. Must not<br>     contain ‘:’.</string></p><p>#挂载的volume卷名,必要字段<br>   name    <string> -required-<br>     This must match the Name of a Volume.</string></p><p>#是否只读<br>   readOnly    <boolean><br>     Mounted read-only if true, read-write otherwise (false or unspecified).<br>     Defaults to false.</boolean></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">为了演示实验是否正常,同时复习之前学到的replicaSet和Service对象,我们来创建一个rs,和svc对象</span><br><span class="line"></span><br><span class="line">####  1. 创建replicaSet对象.包含一个副本,以及上面配置的2个pod容器</span><br></pre></td></tr></table></figure><p>apiVersion: apps/v1<br>kind: ReplicaSet<br>metadata:<br>    name: fortune-rs</p><p>spec:<br>   replicas: 1<br>   selector:<br>       matchLabels:<br>           app: fortune</p><p>   template:<br>      metadata:<br>         name: fortune<br>         labels:<br>           app: fortune<br>      spec:<br>         containers:</p><pre><code>    - image: luksa/fortune      name: html-generator      #磁盘卷挂载配置      volumeMounts:           - name: html  #卷名             mountPath: /var/htdocs #挂载在容器的哪个目录下    - image: nginx:alpine      name: web-servier      volumeMounts:          - name: html            mountPath: /usr/share/nginx/html            readOnly: true      ports:          - name: http            containerPort: 80#磁盘卷定义volumes:#卷名   - name: html     #卷类型     emptyDir: {}</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pod包含2个容器,他们挂载同一个公用的存储卷,当html-generator容器启动时,每10秒启动一次fortune命令输出到/var/htdocs/index.html文件中,因为卷同时也被web-server容器挂载,所以后者能访问到html-generator容器生成的数据</span><br><span class="line"></span><br><span class="line">如果要将emptyDir卷存储在内存上而非磁盘,可以声明medium配置如下配置:</span><br></pre></td></tr></table></figure><p> volumes:</p><pre><code>#卷名   - name: html     #卷类型     emptyDir:         #类型        medium: Memory</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 2.为此ReplicaSet创建一个Service对象.内容如下</span><br></pre></td></tr></table></figure><p>apiVersion: v1<br>kind: Service<br>metadata:<br>   name: fortune-svc</p><p>spec:<br>   selector:<br>     app: fortune</p><p>   ports:</p><pre><code>- port: 80  targetPort: 80</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 3.分别创建svc和rs对象</span><br></pre></td></tr></table></figure><p>kubectl create -f fortune-rs.yaml</p><p>kubectl create -f fortune-svc.yaml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 4.查看资源</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl get rs<br>NAME         DESIRED   CURRENT   READY   AGE<br>fortune-rs   1         1         1       11h</p><p>[root@k8s-master ~]# kubectl get svc<br>NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE<br>fortune-svc          ClusterIP      10.96.123.110   <none>        80/TCP         7m9s</none></p><p>[root@k8s-master ~]# kubectl get pods<br>NAME                READY   STATUS    RESTARTS   AGE<br>fortune-rs-4df5q    2/2     Running   0          11m<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 5.访问Service</span><br></pre></td></tr></table></figure></p><p>[work@k8s-node1 ~]$ curl <a href="http://10.96.123.110" target="_blank" rel="noopener">http://10.96.123.110</a><br>You never have to change anything you got up in the middle of the night<br>to write.<br>        – Saul Bellow<br>[work@k8s-node1 ~]$ curl <a href="http://10.96.123.110" target="_blank" rel="noopener">http://10.96.123.110</a><br>Q:    How much does it cost to ride the Unibus?<br>A:    2 bits.<br>[work@k8s-node1 ~]$ curl <a href="http://10.96.123.110" target="_blank" rel="noopener">http://10.96.123.110</a><br>While you recently had your problems on the run, they’ve regrouped and<br>are making another attack.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">可以看到,每10秒钟访问的内容不一样</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## 二 .hostPath卷</span><br><span class="line"></span><br><span class="line">hostPath卷可以实现持久性存储,如果删除了一个Pod,并且新的pod使用了相同的主机路径的hostPath卷,则新pod会发现上一个pod留下来的数据,但是前提是必须和前一个pod是同一个节点</span><br><span class="line"></span><br><span class="line">这也解释了为什么使用hostPath卷不是一个好主意,因为当Pod被调度到另外一个节点上时,会找不到数据.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 三.NFS卷</span><br><span class="line"></span><br><span class="line">NFS卷可以实现多个节点之间共享存储.但是生产中不建议这样做,因为NFS共享存储传输效率低,稳定性和安全性不高.</span><br><span class="line"></span><br><span class="line">仍然使用上面的例子</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# cp fortune-svc.yaml fortune-nfs-svc.yaml<br>[root@k8s-master ~]# cp fortune-rs.yaml fortune-nfs-rs.yaml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">修改各资源的标签,和rs配置文件中的卷信息:</span><br></pre></td></tr></table></figure></p><p>apiVersion: apps/v1<br>kind: ReplicaSet<br>metadata:<br>    name: fortune-nfs-rs</p><p>spec:<br>   replicas: 1<br>   selector:<br>       matchLabels:<br>           app: fortune-nfs</p><p>   template:<br>      metadata:<br>         name: fortune-nfs<br>         labels:<br>           app: fortune-nfs<br>      spec:<br>         containers:</p><pre><code>    - image: luksa/fortune      name: html-generator      #磁盘卷挂载配置      volumeMounts:           - name: html  #卷名             mountPath: /var/htdocs #挂载在容器的哪个目录下    - image: nginx:alpine      name: web-servier      volumeMounts:          - name: html            mountPath: /usr/share/nginx/html            readOnly: true      ports:          - name: http            containerPort: 80#磁盘卷定义volumes:#卷名   - name: html     #卷类型     nfs:   #NFS服务器地址      server: 172.16.20.2   #NFS服务端共享目录      path: /data/apps/k8s</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">启动rs后,在nfs服务器上已经看到pod写入的数据:</span><br></pre></td></tr></table></figure><p>[work@idc-beta-cron ~]$ cat /data/apps/k8s/index.html<br>Tonight’s the night: Sleep in a eucalyptus tree.</p><p>[work@idc-beta-cron ~]$ cat /data/apps/k8s/index.html<br>Fortune: You will be attacked next Wednesday at 3:15 p.m. by six samurai<br>sword wielding purple fish glued to Harley-Davidson motorcycles.</p><p>Oh, and have a nice day!<br>        – Bryce Nesbitt ‘84</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"></span><br><span class="line">## PV和PVC</span><br><span class="line"></span><br><span class="line">PV和PVC用于kubenetes将存储和pod解耦,存储管理人员配置好PV,然后开发人员只需要声明需要多大的存储卷(pvc)即可,无需关系底层的存储实现方式.</span><br><span class="line"></span><br><span class="line">**实现逻辑大致如下:**</span><br><span class="line"></span><br><span class="line">1.存储管理员配置底层存储方案(NFS,SAN,FC SAN,ceph等)</span><br><span class="line"></span><br><span class="line">2.k8s集群管理员创建一个PV(Psersistent volume,持久卷)</span><br><span class="line"></span><br><span class="line">3.开发人员(或者用户)创建一个PVC(Persistent volume claim,持久卷声明),将PVC和PV绑定</span><br><span class="line"></span><br><span class="line">4.开发人员(或者用户)在pod中引用PVC</span><br><span class="line"></span><br><span class="line">下面用NFS底层存储来演示PV和PVC的工作过程</span><br><span class="line"></span><br><span class="line">#### 1.创建PV</span><br></pre></td></tr></table></figure><p>apiVersion: v1<br>kind: PersistentVolume<br>metadata:<br>  name: mypv1</p><p>spec:<br>   capacity:<br>      storage: 1Gi   #定义PV卷的大小</p><p>   accessModes:</p><pre><code>- ReadWriteOnce   #PV可以读写模式被挂载到单个节点- ReadOnlyMany    #PV以只读模式被挂载到多个节点</code></pre><p>   persistentVolumeReclaimPolicy: Retain  #PV的回收策略.Retain表示PVC释放后,PV会继续保留<br>   storageClassName: nfs   #指定nfs类型<br>   nfs:<br>     path: /data/k8s<br>     server: 172.16.20.1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* accessModes支持ReadWriteOnce ,ReadOnlyMany ,ReadWriteMany </span><br><span class="line"></span><br><span class="line">可以直接使用RWO,ROM,RWM缩写.</span><br><span class="line"></span><br><span class="line">* PersistentVolumeReclaimPolicy回收策略支持:</span><br><span class="line">  - Retain:  PV一直保留,直到管理员手动回收</span><br><span class="line">  - Recycle: 清除PV中的数据</span><br><span class="line">  - Delete:  清除存储上的资源</span><br><span class="line"></span><br><span class="line">* storageClassName: PV的类型</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl create -f nfs-pv.yaml<br>persistentvolume/mypv1 created<br>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Retain           Available           nfs                     5s<br>[root@k8s-master ~]#</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 2.持久卷声明(PVC)</span><br></pre></td></tr></table></figure><p>apiVersion: v1<br>kind: PersistentVolumeClaim<br>metadata:<br>  name: mypvc1</p><p>spec:<br>  accessModes: #访问模式</p><pre><code>- ReadWriteOnce</code></pre><p>  resources: #定义需要的存储空间大小<br>     requests:<br>       storage: 1Gi<br>  storageClassName: nfs #存储类型<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">创建PVC</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl create -f nfs-pvc.yaml<br>persistentvolumeclaim/mypvc1 created<br>[root@k8s-master ~]# kubectl get pvc<br>NAME     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE<br>mypvc1   Bound    mypv1    1Gi        RWO,ROX        nfs            10s<br>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Retain           Bound    default/mypvc1   nfs                     22m<br>[root@k8s-master ~]#<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">default/mypvc1 表示default命名空间的PVC</span><br><span class="line"></span><br><span class="line">Status的Bound表示PV和PVC已经成功绑定</span><br><span class="line"></span><br><span class="line">&gt; 持久卷PV不属于任何名称空间,但是PVC和pod有名称空间概念.PV可以被所有名称空间下的PVC绑定.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 3.创建pod.在pod中调用刚才创建的PVC</span><br></pre></td></tr></table></figure></p><p>apiVersion: apps/v1<br>kind: ReplicaSet<br>metadata:<br>    name: fortune-nfs-pvc-rs</p><p>spec:<br>   replicas: 1<br>   selector:<br>       matchLabels:<br>           app: fortune-pvc-nfs</p><p>   template:<br>      metadata:<br>         name: fortune-pvc-nfs<br>         labels:<br>           app: fortune-pvc-nfs<br>      spec:<br>         containers:</p><pre><code>    - image: luksa/fortune      name: html-generator      #磁盘卷挂载配置      volumeMounts:           - name: html  #卷名             mountPath: /var/htdocs #挂载在容器的哪个目录下    - image: nginx:alpine      name: web-servier      volumeMounts:          - name: html            mountPath: /usr/share/nginx/html            readOnly: true      ports:          - name: http            containerPort: 80#磁盘卷定义volumes:#卷名   - name: html     #卷类型     persistentVolumeClaim:        claimName: mypvc1</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">和empty dir以及Hostpath类似,在引用pvc的时候只是修改一下卷类型.引用mypvc1这个刚才创建的pvc</span><br><span class="line"></span><br><span class="line">创建pod</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl create -f fortune-nfs-pvc.yaml<br>replicaset.apps/fortune-nfs-pvc-rs created</p><p>[root@k8s-master ~]# kubectl get pods<br>NAME                       READY   STATUS    RESTARTS   AGE<br>fortune-nfs-pvc-rs-zlnk2   2/2     Running   0          26s<br>kubia-5452q                0/1     Running   0          5d19h<br>kubia-6mghh                0/1     Running   0          5d19h<br>kubia-nl6rd                0/1     Running   0          5d19h<br>kubia-pvs5z                0/1     Running   0          5d19h<br>ssd-monitor-5vxbn          1/1     Running   0          5d19h<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#### 4.验证</span><br><span class="line"></span><br><span class="line">在nfs服务器上的共享目录/data/k8s上确实看到了Pod容器写入的数据</span><br></pre></td></tr></table></figure></p><p>[work@idc-beta-docker ~]$ ll /data/k8s/<br>total 4<br>-rw-r–r– 1 root root 276 Apr 19 10:24 index.html<br>[work@idc-beta-docker ~]$ cat /data/k8s/index.html<br>Mind!  I don’t mean to say that I know, of my own knowledge, what there is<br>particularly dead about a door-nail.  I might have been inclined, myself,<br>to regard a coffin-nail as the deadest piece of ironmongery in the trade.<br>But the wisdom of our ancestors is in the simile; and my unhallowed hands<br>shall not disturb it, or the Country’s done for.  You will therefore permit<br>me to repeat, emphatically, that Marley was as dead as a door-nail.<br>        – Charles Dickens, “A Christmas Carol”<br>[work@idc-beta-docker ~]$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">-------</span><br><span class="line"></span><br><span class="line">## pv的手动回收</span><br><span class="line"></span><br><span class="line">手动删除Pod,pvc</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl delete pvc mypvc1<br>persistentvolumeclaim “mypvc1” deleted</p><p>[root@k8s-master ~]# kubectl get pvc<br>No resources found in default namespace.</p><p>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM            STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Retain           Released   default/mypvc1   nfs                     11h<br>[root@k8s-master ~]#<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">删除pvc后,pv是Released状态,不可绑定PVC.删除PV,然后重新创建.</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl delete pv mypv1<br>persistentvolume “mypv1” deleted</p><p>[work@idc-beta-docker ~]$ ll /data/k8s/<br>total 4<br>-rw-r–r– 1 root root 100 Apr 19 10:38 index.html</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">删除PV卷,并不会删除存储卷中的数据</span><br><span class="line"></span><br><span class="line">重新创建PV.存储卷中的数据仍然存在,并且PV的状态为Available</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl create -f nfs-pv.yaml<br>persistentvolume/mypv1 created</p><p>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Retain           Available           nfs                     6s</p><p>[work@idc-beta-docker ~]$ ll /data/k8s/<br>total 4<br>-rw-r–r– 1 root root 100 Apr 19 10:38 index.html<br>[work@idc-beta-docker ~]$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"></span><br><span class="line">## PV自动回收</span><br><span class="line"></span><br><span class="line">在pv的配置中修改如下字段</span><br></pre></td></tr></table></figure><p>persistentVolumeReclaimPolicy: Recycle<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">创建PV.Reclaim policy是Recycle</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl create -f nfs-pv.yaml<br>persistentvolume/mypv1 created<br>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Recycle          Available           nfs                     3s<br>[root@k8s-master ~]#</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">创建PVC,绑定到PV,并且创建pod</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl create -f nfs-pvc.yaml<br>persistentvolumeclaim/mypvc1 created</p><p>[root@k8s-master ~]# kubectl create -f fortune-nfs-pvc.yaml<br>replicaset.apps/fortune-nfs-pvc-rs created</p><p>[root@k8s-master ~]# kubectl get pvc<br>NAME     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE<br>mypvc1   Bound    mypv1    1Gi        RWO,ROX        nfs            26s</p><p>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Recycle          Bound    default/mypvc1   nfs                     101s</p><p>[root@k8s-master ~]# kubectl get pods<br>NAME                       READY   STATUS    RESTARTS   AGE<br>fortune-nfs-pvc-rs-9p2xz   2/2     Running   0          16s</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">nfs底层存储卷上数据已经更新</span><br></pre></td></tr></table></figure><p>[work@idc-beta-docker ~]$ cat /data/k8s/index.html<br>Alimony and bribes will engage a large share of your wealth.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 删除Pod,pvc</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl delete pvc mypvc1<br>persistentvolumeclaim “mypvc1” deleted</p><p>[root@k8s-master ~]# kubectl get pvc<br>No resources found in default namespace.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">再次查看PV发现状态又变成了Available</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM            STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Recycle          Released   default/mypvc1   nfs                     5m19s<br>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Recycle          Available           nfs                     5m21s<br><code>`</code></p><blockquote><p>NFS不支持delete回收策略,所以就不演示</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kubernetes-volume&quot;&gt;&lt;a href=&quot;#kubernetes-volume&quot; class=&quot;headerlink&quot; title=&quot;kubernetes volume&quot;&gt;&lt;/a&gt;kubernetes volume&lt;/h2&gt;&lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;在学习docker时,学习了如何将宿主机上的文件系统挂载到容器当中,实现持久化存储,以及多个容器共享同一个文件目录.&lt;/p&gt;
&lt;p&gt;在k8s中,我们也希望pod容器中的数据能够持久化存储.当一个容器因为故障或者其他原因需要删除时,我们希望新的容器能够在上一个容器结束的位置继续运行.&lt;/p&gt;
&lt;p&gt;但是和docker一样,容器默认情况下删除后,所有该容器产生的数据都会消失.并且,虽然容器和宿主机共享CPU,网络,内存等资源,但是并不会共享文件存储.甚至同一个Pod中每个容器都有自己独立的文件系统,彼此互相隔离.&lt;/p&gt;
&lt;p&gt;和docker容器一样,这就需要有一种存储卷能够挂载到Pod容器中,并脱离Pod的生命周期之外,将容器运行产生的数据保存在宿主机或者独立的外部存储卷中&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes headless Service</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/kubernetes%20headless%20Service/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/kubernetes headless Service/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T08:48:46.557Z</updated>
    
    <content type="html"><![CDATA[<hr><h3 id="kubernetes-headless-Service"><a href="#kubernetes-headless-Service" class="headerlink" title="kubernetes headless Service"></a>kubernetes headless Service</h3><p>​     我们以前学习过,Service是Kubernetes项目中用来将一组Pod暴露给外界访问的一种机制.外部客户端通过Service地址可以随机访问到某个具体的Pod</p><p>​     之前学过几种Service类型,包括nodeport,loadbalancer等等.所有这类Service都有一个VIP(虚拟IP),访问Service VIP,Service会将请求转发到后端的Pod上,</p><p>​     还有一种Service是Headless Service(无头服务),这类Service自身不需要VIP,当DNS解析该Service时,会解析出Service后端的Pod地址.这样设置的好处是Kubernetes项目为Pod分配唯一的”可解析身份”,只要知道一个pod的名字和对应的Headless Service名字,就可以通过这条DNS访问到后端的Pod</p><a id="more"></a><hr><h3 id="持久存储"><a href="#持久存储" class="headerlink" title="持久存储"></a>持久存储</h3><p>我们知道通过headless Service使Pod有一个稳定的网络标识,那么存储呢?有状态的应用必须有自己独立的存储,即便这个pod被删除,新创建出来的pod(新pod与旧pod拥有相同的网络表示)也必须挂载相同的存储.</p><p>​      之前在学习kubernetes的存储时,我们学习过PV,PVC存储卷,通过pod模板关联一个持久卷声明就可以为pod提供一个持久卷存储.因为持久卷声明(PVC)和持久卷(PV)是一对一关系.但是之前接触过的ReplicationController,ReplicaSet,Deployment等资源创建的pod是同一个模板创建的,所以共享的是同一个持久卷存储.而StatefulSet要求每个pod都需要有独立的持久卷声明和存储.所以StatefulSet要求关联到一个或多个不同的持久卷声明模板.这些持久卷声明会在pod创建之前准备就绪,并且关联到每个pod中.</p><h3 id="持久卷的创建和删除"><a href="#持久卷的创建和删除" class="headerlink" title="持久卷的创建和删除"></a>持久卷的创建和删除</h3><p>​      扩容一个StatefulSet副本时,会创建2个或者多个对象: pod实例已经与之关联的一个或者多个持久卷声明.但是当StatefulSet缩容时,只会删除一个Pod,而留下持久卷声明.这就意味着删除Pod时,与pod关联的持久卷存储数据并不会被删除.如果持久卷声明被手动删除,那么持久卷上的数据则会消失.</p><p>​     因为缩容会保留持久卷声明,所以在随后的扩容操作中,新的pod实例会使用绑定在持久卷上相同的声明和其上的数据.所以如果因为误操作而缩容一个StatefulSet副本后,可以做一次扩容操作,新的pod实例会运行到与之前完全一致的状态,甚至连pod名字也是一样的</p><hr><h3 id="部署StatefulSet应用"><a href="#部署StatefulSet应用" class="headerlink" title="部署StatefulSet应用"></a>部署StatefulSet应用</h3><p>部署StatefulSet应用之前,需要创建几个不同类型的对象.</p><ul><li><p>一个演示用的docker镜像</p></li><li><p>存储数据文件的持久卷(PV)</p></li><li><p>一个Headless Service服务实例</p></li><li><p>Statefulset模板</p></li></ul><h4 id="准备一个docker镜像"><a href="#准备一个docker镜像" class="headerlink" title="准备一个docker镜像"></a>准备一个docker镜像</h4><p>这里使用书上提供的luksa/kubia-pet镜像,这个镜像是一个Node应用,当应用接收到一个POST请求时,将请求中的body写入到某个文件,当接收到一个GET请求时,返回pod主机名以及改文件中的内容.</p><h4 id="创建持久化存储卷-pv"><a href="#创建持久化存储卷-pv" class="headerlink" title="创建持久化存储卷(pv)"></a>创建持久化存储卷(pv)</h4><p>因为稍后会调度StatefulSet创建3个副本.所以这里需要3个持久卷.如果计划调度更多的副本,则需要创建更多的持久卷..</p><p>之前在学习存储知识的时候介绍过存储卷,所以具体不演示,以下是创建3个PV持久卷的配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-master</span> <span class="string">~]#</span> <span class="string">cat</span> <span class="string">statefulset-kubia-pv.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="comment">#创建一个List列表资源,List的items下列出每个PV的配置</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">List</span></span><br><span class="line"><span class="attr">items:</span></span><br><span class="line"><span class="attr">  - apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">    kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">pv-1</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">       capacity:</span></span><br><span class="line"><span class="attr">         storage:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">       accessModes:</span></span><br><span class="line"><span class="bullet">         -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">       persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span></span><br><span class="line"><span class="attr">       storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">       nfs:</span></span><br><span class="line"><span class="attr">         path:</span> <span class="string">/data/k8s/pv-1</span></span><br><span class="line"><span class="attr">         server:</span> <span class="number">172.16</span><span class="number">.20</span><span class="number">.1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  - apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">    kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">pv-2</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">       capacity:</span></span><br><span class="line"><span class="attr">         storage:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">       accessModes:</span></span><br><span class="line"><span class="bullet">         -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">       persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span></span><br><span class="line"><span class="attr">       storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">       nfs:</span></span><br><span class="line"><span class="attr">         path:</span> <span class="string">/data/k8s/pv-2</span></span><br><span class="line"><span class="attr">         server:</span> <span class="number">172.16</span><span class="number">.20</span><span class="number">.1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  - apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">    kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">pv-3</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">       capacity:</span></span><br><span class="line"><span class="attr">         storage:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">       accessModes:</span></span><br><span class="line"><span class="bullet">         -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">       persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span></span><br><span class="line"><span class="attr">       storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">       nfs:</span></span><br><span class="line"><span class="attr">         path:</span> <span class="string">/data/k8s/pv-3</span></span><br><span class="line"><span class="attr">         server:</span> <span class="number">172.16</span><span class="number">.20</span><span class="number">.1</span></span><br></pre></td></tr></table></figure><blockquote><p>以前接触过在yaml文件中添加—3个横杠使的在一个文件中可以区分定义多个资源,这次定义一个List对象,然后把各个资源作为List对象的各个项目.这2种方法均可以在一个YAML文件中定义多个资源</p></blockquote><p>现在已经定义了个3个底层的PV持久卷</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get pv</span><br><span class="line">NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE</span><br><span class="line">mypv1   1Gi        RWO,ROX        Recycle          Available           nfs                     12d</span><br><span class="line">pv-1    1Gi        RWO            Recycle          Available           nfs                     4s</span><br><span class="line">pv-2    1Gi        RWO            Recycle          Available           nfs                     4s</span><br><span class="line">pv-3    1Gi        RWO            Recycle          Available           nfs                     4s</span><br></pre></td></tr></table></figure><hr><h3 id="创建Headless-Service"><a href="#创建Headless-Service" class="headerlink" title="创建Headless Service"></a>创建Headless Service</h3><p>下面是headless service的配置文件,唯一需要注意的是该类型服务的clusterIP属性必须为None</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">statefulset-kubia-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  clusterIP:</span> <span class="string">None</span></span><br><span class="line">  <span class="comment">#所有标签为statefulset-kubia的Pod都属于这个Service</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">statefulset-kubia</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">      port:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p>创建服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get svc</span><br><span class="line">NAME                    TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">statefulset-kubia-svc   ClusterIP      None            &lt;none&gt;        80/TCP         3s</span><br></pre></td></tr></table></figure><hr><h3 id="创建Statefuleset"><a href="#创建Statefuleset" class="headerlink" title="创建Statefuleset"></a>创建Statefuleset</h3><p>​      statefulset资源的配置和RS,deployment等没有太大的区别,这里使用了一个新的组件volumeClaimTemplates.其中定义了一个持久卷声明.该组件会为每个Pod创建一个独立的持久卷声明.</p><p>​      这个组件是在statefulset资源的spec全局对象下,虽然在pod的template模板中并没有创建持久卷声明(而是直接通过volumeMounts属性来挂在).但是Statefulset在创建时,会自动将volumeClaimTemplate定义的持久卷声明关联到pod中.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-master</span> <span class="string">~]#</span> <span class="string">cat</span> <span class="string">statefulset-kubia.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">statefulset-kubia-v1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  serviceName:</span> <span class="string">statefulset-kubia-v1</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">       app:</span> <span class="string">statefulset-kubia</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">statefulset-kubia</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">statefulset-kubia</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">luksa/kubia-pet</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">            containerPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">data</span></span><br><span class="line"><span class="attr">              mountPath:</span> <span class="string">/var/data</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  volumeClaimTemplates:</span></span><br><span class="line"><span class="attr">       - metadata:</span></span><br><span class="line"><span class="attr">           name:</span> <span class="string">data</span></span><br><span class="line"><span class="attr">         spec:</span></span><br><span class="line"><span class="attr">           resources:</span></span><br><span class="line"><span class="attr">              requests:</span></span><br><span class="line"><span class="attr">                 storage:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">           storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">           accessModes:</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">ReadWriteOnce</span></span><br></pre></td></tr></table></figure><blockquote><p>注意:volumeClaimTemplates组件一定要声明存储类型storageClassName,如果没有声明这一点则Pod一直处于Pending状态.并且会有以下报错信息</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl describe po statefulset-kubia-v1-0</span><br><span class="line"></span><br><span class="line">  Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  error while running &quot;VolumeBinding&quot; filter plugin for pod &quot;statefulset-kubia-v1-0&quot;: pod has unbound immediate PersistentVolumeClaims</span><br></pre></td></tr></table></figure><p>查看PVC提示没有找到PV</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl describe pvc data-statefulset-kubia-v1-0</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason         Age                  From                         Message</span><br><span class="line">  ----    ------         ----                 ----                         -------</span><br><span class="line">  Normal  FailedBinding  55s (x182 over 45m)  persistentvolume-controller  no persistent volumes available for this claim and no storage class is set</span><br></pre></td></tr></table></figure><p>   创建statefulset资源,列出pod资源.和rs,rc,deployment不同的是,他们会一次性创建完所有的pod,而statefulset会在每一个pod完全就绪后,才会创建第二个.</p><p>​    statefulset这样做是因为:状态明确的集群应用对同事有2个集群成员启动引起的竞争情况是非常敏感的.所以依次启动每个成员是比较安全可靠的.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-master</span> <span class="string">~]#</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">pods</span></span><br><span class="line"><span class="string">NAME</span>                     <span class="string">READY</span>   <span class="string">STATUS</span>    <span class="string">RESTARTS</span>   <span class="string">AGE</span></span><br><span class="line"><span class="string">statefulset-kubia-v1-0</span>   <span class="number">0</span><span class="string">/1</span>     <span class="string">Pending</span>   <span class="number">0</span>          <span class="number">74</span><span class="string">s</span></span><br></pre></td></tr></table></figure><hr><p>现在3个Pod副本都已经被创建完成.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get pods</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Running   0          12m</span><br><span class="line">statefulset-kubia-v1-1   1/1     Running   0          12m</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running   0          12m</span><br></pre></td></tr></table></figure><p>statefulset自动创建了3个PVC,并且各自与3个pv自动关联</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pvc</span></span><br><span class="line">NAME                          STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">data-statefulset-kubia-v1-0   Bound    pv-2     1Gi        RWO            nfs            11m</span><br><span class="line">data-statefulset-kubia-v1-1   Bound    pv-3     1Gi        RWO            nfs            11m</span><br><span class="line">data-statefulset-kubia-v1-2   Bound    pv-1     1Gi        RWO            nfs            11m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pv</span></span><br><span class="line">NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                 STORAGECLASS   REASON   AGE</span><br><span class="line">pv-1   1Gi        RWO            Recycle          Bound    default/data-statefulset-kubia-v1-2   nfs                     5h18m</span><br><span class="line">pv-2   1Gi        RWO            Recycle          Bound    default/data-statefulset-kubia-v1-0   nfs                     5h18m</span><br><span class="line">pv-3   1Gi        RWO            Recycle          Bound    default/data-statefulset-kubia-v1-1   nfs                     5h18m</span><br><span class="line">[root@k8s-master ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>​       和RS,RC,Deployment等资源不同的是,Statefulset部署的pod名称并非是随机的,而是pod模板名加上一个序号,这个序号从0开始,依次增加.</p><p>​       PVC的名称格式是PVC的名字+pod名.每个pod自动创建一个PVC,并且该PVC自动关联到一个后端的PV持久卷</p><hr><h3 id="访问POD"><a href="#访问POD" class="headerlink" title="访问POD"></a>访问POD</h3><p>​     由于创建的Service类型是Headless service模式,所以不能通过它来访问pod,而是需要直接连接到每个后端单独的pod.(或者是创建一个普通的Service,但是这样也不允许访问指定的pod)</p><p>​    这次介绍如何通过API服务器与pod通信.API服务器可以通过代理直接连接到指定的pod.可以通过如下的URL</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;apiServerHost&gt;:&lt;port&gt;/api/v1/namespaces/default/pods/pods名称/proxy/&lt;path&gt;</span><br></pre></td></tr></table></figure><p> 在k8s的master节点运行下面命令,下面命令运行一个kubectl proxy.从而可以让proxy去API服务器通信,而不必使用麻烦的授权和SSL证书来直接与API服务器通信</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl proxy</span><br></pre></td></tr></table></figure><p>现在就可以直接访问Pod了.开启另一个master服务器终端.通过curl访问某个Pod.比如访问statefulset-kubia-v1-0这个Pod容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-0/proxy/</span><br><span class="line">You&apos;ve hit statefulset-kubia-v1-0</span><br><span class="line">Data stored on this pod: No data posted yet</span><br></pre></td></tr></table></figure><p>这种访问方式经过了2层的中间代理:</p><h5 id="1-curl命令发送给kubectl-proxy"><a href="#1-curl命令发送给kubectl-proxy" class="headerlink" title="1.curl命令发送给kubectl proxy"></a>1.curl命令发送给kubectl proxy</h5><h5 id="2-kubectl-proxy-带上认证TOKEN转发给API服务器"><a href="#2-kubectl-proxy-带上认证TOKEN转发给API服务器" class="headerlink" title="2. kubectl proxy 带上认证TOKEN转发给API服务器"></a>2. kubectl proxy 带上认证TOKEN转发给API服务器</h5><h5 id="3-API服务器再通过pod容器的实际IP地址将请求转发到后端的Pod"><a href="#3-API服务器再通过pod容器的实际IP地址将请求转发到后端的Pod" class="headerlink" title="3. API服务器再通过pod容器的实际IP地址将请求转发到后端的Pod"></a>3. API服务器再通过pod容器的实际IP地址将请求转发到后端的Pod</h5><p>下面是发送一个post请求到statefulset-kubia-v1-0的例子</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# curl -X POST -d &quot;Hey There ! This greeting was submitted to statefulset-kubia-v1-0&quot; \</span><br><span class="line">&gt; localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-0/proxy/</span><br><span class="line">Data stored on pod statefulset-kubia-v1-0</span><br><span class="line"></span><br><span class="line">#再次用GET请求,就可以返回刚才POST提交的数据</span><br><span class="line">[root@k8s-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-0/proxy/</span><br><span class="line">You&apos;ve hit statefulset-kubia-v1-0</span><br><span class="line">Data stored on this pod: Hey There ! This greeting was submitted to statefulset-kubia-v1-0</span><br></pre></td></tr></table></figure><p>当我们访问其他的pod容器时,并没有返回写入的数据,这和期望的一致,说明每个节点都有各自独立的存储状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-1/proxy/</span><br><span class="line">You&apos;ve hit statefulset-kubia-v1-1</span><br><span class="line">Data stored on this pod: No data posted yet</span><br></pre></td></tr></table></figure><hr><h3 id="删除pod-重新调度"><a href="#删除pod-重新调度" class="headerlink" title="删除pod,重新调度"></a>删除pod,重新调度</h3><p>之前我们在statefulset-kubia-v1-0这个pod节点写入了一条数据,这次我们删除这个Pod,等它被重新调度,然后检查它是否还会返回与之前一致的数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl delete po statefulset-kubia-v1-0</span><br><span class="line">pod &quot;statefulset-kubia-v1-0&quot; deleted</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl get po</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Running   0          2m</span><br><span class="line">statefulset-kubia-v1-1   1/1     Running   0          17h</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running   0          17h</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-0/proxy/</span><br><span class="line">You&apos;ve hit statefulset-kubia-v1-0</span><br><span class="line">Data stored on this pod: Hey There ! This greeting was submitted to statefulset-kubia-v1-0</span><br><span class="line">[root@k8s-master ~]#</span><br></pre></td></tr></table></figure><blockquote><p>删除一个Pod,当Pod重新被调度时不一定是原节点,有可能会调度到另外一个节点</p></blockquote><p>从上面的实验中可以得出2个结论:</p><ul><li>statefulset的pod被重新调度时,会新创建一个和之前一模一样的Pod(包括主机名称,pod名,存储)</li><li>当pod被删除,重新调度后持久化数据与之前一模一样.</li></ul><hr><h3 id="statefulSet滚动更新"><a href="#statefulSet滚动更新" class="headerlink" title="statefulSet滚动更新"></a>statefulSet滚动更新</h3><h4 id="1-7版本之前默认的On-Delete更新策略"><a href="#1-7版本之前默认的On-Delete更新策略" class="headerlink" title="1.7版本之前默认的On Delete更新策略"></a>1.7版本之前默认的On Delete更新策略</h4><p>​     statefulset在1.7版本开始支持滚动更新..在1.7版本之前默认的更新测量是<figure class="highlight plain"><figcaption><span>Delete```.这种侧列和ReplicaSet类似.当更新了配置文件后,旧的pod并不会被自动删除,而是需要手动删除.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">​    下面这个例子中,将镜像更换为luksa/kubia-pet-peers.副本数从3个增加到4个.(为此,我们需要提前再创建一个pv-4</span><br><span class="line"></span><br><span class="line">```shell</span><br><span class="line">#编辑pv配置文件,增加pv-4(前提是nfs服务器上实现存在/data/k8s/pv-4目录</span><br><span class="line">[root@k8s-master ~]# vim statefulset-kubia-pv.yaml</span><br><span class="line"></span><br><span class="line">#更新pv配置文件</span><br><span class="line">[root@k8s-master ~]# kubectl apply -f statefulset-kubia-pv.yaml</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">persistentvolume/pv-1 configured</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">persistentvolume/pv-2 configured</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">persistentvolume/pv-3 configured</span><br><span class="line">persistentvolume/pv-4 created</span><br><span class="line"></span><br><span class="line">#查看PV.</span><br><span class="line">[root@k8s-master ~]# kubectl get pv</span><br><span class="line">NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                 STORAGECLASS   REASON   AGE</span><br><span class="line">pv-1   1Gi        RWO            Recycle          Bound       default/data-statefulset-kubia-v1-2   nfs                     23h</span><br><span class="line">pv-2   1Gi        RWO            Recycle          Bound       default/data-statefulset-kubia-v1-0   nfs                     23h</span><br><span class="line">pv-3   1Gi        RWO            Recycle          Bound       default/data-statefulset-kubia-v1-1   nfs                     23h</span><br><span class="line">pv-4   1Gi        RWO            Recycle          Available                                         nfs                     9s</span><br><span class="line">[root@k8s-master ~]#</span><br></pre></td></tr></table></figure></p><p>更新statefulset配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#更新配置文件</span><br><span class="line">[root@k8s-master ~]# vim statefulset-kubia.yaml</span><br><span class="line">#应用配置文件</span><br><span class="line">[root@k8s-master ~]# kubectl apply -f statefulset-kubia.yaml</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">statefulset.apps/statefulset-kubia-v1 configured</span><br><span class="line"></span><br><span class="line">#查看Pod</span><br><span class="line">[root@k8s-master ~]# kubectl get pods</span><br><span class="line">NAME                     READY   STATUS              RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Running             0          70m</span><br><span class="line">statefulset-kubia-v1-1   1/1     Running             0          18h</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running             0          18h</span><br><span class="line">statefulset-kubia-v1-3   0/1     ContainerCreating   0          5s</span><br><span class="line">[root@k8s-master ~]#</span><br></pre></td></tr></table></figure><p>通过Pod的存活字段可以看到之前旧版本的Pod并没有被自动删除,而是新增了一个副本.这和ReplicaSet的机制类似.</p><hr><h4 id="自动滚动更新策略"><a href="#自动滚动更新策略" class="headerlink" title="自动滚动更新策略"></a>自动滚动更新策略</h4><p>编辑statefulset配置文件,将镜像版本改回到luksa/kubia-pet</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  serviceName:</span> <span class="string">statefulset-kubia-v1</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">4</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">       app:</span> <span class="string">statefulset-kubia</span></span><br><span class="line">  <span class="comment">#在spec字段配置更新策略,默认的type是On Delete,修改为RollingUpdate</span></span><br><span class="line"><span class="attr">  updateStrategy:</span></span><br><span class="line"><span class="attr">     type:</span> <span class="string">RollingUpdate</span></span><br></pre></td></tr></table></figure><p>应用新的配置文件,此时会触发自动更新</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl apply -f statefulset-kubia.yaml</span></span><br><span class="line">statefulset.apps/statefulset-kubia-v1 configured</span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods</span></span><br><span class="line">NAME                     READY   STATUS        RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Running       0          5m22s</span><br><span class="line">statefulset-kubia-v1-1   1/1     Running       0          6m12s</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running       0          6m57s</span><br><span class="line">statefulset-kubia-v1-3   1/1     Terminating   0          7m39s</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods</span></span><br><span class="line">NAME                     READY   STATUS        RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Running       0          6m55s</span><br><span class="line">statefulset-kubia-v1-1   1/1     Terminating   0          7m45s</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running       0          14s</span><br><span class="line">statefulset-kubia-v1-3   1/1     Running       0          54s</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods</span></span><br><span class="line">NAME                     READY   STATUS        RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Terminating   0          7m27s</span><br><span class="line">statefulset-kubia-v1-1   1/1     Running       0          7s</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running       0          46s</span><br><span class="line">statefulset-kubia-v1-3   1/1     Running       0          86s</span><br></pre></td></tr></table></figure><p>发现了什么? 当滚动更新时,kubectl会以倒序的方式,从最末尾一个pod开始依次更新.</p><blockquote><p>StatefulSet的滚动更新策略不同于Deployment可以指定maxSuge参数指定一次同时更新的pod数量,而是只能单个方式进行依次更新</p></blockquote><blockquote><p>StatefulSet还支持partition(分区)的更新策略,具体可以查看官网</p></blockquote><p>无论是何种更新策略.Pod的数据(包括主机名,存储)都会持久化.再次访问第0个pod,存储数据依然存在</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-0/proxy/</span><br><span class="line">You&apos;ve hit statefulset-kubia-v1-0</span><br><span class="line">Data stored on this pod: Hey There ! This greeting was submitted to statefulset-kubia-v1-0</span><br></pre></td></tr></table></figure><hr><h3 id="StatefulSet-如何处理节点失效"><a href="#StatefulSet-如何处理节点失效" class="headerlink" title="StatefulSet 如何处理节点失效"></a>StatefulSet 如何处理节点失效</h3><p>在node2上关闭网卡来模拟这台服务器掉线,观察statefulSet处理节点失效的情况</p><blockquote><p>注意关闭节点网卡前请确保可以通过控制台连接服务器,因为这意味着无法ssh远程登录</p></blockquote><p>node2节点已经关闭,状态为notready</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get node</span><br><span class="line">NAME         STATUS     ROLES    AGE   VERSION</span><br><span class="line">k8s-master   Ready      master   49d   v1.17.3</span><br><span class="line">k8s-node1    Ready      &lt;none&gt;   49d   v1.17.3</span><br><span class="line">k8s-node2    NotReady   &lt;none&gt;   49d   v1.17.3</span><br></pre></td></tr></table></figure><p>过一段时间后,所有node2节点上的Pod为Terminating终止状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide</span></span><br><span class="line">NAME                     READY   STATUS        RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">statefulset-kubia-v1-0   1/1     Terminating   0          33m   10.100.169.174   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-1   1/1     Terminating   0          34m   10.100.169.173   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running       0          34m   10.100.36.97     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-3   1/1     Running       0          35m   10.100.36.99     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h5 id="删除不健康的Pod"><a href="#删除不健康的Pod" class="headerlink" title="删除不健康的Pod"></a>删除不健康的Pod</h5><p>当尝试手动删除pod时,发现永远都无法删除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl delete po statefulset-kubia-v1-0</span><br><span class="line">pod &quot;statefulset-kubia-v1-0&quot; deleted</span><br><span class="line">^@</span><br><span class="line">^@</span><br></pre></td></tr></table></figure><p>在另一个终端上查看该pod.发现虽然pod被Terminating挂起,但是容器仍然处于运行状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl describe pods statefulset-kubia-v1-0</span></span><br><span class="line">Name:                      statefulset-kubia-v1-0</span><br><span class="line">Namespace:                 default</span><br><span class="line">Priority:                  0</span><br><span class="line">Node:                      k8s-node2/172.16.20.253</span><br><span class="line">Start Time:                Sun, 03 May 2020 10:54:17 +0800</span><br><span class="line">Labels:                    app=statefulset-kubia</span><br><span class="line">                           controller-revision-hash=statefulset-kubia-v1-74b44bc68b</span><br><span class="line">                           statefulset.kubernetes.io/pod-name=statefulset-kubia-v1-0</span><br><span class="line">Annotations:               cni.projectcalico.org/podIP: 10.100.169.174/32</span><br><span class="line">Status:                    Terminating (lasts 12m)</span><br><span class="line">Termination Grace Period:  30s</span><br><span class="line">IP:                        10.100.169.174</span><br><span class="line">IPs:</span><br><span class="line">  IP:           10.100.169.174</span><br><span class="line">Controlled By:  StatefulSet/statefulset-kubia-v1</span><br><span class="line">Containers:</span><br><span class="line">  statefulset-kubia:</span><br><span class="line">    Container ID:   docker://099628b95ded3644752a3de799ef338794704aaa4ebe4db5a966b821b2e9a71a</span><br><span class="line">    Image:          luksa/kubia-pet</span><br><span class="line">    Image ID:       docker-pullable://luksa/kubia-pet@sha256:4263bc375d3ae2f73fe7486818cab64c07f9cd4a645a7c71a07c1365a6e1a4d2</span><br><span class="line">    Port:           8080/TCP</span><br><span class="line">    Host Port:      0/TCP</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Sun, 03 May 2020 10:54:21 +0800</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/data from data (rw)</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jfrqr (ro)</span><br></pre></td></tr></table></figure><h5 id="强制删除"><a href="#强制删除" class="headerlink" title="强制删除"></a>强制删除</h5><p>带上参数<figure class="highlight plain"><figcaption><span>--grace-period 0```可以强制删除一个pod</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl delete po statefulset-kubia-v1-0 –force –grace-period 0<br>warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.<br>pod “statefulset-kubia-v1-0” force deleted</p><p>[root@k8s-master ~]# kubectl get pods<br>NAME                     READY   STATUS              RESTARTS   AGE<br>statefulset-kubia-v1-0   0/1     ContainerCreating   0          5s<br>statefulset-kubia-v1-1   1/1     Terminating         0          44m<br>statefulset-kubia-v1-2   1/1     Running             0          44m<br>statefulset-kubia-v1-3   1/1     Running             0          45m</p><p>[root@k8s-master ~]# kubectl get pods -o wide<br>NAME                     READY   STATUS        RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES<br>statefulset-kubia-v1-0   1/1     Running       0          12s   10.100.36.101    k8s-node1   <none>           <none><br>statefulset-kubia-v1-1   1/1     Terminating   0          44m   10.100.169.173   k8s-node2   <none>           <none><br>statefulset-kubia-v1-2   1/1     Running       0          44m   10.100.36.97     k8s-node1   <none>           <none><br>statefulset-kubia-v1-3   1/1     Running       0          45m   10.100.36.99     k8s-node1   <none>           <none><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">此时第0个pod已经重新创建,并且运行到node1节点,而另一个pod依然处于Terminating状态</span><br><span class="line"></span><br><span class="line">##### node2节点恢复正常</span><br><span class="line"></span><br><span class="line">当节点恢复正常后,很快node和pod都全部恢复正常.此时第0个pod依然还是挂在在node1节点.第1个pod的状态迅速从Terminating状态变为Running状态</span><br></pre></td></tr></table></figure></none></none></none></none></none></none></none></none></p><p>[root@k8s-master ~]# kubectl get pods -o wide<br>NAME                     READY   STATUS    RESTARTS   AGE     IP               NODE        NOMINATED NODE   READINESS GATES<br>statefulset-kubia-v1-0   1/1     Running   0          3m17s   10.100.36.101    k8s-node1   <none>           <none><br>statefulset-kubia-v1-1   1/1     Running   0          5s      10.100.169.172   k8s-node2   <none>           <none><br>statefulset-kubia-v1-2   1/1     Running   0          47m     10.100.36.97     k8s-node1   <none>           <none><br>statefulset-kubia-v1-3   1/1     Running   0          48m     10.100.36.99     k8s-node1   <none>           <none><br><code>`</code></none></none></none></none></none></none></none></none></p><hr><h3 id="本章总结"><a href="#本章总结" class="headerlink" title="本章总结"></a>本章总结</h3><p>Stateful和RS,deployment的用法总体没有太大区别,下面是这2种资源的对比</p><table><thead><tr><th style="text-align:center">特性</th><th style="text-align:center">Deployment</th><th style="text-align:center">StatefulSet</th></tr></thead><tbody><tr><td style="text-align:center">是否暴露到外网</td><td style="text-align:center">可以</td><td style="text-align:center">一般不</td></tr><tr><td style="text-align:center">请求面向的对象</td><td style="text-align:center">ServiceName</td><td style="text-align:center">指定pod的域名</td></tr><tr><td style="text-align:center">灵活性</td><td style="text-align:center">通过Service(名称或者IP)访问后端Pod</td><td style="text-align:center">可以访问任意一个pod</td></tr><tr><td style="text-align:center">易用性</td><td style="text-align:center">只需要关心Service信息即可</td><td style="text-align:center">需要知道访问pod的名称,Headless Service名称</td></tr><tr><td style="text-align:center">PV/PVC稳定性</td><td style="text-align:center">无法保障绑定关系</td><td style="text-align:center">可以保障</td></tr><tr><td style="text-align:center">pod名称稳定性</td><td style="text-align:center">使用一个随机的名称后缀,重启后会随机生成另外一个.名称不重复</td><td style="text-align:center">稳定,每次都一样</td></tr><tr><td style="text-align:center">升级更新顺序</td><td style="text-align:center">随机启动.如果pod宕机重启,也是随机分配一个Node节点重新启动</td><td style="text-align:center">pod按顺序依次启动,如果pod宕机,依然使用相同的Node节点和名称</td></tr><tr><td style="text-align:center">停止顺序</td><td style="text-align:center">随机停止</td><td style="text-align:center">倒序停止</td></tr><tr><td style="text-align:center">集群内部服务发现</td><td style="text-align:center">只能通过Service访问随机的一个Pod</td><td style="text-align:center">可以打通pod之间的通信</td></tr><tr><td style="text-align:center">性能开销</td><td style="text-align:center">无需维护pod与node,pvc等关系</td><td style="text-align:center">需要维护额外的关系信息</td></tr></tbody></table><p>通过对比发现</p><ul><li>如果不需要额外数据依赖或者状态维护的部署,优先选择Deployment</li><li>如果单纯要做数据持久化,方式pod宕机数据丢失,直接使用PV/PVC就可以</li><li>如果是有多个副本,且每个副本挂载的PV存储数据不同,并且pod宕机重启后仍然关联到之前的PVC,并且数据需要持久化,考虑使用StatefulSet</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;h3 id=&quot;kubernetes-headless-Service&quot;&gt;&lt;a href=&quot;#kubernetes-headless-Service&quot; class=&quot;headerlink&quot; title=&quot;kubernetes headless Service&quot;&gt;&lt;/a&gt;kubernetes headless Service&lt;/h3&gt;&lt;p&gt;​     我们以前学习过,Service是Kubernetes项目中用来将一组Pod暴露给外界访问的一种机制.外部客户端通过Service地址可以随机访问到某个具体的Pod&lt;/p&gt;
&lt;p&gt;​     之前学过几种Service类型,包括nodeport,loadbalancer等等.所有这类Service都有一个VIP(虚拟IP),访问Service VIP,Service会将请求转发到后端的Pod上,&lt;/p&gt;
&lt;p&gt;​     还有一种Service是Headless Service(无头服务),这类Service自身不需要VIP,当DNS解析该Service时,会解析出Service后端的Pod地址.这样设置的好处是Kubernetes项目为Pod分配唯一的”可解析身份”,只要知道一个pod的名字和对应的Headless Service名字,就可以通过这条DNS访问到后端的Pod&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes API服务器权限(ServiceAccount &amp;&amp; RBAC)</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/kubernetes%20API%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%9D%83%E9%99%90(ServiceAccount%20&amp;&amp;%20RBAC)/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/kubernetes API服务器权限(ServiceAccount &amp;&amp; RBAC)/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T08:48:06.747Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kubernetes-API服务器权限-ServiceAccount-amp-amp-RBAC"><a href="#kubernetes-API服务器权限-ServiceAccount-amp-amp-RBAC" class="headerlink" title="kubernetes API服务器权限(ServiceAccount &amp;&amp; RBAC)"></a>kubernetes API服务器权限(ServiceAccount &amp;&amp; RBAC)</h2><h3 id="ServiceAccount介绍"><a href="#ServiceAccount介绍" class="headerlink" title="ServiceAccount介绍"></a>ServiceAccount介绍</h3><p>​        每个Pod都与一个ServiceAccount相关联,它代表了运行在pod中应用程序的身份证明.每个pod在启动的时候kubernetes会自动挂载ServiceAccount的TOKEN到pod容器的<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">​ServiceAccount只不过是一种运行在pod中的应用程序和API服务器身份认证的一种方式.应用程序通过在请求中传递serviceaccount的token和API服务器通信</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 了解ServiceAccount资源</span><br><span class="line"></span><br><span class="line">​ServiceAccount和pod,Secret,ConfigMap等一样,本身也是一种资源.他们作用在单独的命名空间.kubernetes为每个命名空间自动创建一个默认的ServiceAccount(名字是default),可以像查看其它资源一样使用```kubectl get sa```来查看ServiceAccount列表</span><br><span class="line"></span><br><span class="line">&lt;!--more--&gt;</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl get sa<br>NAME      SECRETS   AGE<br>default   1         51d<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 同一个命名空间下的多个pod可以使用同一个ServiceAccount</span><br><span class="line">* pod只能使用同一个命名空间下的ServiceAccount</span><br><span class="line">* 如果在pod的manifest文件中没有显示的指定ServiceAccount名称.则默认使用default ServiceAccount</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 创建ServiceAccount</span><br><span class="line"></span><br><span class="line">​为什么要去创建新的ServiceAccount,而不是所有pod都使用默认的ServiceAccount? 为了集群的安全性,尽量做到权限最小分配的原则,只对有需要的Pod配置有相应较高权限的ServiceAccount,而其他pod使用default默认ServiceAccount应该不允许他们检索或者修改部署在集群中的任何资源.</span><br><span class="line"></span><br><span class="line">​下面实验一下如何创建其他ServiceAccount,并且分配给Pod</span><br><span class="line"></span><br><span class="line">* 创建ServiceAccount很简单,直接使用```kubectl create serviceaccount```命令</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl create serviceaccount foo<br>serviceaccount/foo created</p><p>[root@k8s-master ~]# kubectl describe sa foo<br>Name:                foo<br>Namespace:           default<br>Labels:              <none><br>Annotations:         <none><br>Image pull secrets:  <none><br>Mountable secrets:   foo-token-ktjj5<br>Tokens:              foo-token-ktjj5<br>Events:              <none><br>[root@k8s-master ~]#<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">上面就创建了一个名为foo的serviceaccount.如果查看这个名为foo-token-ktjj5的TOKEN秘钥可以发现这个ServiceAccount也和default默认的ServiceAccount一样具有相同的条目(CA证书,命名空间,token),当然这2个token本身是不一样的</span><br></pre></td></tr></table></figure></none></none></none></none></p><p>[root@k8s-master ~]# kubectl describe secret foo-token-ktjj5<br>Name:         foo-token-ktjj5<br>Namespace:    default<br>Labels:       <none><br>Annotations:  kubernetes.io/service-account.name: foo<br>              kubernetes.io/service-account.uid: 28cb7741-eaf3-46bb-bd58-3f1ddb748c2e</none></p><p>Type:  kubernetes.io/service-account-token</p><h1 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h1><p>ca.crt:     1025 bytes<br>namespace:  7 bytes<br>token:      eyJhbGciOiJSUzI1NiIsImtpZCI6InI3cE50azROV1pseDJqNmxVQTlSWjF6Vk9YVFFnamZEWFF5RG56YTJhaWMifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImZvby10b2tlbi1rdGpqNSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJmb28iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyOGNiNzc0MS1lYWYzLTQ2YmItYmQ1OC0zZjFkZGI3NDhjMmUiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVmYXVsdDpmb28ifQ.KdfyYVkb7Ye_sV8yUWqU6I8fe8Pck7cyTDdwh08Y9w0f4cwJKqIwsuVn39VCpYIH3PaDsKoxBv7Bpahn6lfiP3MgA4zcWlX4vtBUxJCAt-GBXBTcHkqQ6BwtxFzkY4rgsXLd5HuqsrqrZbxrSM1zfNovPccRgklN3kbz0BuxTmKQ65E9DFhAt5kmzajO3qvAB_ymGeoJLMul1fZEqLnV48UEKN5KFAxVfwo0b4On2LcKOVkmG_P7yO7X6TsE6zEN03kvoxjOd0vnJrGUCT9fu0KlabRyDsyDWsua-Su2kv-gDg3O6zQxqDGgEwRxTb2-7Cbv6PaFbfCwSRnCC8Y6FA<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 将ServiceAccount分配给pod</span><br><span class="line"></span><br><span class="line">将ServiceAccount赋值给Pod非常简单,进需要在pod的manifest文件中的spec字段下指定serviceAccountName即可.例如下面之前学习过的curl镜像和kubectl-proxy的ambassador镜像</span><br></pre></td></tr></table></figure></p><p>apiVersion: v1<br>kind: Pod<br>metadata:<br>  name: curl-custome-sa<br>spec:</p><p>  #指定pod使用哪个serviceaccount<br>  serviceAccountName: foo<br>  containers:</p><pre><code>- name: main  image: tutum/curl  command: [&apos;sleep&apos;,&apos;9999999&apos;]- name: ambassador  image: luksa/kubectl-proxy:1.6.2</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; ServiceAccount必须在pod创建之前新建,后续不能被修改</span><br><span class="line"></span><br><span class="line">* 使用新创建的ServiceAccount访问API服务器.(通过ambassador容器)</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl exec -it curl-custome-sa -c main curl localhost:8001/api/v1/pods<br>{<br>  “kind”: “Status”,<br>  “apiVersion”: “v1”,<br>  “metadata”: {</p><p>  },<br>  “status”: “Failure”,<br>  “message”: “pods is forbidden: User \”system:serviceaccount:default:foo\” cannot list resource \”pods\” in API group \”\” at the cluster scope”,<br>  “reason”: “Forbidden”,<br>  “details”: {<br>    “kind”: “pods”<br>  },<br>  “code”: 403<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">还是无法访问? 这是因为RBAC(基于角色权限控制)插件没有给这个ServiceAccount进行权限授权.</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 介绍RBAC</span><br><span class="line"></span><br><span class="line">​在1.6版本开始,集群安全性显著提高.RBAC在集群中默认开启.RBAC会阻止未授权的用户查看和修改集群状态.API服务器对外暴露了REST接口,用户可以通过向服务器发送HTTP请求来执行动作.</span><br><span class="line"></span><br><span class="line">​REST客户端发送GET,POST,PUT,DELETE和其他类型的HTTP请求到特定的URL资源.这些资源可以是kubernetes的Pod,Service,Secret等等.</span><br><span class="line"></span><br><span class="line">​RBAC支持的支持一些动词(例如,get,create,update)等映射到客户端请求的HTTP方法(GET,POST,PUT).完整的映射方法如下表</span><br><span class="line"></span><br><span class="line">| HTTP方法 |        RBAC动词        |</span><br><span class="line">| :------: | :--------------------: |</span><br><span class="line">| GET,HEAD | get(或者watch用于监听) |</span><br><span class="line">|   POST   |         create         |</span><br><span class="line">|   PUT    |         update         |</span><br><span class="line">|  PATCH   |         patch          |</span><br><span class="line">|  DELETE  |         delete         |</span><br><span class="line"></span><br><span class="line">​除了可以对全部资源类型应用安装权限,RBAC规则还可以引用于特定的资源实例,以及非资源URL路径.(并不是API服务器对外暴露的每个路径都映射到一个资源),例如/api路径,/healthz健康路径</span><br><span class="line"></span><br><span class="line">​RBAC授权插件将用户角色作为决定用户能否执行操作的关键因素主体.(可以是一个人,一个ServiceAccount,或者一组人,一组ServiceAccount).一个用户可以有多个角色.</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 介绍RBAC资源</span><br><span class="line"></span><br><span class="line">RBAC授权规则通过四种资源来进行配置.可以分为两个组:</span><br><span class="line"></span><br><span class="line">* Role(角色)和ClusterRole(集群角色).他们指定了在资源上可以执行哪些动作</span><br><span class="line">* RoleBinding(角色绑定)和ClusterRoleBinding.(集群角色绑定).他们将上述角色绑定到特定的用户,组或者ServiceAccount上</span><br><span class="line"></span><br><span class="line">&gt; 角色定义了可以有什么访问权限,而角色绑定决定了谁可以做这些操作</span><br><span class="line"></span><br><span class="line">角色和集群角色,或者角色绑定和集群角色绑定之间的区别在于角色和角色绑定是命名空间资源.而集群角色和集群角色绑定是集群级别的资源(独立资源,不属于任何命名空间).</span><br><span class="line"></span><br><span class="line">* 单个命名空间可以创建多个角色,以及多个角色绑定</span><br><span class="line">* 尽管角色绑定是在命名空间下的.但是也可以引用其他命名空间下的集群角色.</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 演示RBAC的实践</span><br><span class="line"></span><br><span class="line">* ##### 创建2个命名空间,然后在2个命名空间下运行```kubectl-proxy``` pod.</span><br><span class="line"></span><br><span class="line">下面这个配置创建了```foo```,```bar```这2个名称空间.以及在每个名称空间下创建了一个```kubectl-proxy```的test容器</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl create ns foo</span><br><span class="line">namespace/foo created</span><br><span class="line">[root@k8s-master ~]# kubectl create ns bar</span><br><span class="line">namespace/bar created</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl run test --image=luksa/kubectl-proxy -n foo</span><br><span class="line">kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.</span><br><span class="line">deployment.apps/test created</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl run test --image=luksa/kubectl-proxy -n bar</span><br><span class="line">kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.</span><br><span class="line">deployment.apps/test created</span><br></pre></td></tr></table></figure></p><p>列出各名称空间下的pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get po -n foo</span></span><br><span class="line">NAME                    READY   STATUS    RESTARTS   AGE</span><br><span class="line"><span class="built_in">test</span>-7b4bb6b9ff-qpppf   1/1     Running   0          28m</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get po -n bar</span></span><br><span class="line">NAME                    READY   STATUS    RESTARTS   AGE</span><br><span class="line"><span class="built_in">test</span>-7b4bb6b9ff-jl6v5   1/1     Running   0          42m</span><br></pre></td></tr></table></figure><p>此时由于还没有授权,进入pod容器内部,对API服务器发起http访问请求时.API服务器不允许Pod访问services服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl exec -it test-7b4bb6b9ff-qpppf -n foo sh</span><br><span class="line"></span><br><span class="line">/ # curl localhost:8001/api/v1/namespaces/foo/services</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Status&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;status&quot;: &quot;Failure&quot;,</span><br><span class="line">  &quot;message&quot;: &quot;services is forbidden: User \&quot;system:serviceaccount:foo:default\&quot; cannot list resource \&quot;services\&quot; in API group \&quot;\&quot; in the namespace \&quot;foo\&quot;&quot;,</span><br><span class="line">  &quot;reason&quot;: &quot;Forbidden&quot;,</span><br><span class="line">  &quot;details&quot;: &#123;</span><br><span class="line">    &quot;kind&quot;: &quot;services&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;code&quot;: 403</span><br><span class="line">&#125;/ #</span><br></pre></td></tr></table></figure><hr><ul><li><h5 id="使用Role和RoleBinding"><a href="#使用Role和RoleBinding" class="headerlink" title="使用Role和RoleBinding"></a>使用Role和RoleBinding</h5><h5 id="创建Role"><a href="#创建Role" class="headerlink" title="创建Role"></a>创建Role</h5><p>Role资源定义了哪些操作可以在哪些资源上执行.下面的配置文件定义了一个Role.允许用户获取并列出foo命名空间中的服务</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  #该role所在的命名空间,如果没有填写,则此role应用在默认的命名空间</span><br><span class="line">  namespace: foo</span><br><span class="line">  name: service-reader</span><br><span class="line">#以下是此role定义的规则</span><br><span class="line">rules:</span><br><span class="line">    #Service是核心apiGroup的资源,所以没有apiGroup名,就是&quot;&quot;空</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    #该Role角色授权的规则,允许get,list访问权限</span><br><span class="line">    verbs: [&quot;get&quot;,&quot;list&quot;]</span><br><span class="line">   #这个规则和服务有关,(必须使用复数)</span><br><span class="line">    resources: [&quot;services&quot;]</span><br><span class="line">~</span><br></pre></td></tr></table></figure><p>Role配置文件中定义了以下信息:</p><ul><li>Role所处的名称空间</li><li>Role角色名</li><li>verbs: 角色允许的访问权限</li><li>resources: 角色允许访问的资源</li></ul><p>创建role</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl create -f serviceaccount-service-reader.yaml</span></span><br><span class="line">role.rbac.authorization.k8s.io/service-reader created</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get role -n foo</span></span><br><span class="line">NAME             AGE</span><br><span class="line">service-reader   6m18s</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl describe role -n foo service-reader</span></span><br><span class="line">Name:         service-reader</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources  Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  ---------  -----------------  --------------  -----</span><br><span class="line">  services   []                 []              [get list]</span><br><span class="line">[root@k8s-master ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure><ul><li><h5 id="将Role角色绑定到ServiceAccount"><a href="#将Role角色绑定到ServiceAccount" class="headerlink" title="将Role角色绑定到ServiceAccount"></a>将Role角色绑定到ServiceAccount</h5><p>​      Role角色定义了哪些操作可以执行,但是没有指定谁拥有该角色.要做到这一点,需要将角色绑定到一个主体上.它可以是一个usr(用户),或者一个ServiceAccount.或者一个组.</p><p>​        通过创建一个RoleBinding资源来实现将角色绑定到主体.使用<figure class="highlight plain"><figcaption><span>create rolebinding```.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">完整格式是```kubectl create rolebinding 绑定名 --role=角色名 --serviceaccount=命名空间:ServiceAccount名 -n 命名空间```.下面是具体的例子实现</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl create rolebinding test --role=service-reader --serviceaccount=foo:default -n foo</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/test created</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl get rolebinding test -n foo  -o yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: &quot;2020-05-05T09:45:58Z&quot;</span><br><span class="line">  name: test</span><br><span class="line">  namespace: foo</span><br><span class="line">  resourceVersion: &quot;10719596&quot;</span><br><span class="line">  selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/foo/rolebindings/test</span><br><span class="line">  uid: a400ece9-ceb7-4a57-8687-f385a69b9190</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: service-reader</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: default</span><br><span class="line">  namespace: foo</span><br></pre></td></tr></table></figure></p><p>​        在上面的例子中创建了一个test的rolebinding的角色绑定资源.将<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">​通过yaml文件显示,RoleBinding角色绑定引用了单一的角色(从```roleRef```字段可以看到引用的Role角色信息).RoleBinding可以将单个角色绑定到多个主体.</span><br><span class="line"></span><br><span class="line">​因为这个RoleBinding将service-reader这个Role角色绑定到了foo命名空间下的ServiceAccount下.所以现在foo下的pod可以访问集群的Service资源了</span><br></pre></td></tr></table></figure></p><p>#进入foo命名空间下的Pod容器内部,通过API服务器访问services<br>/ # curl localhost:8001/api/v1/namespaces/foo/services<br>{<br>  “kind”: “ServiceList”,<br>  “apiVersion”: “v1”,<br>  “metadata”: {</p><pre><code>&quot;selfLink&quot;: &quot;/api/v1/namespaces/foo/services&quot;,&quot;resourceVersion&quot;: &quot;10720460&quot;</code></pre><p>  },<br>  “items”: []  #items列表为空很正常,因为foo名称空间下没有任何Serivce资源<br>}/ #</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* ##### 在RoleBinding角色绑定中使用其他名称空间的ServiceAccount</span><br><span class="line"></span><br><span class="line">  ​刚才在foo的名称空间下创建了Role和RoleBinding.并且foo下的pod可以正常通过API服务器访问Service资源了.但是很显然bar名称空间下pod没有任何访问权限.</span><br><span class="line"></span><br><span class="line">  ​可以修改foo名称空间中的RoleBinding,并添加另一个pod的ServiceAccount.即使这个ServiceAccount在另一个不同的名称空间中.</span><br><span class="line"></span><br><span class="line">  ​修改RoleBinding资源名为test的配置文件</span><br></pre></td></tr></table></figure></li></ul><p>[root@k8s-master ~]# kubectl edit rolebinding test -n foo<br>rolebinding.rbac.authorization.k8s.io/test edited</p><p>#在配置文件中最后新增一个subjects配置.引用来自bar名称空间中的default ServiceAccount<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: RoleBinding<br>metadata:<br>  creationTimestamp: “2020-05-05T09:45:58Z”<br>  name: test<br>  namespace: foo<br>  resourceVersion: “10719596”<br>  selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/foo/rolebindings/test<br>  uid: a400ece9-ceb7-4a57-8687-f385a69b9190<br>roleRef:<br>  apiGroup: rbac.authorization.k8s.io<br>  kind: Role<br>  name: service-reader<br>subjects:</p><ul><li>kind: ServiceAccount<br>name: default<br>namespace: foo</li><li>kind: ServiceAccount<br>name: default<br>namespace: bar<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">此时,bar名称空间下的Pod可以访问foo名称空间下的Service资源(但是仍然不能访问bar自己名称空间下的资源)</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">/ # curl localhost:8001/api/v1/namespaces/foo/services</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;ServiceList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;/api/v1/namespaces/foo/services&quot;,</span><br><span class="line">    &quot;resourceVersion&quot;: &quot;10722489&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: []</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">/ # curl localhost:8001/api/v1/namespaces/bar/services</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Status&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;status&quot;: &quot;Failure&quot;,</span><br><span class="line">  &quot;message&quot;: &quot;services is forbidden: User \&quot;system:serviceaccount:bar:default\&quot; cannot list resource \&quot;services\&quot; in API group \&quot;\&quot; in the namespace \&quot;bar\&quot;&quot;,</span><br><span class="line">  &quot;reason&quot;: &quot;Forbidden&quot;,</span><br><span class="line">  &quot;details&quot;: &#123;</span><br><span class="line">    &quot;kind&quot;: &quot;services&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;code&quot;: 403</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="ClusterRole和ClusterRoleBinding"><a href="#ClusterRole和ClusterRoleBinding" class="headerlink" title="ClusterRole和ClusterRoleBinding"></a>ClusterRole和ClusterRoleBinding</h3><p>​    Role和RoleBinding都是名称空间的资源,他们属于一个单一的名称空间资源上.但是如上面那个例子,RoleBinding可以引用来自其他名称空间中的ServiceAccount.</p><p>​    除了这些名称空间里的资源,还存在2个集群级别的RBAC资源:<strong>ClusterRole</strong>和<strong>ClusterRoleBinding</strong>.之所以存在集群级别的 资源是因为常规的<strong>Role</strong>和<strong>RoleBinding</strong>只允许访问和角色在同一个名称空间中的资源.如果希望允许不同名称空间下的Pod可以互相访问资源,则需要在每个名称空间中创建一个相同的<strong>Role</strong>和<strong>RoleBinding</strong>.</p><p>​    另外,一些特定的资源完全不在任何名称空间中(例如.Node,PV,namespace等等).我们也提过API服务器对外暴露的一些不表示资源的URL路径(例如/healthz).常规<strong>Role</strong>和<strong>RoleBinding</strong>不能对这些类型的URL进行授权.但是<strong>ClusterRole</strong>可以.</p><h5 id="创建ClusterRole"><a href="#创建ClusterRole" class="headerlink" title="创建ClusterRole"></a>创建ClusterRole</h5><p>命令格式和Role类似:<figure class="highlight plain"><figcaption><span>create clusterrole 名字```.下面这个例子创建一个pv-reader的集群角色,允许只读访问persistentvolumes资源</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl create clusterrole pv-reader --verb=get,list --resource=persistentvolumes</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/pv-reader created</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl get clusterrole pv-reader -o yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: &quot;2020-05-05T10:26:01Z&quot;</span><br><span class="line">  name: pv-reader</span><br><span class="line">  resourceVersion: &quot;10725342&quot;</span><br><span class="line">  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/pv-reader</span><br><span class="line">  uid: 2c6d3083-3385-46b3-9ea6-b479f57e7db5</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - persistentvolumes</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">[root@k8s-master ~]#</span><br></pre></td></tr></table></figure></p><h5 id="创建ClusterRoleBinding"><a href="#创建ClusterRoleBinding" class="headerlink" title="创建ClusterRoleBinding"></a>创建ClusterRoleBinding</h5><p>命令和RoleBinding使用方法类似: <figure class="highlight plain"><figcaption><span>create clusterrolebind```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">下面的命令创建了个一个pv-test的```ClusterRoleBinding```集群角色绑定对线,指定的角色是```pv-reader```,分配给的ServiceAccount是```foo```名称空间下的default默认ServiceAccount</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl create clusterrolebinding pv-test –clusterrole=pv-reader –serviceaccount=foo:default<br>clusterrolebinding.rbac.authorization.k8s.io/pv-test created<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">现在foo名称空间下的Pod就可以访问集群下的persistentvolume(PV)资源了</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">/ # curl localhost:8001/api/v1/persistentvolumes</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;PersistentVolumeList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;/api/v1/persistentvolumes&quot;,</span><br><span class="line">    &quot;resourceVersion&quot;: &quot;10858720&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;pv-1&quot;,</span><br><span class="line">        &quot;selfLink&quot;: &quot;/api/v1/persistentvolumes/pv-1&quot;,</span><br><span class="line">        &quot;uid&quot;: &quot;544a9c07-1c41-41c5-8ed4-da41b4d11cf5&quot;,</span><br><span class="line">        &quot;resourceVersion&quot;: &quot;10245103&quot;,</span><br><span class="line">        &quot;creationTimestamp&quot;: &quot;2020-05-02T02:54:43Z&quot;,</span><br><span class="line">        .......</span><br></pre></td></tr></table></figure></p><blockquote><p>由于指定了绑定到foo名称空间下的ServiceAccount.所以显而易见,bar名称空间下的pod没有权限访问perssitentvolumes资源</p></blockquote><hr><h3 id="ClusterRole授权访问指定命名空间中的资源"><a href="#ClusterRole授权访问指定命名空间中的资源" class="headerlink" title="ClusterRole授权访问指定命名空间中的资源"></a>ClusterRole授权访问指定命名空间中的资源</h3><p><strong>ClusterRole</strong>不是必须一直和集群级别的<strong>ClusterRoleBinding</strong>绑定使用.他们也可以和常规的<strong>RoleBinding</strong>进行捆绑.</p><p>在kubernetes中,有很多系统预定义的RBAC角色资源.通过<figure class="highlight plain"><figcaption><span>get```命令就可以看到系统预定义的角色.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>kubectl get role -n kube-system<br>kubectl get clusterrole<br>kubectl get rolebinding -n kube-system<br>kubectl get clusterrolebinding<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">下面是一个系统预定义的名为view的集群角色</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl get clusterrole view -o yaml</span><br><span class="line"></span><br><span class="line">  name: view</span><br><span class="line">  resourceVersion: &quot;413&quot;</span><br><span class="line">  selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/view</span><br><span class="line">  uid: 0aa05d0e-69b3-45df-8f92-be922386dbfe</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - configmaps</span><br><span class="line">  - endpoints</span><br><span class="line">  - persistentvolumeclaims</span><br><span class="line">  - persistentvolumeclaims/status</span><br><span class="line">  - pods</span><br><span class="line">  - replicationcontrollers</span><br><span class="line">  - replicationcontrollers/scale</span><br><span class="line">  - serviceaccounts</span><br><span class="line">  - services</span><br><span class="line">  - services/status</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  .......</span><br></pre></td></tr></table></figure></p><p>​        这个ClusterRole又很多规则,允许只读(get,list,watch)访问各种资源.这些资源都是有名称空间的.这个名为View的ClusterRole的作用取决于它是和<strong>ClusterRoleBinding</strong>绑定还是和<strong>RoleBinding</strong>绑定.</p><ul><li><p>如果创建了一个<strong>ClusterRoleBinding</strong>,并且引用了View这个<strong>ClusterRole</strong>.那么绑定的主体可以在所有名称空间中查看指定资源</p></li><li><p>如果创建的是一个<strong>RoleBinding</strong>,并且引用了View这个<strong>ClusterRole</strong>,那么在绑定中列出的主体只能查看在RoleBinding名称空间中的资源.</p></li></ul><p>下面观察一下一个ClusterRole绑定到<strong>ClusterRoleBinding</strong>和<strong>RoleBinding</strong>的不同效果</p><p>在实验之前,确认一下默认情况下foo名称空间下的pod无论是访问集群下的pod还是名称空间下的pod都没有权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">/ <span class="comment"># curl localhost:8001/api/v1/pods</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"kind"</span>: <span class="string">"Status"</span>,</span><br><span class="line">  <span class="string">"apiVersion"</span>: <span class="string">"v1"</span>,</span><br><span class="line">  <span class="string">"metadata"</span>: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"status"</span>: <span class="string">"Failure"</span>,</span><br><span class="line">  <span class="string">"message"</span>: <span class="string">"pods is forbidden: User \"system:serviceaccount:foo:default\" cannot list resource \"pods\" in API group \"\" at the cluster scope"</span>,</span><br><span class="line">  <span class="string">"reason"</span>: <span class="string">"Forbidden"</span>,</span><br><span class="line">  <span class="string">"details"</span>: &#123;</span><br><span class="line">    <span class="string">"kind"</span>: <span class="string">"pods"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"code"</span>: 403</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">/ <span class="comment"># curl localhost:8001/api/v1/namespaces/foo/pods</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"kind"</span>: <span class="string">"Status"</span>,</span><br><span class="line">  <span class="string">"apiVersion"</span>: <span class="string">"v1"</span>,</span><br><span class="line">  <span class="string">"metadata"</span>: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"status"</span>: <span class="string">"Failure"</span>,</span><br><span class="line">  <span class="string">"message"</span>: <span class="string">"pods is forbidden: User \"system:serviceaccount:foo:default\" cannot list resource \"pods\" in API group \"\" in the namespace \"foo\""</span>,</span><br><span class="line">  <span class="string">"reason"</span>: <span class="string">"Forbidden"</span>,</span><br><span class="line">  <span class="string">"details"</span>: &#123;</span><br><span class="line">    <span class="string">"kind"</span>: <span class="string">"pods"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"code"</span>: 403</span><br></pre></td></tr></table></figure><h4 id="1-ClusterRole绑定到ClusterRoleBinding"><a href="#1-ClusterRole绑定到ClusterRoleBinding" class="headerlink" title="1.ClusterRole绑定到ClusterRoleBinding"></a>1.ClusterRole绑定到ClusterRoleBinding</h4><p>下面的命令将view这个ClusterRole绑定到foo名称空间下的ServiceAccount</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl create clusterrolebinding view-test --clusterrole=view --serviceaccount=foo:default</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/view-test created</span><br></pre></td></tr></table></figure><p>现在pod能列出foo名称空间下的Pod了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/ # curl localhost:8001/api/v1/namespaces/foo/pods</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;PodList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;/api/v1/namespaces/foo/pods&quot;,</span><br><span class="line">    &quot;resourceVersion&quot;: &quot;10865044&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;test-7b4bb6b9ff-qpppf&quot;,</span><br></pre></td></tr></table></figure><p>而且也可以列出bar名称空间下的Pod了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/ # curl localhost:8001/api/v1/namespaces/bar/pods</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;PodList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;/api/v1/namespaces/bar/pods&quot;,</span><br><span class="line">    &quot;resourceVersion&quot;: &quot;10865200&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;test-7b4bb6b9ff-jl6v5&quot;</span><br></pre></td></tr></table></figure><p>还可以使用/api/v1/pods的URL路径来检索所有名称空间中的pod</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">/ # curl localhost:8001/api/v1/pods | more</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;PodList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;/api/v1/pods&quot;,</span><br><span class="line">    &quot;resourceVersion&quot;: &quot;10872966&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;test-7b4bb6b9ff-jl6v5&quot;,</span><br><span class="line">        &quot;generateName&quot;: &quot;test-7b4bb6b9ff-&quot;,</span><br><span class="line">        &quot;namespace&quot;: &quot;bar&quot;,</span><br><span class="line">        ......</span><br></pre></td></tr></table></figure><p><strong>总结:</strong> 正如预期的那样.将ClusterRole绑定到ClusterRoleBinding.便可以获取整个集群中(包括所有名称空间)下的所有pod的列表.</p><hr><h4 id="2-将ClusterRole绑定到RoleBinding"><a href="#2-将ClusterRole绑定到RoleBinding" class="headerlink" title="2.将ClusterRole绑定到RoleBinding."></a>2.将ClusterRole绑定到RoleBinding.</h4><p>先删除刚才创建的view-test的绑定关系删除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl delete clusterrolebinding view-test</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io &quot;view-test&quot; deleted</span><br></pre></td></tr></table></figure><p>创建一个RoleBinding替代</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl create rolebinding view-test --clusterrole=view --serviceaccount=foo:default -n foo</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/view-test created</span><br></pre></td></tr></table></figure><p>现在foo名称空间有一个RoleBinding,将同一个名称空间中的default ServiceAccount绑定到view这个<strong>ClusterRole</strong></p><p>现在pod仍然可以访问当前名称空间下的Pod</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/ # curl localhost:8001/api/v1/namespaces/foo/pods</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;PodList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;/api/v1/namespaces/foo/pods&quot;,</span><br><span class="line">    &quot;resourceVersion&quot;: &quot;10876697&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;test-7b4bb6b9ff-qpppf&quot;,</span><br></pre></td></tr></table></figure><p>但是现在已经无法访问bar名称空间下的pod了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">/ # curl localhost:8001/api/v1/namespaces/bar/pods</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Status&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;status&quot;: &quot;Failure&quot;,</span><br><span class="line">  &quot;message&quot;: &quot;pods is forbidden: User \&quot;system:serviceaccount:foo:default\&quot; cannot list resource \&quot;pods\&quot; in API group \&quot;\&quot; in the namespace \&quot;bar\&quot;&quot;,</span><br><span class="line">  &quot;reason&quot;: &quot;Forbidden&quot;,</span><br><span class="line">  &quot;details&quot;: &#123;</span><br><span class="line">    &quot;kind&quot;: &quot;pods&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;code&quot;: 403</span><br></pre></td></tr></table></figure><p>整个集群下的pod资源当然也无法访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">/ # curl localhost:8001/api/v1/pods</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Status&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line"></span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;status&quot;: &quot;Failure&quot;,</span><br><span class="line">  &quot;message&quot;: &quot;pods is forbidden: User \&quot;system:serviceaccount:foo:default\&quot; cannot list resource \&quot;pods\&quot; in API group \&quot;\&quot; at the cluster scope&quot;,</span><br><span class="line">  &quot;reason&quot;: &quot;Forbidden&quot;,</span><br><span class="line">  &quot;details&quot;: &#123;</span><br><span class="line">    &quot;kind&quot;: &quot;pods&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;code&quot;: 403</span><br></pre></td></tr></table></figure><hr><h3 id="总结Role-ClusterRole-RoleBinding和ClusterRoleBinding的组合"><a href="#总结Role-ClusterRole-RoleBinding和ClusterRoleBinding的组合" class="headerlink" title="总结Role,ClusterRole,RoleBinding和ClusterRoleBinding的组合"></a>总结Role,ClusterRole,RoleBinding和ClusterRoleBinding的组合</h3><table><thead><tr><th>访问资源</th><th>角色类型</th><th>绑定类型</th></tr></thead><tbody><tr><td>集群级别的资源(Nodes,PersistentVolumes,……)</td><td>ClusterRole</td><td>ClusterRoleBinding</td></tr><tr><td>非资源URL(/api,/healthz……)</td><td>ClusterRole</td><td>ClusterRoleBinding</td></tr><tr><td>在任何名称空间中的资源(跨所有名称空间的资源)</td><td>ClusterRole</td><td>ClusterRoleBinding</td></tr><tr><td>具体名称空间中的资源(在多个名称空间中重用这个相同的ClusterRole</td><td>ClusterRole</td><td>RoleBinding</td></tr><tr><td>在具体名称空间中的资源(Role必须在每个名称空间中定义)</td><td>Role</td><td>RoleBinding</td></tr></tbody></table><hr><h3 id="了解默认的ClusterRole和ClusterRoleBinding"><a href="#了解默认的ClusterRole和ClusterRoleBinding" class="headerlink" title="了解默认的ClusterRole和ClusterRoleBinding"></a>了解默认的ClusterRole和ClusterRoleBinding</h3><p>Kubernetes提供了默认的ClusterRole和ClusterRoleBinding.API服务器启动时都会更新他们,这保证了如果你错误地删除角色和绑定,Kubernetes会重新创建默认的角色和绑定</p><figure class="highlight plain"><figcaption><span>get clusterrolebinding```和```kubectl get clusterrole```命令显示了系统默认的ClusterRole和ClusterRoleBinding</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在前面的例子中,使用了默认的view ClusterRole.它允许读取一个名称空间中的大多数资源(除了Role,RoleBinding,Secret).</span><br><span class="line"></span><br><span class="line">下面是一些其他的默认**ClusterRole**:</span><br></pre></td></tr></table></figure><p>#允许修改一个名称空间中的资源,不允许查看和修改Role,RoleBinding<br>ClusterRole: edit</p><p>#一个名称空间中的资源完全控制权.可以读取和修改名称空间中的任何资源.和上面的区别在于是否有权限查看修改Role,RoleBinding<br>ClusterRole: admin</p><p>#集群完全控制权限<br>ClusterRole: cluster-admin<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">显然将admin或者cluster-admin授予pod应用程序或者用户是一个坏主意,和安全问题一样,最好是仅仅给每个人提供他们所需的权限(最小权限原则)</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 总结:</span><br></pre></td></tr></table></figure></p><p>#创建ServiceAccount<br>kubectl create serviceaccount 服务账号名 </p><p>#将ServiceAccount分配给pod<br>在pod的manifest配置文件中指定ServiceAccount名:<br>serviceAccountName: 服务账号名</p><p>#RBAC4种角色资源<br>Role: 常规角色.定义了对某个资源具有某种访问权限<br>RoleBinding: 常规角色绑定.定义了该角色绑定到哪个主体<br>ClusterRole: 集群角色.同上<br>ClusterRoleBinding: 集群角色绑定.同上</p><p>区别在于:<br>常规角色和集群角色绑定作用于某个名称空间下.<br>集群角色和集群角色绑定作用于整个集群,不受限于任何名称空间</p><p>#了解集群默认集群角色和集群角色绑定<br><code>`</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;kubernetes-API服务器权限-ServiceAccount-amp-amp-RBAC&quot;&gt;&lt;a href=&quot;#kubernetes-API服务器权限-ServiceAccount-amp-amp-RBAC&quot; class=&quot;headerlink&quot; title
      
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>使用kubeadmin安装kubernetes集群文档</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/%E4%BD%BF%E7%94%A8kubeadmin%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4%E6%96%87%E6%A1%A3/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/使用kubeadmin安装kubernetes集群文档/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T10:06:08.209Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用kubeadmin安装kubernetes集群文档"><a href="#使用kubeadmin安装kubernetes集群文档" class="headerlink" title="使用kubeadmin安装kubernetes集群文档"></a>使用kubeadmin安装kubernetes集群文档</h2><h3 id="集群环境"><a href="#集群环境" class="headerlink" title="集群环境"></a>集群环境</h3><table><thead><tr><th>主机名</th><th>IP地址</th><th>节点角色</th><th>操作系统</th><th>Service网段</th><th>pod网段</th></tr></thead><tbody><tr><td>k8s-m01</td><td>10.111.2.58</td><td>master</td><td>CentOS7.6</td><td>10.100.0.0/20</td><td>10.96.0.0/12</td></tr><tr><td>k8s-n01</td><td>10.111.2.56</td><td>node</td><td>CentOS7.6</td><td>10.100.0.0/20</td><td>10.96.0.0/12</td></tr><tr><td>k8s-n02</td><td>10.111.2.55</td><td>node</td><td>CentOS7.6</td><td>10.100.0.0/20</td><td>10.96.0.0/12</td></tr></tbody></table><hr><h3 id="服务器-网络要求"><a href="#服务器-网络要求" class="headerlink" title="服务器,网络要求"></a>服务器,网络要求</h3><ul><li><p>service和Pod网段可以任意指定,但是不能和物理机网段,或者当前其他物理网段有冲突</p></li><li><p>服务器配置至少是2核2G</p></li></ul><a id="more"></a><h3 id="软件版本"><a href="#软件版本" class="headerlink" title="软件版本"></a>软件版本</h3><ul><li><p>kubernetes v1.15.3</p></li><li><p>docker 18.09.7</p></li></ul><h3 id="安装文档参考"><a href="#安装文档参考" class="headerlink" title="安装文档参考:"></a>安装文档参考:</h3><p>大部分文档资料来源: <a href="https://www.kuboard.cn/install/install-k8s.html#introduction" target="_blank" rel="noopener">https://www.kuboard.cn/install/install-k8s.html#introduction</a></p><p>但是我没有参考这个教程安装calico网络插件..而是安装flannal网络插件</p><h3 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h3><blockquote><p> 以下所有步骤都是用root账户执行</p></blockquote><p><strong>一.在master节点执行以下安装步骤</strong></p><p>1.设置三台服务器的主机名:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#master节点:</span><br><span class="line">hostnamectl --static set-hostname  k8s-m01</span><br><span class="line"></span><br><span class="line">#node1节点</span><br><span class="line">hostnamectl --static set-hostname  k8s-n01</span><br><span class="line"></span><br><span class="line">#node2节点</span><br><span class="line">hostnamectl --static set-hostname  k8s-02</span><br></pre></td></tr></table></figure><p>2.添加host解析</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#在3台服务器上执行</span><br><span class="line"></span><br><span class="line">echo &quot;10.111.2.58   k8s-m01</span><br><span class="line">10.111.2.56  k8s-n01</span><br><span class="line">10.111.2.55 k8s-n02</span><br><span class="line">&quot; &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><p>3.安装以下软件:</p><ul><li>docker</li><li>nfs-utils</li><li>kubectl</li><li>kubeadm</li><li>kubelet</li></ul><p>一键安装:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -sSL https://kuboard.cn/install-script/v1.15.3/install-kubelet.sh | sh</span><br></pre></td></tr></table></figure><p>也可以采用手动安装的方式.(以下手动安装步骤实际上就是上面的Install-kubelet.sh文件内容.):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line"># 在 master 节点和 worker 节点都要执行</span><br><span class="line"></span><br><span class="line"># 安装 docker</span><br><span class="line"># 参考文档如下</span><br><span class="line"># https://docs.docker.com/install/linux/docker-ce/centos/ </span><br><span class="line"># https://docs.docker.com/install/linux/linux-postinstall/</span><br><span class="line"></span><br><span class="line"># 卸载旧版本</span><br><span class="line">yum remove -y docker \</span><br><span class="line">docker-client \</span><br><span class="line">docker-client-latest \</span><br><span class="line">docker-common \</span><br><span class="line">docker-latest \</span><br><span class="line">docker-latest-logrotate \</span><br><span class="line">docker-logrotate \</span><br><span class="line">docker-selinux \</span><br><span class="line">docker-engine-selinux \</span><br><span class="line">docker-engine</span><br><span class="line"></span><br><span class="line"># 设置 yum repository</span><br><span class="line">yum install -y yum-utils \</span><br><span class="line">device-mapper-persistent-data \</span><br><span class="line">lvm2</span><br><span class="line">yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line"></span><br><span class="line"># 安装并启动 docker</span><br><span class="line">yum install -y docker-ce-18.09.7 docker-ce-cli-18.09.7 containerd.io</span><br><span class="line">systemctl enable docker</span><br><span class="line">systemctl start docker</span><br><span class="line"></span><br><span class="line"># 安装 nfs-utils</span><br><span class="line"># 必须先安装 nfs-utils 才能挂载 nfs 网络存储</span><br><span class="line">yum install -y nfs-utils</span><br><span class="line"></span><br><span class="line"># 关闭 防火墙</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line"></span><br><span class="line"># 关闭 SeLinux</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config</span><br><span class="line"></span><br><span class="line"># 关闭 swap</span><br><span class="line">swapoff -a</span><br><span class="line">yes | cp /etc/fstab /etc/fstab_bak</span><br><span class="line">cat /etc/fstab_bak |grep -v swap &gt; /etc/fstab</span><br><span class="line"></span><br><span class="line"># 修改 /etc/sysctl.conf</span><br><span class="line"># 如果有配置，则修改</span><br><span class="line">sed -i &quot;s#^net.ipv4.ip_forward.*#net.ipv4.ip_forward=1#g&quot;  /etc/sysctl.conf</span><br><span class="line">sed -i &quot;s#^net.bridge.bridge-nf-call-ip6tables.*#net.bridge.bridge-nf-call-ip6tables=1#g&quot;  /etc/sysctl.conf</span><br><span class="line">sed -i &quot;s#^net.bridge.bridge-nf-call-iptables.*#net.bridge.bridge-nf-call-iptables=1#g&quot;  /etc/sysctl.conf</span><br><span class="line"># 可能没有，追加</span><br><span class="line">echo &quot;net.ipv4.ip_forward = 1&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo &quot;net.bridge.bridge-nf-call-ip6tables = 1&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo &quot;net.bridge.bridge-nf-call-iptables = 1&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line"># 执行命令以应用</span><br><span class="line">sysctl -p</span><br><span class="line"></span><br><span class="line"># 配置K8S的yum源</span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">repo_gpgcheck=0</span><br><span class="line">gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg</span><br><span class="line">       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 卸载旧版本</span><br><span class="line">yum remove -y kubelet kubeadm kubectl</span><br><span class="line"></span><br><span class="line"># 安装kubelet、kubeadm、kubectl</span><br><span class="line">yum install -y kubelet-1.15.3 kubeadm-1.15.3 kubectl-1.15.3</span><br><span class="line"></span><br><span class="line"># 修改docker Cgroup Driver为systemd</span><br><span class="line"># # 将/usr/lib/systemd/system/docker.service文件中的这一行 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock</span><br><span class="line"># # 修改为 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd</span><br><span class="line"># 如果不修改，在添加 worker 节点时可能会碰到如下错误</span><br><span class="line"># [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. </span><br><span class="line"># Please follow the guide at https://kubernetes.io/docs/setup/cri/</span><br><span class="line">sed -i &quot;s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g&quot; /usr/lib/systemd/system/docker.service</span><br><span class="line"></span><br><span class="line"># 设置 docker 镜像，提高 docker 镜像下载速度和稳定性</span><br><span class="line"># 如果您访问 https://hub.docker.io 速度非常稳定，亦可以跳过这个步骤</span><br><span class="line">curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io</span><br><span class="line"></span><br><span class="line"># 重启 docker，并启动 kubelet</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart docker</span><br><span class="line">systemctl enable kubelet &amp;&amp; systemctl start kubelet</span><br><span class="line"></span><br><span class="line">docker version</span><br></pre></td></tr></table></figure><p>4.初始化master节点</p><p>一键初始化 .执行以下命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 只在 master 节点执行</span><br><span class="line"># 替换 x.x.x.x 为 master 节点实际 IP（请使用内网 IP）</span><br><span class="line"># export 命令只在当前 shell 会话中有效，开启新的 shell 窗口后，如果要继续安装过程，请重新执行此处的 export 命令</span><br><span class="line"></span><br><span class="line">export MASTER_IP=x.x.x.x</span><br><span class="line"></span><br><span class="line"># 替换 apiserver.demo 为 您想要的 dnsName (不建议使用 master 的 hostname 作为 APISERVER_NAME)</span><br><span class="line"></span><br><span class="line">export APISERVER_NAME=apiserver.demo</span><br><span class="line">export POD_SUBNET=10.100.0.1/20</span><br><span class="line">echo &quot;$&#123;MASTER_IP&#125;    $&#123;APISERVER_NAME&#125;&quot; &gt;&gt; /etc/hosts</span><br><span class="line">curl -sSL https://kuboard.cn/install-script/v1.15.3/init-master.sh | sh</span><br></pre></td></tr></table></figure><p>也可以手工方式初始化:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># 只在 master 节点执行</span><br><span class="line"># 替换 x.x.x.x 为 master 节点实际 IP（请使用内网 IP）</span><br><span class="line"># export 命令只在当前 shell 会话中有效，开启新的 shell 窗口后，如果要继续安装过程，请重新执行此处的 export 命令</span><br><span class="line">export MASTER_IP=x.x.x.x</span><br><span class="line"># 替换 apiserver.demo 为 您想要的 dnsName (不建议使用 master 的 hostname 作为 APISERVER_NAME)</span><br><span class="line">export APISERVER_NAME=apiserver.demo</span><br><span class="line">export POD_SUBNET=10.100.0.1/20</span><br><span class="line">echo &quot;$&#123;MASTER_IP&#125;    $&#123;APISERVER_NAME&#125;&quot; &gt;&gt; /etc/hosts</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 只在 master 节点执行</span><br><span class="line"></span><br><span class="line"># 查看完整配置选项 https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2</span><br><span class="line"></span><br><span class="line">rm -f ./kubeadm-config.yaml</span><br><span class="line">cat &lt;&lt;EOF &gt; ./kubeadm-config.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.15.3</span><br><span class="line">imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers</span><br><span class="line">controlPlaneEndpoint: &quot;$&#123;APISERVER_NAME&#125;:6443&quot;</span><br><span class="line">networking:</span><br><span class="line">  serviceSubnet: &quot;10.96.0.0/12&quot;</span><br><span class="line">  podSubnet: &quot;$&#123;POD_SUBNET&#125;&quot;</span><br><span class="line">  dnsDomain: &quot;cluster.local&quot;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># kubeadm init</span><br><span class="line"># 根据您服务器网速的情况，您需要等候 3 - 10 分钟</span><br><span class="line">kubeadm init --config=kubeadm-config.yaml --upload-certs</span><br><span class="line"></span><br><span class="line"># 配置 kubectl</span><br><span class="line">rm -rf /root/.kube/</span><br><span class="line">mkdir /root/.kube/</span><br><span class="line">cp -i /etc/kubernetes/admin.conf /root/.kube/config</span><br><span class="line"></span><br><span class="line"># 安装 calico 网络插件</span><br><span class="line"># 参考文档 https://docs.projectcalico.org/v3.8/getting-started/kubernetes/</span><br><span class="line">rm -f calico.yaml</span><br><span class="line">wget https://docs.projectcalico.org/v3.8/manifests/calico.yaml</span><br><span class="line">sed -i &quot;s#192\.168\.0\.0/16#$&#123;POD_SUBNET&#125;#&quot; calico.yaml</span><br><span class="line">kubectl apply -f calico.yaml</span><br></pre></td></tr></table></figure><blockquote><p>如果不像安卓calico网络插件,而是安装flannal插件,可以将上个安装calico网络插件步骤替换成如下flannal部署方式:</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml</span><br></pre></td></tr></table></figure><p>5.检查初始化结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 只在 master 节点执行</span><br><span class="line"></span><br><span class="line"># 执行如下命令，等待 3-10 分钟，直到所有的容器组处于 Running 状态</span><br><span class="line">watch kubectl get pod -n kube-system -o wide</span><br><span class="line"></span><br><span class="line"># 查看 master 节点初始化结果</span><br><span class="line">kubectl get nodes</span><br></pre></td></tr></table></figure><p>6.获得join命令参数:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 只在 master 节点执行</span><br><span class="line">kubeadm token create --print-join-command</span><br><span class="line"></span><br><span class="line">执行这个命令会输出如下类似信息:</span><br><span class="line"></span><br><span class="line"># kubeadm token create 命令的输出</span><br><span class="line">kubeadm join apiserver.demo:6443 --token mpfjma.4vjjg8flqihor4vt     --discovery-token-ca-cert-hash sha256:6f7a8e40a810323672de5eee6f4d19aa2dbdb38411845a1bf5dd63485c43d303</span><br></pre></td></tr></table></figure><hr><p>二.初始化node节点</p><p>在两台node节点分别执行:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 只在 worker 节点执行</span><br><span class="line"># 替换 $&#123;MASTER_IP&#125; 为 master 节点实际 IP</span><br><span class="line"># 替换 $&#123;APISERVER_NAME&#125; 为初始化 master 节点时所使用的 APISERVER_NAME</span><br><span class="line">echo &quot;$&#123;MASTER_IP&#125;    $&#123;APISERVER_NAME&#125;&quot; &gt;&gt; /etc/hosts</span><br><span class="line"></span><br><span class="line"># 替换为 master 节点上 kubeadm token create 命令的输出</span><br><span class="line">kubeadm join apiserver.demo:6443 --token mpfjma.4vjjg8flqihor4vt     --discovery-token-ca-cert-hash sha256:6f7a8e40a810323672de5eee6f4d19aa2dbdb38411845a1bf5dd63485c43d303</span><br></pre></td></tr></table></figure><p>四.检查初始化结果</p><p>在master节点上执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 只在 master 节点执行</span><br><span class="line">kubectl get nodes</span><br><span class="line"></span><br><span class="line">#输出结果如下:</span><br><span class="line"></span><br><span class="line">[root@k8s-m01 ~]# kubectl get nodes</span><br><span class="line">NAME      STATUS     ROLES    AGE     VERSION</span><br><span class="line">k8s-m01   Ready      master   7m52s   v1.15.3</span><br><span class="line">k8s-n01   Ready      &lt;none&gt;   50s     v1.15.3</span><br><span class="line">k8s-n02   NotReady   &lt;none&gt;   5s      v1.15.3</span><br></pre></td></tr></table></figure><p>至此,已经完成了kubeadm工具安装部署kubernetes集群的工作</p><hr><h2 id="移除node节点"><a href="#移除node节点" class="headerlink" title="移除node节点"></a>移除node节点</h2><p><strong>方式一.</strong></p><ul><li>在node节点上执行</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 只在 worker 节点执行</span><br><span class="line">kubeadm reset</span><br></pre></td></tr></table></figure><p><strong>方式二.</strong></p><ul><li>在master节点上执行</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 只在 master 节点执行</span><br><span class="line">kubectl delete node k8s-n01</span><br></pre></td></tr></table></figure><hr><h2 id="重置k8s集群配置"><a href="#重置k8s集群配置" class="headerlink" title="重置k8s集群配置"></a>重置k8s集群配置</h2><p>在master节点上执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm reset</span><br></pre></td></tr></table></figure><p>然后在node节点上执行reset,再重新添加回集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm reset</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;使用kubeadmin安装kubernetes集群文档&quot;&gt;&lt;a href=&quot;#使用kubeadmin安装kubernetes集群文档&quot; class=&quot;headerlink&quot; title=&quot;使用kubeadmin安装kubernetes集群文档&quot;&gt;&lt;/a&gt;使用kubeadmin安装kubernetes集群文档&lt;/h2&gt;&lt;h3 id=&quot;集群环境&quot;&gt;&lt;a href=&quot;#集群环境&quot; class=&quot;headerlink&quot; title=&quot;集群环境&quot;&gt;&lt;/a&gt;集群环境&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;主机名&lt;/th&gt;
&lt;th&gt;IP地址&lt;/th&gt;
&lt;th&gt;节点角色&lt;/th&gt;
&lt;th&gt;操作系统&lt;/th&gt;
&lt;th&gt;Service网段&lt;/th&gt;
&lt;th&gt;pod网段&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;k8s-m01&lt;/td&gt;
&lt;td&gt;10.111.2.58&lt;/td&gt;
&lt;td&gt;master&lt;/td&gt;
&lt;td&gt;CentOS7.6&lt;/td&gt;
&lt;td&gt;10.100.0.0/20&lt;/td&gt;
&lt;td&gt;10.96.0.0/12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k8s-n01&lt;/td&gt;
&lt;td&gt;10.111.2.56&lt;/td&gt;
&lt;td&gt;node&lt;/td&gt;
&lt;td&gt;CentOS7.6&lt;/td&gt;
&lt;td&gt;10.100.0.0/20&lt;/td&gt;
&lt;td&gt;10.96.0.0/12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;k8s-n02&lt;/td&gt;
&lt;td&gt;10.111.2.55&lt;/td&gt;
&lt;td&gt;node&lt;/td&gt;
&lt;td&gt;CentOS7.6&lt;/td&gt;
&lt;td&gt;10.100.0.0/20&lt;/td&gt;
&lt;td&gt;10.96.0.0/12&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h3 id=&quot;服务器-网络要求&quot;&gt;&lt;a href=&quot;#服务器-网络要求&quot; class=&quot;headerlink&quot; title=&quot;服务器,网络要求&quot;&gt;&lt;/a&gt;服务器,网络要求&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;service和Pod网段可以任意指定,但是不能和物理机网段,或者当前其他物理网段有冲突&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;服务器配置至少是2核2G&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>kubernets探针</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/kubernetes%E6%8E%A2%E9%92%88/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/kubernetes探针/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T10:05:50.556Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kubernets探针"><a href="#kubernets探针" class="headerlink" title="kubernets探针"></a>kubernets探针</h2><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>kubernetes一共有2种探针:</p><p><strong>存活探针</strong></p><p><strong>就绪探针</strong></p><hr><h2 id="就绪探针"><a href="#就绪探针" class="headerlink" title="就绪探针"></a>就绪探针</h2><h3 id="就绪探针工作介绍"><a href="#就绪探针工作介绍" class="headerlink" title="就绪探针工作介绍"></a>就绪探针工作介绍</h3><p>就绪探针会定期调用,检查特定的pod是否准备就绪接收客户端的请求.当容器启动时,会等待一个时间,然后执行第一次准备就绪检查.如果某个Pod没有通过探针探测,则会从服务中删除该pod,如果pod再次准备就绪,则会重新添加到Service</p><p>就绪探针确保客户端只与正常的Pod交互.</p><hr><h3 id="就绪探针的类型"><a href="#就绪探针的类型" class="headerlink" title="就绪探针的类型"></a>就绪探针的类型</h3><ul><li>Exec探针 . 使用command命令,如果命令结果返回0,则说明pod准备就绪.</li><li>HTTP GET探针. 向容器发送HTTP GET.通过响应状态码判断pod容器是否准备好</li><li>TCP socket探针.尝试连接Pod容器的TCP端口.判断pod容器的某个端口是否正常工作</li></ul><hr><a id="more"></a><h3 id="向pod容器添加探针"><a href="#向pod容器添加探针" class="headerlink" title="向pod容器添加探针"></a>向pod容器添加探针</h3><p>编辑kubia的rc配置文件,在spec.containers下添加readinessProbe配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ReplicationController</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia</span><br><span class="line">spec:</span><br><span class="line">  replicas: 4</span><br><span class="line">  selector:</span><br><span class="line">      app: kubia</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: kubia</span><br><span class="line"></span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - name: kubia</span><br><span class="line">          image: luksa/kubia</span><br><span class="line">          ports:</span><br><span class="line">          - containerPort: 8080</span><br><span class="line">          readinessProbe:</span><br><span class="line">            exec: #存活探针类型</span><br><span class="line">              command: #探针命令</span><br><span class="line">                - ls</span><br><span class="line">                - /var/ready</span><br></pre></td></tr></table></figure><blockquote><p>重新编辑rc文件后,存活探针并不会对已经存在的Pod生效.删除所有pod,等待rc重新创建</p></blockquote><p>让yaml文件生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl apply -f kubia-rc.yaml</span><br><span class="line">replicationcontroller/kubia unchanged</span><br></pre></td></tr></table></figure><p>删除Pods,重新创建后,查看pods的状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get pods</span><br><span class="line">NAME                READY   STATUS    RESTARTS   AGE</span><br><span class="line">kubia-6gvhg         0/1     Running   0          61s</span><br><span class="line">kubia-qlpdt         0/1     Running   0          61s</span><br><span class="line">kubia-w8c82         0/1     Running   0          61s</span><br><span class="line">kubia-w9spj         0/1     Running   0          61s</span><br></pre></td></tr></table></figure><p>pods的READY是0.表示pod容器虽然正在运行中,但是未准备就绪</p><p>向其中一个pod容器创建/var/ready文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl exec kubia-6gvhg -- touch /var/ready</span><br></pre></td></tr></table></figure><p>但是容器并不会马上就绪.这是因为pod默认每隔10s探测一次.通过<figure class="highlight plain"><figcaption><span>describe pod kubia-6gvhg```可以看到就绪探针的策略</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl describe pods kubia-6gvhg</p><p>Readiness:      exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1 #failure=3<br>Warning  Unhealthy  2m53s (x30 over 7m43s)  kubelet, k8s-node2  Readiness probe failed: ls: cannot access /var/ready: No such file or directory<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">10s后,可以看到就只有一个pod准备就绪</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl get pods<br>NAME                READY   STATUS    RESTARTS   AGE<br>kubia-6gvhg         1/1     Running   0          9m45s<br>kubia-qlpdt         0/1     Running   0          9m45s<br>kubia-w8c82         0/1     Running   0          9m45s<br>kubia-w9spj         0/1     Running   0          9m45s<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">通过测试发现,客户端的请求永远只转发到已经准备就绪的Pod容器内,而其他容器则不接受请求</span><br></pre></td></tr></table></figure></p><p>[work@k8s-node1 ~]$ while true;do curl <a href="http://10.96.170.37;sleep" target="_blank" rel="noopener">http://10.96.170.37;sleep</a> 1;done<br>You’ve hit kubia-6gvhg<br>You’ve hit kubia-6gvhg<br>You’ve hit kubia-6gvhg<br>You’ve hit kubia-6gvhg<br>You’ve hit kubia-6gvhg<br>You’ve hit kubia-6gvhg<br>You’ve hit kubia-6gvhg<br><code>`</code></p><p>其他两种探测类型的是否方法也类似</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kubernets探针&quot;&gt;&lt;a href=&quot;#kubernets探针&quot; class=&quot;headerlink&quot; title=&quot;kubernets探针&quot;&gt;&lt;/a&gt;kubernets探针&lt;/h2&gt;&lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;kubernetes一共有2种探针:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;存活探针&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;就绪探针&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;就绪探针&quot;&gt;&lt;a href=&quot;#就绪探针&quot; class=&quot;headerlink&quot; title=&quot;就绪探针&quot;&gt;&lt;/a&gt;就绪探针&lt;/h2&gt;&lt;h3 id=&quot;就绪探针工作介绍&quot;&gt;&lt;a href=&quot;#就绪探针工作介绍&quot; class=&quot;headerlink&quot; title=&quot;就绪探针工作介绍&quot;&gt;&lt;/a&gt;就绪探针工作介绍&lt;/h3&gt;&lt;p&gt;就绪探针会定期调用,检查特定的pod是否准备就绪接收客户端的请求.当容器启动时,会等待一个时间,然后执行第一次准备就绪检查.如果某个Pod没有通过探针探测,则会从服务中删除该pod,如果pod再次准备就绪,则会重新添加到Service&lt;/p&gt;
&lt;p&gt;就绪探针确保客户端只与正常的Pod交互.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;就绪探针的类型&quot;&gt;&lt;a href=&quot;#就绪探针的类型&quot; class=&quot;headerlink&quot; title=&quot;就绪探针的类型&quot;&gt;&lt;/a&gt;就绪探针的类型&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Exec探针 . 使用command命令,如果命令结果返回0,则说明pod准备就绪.&lt;/li&gt;
&lt;li&gt;HTTP GET探针. 向容器发送HTTP GET.通过响应状态码判断pod容器是否准备好&lt;/li&gt;
&lt;li&gt;TCP socket探针.尝试连接Pod容器的TCP端口.判断pod容器的某个端口是否正常工作&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes 高级调度</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/kubernetes%20%E9%AB%98%E7%BA%A7%E8%B0%83%E5%BA%A6/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/kubernetes 高级调度/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T08:49:19.131Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kubernetes-高级调度"><a href="#kubernetes-高级调度" class="headerlink" title="kubernetes 高级调度"></a>kubernetes 高级调度</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>本章主要介绍pod的高级调度功能.主要涵盖2方面调度特性:</p><ul><li>节点污染和pod容忍度</li><li>节点亲缘性</li></ul><hr><h3 id="一-节点污染和pod容忍度"><a href="#一-节点污染和pod容忍度" class="headerlink" title="一.节点污染和pod容忍度"></a>一.节点污染和pod容忍度</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p><strong>节点污点</strong>: 该特性用于限制哪些Pod可以被调度到某一个节点.</p><p><strong>pod容忍度</strong>:当一个Pod容忍某个节点的污点,这个pod才能被调度到该节点</p><p>默认情况下k8s集群中的master主节点就设置了污点,.这样才能保证只有控制面板等系统组件才能部署在主节点上.应用pod只能被调度到工作节点</p><hr><h4 id="显示节点污点信息"><a href="#显示节点污点信息" class="headerlink" title="显示节点污点信息"></a>显示节点污点信息</h4><a id="more"></a><p>通过<figure class="highlight plain"><figcaption><span>describe node```可以查看节点的污点信息.例如下面是master节点污点信息</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl describe node k8s-master</span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoSchedule</span><br></pre></td></tr></table></figure></p><p>master节点包含一个污点(taints).污点包含一个key,value,effect.格式为:<key>=<value>:<effect></effect></value></key></p><p>上面的污点信息包含一个<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">这个污点将阻止pod调度到这个节点上,除非有pod能容忍这个污点.通常能容忍这个污点的Pod都是kubernetes系统级别的Pod.</span><br><span class="line"></span><br><span class="line">例如观察一个Kube-system名称空间下的coredns系统级别的pod信息:</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl describe po coredns-7f9c544f75-9sh28 -n kube-system</span><br><span class="line"></span><br><span class="line">Tolerations:     CriticalAddonsOnly</span><br><span class="line">                 node-role.kubernetes.io/master:NoSchedule</span><br><span class="line">                 node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br></pre></td></tr></table></figure></p><p>该pod容忍度(tolerations)包含了4个容忍度,其中<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##### 污点效果</span><br><span class="line"></span><br><span class="line">上面列出的pod展示了2种污点effect(污点效果):```NoSchedule```和```NoExecute```.每一个污点都可以关联一个effect.下面介绍各污点效果.</span><br><span class="line"></span><br><span class="line">* **NoSechedule**: 如果pod没有容忍这些污点,这pod不能被调度到包含这个污点的节点</span><br><span class="line">* **PreferNoSechedule** NoSechedule的宽松版本,表示尽量阻止pod被调度到这个节点.但是如果没有其他节点可以调度,pod依然会被调度到这个节点</span><br><span class="line">* **NoExecute** 上面两者只在调度期间起作用,而NoExecute也会影响正在节点上运行中的pod.如果在一个节点上添加了NoExecute污点.那么在这个节点上正在运行的pod如果没有容忍这个污点,会被这个节点驱除</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 添加自定义污点</span><br><span class="line"></span><br><span class="line">如果一个单独K8s集群上同时有生产环境和非生产环境的应用.那么可以在生产环境上添加自定义污点来防止其他环境的Pod调度到生产节点.</span><br><span class="line"></span><br><span class="line">添加污点命令:```kubectl taint</span><br></pre></td></tr></table></figure></p><p>命令格式: <figure class="highlight plain"><figcaption><span>taint node NodeName key</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">例如下面命令将node1的节点添加一个key为```node-type```.value为```production```.效果为```NoSchedule```的污点</span><br></pre></td></tr></table></figure></p><p>root@k8s-master ~]# kubectl taint node k8s-node1 node-type=production:NoSchedule<br>node/k8s-node1 tainted<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">下面部署一个常规的多副本pod.此时所有pod都被调度到k8s-node2节点上.无法调度到node1</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl run test --image busybox --replicas 5 -- sleep 999</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl get pods -o wide  </span><br><span class="line">test-69c6778cfb-2jgbn           1/1     Running   0          4m38s   10.100.169.188   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">test-69c6778cfb-7lbnw           1/1     Running   0          4m38s   10.100.169.187   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">test-69c6778cfb-c4vrx           1/1     Running   0          4m38s   10.100.169.190   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">test-69c6778cfb-grkhc           1/1     Running   0          4m38s   10.100.169.185   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">test-69c6778cfb-xc2sp           1/1     Running   0          4m38s   10.100.169.130   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure></p><hr><h4 id="在pod上添加污点容忍度"><a href="#在pod上添加污点容忍度" class="headerlink" title="在pod上添加污点容忍度"></a>在pod上添加污点容忍度</h4><p>如果想让pod部署到node1这个已经打了<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: prod</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">       node-type: production</span><br><span class="line">  replicas: 5</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">       name: taint-kubia-v1</span><br><span class="line">       labels:</span><br><span class="line">         node-type: production</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">           - image: busybox</span><br><span class="line">             command: [&quot;sleep&quot;]</span><br><span class="line">             args: [&quot;999999&quot;]</span><br><span class="line">             name: kubia-busybox</span><br><span class="line">      tolerations:</span><br><span class="line">        - key: node-type</span><br><span class="line">          operator: Equal</span><br><span class="line">          value: production</span><br><span class="line">          effect: NoSchedule</span><br></pre></td></tr></table></figure></p><p>在pod的<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl get pods -o wide</span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE    IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">prod-59b4554db6-d24d5           1/1     Running   0          24s    10.100.169.191   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-59b4554db6-kmzj2           1/1     Running   0          24s    10.100.36.114    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-59b4554db6-nlc8s           1/1     Running   0          24s    10.100.36.116    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-59b4554db6-pvzrw           1/1     Running   0          24s    10.100.36.115    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-59b4554db6-t8n5c           1/1     Running   0          24s    10.100.169.131   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure></p><hr><h4 id="在node节点添加污点-驱逐正在该节点上运行的pod"><a href="#在node节点添加污点-驱逐正在该节点上运行的pod" class="headerlink" title="在node节点添加污点,驱逐正在该节点上运行的pod"></a>在node节点添加污点,驱逐正在该节点上运行的pod</h4><p>当前在node2节点上运行了以下pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide  | grep k8s-node2</span></span><br><span class="line">cloud-busybox-99c65b774-6bbbc   1/1     Running   80         3d8h    10.100.169.183   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">cloud-busybox-99c65b774-zcw78   1/1     Running   80         3d8h    10.100.169.182   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">curl-custome-sa                 2/2     Running   0          8d      10.100.169.177   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-59b4554db6-d24d5           1/1     Running   4          9m49s   10.100.169.191   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-59b4554db6-t8n5c           1/1     Running   4          9m49s   10.100.169.131   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-1          1/1     Running   0          10d     10.100.169.172   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-2jgbn           1/1     Running   80         22h     10.100.169.188   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-7lbnw           1/1     Running   80         22h     10.100.169.187   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-c4vrx           1/1     Running   80         22h     10.100.169.190   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-grkhc           1/1     Running   80         22h     10.100.169.185   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-xc2sp           1/1     Running   80         22h     10.100.169.130   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>如果想要在node2节点上打上污点信息,只允许non-production环境的pod才能调度到该节点上.可以直接打上污点标签</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl taint node k8s-node2 node-type=non-production:NoExecute</span></span><br><span class="line">node/k8s-node2 tainted</span><br><span class="line"></span><br><span class="line"><span class="comment">#此时该节点上的所有pod都被终止</span></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide  | grep k8s-node2</span></span><br><span class="line">cloud-busybox-99c65b774-6bbbc   1/1     Terminating         80         3d8h   10.100.169.183   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">cloud-busybox-99c65b774-zcw78   1/1     Terminating         80         3d8h   10.100.169.182   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">curl-custome-sa                 2/2     Terminating         0          8d     10.100.169.177   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-1          1/1     Terminating         0          10d    10.100.169.172   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-2jgbn           1/1     Terminating         80         22h    10.100.169.188   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-7lbnw           1/1     Terminating         80         22h    10.100.169.187   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-c4vrx           1/1     Terminating         80         22h    10.100.169.190   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-grkhc           1/1     Terminating         80         22h    10.100.169.185   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-xc2sp           1/1     Terminating         80         22h    10.100.169.130   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">#所有pod都被调度到node1节点.(注意有一个运行在Node2上的statefulset类型的Pod被终止了,由于statefulset的特性,该pod不会被自动调度到其他节点,除非手动强制删除)</span></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide</span></span><br><span class="line">NAME                            READY   STATUS        RESTARTS   AGE     IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">cloud-busybox-99c65b774-j5ggn   0/1     Pending       0          144m    &lt;none&gt;           &lt;none&gt;      &lt;none&gt;           &lt;none&gt;</span><br><span class="line">cloud-busybox-99c65b774-ksj8x   1/1     Running       82         3d10h   10.100.36.103    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">cloud-busybox-99c65b774-qtzv9   0/1     Pending       0          144m    &lt;none&gt;           &lt;none&gt;      &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-6d8fdb7654-9xlqq           1/1     Running       0          7m23s   10.100.36.109    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-6d8fdb7654-cwsfp           1/1     Running       0          7m23s   10.100.36.108    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-6d8fdb7654-gbc79           1/1     Running       0          7m10s   10.100.36.119    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-6d8fdb7654-lsjc4           1/1     Running       0          7m15s   10.100.36.110    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-6d8fdb7654-mcmt7           1/1     Running       0          7m23s   10.100.36.111    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-0          1/1     Running       0          10d     10.100.36.101    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-1          0/1     Terminating   0          10d     10.100.169.172   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-2          1/1     Running       0          10d     10.100.36.97     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-3          1/1     Running       0          10d     10.100.36.99     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><hr><h4 id="了解污点和污点容忍度使用场景"><a href="#了解污点和污点容忍度使用场景" class="headerlink" title="了解污点和污点容忍度使用场景"></a>了解污点和污点容忍度使用场景</h4><p>​       节点可以拥有多个污点信息,而pod也可以有多个污点容忍度.污点容忍度可以通过操作符<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 二.节点亲缘性(node affinity)</span><br><span class="line"></span><br><span class="line">污点可以让pod远离特定的节点.节点亲缘性是一种更新的机制.这种机制允许Kubernetes将pod只调度到某个节点上面</span><br><span class="line"></span><br><span class="line">与节点选择器类似,每个pod可以定义自己的节点亲缘性规则,这些规则可以允许你指定硬性限制或者偏好.如果指定一种偏好的话,你将告知Kubernetes对于某个特定的pod.它更倾向于调度到某些节点上.如果没法实现的话,pod将被调度到其他节点.</span><br><span class="line"></span><br><span class="line">##### 检查默认node节点标签:</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl describe node k8s-node2</span><br><span class="line">Name:               k8s-node2</span><br><span class="line">Roles:              &lt;none&gt;</span><br><span class="line">#Labels表示该节点拥有的默认的标签.</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/arch=amd64</span><br><span class="line">                    kubernetes.io/hostname=k8s-node2</span><br><span class="line">                    kubernetes.io/os=linux</span><br><span class="line">Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock</span><br><span class="line">                    node.alpha.kubernetes.io/ttl: 0</span><br><span class="line">                    projectcalico.org/IPv4Address: 172.16.20.253/24</span><br><span class="line">                    projectcalico.org/IPv4IPIPTunnelAddr: 10.100.169.128</span><br><span class="line">                    volumes.kubernetes.io/controller-managed-attach-detach: true</span><br></pre></td></tr></table></figure></p><h5 id="给节点打一个标签"><a href="#给节点打一个标签" class="headerlink" title="给节点打一个标签."></a>给节点打一个标签.</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#给node1节点打一个标签.disk=ssd</span><br><span class="line">[root@k8s-master ~]# kubectl label node k8s-node1 disk=ssd</span><br></pre></td></tr></table></figure><h5 id="回顾nodeSelector节点选择器"><a href="#回顾nodeSelector节点选择器" class="headerlink" title="回顾nodeSelector节点选择器"></a>回顾nodeSelector节点选择器</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下面是一个deployment的配置文件,Pod指定了标签选择器</span></span><br><span class="line"><span class="string">[root@k8s-master</span> <span class="string">~]#</span> <span class="string">cat</span> <span class="string">node-affinity-nodeslector.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kubia-nodeselector-ssd</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">4</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">     matchLabels:</span></span><br><span class="line"><span class="attr">        app:</span>  <span class="string">kubia-nodeselector</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">kubia-nodeselector-ssd</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">         app:</span>  <span class="string">kubia-nodeselector</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      nodeSelector:</span></span><br><span class="line"><span class="attr">         disk:</span> <span class="string">ssd</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - image:</span> <span class="string">luksa/kubia</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">kubia-nodeselector-ssd</span></span><br></pre></td></tr></table></figure><p>所有节点都调度到符合这个标签的node1节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide | grep kubia-nodeselector-ssd</span></span><br><span class="line">kubia-nodeselector-ssd-7d5b94f9bd-cqmsv   1/1     Running       0          2m2s    10.100.36.124    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-nodeselector-ssd-7d5b94f9bd-djkxr   1/1     Running       0          2m2s    10.100.36.125    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-nodeselector-ssd-7d5b94f9bd-htggw   1/1     Running       0          2m2s    10.100.36.126    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-nodeselector-ssd-7d5b94f9bd-vm2rv   1/1     Running       0          2m2s    10.100.36.127    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><hr><h4 id="节点亲缘性配置"><a href="#节点亲缘性配置" class="headerlink" title="节点亲缘性配置"></a>节点亲缘性配置</h4><p>将上面的节点标签选择器换成用节点亲缘性来表达</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-master</span> <span class="string">~]#</span> <span class="string">cat</span>  <span class="string">node-affinity-disk-ssd.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kubia-nodeaffinity-ssd</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">4</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">     matchLabels:</span></span><br><span class="line"><span class="attr">        app:</span>  <span class="string">kubia-nodeaffinity</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">kubia-nodeaffinity</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">         app:</span>  <span class="string">kubia-nodeaffinity</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        nodeAffinity:</span></span><br><span class="line"><span class="attr">          requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line"><span class="attr">             nodeSelectorTerms:</span></span><br><span class="line"><span class="attr">               - matchExpressions:</span></span><br><span class="line"><span class="attr">                    - key:</span> <span class="string">disk</span></span><br><span class="line"><span class="attr">                      operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">                      values:</span></span><br><span class="line"><span class="bullet">                         -</span> <span class="string">ssd</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - image:</span> <span class="string">luksa/kubia</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">kubia-nodeselector-ssd</span></span><br></pre></td></tr></table></figure><p>第一印象是节点亲缘性写法比节点选择器要复杂的多(这是因为节点亲缘性的表达能力更强).</p><h5 id="节点亲缘性属性"><a href="#节点亲缘性属性" class="headerlink" title="节点亲缘性属性"></a>节点亲缘性属性</h5><p>pod的<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* **nodeAffinity** 顾名思义,节点亲缘性调度规则</span><br><span class="line">* **podAffinity**  Pod亲缘性调度规则,例如某类节点调度到同一个节点,地域,可用区</span><br><span class="line">* **podAntiAffinity** 和podAffinity完全相反,pod反亲缘性调度规则,</span><br><span class="line"></span><br><span class="line">在这个例子中石油了**nodeAffinity**表示节点亲缘性规则.在这个规则下有一个极其长的属性.把这个属性的名字分成两段,然后分别看下他们的含义:</span><br><span class="line"></span><br><span class="line">* **requiredDuringScheduling** 表明该字段下定义的规则.为了让pod能够调度到该节点,明确指出该节点必须包含的标签.重点包含以下2个条件.也就是说在pod调度期间,节点必须要包含标签:</span><br><span class="line">  - **required**(必须),</span><br><span class="line">  - **DuringScheduling**(在pod调度期间)</span><br><span class="line">* ...**IgnoredDuringExecution** 表示该规则不会影响已经在节点上运行的pod.重点包含以下2个条件.也就是说无论节点是否具备某个标签,或者无论pod怎么调度,都不会影响已经运行的Pod</span><br><span class="line">  - **Ignored**(忽略)</span><br><span class="line">  - **DuringExecution**(在pod已运行期间)</span><br><span class="line"></span><br><span class="line">接下来的字段属性比较容易理解,</span><br><span class="line"></span><br><span class="line">* **nodeSelectorTerms**: 对象类型,表示一组节点选择器项列表</span><br><span class="line">* **matchExpressions**: 匹配表达式,也是一个对象类型.包含以下属性</span><br><span class="line">  - **key**: 必要字段.</span><br><span class="line">  - **operator**: 必要字段,key和value关系的表达式,有**In**,**NotIn**,**Exists**,**DoesNotExist**,**Gt**,**Lt**</span><br><span class="line">  - **values**: key的值,是一个字符串的数组格式.</span><br><span class="line"></span><br><span class="line">&gt; 通过```kubectl explain pods.spec.affinity```可以查询属性字段的含义以及配置用法</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### pod调度节点优先</span><br><span class="line"></span><br><span class="line">节点亲缘性还有一个属性```preferredDuringSchedulingIgnoredDuringExecution```这个属性字段指定调度器优先考虑哪些节点.</span><br><span class="line"></span><br><span class="line">想象一下如果你在阿里云同一地域下的多个可用区都有部署节点服务器,你想要将Pod优先部署在zone1,并且是部署在专有服务器上.如果zone1下没有足够的服务器资源,那么部署到其他可用区(zone2)也是可以接受的.那么节点亲缘性就可以实现这种功能.</span><br><span class="line"></span><br><span class="line">##### 给节点加上可用区等信息标签</span><br><span class="line"></span><br><span class="line">下面的配置中给2个节点加上标签,模拟他们在不同的可用区服务器,以及属于不同的服务器类型(专用和共享)</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl label node k8s-node1 zone=zone1</span><br><span class="line">node/k8s-node1 labeled</span><br><span class="line">[root@k8s-master ~]# kubectl label node k8s-node1 share-type=dedicated</span><br><span class="line">node/k8s-node1 labeled</span><br><span class="line">[root@k8s-master ~]# kubectl label node k8s-node2 share-type=shared</span><br><span class="line">node/k8s-node2 labeled</span><br><span class="line">[root@k8s-master ~]# kubectl label node k8s-node2 zone=zone2</span><br><span class="line">node/k8s-node2 labeled</span><br><span class="line">[root@k8s-master ~]#</span><br></pre></td></tr></table></figure></p><h5 id="指定优先级节点亲缘性规则"><a href="#指定优先级节点亲缘性规则" class="headerlink" title="指定优先级节点亲缘性规则"></a>指定优先级节点亲缘性规则</h5><p>下面创建一个deployment资源,pod节点指定了优先调度到zone1区域(权重80),次优先调度到dedicated服务器(权重20)</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">node-affinity-weight</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">       app:</span> <span class="string">node-affinity-weight</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">node-affinity-weight</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">         app:</span> <span class="string">node-affinity-weight</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - image:</span> <span class="string">luksa/kubia</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">kubia</span></span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        nodeAffinity:</span></span><br><span class="line">          <span class="comment">#优先调度到某个节点,注意不是必须要调度到该节点</span></span><br><span class="line"><span class="attr">          preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">             <span class="comment">#权重越高,优先级越高,优先调度到zone1</span></span><br><span class="line"><span class="attr">              - weight:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">                preference:</span></span><br><span class="line"><span class="attr">                  matchExpressions:</span></span><br><span class="line"><span class="attr">                   - key:</span> <span class="string">zone</span></span><br><span class="line"><span class="attr">                     operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">                     values:</span></span><br><span class="line"><span class="bullet">                        -</span> <span class="string">zone1</span></span><br><span class="line"></span><br><span class="line">             <span class="comment">#如果以上需求不能满足,则调度到满足以下标签的节点</span></span><br><span class="line"><span class="attr">              - weight:</span> <span class="number">20</span></span><br><span class="line"><span class="attr">                preference:</span></span><br><span class="line"><span class="attr">                  matchExpressions:</span></span><br><span class="line"><span class="attr">                    - key:</span> <span class="string">share-type</span></span><br><span class="line"><span class="attr">                      operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">                      values:</span></span><br><span class="line"><span class="bullet">                        -</span> <span class="string">dedicated</span></span><br></pre></td></tr></table></figure><p>此时大部分Pod都优先部署到node1节点.之所以不是所有Pod都被调度到Node1节点是因为除了节点亲缘性优先级外还有其他的优先级函数来决定节点被调度到哪.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide</span></span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">node-affinity-weight-7777f464d6-8h9qq   1/1     Running   0          33s   10.100.36.75     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-8xchv   1/1     Running   0          33s   10.100.169.132   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-bnsvs   1/1     Running   0          33s   10.100.36.79     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-bxbpb   1/1     Running   0          33s   10.100.36.71     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-bznfw   1/1     Running   0          33s   10.100.36.78     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-cgdfq   1/1     Running   0          33s   10.100.36.77     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-flvcf   1/1     Running   0          33s   10.100.169.135   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-pb666   1/1     Running   0          33s   10.100.169.134   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-tqd5k   1/1     Running   0          33s   10.100.36.74     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-vpknb   1/1     Running   0          33s   10.100.36.76     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><hr><h3 id="pod硬亲缘性"><a href="#pod硬亲缘性" class="headerlink" title="pod硬亲缘性"></a>pod硬亲缘性</h3><p>上一节已经了解节点亲缘性规则是如何影响Pod被调度到哪个节点的.这些规则只影响了Pod和节点之前的亲缘性.但是有时候也希望有能力指定pod自身之间的亲缘性.</p><p>例如,有一个前端pod,和一个后端Pod,我们期望将前端Pod部署到和后端pod同一个节点,降低前后端pod之间的访问延时,提高应用性能.使用Pod亲缘性可以满足这种场景的需求.</p><blockquote><p> pod亲缘性和节点亲缘性区别是.pod亲缘性不关心pod被调度到具体哪个节点,而是只要求被调度到其他某组pod同一个节点上.</p></blockquote><p>在下面的例子中部署一个1后端pod.和5个前端pod.使用节点亲缘性将5个前端Pod部署到后端pod相同的节点上</p><h5 id="部署一个后端pod"><a href="#部署一个后端pod" class="headerlink" title="部署一个后端pod"></a>部署一个后端pod</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#启动一个backend的pod.指定一个标签</span><br><span class="line">[root@k8s-master ~]# kubectl run backend -l app=backend --image busybox -- sleep 999999</span><br></pre></td></tr></table></figure><h5 id="部署前端Pod亲缘性"><a href="#部署前端Pod亲缘性" class="headerlink" title="部署前端Pod亲缘性"></a>部署前端Pod亲缘性</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">pod-affinity</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">       app:</span> <span class="string">frontend</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">luksa/kubia</span></span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line">        <span class="comment">#节点亲缘性</span></span><br><span class="line"><span class="attr">        podAffinity:</span></span><br><span class="line">         <span class="comment">#一个required强制性要求而不是prefered</span></span><br><span class="line"><span class="attr">          requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">               <span class="comment">#此pod必须被调度到满足以下条件的节点.该节点运行了app:backend标签的Pod</span></span><br><span class="line"><span class="attr">             - topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">               <span class="comment">#满足以下标签选择器的pod</span></span><br><span class="line"><span class="attr">               labelSelector:</span></span><br><span class="line"><span class="attr">                  matchLabels:</span></span><br><span class="line"><span class="attr">                      app:</span> <span class="string">backend</span></span><br></pre></td></tr></table></figure><p>代码清单显示要求将pod调度到和其他包含app=backend标签的Pod所在的相同节点上(通过topologyKey字段指定)</p><blockquote><p>除了使用matchLabels字段外,也可以使用表达能力更强的matchExpressions字段</p></blockquote><p>此时所有的前端pod都被部署到后端Pod同一个节点,没有被部署到其他节点(node1)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide</span></span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE     IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">backend-7d4c66b6b5-4mhpl        1/1     Running   0          43m     10.100.169.133   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-694cf9455c-6h2dq   1/1     Running   0          2m46s   10.100.169.139   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-694cf9455c-jtmd8   1/1     Running   0          2m46s   10.100.169.140   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-694cf9455c-p6m7d   1/1     Running   0          2m46s   10.100.169.141   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-694cf9455c-qbr4c   1/1     Running   0          2m46s   10.100.169.137   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-694cf9455c-vfp84   1/1     Running   0          2m46s   10.100.169.138   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><blockquote><p>有趣的是,如果现在删除了后端Pod,调度器会将后端pod还是调度到node2.即便后端pod没有定义任何Pod亲缘性规则.</p><p>这种设置很合理,因为如何后端Pod被调度到其他节点,前端pod的亲缘性规则就被打破了</p></blockquote><h5 id="topologyKey"><a href="#topologyKey" class="headerlink" title="topologyKey"></a>topologyKey</h5><p>在上面的例子中topology属性为kubernetes.io/hostname.除此之外还有其他属性:</p><ul><li><strong>failure-domain.beta.kubernetes.io/zone</strong> 云服务提供商不同的不同可用区</li><li><strong>failure-domain.beta.kubernetes.io/region</strong> 云服务提供商不同的不同地域</li><li>自定义键</li></ul><hr><h3 id="pod软亲缘性"><a href="#pod软亲缘性" class="headerlink" title="pod软亲缘性"></a>pod软亲缘性</h3><p>pod软亲缘性稍微的区别就是没有硬性要求一定要部署到某个节点.拿前面一个例子来说,软亲缘性只是优先将前端pod调度到和后端Pod相同的节点,但是如果条件不满足要求,也可以调度到其他节点</p><p>软亲缘性的配置文件和硬亲缘性的主要区别就在于.<strong>required</strong>开头的属性是硬性要求.<strong>preferred</strong>是软要求</p><p>下面将之前的前端pod的deployment删除,然后部署下面的软podAffinity</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">pod-affinity-frontend-preferred</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">       app:</span> <span class="string">frontend</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">     metadata:</span></span><br><span class="line"><span class="attr">       name:</span> <span class="string">pod-affinity-frontend-preferred</span></span><br><span class="line"><span class="attr">       labels:</span></span><br><span class="line"><span class="attr">         app:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">     spec:</span></span><br><span class="line"><span class="attr">       containers:</span></span><br><span class="line"><span class="attr">         - name:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">           image:</span> <span class="string">luksa/kubia</span></span><br><span class="line"><span class="attr">       affinity:</span></span><br><span class="line"><span class="attr">          podAffinity:</span></span><br><span class="line"><span class="attr">             preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line"><span class="attr">                - weight:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">                  podAffinityTerm:</span></span><br><span class="line"><span class="attr">                      topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line"><span class="attr">                      labelSelector:</span></span><br><span class="line"><span class="attr">                        matchExpressions:</span></span><br><span class="line"><span class="attr">                            - key:</span> <span class="string">app</span></span><br><span class="line"><span class="attr">                              operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">                              values:</span></span><br><span class="line"><span class="bullet">                                -</span> <span class="string">backend</span></span><br></pre></td></tr></table></figure><p>此时看到几乎所有的前端pod都部署到和后端Pod相同的节点上.但是其中还是有一个pod被部署到node1.</p><p>正如<strong>nodeAffinity</strong>的软亲缘性一样.kubernetes不会将所有的Pod都完全按照要求部署到同一个节点,还有其他优先调度函数在起作用,将pod调度到其他节点.</p><p>这样的设置是有道理的,因为Kubernets考虑到如果该节点出现宕机故障会导致整个应用不可用.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide</span></span><br><span class="line">NAME                                               READY   STATUS    RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">backend-7d4c66b6b5-hvkph                           1/1     Running   0          41m   10.100.169.142   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-frontend-preferred-7b7b9fb784-4jc9r   1/1     Running   0          17s   10.100.36.80     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-frontend-preferred-7b7b9fb784-bd97q   1/1     Running   0          17s   10.100.169.146   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-frontend-preferred-7b7b9fb784-rfgp7   1/1     Running   0          17s   10.100.169.145   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-frontend-preferred-7b7b9fb784-s7qtc   1/1     Running   0          17s   10.100.169.143   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-frontend-preferred-7b7b9fb784-skhhq   1/1     Running   0          17s   10.100.169.144   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><hr><h3 id="pod非亲缘性"><a href="#pod非亲缘性" class="headerlink" title="pod非亲缘性"></a>pod非亲缘性</h3><p>和pod亲缘性完全相反,pod非亲缘性让两组pod远离彼此,不被调度到同一个节点.这种场景比较常见.例如当2个集合的pod调度到同一个节点上会影响彼此的性能.</p><p>pod非亲缘性和pod亲缘性配置几乎一模一样,唯一的区别就是podAntiAffinity替代podAffinity</p><p>删除之前的例子,使用下面的Pod配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">pod-antiaffinity</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">       app:</span> <span class="string">frontend</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">luksa/kubia</span></span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line">        <span class="comment">#节点亲缘性</span></span><br><span class="line"><span class="attr">        podAntiAffinity:</span></span><br><span class="line">         <span class="comment">#一个required强制性要求而不是prefered</span></span><br><span class="line"><span class="attr">          requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">               <span class="comment">#此pod必须不被调度到运行了app:backend标签Pod的节点</span></span><br><span class="line"><span class="attr">             - topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">               <span class="comment">#满足以下标签选择器的pod</span></span><br><span class="line"><span class="attr">               labelSelector:</span></span><br><span class="line"><span class="attr">                  matchLabels:</span></span><br><span class="line"><span class="attr">                      app:</span> <span class="string">backend</span></span><br></pre></td></tr></table></figure><p>前端的所有pod都被调度到和后端pod不一样的节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide</span></span><br><span class="line">NAME                                READY   STATUS    RESTARTS   AGE    IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">backend-7d4c66b6b5-hvkph            1/1     Running   0          168m   10.100.169.142   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-antiaffinity-5b9fd9846c-k5flw   1/1     Running   0          61s    10.100.36.81     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-antiaffinity-5b9fd9846c-t4l8z   1/1     Running   0          61s    10.100.36.89     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-antiaffinity-5b9fd9846c-wp5mf   1/1     Running   0          61s    10.100.36.83     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-antiaffinity-5b9fd9846c-xkqhm   1/1     Running   0          61s    10.100.36.82     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-antiaffinity-5b9fd9846c-xzg7t   1/1     Running   0          61s    10.100.36.84     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>pod软非亲缘性就不再介绍了</p><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>如果节点上添加了1个污点信息.除非Pod容忍这些污点,否则pod不会被调度到该节点</li><li>有3种类型的污点:<code>Noschdule</code>完全阻止pod调度,<code>PreferNoSchedule</code>不完全阻止,<code>NoExecute</code>将已经在运行的Pod从节点驱逐</li><li><code>NoExecute</code>污点的节点可以设置等待时间,当节点不可用时,pod重新调度时,最长等待时间</li><li>节点亲缘性允许指定pod应该被调度到哪些节点.有硬性(<strong>required</strong>)要求,也有软性优先级(<strong>preferred</strong>)</li><li>pod亲缘性用于将pod调度到和另一个pod相同的一个节点.(基于pod的标签)</li><li>pod亲缘性的topologyKey属性表示了被调度的pod和另一组pod的距离.(在同一个节点,同一个机柜,同一个可用区,或者同一个地域)</li><li>pod非亲缘性和亲缘性相反,用于将pod调度到远离某组pod的节点</li><li>无论是节点亲缘性还是pod亲缘性可以设置是硬性要求还是软性优选选择</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kubernetes-高级调度&quot;&gt;&lt;a href=&quot;#kubernetes-高级调度&quot; class=&quot;headerlink&quot; title=&quot;kubernetes 高级调度&quot;&gt;&lt;/a&gt;kubernetes 高级调度&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;本章主要介绍pod的高级调度功能.主要涵盖2方面调度特性:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点污染和pod容忍度&lt;/li&gt;
&lt;li&gt;节点亲缘性&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&quot;一-节点污染和pod容忍度&quot;&gt;&lt;a href=&quot;#一-节点污染和pod容忍度&quot; class=&quot;headerlink&quot; title=&quot;一.节点污染和pod容忍度&quot;&gt;&lt;/a&gt;一.节点污染和pod容忍度&lt;/h3&gt;&lt;h4 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;节点污点&lt;/strong&gt;: 该特性用于限制哪些Pod可以被调度到某一个节点.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;pod容忍度&lt;/strong&gt;:当一个Pod容忍某个节点的污点,这个pod才能被调度到该节点&lt;/p&gt;
&lt;p&gt;默认情况下k8s集群中的master主节点就设置了污点,.这样才能保证只有控制面板等系统组件才能部署在主节点上.应用pod只能被调度到工作节点&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&quot;显示节点污点信息&quot;&gt;&lt;a href=&quot;#显示节点污点信息&quot; class=&quot;headerlink&quot; title=&quot;显示节点污点信息&quot;&gt;&lt;/a&gt;显示节点污点信息&lt;/h4&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>harbor私有仓库部署</title>
    <link href="https://jesse.top/2020/06/26/docker/harbor%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E9%83%A8%E7%BD%B2/"/>
    <id>https://jesse.top/2020/06/26/docker/harbor私有仓库部署/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T03:03:12.763Z</updated>
    
    <content type="html"><![CDATA[<h2 id="harbor私有仓库部署"><a href="#harbor私有仓库部署" class="headerlink" title="harbor私有仓库部署"></a>harbor私有仓库部署</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>harbor是docker的私有仓库,可以部署在局域网服务器上,用来管理docker镜像.虽然docker官方也提供公共镜像仓库,但是由于是境外网站,拉取镜像速度非常慢,而且有被墙的可能.部署私有仓库非常有必要</p><p>harbor是vmware公司开源的企业级的docker registry管理项目.</p><blockquote><p>处于数据脱敏需要,以下内容中隐藏了真实域名.而使用hub.xxxxxx.com替代</p></blockquote><hr><h3 id="社区"><a href="#社区" class="headerlink" title="社区"></a>社区</h3><p>harbor github: <a href="https://github.com/goharbor/harbor" target="_blank" rel="noopener">goharbor/harbor</a></p><p>官网文档介绍: <a href="https://goharbor.io/docs/1.10/" target="_blank" rel="noopener">harbor doc</a></p><p>在部署中遇到的各种坑,都可以通过查阅文档,或者搜索github的issue解决</p><hr><h3 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h3><p>harbor是docker-compose部署的.包括一系列组件:nginx.core,log,register等等.</p><p>但是由于本机已经存在一个Nginx镜像.所以用Nginx代理到harbor的Nginx.<strong>如果是独立的服务器部署Harbor的话,则不会存在这个问题,可以直接跳过这一章节.</strong></p><p>nginx代理框架大概是:</p><p><strong>nginx—-&gt;harbor-nginx—–&gt;habor</strong></p><p>由于docker提交镜像需要Https协议,所以:</p><p><strong>nginx—301跳转到nginx https—–&gt;harbor-nginx http—-&gt; habor</strong></p><p>但是这样的部署方式,有一个问题:</p><p>私有仓库可以正常login但是push镜像的时候,又提示未验证.</p><p>该问题尝试过很多解决方案,但是均无法解决</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-docker ~]# docker login --username=admin -pHarbor12345 hub.xxxxxx.com</span><br><span class="line">WARNING! Using --password via the CLI is insecure. Use --password-stdin.</span><br><span class="line">WARNING! Your password will be stored unencrypted in /root/.docker/config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/login/#credentials-store</span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br><span class="line">[root@idc-function-docker ~]# docker push hub.xxxxxx.com/master/nginx:latest</span><br><span class="line">The push refers to repository [hub.xxxxxx.com/master/nginx]</span><br><span class="line">d37eecb5b769: Pushing [==================================================&gt;]  3.584kB</span><br><span class="line">99134ec7f247: Preparing</span><br><span class="line">c3a984abe8a8: Preparing</span><br><span class="line">unauthorized: authentication required</span><br></pre></td></tr></table></figure><p>所以现在的架构是</p><p><strong>nginx—301跳转到Nginx https——&gt; harbor-nginx https——&gt;harbor</strong></p><hr><h3 id="harbor部署前提条件"><a href="#harbor部署前提条件" class="headerlink" title="harbor部署前提条件"></a>harbor部署前提条件</h3><ul><li>安装docker-ce</li><li>安装docker-composer</li><li>准备一个空目录.比如/data.或者/data/harbor (注意,最好是空目录,不要和其他项目混杂一起)</li><li>安装好https域名证书</li></ul><hr><h3 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h3><p>docker和docker-compose的安装就略过了.这里提一句,我用的是acme.sh部署letsencrypt的证书</p><p>接下来开始部署harbor</p><ul><li>1.去github下载离线安装包.离线安装包虽然比较大,但是安装过程快速,且不会中断</li></ul><p>这里安装的是最新版,v1.10.1.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在github下载 harbor-offline-installer-v1.10.1.tgz</span><br><span class="line">tar xvf 解压</span><br></pre></td></tr></table></figure><ul><li>2.解压后,进入harbor文件,编辑harbor.yaml配置文件.需要改动以下几个地方</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">hostname hub.xxxxxx.com  #定义主机名,可以使用IP,也可以用域名</span><br><span class="line"></span><br><span class="line">#定义https的服务器证书和秘钥的文件路径</span><br><span class="line">https:</span><br><span class="line">  # https port for harbor, default is 443</span><br><span class="line">  port: 443</span><br><span class="line">  # The path of cert and key files for nginx</span><br><span class="line">  certificate: /data/letsencrypt/hub.xxxxxx.com/fullchain.cer</span><br><span class="line">  private_key: /data/letsencrypt/hub.xxxxxx.com/hub.xxxxxx.com.key</span><br><span class="line">  </span><br><span class="line">#这是harbor命令行和浏览器登陆的初始密码..用户名是admin</span><br><span class="line">harbor_admin_password: Harbor12345</span><br><span class="line"></span><br><span class="line">#这是数据目录,最好是一个空目录</span><br><span class="line">data_volume: /data/apps/harbor</span><br><span class="line"></span><br><span class="line">#日志文件保存路径</span><br><span class="line">log:</span><br><span class="line">    location: /data/logs/harbor</span><br></pre></td></tr></table></figure><ul><li>3.执行install.sh文件.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#该脚本会检查主机的环境,拉取相关镜像.以及根据配置文件生成docker-compose文件.</span><br><span class="line">[root@idc-function-docker harbor]#./install.sh</span><br></pre></td></tr></table></figure><blockquote><p> 这个脚本执行到最后会报错,提示80端口和nginx容器已经被占用了.但是没关系.</p></blockquote><ul><li>4.修改docker-composer文件.(如果本机上没有nginx或者80端口没有被占用,这一步可以不做)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#将Nginx容器名改成hub-nginx</span><br><span class="line">#将80和443的端口映射修改一下,比如我这里</span><br><span class="line">proxy:</span><br><span class="line">    image: goharbor/nginx-photon:v1.10.1</span><br><span class="line">    container_name: hub-nginx</span><br><span class="line">ports:</span><br><span class="line">      - 8081:8080</span><br><span class="line">      - 4443:8443</span><br></pre></td></tr></table></figure><ul><li><ol start="5"><li>启动docker-compose</li></ol></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-docker harbor]#docker-compose up -d</span><br></pre></td></tr></table></figure><ul><li><ol start="6"><li>宿主机的nginx开启代理. (如果不是采用nginx代理的话,可以忽略这一步)</li></ol></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">#我这里是有一个独立的Nginx容器</span><br><span class="line">[root@idc-function-docker harbor]# cat/data/conf/nginx/conf.d/dwd-docker-hub.conf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line"></span><br><span class="line">  server_name hub.xxxxxx.com;</span><br><span class="line">  listen 443 ssl http2;</span><br><span class="line">  ssl_certificate  /data/letsencrypt/hub.xxxxxx.com/fullchain.cer;</span><br><span class="line">  ssl_certificate_key /data/letsencrypt/hub.xxxxxx.com/hub.xxxxxx.com.key;</span><br><span class="line">  include /data/letsencrypt/options-ssl-nginx.conf; # managed by Certbot</span><br><span class="line">  ssl_dhparam /data/letsencrypt/ssl-dhparams.pem; # managed by Certbot</span><br><span class="line"></span><br><span class="line">  add_header      Strict-Transport-Security &quot;max-age=31536000&quot; always;</span><br><span class="line">  location / &#123;</span><br><span class="line"></span><br><span class="line">    proxy_pass https://172.16.20.30:4443; #代理到宿主机的4443端口,宿主机会将4443代理到hub-docer容器的443端口</span><br><span class="line">    client_max_body_size 2000m; #这里要定义大一点,否则提交镜像的时候,会提示 413 Request Entity Too Large</span><br><span class="line">                proxy_buffering off;</span><br><span class="line">                proxy_ssl_verify off;</span><br><span class="line">                proxy_set_header Host $http_host;</span><br><span class="line">                proxy_set_header Upgrade $http_upgrade;</span><br><span class="line">                proxy_set_header Connection &quot;Upgrade&quot;;</span><br><span class="line">                proxy_set_header X-Forwarded-Proto $scheme;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    if ($host = hub.xxxxxx.com)&#123;</span><br><span class="line">        return 301 https://$host$request_uri;</span><br><span class="line">    &#125; # managed by Certbot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  listen 80;</span><br><span class="line">    server_name hub.xxxxxx.com;</span><br><span class="line">    return 404; # managed by Certbot</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至此,部署就完成了.</p><p>我之前在部署的过程中遇到无数的坑,,可能都是由于harbor没有使用Https引起的</p><hr><p>浏览器访问<a href="https://hub.xxxxxx.com,用初始的账号密码登陆,可以新建项目,创建用户等" target="_blank" rel="noopener">https://hub.xxxxxx.com,用初始的账号密码登陆,可以新建项目,创建用户等</a>.</p><p>我这里创建了一个master的公开项目</p><hr><p>登陆和push镜像没有任何问题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root@idc-function-docker ~]# docker login --username=admin -pHarbor12345 hub.xxxxxx.com</span><br><span class="line">WARNING! Using --password via the CLI is insecure. Use --password-stdin.</span><br><span class="line">WARNING! Your password will be stored unencrypted in /root/.docker/config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/login/#credentials-store</span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br><span class="line">[root@idc-function-docker ~]# docker push hub.xxxxxx.com/master/nginx:latest</span><br><span class="line">The push refers to repository [hub.xxxxxx.com/master/nginx]</span><br><span class="line">d37eecb5b769: Layer already exists</span><br><span class="line">99134ec7f247: Layer already exists</span><br><span class="line">c3a984abe8a8: Layer already exists</span><br><span class="line">latest: digest: sha256:7ac7819e1523911399b798309025935a9968b277d86d50e5255465d6592c0266 size: 948</span><br><span class="line">[root@idc-function-docker ~]#</span><br></pre></td></tr></table></figure><p>在另外一台客户端上,尝试pull镜像不需要密码.如果需要密码pull镜像,可以将项目设置为私有</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker pull  hub.xxxxxx.com/master/nginx:latest</span><br><span class="line">latest: Pulling from master/nginx</span><br><span class="line">Digest: sha256:7ac7819e1523911399b798309025935a9968b277d86d50e5255465d6592c0266</span><br><span class="line">Status: Downloaded newer image for hub.xxxxxx.com/master/nginx:latest</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;harbor私有仓库部署&quot;&gt;&lt;a href=&quot;#harbor私有仓库部署&quot; class=&quot;headerlink&quot; title=&quot;harbor私有仓库部署&quot;&gt;&lt;/a&gt;harbor私有仓库部署&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;harbor是docker的私有仓库,可以部署在局域网服务器上,用来管理docker镜像.虽然docker官方也提供公共镜像仓库,但是由于是境外网站,拉取镜像速度非常慢,而且有被墙的可能.部署私有仓库非常有必要&lt;/p&gt;
&lt;p&gt;harbor是vmware公司开源的企业级的docker registry管理项目.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;处于数据脱敏需要,以下内容中隐藏了真实域名.而使用hub.xxxxxx.com替代&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&quot;社区&quot;&gt;&lt;a href=&quot;#社区&quot; class=&quot;headerlink&quot; title=&quot;社区&quot;&gt;&lt;/a&gt;社区&lt;/h3&gt;&lt;p&gt;harbor github: &lt;a href=&quot;https://github.com/goharbor/harbor&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;goharbor/harbor&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;官网文档介绍: &lt;a href=&quot;https://goharbor.io/docs/1.10/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;harbor doc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在部署中遇到的各种坑,都可以通过查阅文档,或者搜索github的issue解决&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;框架&quot;&gt;&lt;a href=&quot;#框架&quot; class=&quot;headerlink&quot; title=&quot;框架&quot;&gt;&lt;/a&gt;框架&lt;/h3&gt;&lt;p&gt;harbor是docker-compose部署的.包括一系列组件:nginx.core,log,register等等.&lt;/p&gt;
&lt;p&gt;但是由于本机已经存在一个Nginx镜像.所以用Nginx代理到harbor的Nginx.&lt;strong&gt;如果是独立的服务器部署Harbor的话,则不会存在这个问题,可以直接跳过这一章节.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;nginx代理框架大概是:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nginx—-&amp;gt;harbor-nginx—–&amp;gt;habor&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;由于docker提交镜像需要Https协议,所以:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nginx—301跳转到nginx https—–&amp;gt;harbor-nginx http—-&amp;gt; habor&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;但是这样的部署方式,有一个问题:&lt;/p&gt;
&lt;p&gt;私有仓库可以正常login但是push镜像的时候,又提示未验证.&lt;/p&gt;
&lt;p&gt;该问题尝试过很多解决方案,但是均无法解决&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
</feed>

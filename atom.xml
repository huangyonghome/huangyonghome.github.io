<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jesse&#39;s home</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jesse.top/"/>
  <updated>2021-05-09T15:06:26.204Z</updated>
  <id>https://jesse.top/</id>
  
  <author>
    <name>Jesse</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Docker原理----1.Namespace</title>
    <link href="https://jesse.top/2021/03/20/docker/Docker%E5%8E%9F%E7%90%86----1.Namespace/"/>
    <id>https://jesse.top/2021/03/20/docker/Docker原理----1.Namespace/</id>
    <published>2021-03-20T03:59:58.000Z</published>
    <updated>2021-05-09T15:06:26.204Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Docker原理—-Namespace"><a href="#Docker原理—-Namespace" class="headerlink" title="Docker原理—-Namespace"></a>Docker原理—-Namespace</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>容器其实是一种沙盒技术,顾名思义,沙盒就像是一个集装箱一样,把你的应用”装”起来.这样应用和应用之间就因为有了边界而不至于互相干扰.而被装进集装箱的应用,也可以被方便的搬来搬去.</p><p>容器的核心功能就是通过约束和修改进程的动态表现,从而为其创造出一个”边界”.对于Docker以及大多数的Linux容器来说.<code>cgroup</code> 是用来约束的手段,而<code>Namespace</code>则是用来修改进程视图的主要方法</p><h3 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h3><p>linux内核提拱了6种namespace隔离的系统调用，如下图所示，但是真正的容器还需要处理许多其他工作。</p><table><thead><tr><th>namespace</th><th>系统调用参数</th><th><strong>隔离内容</strong></th></tr></thead><tbody><tr><td>UTS</td><td>CLONE_NEWUTS</td><td>主机名或域名</td></tr><tr><td>IPC</td><td>CLONE_NEWIPC</td><td>信号量、消息队列和共享内存</td></tr><tr><td>PID</td><td>CLONE_NEWPID</td><td>进程编号</td></tr><tr><td>Network</td><td>CLONE_NEWNET</td><td>网络设备、网络战、端口等</td></tr><tr><td>Mount</td><td>CLONE_NEWNS</td><td>挂载点（文件系统）</td></tr><tr><td>User</td><td>CLONE_NEWUSER</td><td>用户组和用户组</td></tr></tbody></table><p>实际上，linux内核实现namespace的主要目的，就是为了实现轻量级虚拟化技术服务。在同一个namespace下的进程合一感知彼此的变化，而对外界的进程一无所知。这样就可以让容器中的进程产生错觉，仿佛自己置身一个独立的系统环境中，以达到隔离的目的。</p><p>下面拿个简单例子来讲解部分Namespace技术</p><a id="more"></a><h4 id="PID-namespace"><a href="#PID-namespace" class="headerlink" title="PID namespace"></a>PID namespace</h4><p>启动一个容器,在容器里<code>/bin/sh</code>的进程的PID是1.也就是容器的第一号进程,也称之为容器的主进程.另外还可以注意到该容器只能看到2个进程,一个是启动的PID为1的主进程,一个是手动在容器内部执行的ps进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[work@docker-dev elasticsearch]$ docker run -it busybox /bin/sh</span><br><span class="line">/ # ps aux</span><br><span class="line">PID   USER     TIME  COMMAND</span><br><span class="line">    1 root      0:00 /bin/sh</span><br><span class="line">    6 root      0:00 ps aux</span><br><span class="line">/ #</span><br></pre></td></tr></table></figure><p>接下来在Docker宿主机上查找<code>/bin/sh</code>这个进程,发现他的PID为8058,而它的父进程是<code>containerd-shim</code>,再继续往上找,他的父进程的PID为1571,也就是<code>/usr/bin/containerd</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[work@docker-dev ~]$ ps -ef | grep &quot;/bin/sh&quot;</span><br><span class="line">work      8024  7016  0 22:09 pts/4    00:00:00 docker run -it busybox /bin/sh</span><br><span class="line">root      8058  8040  0 22:09 pts/0    00:00:00 /bin/sh</span><br><span class="line">work      8118  8094  0 22:10 pts/0    00:00:00 grep --color=auto /bin/sh</span><br><span class="line"></span><br><span class="line">[work@docker-dev ~]$ ps -ef | grep 8040</span><br><span class="line">root      8040  1571  0 22:09 ?        00:00:00 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/17e50d35b5bc523baa1acceb8710accf13b6859365e042278555f336a6642aff -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc</span><br><span class="line">root      8058  8040  0 22:09 pts/0    00:00:00 /bin/sh</span><br><span class="line">work      8120  8094  0 22:10 pts/0    00:00:00 grep --color=auto 8040</span><br><span class="line"></span><br><span class="line">[work@docker-dev ~]$ ps aux | grep 1571 | head -n 1</span><br><span class="line">root      1571  0.0  0.3 1632744 53924 ?       Ssl  Feb01  36:04 /usr/bin/containerd</span><br></pre></td></tr></table></figure><p><strong>这说明了什么呢</strong>?</p><p>从上面的例子中可以看到Docker容器内的主进程其实是直接运行在Docker宿主机上的.他在宿主机上是个普通的进程,Docker对这个进程实施了一个”障眼法”,让他看不到其他的所有进程.让该进程误认为自己是第一个启动的PID为1的进程.</p><p>这种技术就是Linux里面的<code>Namespace</code>机制.</p><h3 id="docker-exec"><a href="#docker-exec" class="headerlink" title="docker exec"></a>docker exec</h3><p>我们知道通过<code>docker exec</code>命令可以进入一个容器的内部.在了解了 Linux Namespace 的隔离机制后，你应该会很自然地想到一个问题：docker exec 是怎么做到进入容器里的呢？</p><p>实际上，Linux Namespace 创建的隔离空间虽然看不见摸不着，但一个进程的 Namespace 信息在宿主机上是确确实实存在的，并且是以一个文件的方式存在。</p><p>该命令的原理其实就是获取当前容器进程的PID,以及namespace文件.然后加入该namespace.这样通过共享同一个namespace.从而获取容器内部的信息.</p><p>下面通过一个示例演示如何进入一个容器内部</p><ul><li>启动一个临时容器</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name c1 centos-demo sleep 999</span><br></pre></td></tr></table></figure><ul><li>查看容器进程的PID</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev ~]# docker inspect c1 -f &#123;&#123;.State.Pid&#125;&#125;</span><br><span class="line">22733</span><br></pre></td></tr></table></figure><ul><li>查看该进程的namespace</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev ~]# ll /proc/22733/ns</span><br><span class="line">total 0</span><br><span class="line">lrwxrwxrwx 1 root root 0 May  9 17:15 ipc -&gt; ipc:[4026532170]</span><br><span class="line">lrwxrwxrwx 1 root root 0 May  9 17:15 mnt -&gt; mnt:[4026532168]</span><br><span class="line">lrwxrwxrwx 1 root root 0 May  9 17:14 net -&gt; net:[4026532173]</span><br><span class="line">lrwxrwxrwx 1 root root 0 May  9 17:15 pid -&gt; pid:[4026532171]</span><br><span class="line">lrwxrwxrwx 1 root root 0 May  9 17:15 user -&gt; user:[4026531837]</span><br><span class="line">lrwxrwxrwx 1 root root 0 May  9 17:15 uts -&gt; uts:[4026532169]</span><br></pre></td></tr></table></figure><p>可以看到，一个进程的每种 Linux Namespace，都在它对应的 /proc/[进程号]/ns 下有一个对应的虚拟文件，并且链接到一个真实的 Namespace 文件上。</p><p>有了这样一个可以“hold 住”所有 Linux Namespace 的文件，我们就可以对 Namespace 做一些很有意义事情了，比如：加入到一个已经存在的 Namespace 当中。</p><p><strong>这也就意味着：一个进程，可以选择加入到某个进程已有的 Namespace 当中，从而达到“进入”这个进程所在容器的目的，这正是 docker exec 的实现原理。</strong></p><p>而这个操作所依赖的，乃是一个名叫 setns() 的 Linux 系统调用。它的调用方法，我可以用如下一段小程序为你说明：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#define _GNU_SOURCE</span><br><span class="line">#include &lt;fcntl.h&gt;</span><br><span class="line">#include &lt;sched.h&gt;</span><br><span class="line">#include &lt;unistd.h&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"> </span><br><span class="line">#define errExit(msg) do &#123; perror(msg); exit(EXIT_FAILURE);&#125; while (0)</span><br><span class="line"> </span><br><span class="line">int main(int argc, char *argv[]) &#123;</span><br><span class="line">    int fd;</span><br><span class="line">    </span><br><span class="line">    fd = open(argv[1], O_RDONLY);</span><br><span class="line">    if (setns(fd, 0) == -1) &#123;</span><br><span class="line">        errExit(&quot;setns&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    execvp(argv[2], &amp;argv[2]); </span><br><span class="line">    errExit(&quot;execvp&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码功能非常简单：它一共接收两个参数，第一个参数是 argv[1]，即当前进程要加入的 Namespace 文件的路径，比如 /proc/22733/ns/net；而第二个参数，则是你要在这个 Namespace 里运行的进程，比如 /bin/bash。</p><p>这段代码的的核心操作，则是通过 open() 系统调用打开了指定的 Namespace 文件，并把这个文件的描述符 fd 交给 setns() 使用。在 setns() 执行后，当前进程就加入了这个文件对应的 Linux Namespace 当中了。</p><p>现在，你可以编译执行一下这个程序，加入到容器进程（PID=22733）的net也就是 Network Namespace 中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev ~]#gcc -o set_ns set_ns.c </span><br><span class="line">[root@docker-dev ~]#./set_ns /proc/22733/ns/net /bin/bash </span><br><span class="line">$ ifconfig</span><br><span class="line">eth0      Link encap:Ethernet  HWaddr 02:42:ac:11:00:02  </span><br><span class="line">          inet addr:172.17.0.2  Bcast:0.0.0.0  Mask:255.255.0.0</span><br><span class="line">          inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</span><br><span class="line">          RX packets:12 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:10 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">   collisions:0 txqueuelen:0 </span><br><span class="line">          RX bytes:976 (976.0 B)  TX bytes:796 (796.0 B)</span><br><span class="line"> </span><br><span class="line">lo        Link encap:Local Loopback  </span><br><span class="line">          inet addr:127.0.0.1  Mask:255.0.0.0</span><br><span class="line">          inet6 addr: ::1/128 Scope:Host</span><br><span class="line">          UP LOOPBACK RUNNING  MTU:65536  Metric:1</span><br><span class="line">          RX packets:0 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">  collisions:0 txqueuelen:1000 </span><br><span class="line">          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)</span><br></pre></td></tr></table></figure><p>正如上所示，当我们执行 ifconfig 命令查看网络设备时，我会发现能看到的网卡“变少”了：只有两个。而我的宿主机则至少有四个网卡。这是怎么回事呢？</p><p>实际上，在 setns() 之后我看到的这两个网卡，正是我在前面启动的 Docker 容器里的网卡。也就是说，我新创建的这个 /bin/bash 进程，由于加入了该容器进程（PID=22733）的 Network Namepace，它看到的网络设备与这个容器里是一样的，即：/bin/bash 进程的网络设备视图，也被修改了。</p><p>而一旦一个进程加入到了另一个 Namespace 当中，在宿主机的 Namespace 文件上，也会有所体现。</p><p>在宿主机上，你可以用 ps 指令找到这个 set_ns 程序执行的 /bin/bash 进程，其真实的 PID 是 22821：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[work@docker-dev ~]$ ps aux | grep &quot;/bin/bash&quot;</span><br><span class="line">root     22821  0.0  0.0 115676  2084 pts/0    S+   17:20   0:00 /bin/bash</span><br></pre></td></tr></table></figure><p>这时，如果按照前面介绍过的方法，查看一下这个 PID=22733的进程的 Namespace，你就会发现这样一个事实：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@docker-dev ~]# ll /proc/22733/ns/net</span><br><span class="line">lrwxrwxrwx 1 root root 0 May  9 17:14 /proc/22733/ns/net -&gt; net:[4026532173]</span><br><span class="line">[root@docker-dev ~]# ll /proc/22821/ns/net</span><br><span class="line">lrwxrwxrwx 1 root root 0 May  9 17:25 /proc/22821/ns/net -&gt; net:[4026532173]</span><br></pre></td></tr></table></figure><p>在 /proc/[PID]/ns/net 目录下，这个 PID=22821进程，与我们前面的 Docker 容器进程（PID=22733）指向的 Network Namespace 文件完全一样。这说明这两个进程，共享了这个名叫 net:[4026532173] 的 Network Namespace。</p><p>刚才我们演示了共享了net这个网络名称空间,同样的道理,还可以共享其他名称空间.比如:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev ~]#  ./set_ns /proc/22733/ns/mnt /bin/bash</span><br><span class="line">[root@docker-dev /]# ls ~</span><br><span class="line">anaconda-ks.cfg</span><br><span class="line">[root@docker-dev /]# exit</span><br><span class="line">exit</span><br><span class="line">[root@docker-dev ~]# ls</span><br><span class="line">1.txt            centos          hsq-openapi-php              memory.limit_in_bytez~  ns.c            set_ns    tradecenter-nginx-php-fpm  zookeeper-3.4.13.tar.gz</span><br><span class="line">2.txt            cert            jdk-8u281-linux-i586.tar.gz  mem.py                  php             set_ns.c  virtual_root</span><br><span class="line">A                D               jenkins-fpm.txt              nginx                   php-fpm         tasks~    worker</span><br><span class="line">anaconda-ks.cfg  Dockerfile      jenkins-php-rpc.txt          nginx-php               php-rpc         tasky~    www.conf</span><br><span class="line">B                Dockerfile.bak  jenkins-slave                nginx-php-rpc           php-rpc:7.1.33  taskz~    yar-2.0.5.tgz</span><br><span class="line">C                fpm.txt         memory.limit_in_bytes~       ns                      php-rpc.txt     test      zookeeper-0.6.4.tgz</span><br></pre></td></tr></table></figure><p>当进入容器进程的mnt名称空间后,进程视图切换到了<code>/</code>根目录下,并且<code>root</code>家目录的内容也发生了变化.</p><p>以上就是<code>docker exec</code>命令背后的详细原理.</p><hr><p><strong>总结</strong></p><p><code>Namespace</code>的视图隔离就是Linux容器最基本的实现原理.所以Docker实际上是在创建容器进程时，指定了这个进程所需要启用的一组 Namespace 参数。这样，容器就只能“看”到当前 Namespace 所限定的资源、文件、设备、状态，或者配置。而对于宿主机以及其他不相关的程序，它就完全看不到了。</p><p><strong>所以说，容器，其实是一种特殊的进程而已。</strong></p><p>在理解了 Namespace 的工作方式之后，你就会明白，跟真实存在的虚拟机不同，在使用 Docker 的时候，并没有一个真正的“Docker 容器”运行在宿主机里面。Docker 项目帮助用户启动的，还是原来的应用进程，只不过在创建这些进程时，Docker 为它们加上了各种各样的 Namespace 参数。</p><p>这时，这些进程就会觉得自己是各自 PID Namespace 里的第 1 号进程，只能看到各自 Mount Namespace 里挂载的目录和文件，只能访问到各自 Network Namespace 里的网络设备，就仿佛运行在一个个“容器”里面，与世隔绝。</p><h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><p>Q: 你是否知道最新的 Docker 项目默认会为容器启用哪些 Namespace 吗？</p><p>A: PID, UTS, network, user, mount, IPC, cgroup</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Docker原理—-Namespace&quot;&gt;&lt;a href=&quot;#Docker原理—-Namespace&quot; class=&quot;headerlink&quot; title=&quot;Docker原理—-Namespace&quot;&gt;&lt;/a&gt;Docker原理—-Namespace&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;容器其实是一种沙盒技术,顾名思义,沙盒就像是一个集装箱一样,把你的应用”装”起来.这样应用和应用之间就因为有了边界而不至于互相干扰.而被装进集装箱的应用,也可以被方便的搬来搬去.&lt;/p&gt;
&lt;p&gt;容器的核心功能就是通过约束和修改进程的动态表现,从而为其创造出一个”边界”.对于Docker以及大多数的Linux容器来说.&lt;code&gt;cgroup&lt;/code&gt; 是用来约束的手段,而&lt;code&gt;Namespace&lt;/code&gt;则是用来修改进程视图的主要方法&lt;/p&gt;
&lt;h3 id=&quot;Namespace&quot;&gt;&lt;a href=&quot;#Namespace&quot; class=&quot;headerlink&quot; title=&quot;Namespace&quot;&gt;&lt;/a&gt;Namespace&lt;/h3&gt;&lt;p&gt;linux内核提拱了6种namespace隔离的系统调用，如下图所示，但是真正的容器还需要处理许多其他工作。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;namespace&lt;/th&gt;
&lt;th&gt;系统调用参数&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;隔离内容&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;UTS&lt;/td&gt;
&lt;td&gt;CLONE_NEWUTS&lt;/td&gt;
&lt;td&gt;主机名或域名&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IPC&lt;/td&gt;
&lt;td&gt;CLONE_NEWIPC&lt;/td&gt;
&lt;td&gt;信号量、消息队列和共享内存&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PID&lt;/td&gt;
&lt;td&gt;CLONE_NEWPID&lt;/td&gt;
&lt;td&gt;进程编号&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Network&lt;/td&gt;
&lt;td&gt;CLONE_NEWNET&lt;/td&gt;
&lt;td&gt;网络设备、网络战、端口等&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mount&lt;/td&gt;
&lt;td&gt;CLONE_NEWNS&lt;/td&gt;
&lt;td&gt;挂载点（文件系统）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;User&lt;/td&gt;
&lt;td&gt;CLONE_NEWUSER&lt;/td&gt;
&lt;td&gt;用户组和用户组&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;实际上，linux内核实现namespace的主要目的，就是为了实现轻量级虚拟化技术服务。在同一个namespace下的进程合一感知彼此的变化，而对外界的进程一无所知。这样就可以让容器中的进程产生错觉，仿佛自己置身一个独立的系统环境中，以达到隔离的目的。&lt;/p&gt;
&lt;p&gt;下面拿个简单例子来讲解部分Namespace技术&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker原理----2.Cgroup</title>
    <link href="https://jesse.top/2021/03/20/docker/Docker%E5%8E%9F%E7%90%86----2.Cgroup/"/>
    <id>https://jesse.top/2021/03/20/docker/Docker原理----2.Cgroup/</id>
    <published>2021-03-20T03:59:58.000Z</published>
    <updated>2021-05-09T15:06:00.394Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Docker原理—-Cgroup"><a href="#Docker原理—-Cgroup" class="headerlink" title="Docker原理—-Cgroup"></a>Docker原理—-Cgroup</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>在上篇介绍完容器的”隔离”技术之后,我们再来研究一下容器的”限制”问题</p><p>也许你会好奇，我们不是已经通过 Linux Namespace 创建了一个“容器”吗，为什么还需要对容器做“限制”呢？</p><p>我还是以 PID Namespace 为例，来给你解释这个问题。</p><p>虽然容器的第一号进程只能看到容器里的情况,但是由于是直接运行在宿主机上,所以它和宿主机上其他所有进程之间依然是平等的竞争关系.这就意味着虽然该进程在视图上被隔离起来了,但是他能够使用宿主机上的所有资源(比如CPU,内存).</p><p>这显然不是一个”沙盒”应该表现出来的合理行为</p><p>而Linux Cgroups就是Linux内核中用来为进程设置资源限制的一个重要功能</p><a id="more"></a><h3 id="Cgroups"><a href="#Cgroups" class="headerlink" title="Cgroups"></a>Cgroups</h3><p><strong>Linux Cgroups 的全称是 Linux Control Group。它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。</strong></p><p>此外，Cgroups 还能够对进程进行优先级设置、审计，以及将进程挂起和恢复等操作。在今天的分享中，我只和你重点探讨它与容器关系最紧密的“限制”能力，并通过一组实践来带你认识一下 Cgroups。</p><p>从字面上理解，cgroups就是把任务放到一个组里面统一加以控制。本质上来说，cgroups是内核附加在程序上的一系列hook，通过程序运行时对资源的调度触发相应的钩子以达到资源跟踪和限制的目的。在cgroup里，任务(task)就是系统的一个进程或者线程。</p><h5 id="cgroups的四大作用："><a href="#cgroups的四大作用：" class="headerlink" title="cgroups的四大作用："></a>cgroups的四大作用：</h5><ul><li>资源限制： 比如设定任务内存使用的上限。</li><li>优先级分配： 比如给任务分配CPU的时间片数量和磁盘IO的带宽大小来控制任务运行的优先级。</li><li>资源统计：比如统计CPU的使用时长、内存用量等。这个功能非常适用于计费。</li><li>任务控制：cgroups可以对任务执行挂起、恢复等操作。</li></ul><p>在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下。，我可以用 mount 指令把它们展示出来，这条命令是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)</span><br><span class="line">cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)</span><br><span class="line">cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)</span><br><span class="line">cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)</span><br><span class="line">cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)</span><br><span class="line">cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)</span><br><span class="line">cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)</span><br><span class="line">cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls)</span><br><span class="line">cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)</span><br><span class="line">cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)</span><br><span class="line">cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)</span><br></pre></td></tr></table></figure><p>cgroups以<strong>操作文件的方式</strong>作为API。它的操作目录是<code>/sys/fs/cgroup</code>。我们来看看这个目录下有什么内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@edward-rhel7-2 cloud-user]# ls /sys/fs/cgroup</span><br><span class="line">blkio  cpu  cpuacct  cpu,cpuacct  cpuset  devices  freezer  hugetlb  memory  net_cls  net_cls,net_prio  net_prio  perf_event  pids  systemd</span><br></pre></td></tr></table></figure><p>可以看到，在 /sys/fs/cgroup 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统(sub system)。子系统就是资源调度器。比如CPU子系统可以控制CPU的时间分配，memory子系统可以限制内存的使用量.这些都是我这台机器当前可以被 Cgroups 进行限制的资源种类。而在子系统对应的资源种类下，你就可以看到该类资源具体可以被限制的方法。</p><hr><h3 id="Cgroup对CPU资源限制"><a href="#Cgroup对CPU资源限制" class="headerlink" title="Cgroup对CPU资源限制"></a>Cgroup对CPU资源限制</h3><p>对 CPU 子系统来说，我们就可以看到如下几个配置文件，这个指令是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ls /sys/fs/cgroup/cpu</span><br><span class="line">cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release</span><br><span class="line">cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks</span><br></pre></td></tr></table></figure><p>如果熟悉 Linux CPU 管理的话，你就会在它的输出里注意到 cfs_period 和 cfs_quota 这样的关键词。这两个参数需要组合使用，可以用来限制进程在长度为 cfs_period 的一段时间内，只能被分配到总量为 cfs_quota 的 CPU 时间。</p><p>而这样的配置文件又如何使用呢？</p><p>你需要在对应的子系统下面创建一个目录，比如，我们现在进入 /sys/fs/cgroup/cpu 目录下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:/sys/fs/cgroup/cpu$ mkdir container</span><br><span class="line">root@ubuntu:/sys/fs/cgroup/cpu$ ls container/</span><br><span class="line">cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release</span><br><span class="line">cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks</span><br></pre></td></tr></table></figure><p>这个目录就称为一个“控制组”。你会发现，操作系统会在你新创建的 container 目录下，自动生成该子系统对应的资源限制文件。</p><p>现在，我们在后台执行这样一条脚本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ while : ; do : ; done &amp;</span><br><span class="line">[1] 8498</span><br></pre></td></tr></table></figure><p>显然，它执行了一个死循环，可以把计算机的 CPU 吃到 100%，根据它的输出，我们可以看到这个脚本在后台运行的进程号（PID）是 8498。</p><p>这样，我们可以用 top 指令来确认一下 CPU 有没有被打满.在输出里可以看到，CPU 的使用率已经 100% 了（%Cpu0 :100.0 us）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Tasks: 154 total,   2 running, 152 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu0  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu1  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu2  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu3  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu4  :100.0 us,  0.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu5  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu6  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu7  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br></pre></td></tr></table></figure><p>而此时，我们可以通过查看 container 目录下的文件，看到 container 控制组里的 CPU quota 还没有任何限制（即：-1），CPU period 则是默认的 100 ms（100000 us）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us </span><br><span class="line">-1</span><br><span class="line">$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_period_us </span><br><span class="line">100000</span><br></pre></td></tr></table></figure><p>接下来，我们可以通过修改这些文件的内容来设置限制。</p><p>比如，向 container 组里的 cfs_quota 文件写入 20 ms（20000 us):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 20000 &gt; /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us</span><br></pre></td></tr></table></figure><p>结合前面的介绍，你应该能明白这个操作的含义，它意味着在每 100 ms 的时间里，被该控制组限制的进程只能使用 20 ms 的 CPU 时间，也就是说这个进程只能使用到 20% 的 CPU 带宽。</p><p>接下来，我们把被限制的进程的 PID 写入 container 组里的 tasks 文件，上面的设置就会对该进程生效了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 8498 &gt; /sys/fs/cgroup/cpu/container/tasks</span><br></pre></td></tr></table></figure><p>我们可以用 top 指令查看一下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Tasks: 154 total,   2 running, 152 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu0  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu1  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu2  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu3  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu4  : 19.8 us,  0.0 sy,  0.0 ni, 80.2 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu5  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu6  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu7  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br></pre></td></tr></table></figure><p>可以看到，计算机的 CPU 使用率立刻降到了 20%以内</p><p>除 CPU 子系统外，Cgroups 的每一项子系统都有其独有的资源限制能力，比如：</p><ul><li>blkio，为块设备设定I/O 限制，一般用于磁盘等设备；</li><li>cpuset，为进程分配单独的 CPU 核和对应的内存节点；</li><li>memory，为进程设定内存使用的限制。</li></ul><p><strong>Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合</strong>。而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了。</p><p>而至于在这些控制组下面的资源文件里填上什么值，就靠用户执行 docker run 时的参数指定了，比如这样一条命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev container]# docker run -it -d --cpu-period=100000 --cpu-quota=20000 busybox /bin/sh</span><br><span class="line">c992cf3cc50c8f1e32780aed17058d4dcaf91048b2b5fbf0a5134078a983e95b</span><br></pre></td></tr></table></figure><p>在启动这个容器后，我们可以通过查看 Cgroups 文件系统下，CPU 子系统中，“docker”这个控制组里的资源限制文件的内容来确认：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev container]# cat /sys/fs/cgroup/cpu/docker/c992cf3cc50c/cpu.cfs_quota_us</span><br><span class="line">20000</span><br><span class="line">[root@docker-dev container]# cat /sys/fs/cgroup/cpu/docker/c992cf3cc50c/cpu.cfs_period_us</span><br><span class="line">100000</span><br></pre></td></tr></table></figure><p>这就意味着这个 Docker 容器，只能使用到 20% 的 CPU 带宽。</p><hr><h3 id="Cgroup对内存限制"><a href="#Cgroup对内存限制" class="headerlink" title="Cgroup对内存限制"></a>Cgroup对内存限制</h3><p>内存资源和CPU不同,CPU属于可压缩资源.当进程触发CPU限制阈值时,进程仍然可以正常运行,只是进程能使用的CPU分片时间受到限制.然而内存属于不可压缩资源,当进程触发内存资源阈值时,进程会立刻被杀死,也就是触发OOM事件.</p><p>下面用python的递归模拟一个内存占用的程序</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import time</span><br><span class="line">import sys</span><br><span class="line">sys.setrecursionlimit(30000)</span><br><span class="line"></span><br><span class="line">class Recursion:</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.level = 0</span><br><span class="line"></span><br><span class="line">    def rec(self):</span><br><span class="line">        self.level += 1</span><br><span class="line">        if self.level &gt; 1000:</span><br><span class="line">            time.sleep(1)</span><br><span class="line">        self.rec()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Recursion().rec()</span><br></pre></td></tr></table></figure><p>在<code>/sys/fs/cgroup/memory/</code> 目录下创建一个测试文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev ~]# cd /sys/fs/cgroup/memory/</span><br><span class="line">[root@docker-dev memory]# mkdir mem_test</span><br></pre></td></tr></table></figure><p>在该目录下,限制内存阈值,这里设置为10K</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev ~]# cd /sys/fs/cgroup/memory/mem_test/</span><br><span class="line">[root@docker-dev mem_test]# echo 10k &gt; memory.limit_in_bytes</span><br><span class="line">[root@docker-dev mem_test]# cat memory.limit_in_bytes</span><br><span class="line">8192</span><br></pre></td></tr></table></figure><p>运行python程序,同时开启另一个shell终端,获取该进程的PID</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev ~]# python3 mem.py</span><br><span class="line"></span><br><span class="line">[root@docker-dev mem_test]# ps aux | grep python3</span><br><span class="line">root     22958  0.0  0.0 125908  6468 pts/1    S+   17:58   0:00 python3 mem.py</span><br></pre></td></tr></table></figure><p>将22958这个PID写入到<code>mem_test</code>目录下的tasks文件内</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev mem_test]# echo 22958 &gt; tasks</span><br></pre></td></tr></table></figure><p>此时.python3的进程会被杀死,出现OOM现象</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev mem_test]# less /var/log/messages | grep oom</span><br><span class="line">May  9 18:00:28 docker-dev kernel: python3 invoked oom-killer: gfp_mask=0xd0, order=0, oom_score_adj=0</span><br><span class="line">May  9 18:00:28 docker-dev kernel: [&lt;ffffffff9f5c24ce&gt;] oom_kill_process+0x25e/0x3f0</span><br><span class="line">May  9 18:00:28 docker-dev kernel: [&lt;ffffffff9f640c06&gt;] mem_cgroup_oom_synchronize+0x546/0x570</span><br></pre></td></tr></table></figure><hr><h3 id="docker-cgroup"><a href="#docker-cgroup" class="headerlink" title="docker cgroup"></a>docker cgroup</h3><p>通过上面2个小例子,我们演示了cgroup对本机进程的资源限制效果.docker在启动容器时也允许我们对该容器的CPU和内存进行一些资源限制.但是其资源限制的本质也同样是利用cgroup的功能.下面我们运行一个容器.该容器运行一个上文中的while死循环,但是这次我们对容器的CPU资源进行限制.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev mem_test]# docker run -d --name c2 --cpu-period=100000 --cpu-quota=20000  hub.doweidu.com/base/centos-demo:7 bash -c &quot;while : ; do : ; done&quot;</span><br><span class="line">7b1cb8734d905dd25eb1cdcf4cb63ebf8c7e6182d90639db4ad15bc99ba19f63</span><br><span class="line">[root@docker-dev mem_test]#</span><br></pre></td></tr></table></figure><p>通过top命令,我们可以看到容器的CPU限制已经生效了.cpu3被限制在20%的使用率之内</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">top - 20:53:50 up 31 days, 10:09,  2 users,  load average: 0.00, 0.01, 0.05</span><br><span class="line">Tasks: 161 total,   3 running, 158 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu0  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu1  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu2  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu3  : 19.3 us,  0.0 sy,  0.0 ni, 80.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu4  :  1.7 us,  0.0 sy,  0.0 ni, 98.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu5  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu6  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu7  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">KiB Mem : 16265540 total, 13309868 free,   783868 used,  2171804 buff/cache</span><br><span class="line">KiB Swap:  4194300 total,  4193012 free,     1288 used. 14757664 avail Mem</span><br></pre></td></tr></table></figure><p>在<code>/sys/fs/cgroup/cpu/docker</code> 目录下.可以看到新生成了一个目录<code>7b1cb8734d905dd25eb1cdcf4cb63ebf8c7e6182d90639db4ad15bc99ba19f63</code> .其实这就是我们上一步中刚启动的容器的ID.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev docker]# ll</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 2 root root 0 May  9 20:53 7b1cb8734d905dd25eb1cdcf4cb63ebf8c7e6182d90639db4ad15bc99ba19f63</span><br><span class="line">-rw-r--r-- 1 root root 0 Apr  8 11:04 cgroup.clone_children</span><br><span class="line">--w--w--w- 1 root root 0 Apr  8 11:04 cgroup.event_control</span><br><span class="line">-rw-r--r-- 1 root root 0 Apr  8 11:04 cgroup.procs</span><br><span class="line">-r--r--r-- 1 root root 0 Apr  8 11:04 cpuacct.stat</span><br><span class="line">-rw-r--r-- 1 root root 0 Apr  8 11:04 cpuacct.usage</span><br><span class="line">-r--r--r-- 1 root root 0 Apr  8 11:04 cpuacct.usage_percpu</span><br><span class="line">-rw-r--r-- 1 root root 0 Apr  8 11:04 cpu.cfs_period_us</span><br><span class="line">-rw-r--r-- 1 root root 0 Apr  8 11:04 cpu.cfs_quota_us</span><br><span class="line">-rw-r--r-- 1 root root 0 Apr  8 11:04 cpu.rt_period_us</span><br><span class="line">-rw-r--r-- 1 root root 0 Apr  8 11:04 cpu.rt_runtime_us</span><br><span class="line">-rw-r--r-- 1 root root 0 Apr  8 11:04 cpu.shares</span><br><span class="line">-r--r--r-- 1 root root 0 Apr  8 11:04 cpu.stat</span><br><span class="line">-rw-r--r-- 1 root root 0 Apr  8 11:04 notify_on_release</span><br><span class="line">-rw-r--r-- 1 root root 0 Apr  8 11:04 tasks</span><br></pre></td></tr></table></figure><p>进入该容器ID的目录内.可以看到<code>cpu.cfs_quota_us</code>文件已经设置了限额20000us.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev docker]# cd 7b1cb8734d905dd25eb1cdcf4cb63ebf8c7e6182d90639db4ad15bc99ba19f63</span><br><span class="line"></span><br><span class="line">[root@docker-dev 7b1cb8734d905dd25eb1cdcf4cb63ebf8c7e6182d90639db4ad15bc99ba19f63]# cat cpu.cfs_quota_us</span><br><span class="line">20000</span><br></pre></td></tr></table></figure><p>查看该容器的Pid以及tasks可以看到.docker自动将容器的进程Pid写入到了tasks文件中.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev 7b1cb8734d905dd25eb1cdcf4cb63ebf8c7e6182d90639db4ad15bc99ba19f63]# docker inspect c2 -f &#123;&#123;.State.Pid&#125;&#125;</span><br><span class="line">23209</span><br><span class="line"></span><br><span class="line">[root@docker-dev 7b1cb8734d905dd25eb1cdcf4cb63ebf8c7e6182d90639db4ad15bc99ba19f63]# cat tasks</span><br><span class="line">23209</span><br><span class="line">[root@docker-dev 7b1cb8734d905dd25eb1cdcf4cb63ebf8c7e6182d90639db4ad15bc99ba19f63]#</span><br></pre></td></tr></table></figure><p>通过上面的例子中可以看到,docker使用cgroup解决了多个容器之前的资源竞争和互相干扰的问题.</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过以上讲述，你现在应该能够理解，一个正在运行的 Docker 容器，其实就是一个启用了多个 Linux Namespace 的应用进程，而这个进程能够使用的资源量，则受 Cgroups 配置的限制。</p><p>这也是容器技术中一个非常重要的概念，即：<strong>容器是一个“单进程”模型。</strong></p><p>由于一个容器的本质就是一个进程，用户的应用进程实际上就是容器里 PID=1 的进程，也是其他后续创建的所有进程的父进程。这就意味着，在一个容器中，你没办法同时运行两个不同的应用，除非你能事先找到一个公共的 PID=1 的程序来充当两个不同应用的父进程，这也是为什么很多人都会用 systemd 或者 supervisord 这样的软件来代替应用本身作为容器的启动进程。</p><p>但是，在后面分享容器设计模式时，我还会推荐其他更好的解决办法。这是因为容器本身的设计，就是希望容器和应用能够<strong>同生命周期</strong>，这个概念对后续的容器编排非常重要。否则，一旦出现类似于“容器是正常运行的，但是里面的应用早已经挂了”的情况，编排系统处理起来就非常麻烦了。</p><p>另外，跟 Namespace 的情况类似，Cgroups 对资源的限制能力也有很多不完善的地方，被提及最多的自然是 /proc 文件系统的问题。</p><p>众所周知，Linux 下的 /proc 目录存储的是记录当前内核运行状态的一系列特殊文件，用户可以通过访问这些文件，查看系统以及当前正在运行的进程的信息，比如 CPU 使用情况、内存占用率等，这些文件也是 top 指令查看系统信息的主要数据来源。</p><p>但是，你如果在容器里执行 top 指令，就会发现，它显示的信息居然是宿主机的 CPU 和内存数据，而不是当前容器的数据。</p><p>造成这个问题的原因就是，/proc 文件系统并不知道用户通过 Cgroups 给这个容器做了什么样的资源限制，即：/proc 文件系统不了解 Cgroups 限制的存在。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Docker原理—-Cgroup&quot;&gt;&lt;a href=&quot;#Docker原理—-Cgroup&quot; class=&quot;headerlink&quot; title=&quot;Docker原理—-Cgroup&quot;&gt;&lt;/a&gt;Docker原理—-Cgroup&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;在上篇介绍完容器的”隔离”技术之后,我们再来研究一下容器的”限制”问题&lt;/p&gt;
&lt;p&gt;也许你会好奇，我们不是已经通过 Linux Namespace 创建了一个“容器”吗，为什么还需要对容器做“限制”呢？&lt;/p&gt;
&lt;p&gt;我还是以 PID Namespace 为例，来给你解释这个问题。&lt;/p&gt;
&lt;p&gt;虽然容器的第一号进程只能看到容器里的情况,但是由于是直接运行在宿主机上,所以它和宿主机上其他所有进程之间依然是平等的竞争关系.这就意味着虽然该进程在视图上被隔离起来了,但是他能够使用宿主机上的所有资源(比如CPU,内存).&lt;/p&gt;
&lt;p&gt;这显然不是一个”沙盒”应该表现出来的合理行为&lt;/p&gt;
&lt;p&gt;而Linux Cgroups就是Linux内核中用来为进程设置资源限制的一个重要功能&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker原理----3.mount挂载</title>
    <link href="https://jesse.top/2021/03/20/docker/Docker%E5%8E%9F%E7%90%86----3.mount%E6%8C%82%E8%BD%BD/"/>
    <id>https://jesse.top/2021/03/20/docker/Docker原理----3.mount挂载/</id>
    <published>2021-03-20T03:59:58.000Z</published>
    <updated>2021-05-09T15:06:45.280Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Docker原理—-3-mount挂载"><a href="#Docker原理—-3-mount挂载" class="headerlink" title="Docker原理—-3.mount挂载"></a>Docker原理—-3.mount挂载</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>而正如我前面所说的，Namespace 的作用是“隔离”，它让应用进程只能看到该 Namespace 内的“世界”；而 Cgroups 的作用是“限制”，它给这个“世界”围上了一圈看不见的墙。这么一折腾，进程就真的被“装”在了一个与世隔绝的房间里，而这些房间就是 PaaS 项目赖以生存的应用“沙盒”。</p><p>可是，还有一个问题不知道你有没有仔细思考过：这个房间四周虽然有了墙，但是如果容器进程低头一看地面，又是怎样一副景象呢？</p><p>换句话说，<strong>容器里的进程看到的文件系统又是什么样子的呢？</strong></p><a id="more"></a><h3 id="chroot"><a href="#chroot" class="headerlink" title="chroot"></a>chroot</h3><p>Docker容器借助<code>chroot</code>挂载一个虚拟根目录到容器.我们在Linux操作系统里可以很方便的演练<code>chroot</code>是如何工作的.chroot的作用就是帮助你<code>change root file system</code> ,即改变进程的根目录到你指定的位置.</p><p>假设，我们现在有一个 $HOME/test 目录，想要把它作为一个 /bin/bash 进程的根目录。</p><p>首先，创建一个 test 目录和几个 lib 文件夹:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p $HOME/test</span><br><span class="line">$ mkdir -p $HOME/test/&#123;bin,lib64,lib&#125;</span><br></pre></td></tr></table></figure><p>然后，把 bash 命令拷贝到 test 目录对应的 bin 路径下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp -v /bin/&#123;bash,ls&#125; $HOME/test/bin</span><br></pre></td></tr></table></figure><p>接下来，把 bash 命令需要的所有 so 文件，也拷贝到 test 目录对应的 lib 路径下。找到 so 文件可以用 ldd 命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ T=$HOME/virtual_root</span><br><span class="line">$ list=&quot;$(ldd /bin/ls | egrep -o &apos;/lib.*\.[0-9]&apos;)&quot;</span><br><span class="line">$ for i in $list; do cp -v &quot;$i&quot; &quot;$&#123;T&#125;$&#123;i&#125;&quot;; done</span><br><span class="line"></span><br><span class="line">#还需要拷贝下面这个so文件</span><br><span class="line">cp -v /usr/lib64/libtinfo.so.5 $T/lib64/</span><br></pre></td></tr></table></figure><p>最后，执行 chroot 命令，告诉操作系统，我们将使用 $HOME/test 目录作为 /bin/bash 进程的根目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chroot $HOME/test /bin/bash</span><br></pre></td></tr></table></figure><p>这时，你如果执行 “ls /“，就会看到，它返回的都是 $HOME/test 目录下面的内容，而不是宿主机的内容。</p><p>更重要的是，对于被 chroot 的进程来说，它并不会感受到自己的根目录已经被“修改”成 $HOME/test 了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev test]# chroot $HOME/test /bin/bash</span><br><span class="line">bash-4.2# ./bin/ls /</span><br><span class="line">bin  lib  lib64</span><br><span class="line"></span><br><span class="line">bash-4.2# ./bin/ls</span><br><span class="line">bin  lib  lib64</span><br></pre></td></tr></table></figure><p>这种视图被修改的原理，是不是跟我之前介绍的 Linux Namespace 很类似呢？</p><hr><h3 id="rootfs"><a href="#rootfs" class="headerlink" title="rootfs"></a>rootfs</h3><p><strong>实际上，Mount Namespace 正是基于对 chroot 的不断改良才被发明出来的，它也是 Linux 操作系统里的第一个 Namespace。</strong></p><p>当然，为了能够让容器的这个根目录看起来更“真实”，我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如 Ubuntu16.04 的 ISO。这样，在容器启动之后，我们在容器里通过执行 “ls /“ 查看根目录下的内容，就是 Ubuntu 16.04 的所有目录和文件。</p><p><strong>而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。</strong></p><p>所以，一个最常见的 rootfs，或者说容器镜像，会包括如下所示的一些目录和文件，比如 /bin，/etc，/proc 等等：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls /</span><br><span class="line">bin dev etc home lib lib64 mnt opt proc root run sbin sys tmp usr var</span><br></pre></td></tr></table></figure><p>现在，你应该可以理解，对 Docker 项目来说，它最核心的原理实际上就是为待创建的用户进程：</p><ol><li>启用 Linux Namespace 配置；</li><li>设置指定的 Cgroups 参数； </li><li>切换进程的根目录（Change Root）。</li></ol><p>这样，一个完整的容器就诞生了。不过，Docker 项目在最后一步的切换上会优先使用 pivot_root 系统调用，如果系统不支持，才会使用 chroot。</p><p>另外，<strong>需要明确的是，rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。</strong></p><p>所以说，rootfs 只包括了操作系统的“躯壳”，并没有包括操作系统的“灵魂”。</p><p>那么，对于容器来说，这个操作系统的“灵魂”又在哪里呢？</p><p>实际上，同一台机器上的所有容器，都共享宿主机操作系统的内核。</p><p>这就意味着，如果你的应用程序需要配置内核参数、加载额外的内核模块，以及跟内核进行直接的交互，你就需要注意了：这些操作和依赖的对象，都是宿主机操作系统的内核，它对于该机器上的所有容器来说是一个“全局变量”，牵一发而动全身。</p><p>这也是容器相比于虚拟机的主要缺陷之一：毕竟后者不仅有模拟出来的硬件机器充当沙盒，而且每个沙盒里还运行着一个完整的 Guest OS 给应用随便折腾。</p><p>不过，<strong>正是由于 rootfs 的存在，容器才有了一个被反复宣传至今的重要特性：一致性。</strong></p><p>什么是容器的“一致性”呢？</p><p>过去由于云端与本地服务器环境不同，应用的打包过程，一直是使用 PaaS 时最“痛苦”的一个步骤。</p><p>但有了容器之后，更准确地说，有了容器镜像（即 rootfs）之后，这个问题被非常优雅地解决了。</p><p><strong>由于 rootfs 里打包的不只是应用，而是整个操作系统的文件和目录，也就意味着，应用以及它运行所需要的所有依赖，都被封装在了一起。</strong></p><p>有了容器镜像“打包操作系统”的能力，这个最基础的依赖环境也终于变成了应用沙盒的一部分。这就赋予了容器所谓的一致性：无论在本地、云端，还是在一台任何地方的机器上，用户只需要解压打包好的容器镜像，那么这个应用运行所需要的完整的执行环境就被重现出来了。</p><p><strong>这种深入到操作系统级别的运行环境一致性，打通了应用在本地开发和远端执行环境之间难以逾越的鸿沟。</strong></p><p>不过，这时你可能已经发现了另一个非常棘手的问题：难道我每开发一个应用，或者升级一下现有的应用，都要重复制作一次 rootfs 吗？</p><p>比如，我现在用 Ubuntu 操作系统的 ISO 做了一个 rootfs，然后又在里面安装了 Java 环境，用来部署我的 Java 应用。那么，我的另一个同事在发布他的 Java 应用时，显然希望能够直接使用我安装过 Java 环境的 rootfs，而不是重复这个流程。</p><p>一种比较直观的解决办法是，我在制作 rootfs 的时候，每做一步“有意义”的操作，就保存一个 rootfs 出来，这样其他同事就可以按需求去用他需要的 rootfs 了。</p><p>但是，这个解决办法并不具备推广性。原因在于，一旦你的同事们修改了这个 rootfs，新旧两个 rootfs 之间就没有任何关系了。这样做的结果就是极度的碎片化。</p><p>那么，既然这些修改都基于一个旧的 rootfs，我们能不能以增量的方式去做这些修改呢？这样做的好处是，所有人都只需要维护相对于 base rootfs 修改的增量内容，而不是每次修改都制造一个“fork”。</p><p>答案当然是肯定的。</p><p>这也正是为何，Docker 公司在实现 Docker 镜像时并没有沿用以前制作 rootfs 的标准流程，而是做了一个小小的创新：</p><blockquote><p>Docker 在镜像的设计中，引入了层（layer）的概念。也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs。</p></blockquote><hr><h3 id="AUFS"><a href="#AUFS" class="headerlink" title="AUFS"></a>AUFS</h3><p>当然，这个想法不是凭空臆造出来的，而是用到了一种叫作联合文件系统（Union File System）的能力。</p><p>Union File System 也叫 UnionFS，最主要的功能是将多个不同位置的目录联合挂载（union mount）到同一个目录下。比如，我现在有两个目录 A 和 B，它们分别有两个文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ tree</span><br><span class="line">.</span><br><span class="line">├── A</span><br><span class="line">│  ├── a</span><br><span class="line">│  └── x</span><br><span class="line">└── B</span><br><span class="line">  ├── b</span><br><span class="line">  └── x</span><br></pre></td></tr></table></figure><p>然后，我使用联合挂载的方式，将这两个目录挂载到一个公共的目录 C 上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir C</span><br><span class="line">$ mount -t aufs -o dirs=./A:./B none ./C</span><br></pre></td></tr></table></figure><p>这时，我再查看目录 C 的内容，就能看到目录 A 和 B 下的文件被合并到了一起：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ tree ./C</span><br><span class="line">./C</span><br><span class="line">├── a</span><br><span class="line">├── b</span><br><span class="line">└── x</span><br></pre></td></tr></table></figure><p>可以看到，在这个合并后的目录 C 里，有 a、b、x 三个文件，并且 x 文件只有一份。这，就是“合并”的含义。此外，如果你在目录 C 里对 a、b、x 文件做修改，这些修改也会在对应的目录 A、B 中生效。</p><hr><h3 id="Overlay2"><a href="#Overlay2" class="headerlink" title="Overlay2"></a>Overlay2</h3><p>AUFS是最古老的联合挂载文件系统,也是docker最初使用的文件系统.在新版本的docker中.使用的是overlay2文件系统.也是目前docker场景下性能最优秀的文件系统.overlay2也是在AUFS的基础之上发展而来.其原理和AUFS有相似之处.下面演示一下overlay2文件系统的用法</p><ul><li>当前我准备了5个目录.其中A,B每个目录下都有个a文件,其内容如下</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev ~]# tree -L 2 A B C D worker</span><br><span class="line">A</span><br><span class="line">├── a</span><br><span class="line">└── x</span><br><span class="line">B</span><br><span class="line">├── a</span><br><span class="line">├── b</span><br><span class="line">└── x</span><br><span class="line">C</span><br><span class="line">D</span><br><span class="line">worker</span><br><span class="line"></span><br><span class="line">[root@docker-dev ~]# cat A/a</span><br><span class="line">hello A haha</span><br><span class="line">[root@docker-dev ~]# cat B/a</span><br><span class="line">hello B</span><br></pre></td></tr></table></figure><p>和AUFS的工作方式类似,将ABC挂载到D这个目录下,其中A,B是底层不可修改目录,C是可读写目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev ~]# mount -t overlay overlay -o lowerdir=A:B,upperdir=C,workdir=worker D</span><br><span class="line"></span><br><span class="line">[root@docker-dev ~]# mount | grep overlay</span><br><span class="line">overlay on /root/D type overlay (rw,relatime,lowerdir=A:B,upperdir=C,workdir=worker)</span><br></pre></td></tr></table></figure><p>再次查看目录结构,发现D目录下多了三个从目录A和目录B合并过来的a,b,x文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev ~]# tree -L 2 A B C D worker</span><br><span class="line">A</span><br><span class="line">├── a</span><br><span class="line">└── x</span><br><span class="line">B</span><br><span class="line">├── a</span><br><span class="line">├── b</span><br><span class="line">└── x</span><br><span class="line">C</span><br><span class="line">D</span><br><span class="line">├── a</span><br><span class="line">├── b</span><br><span class="line">└── x</span><br><span class="line">worker</span><br><span class="line">└── work</span><br></pre></td></tr></table></figure><p>此时在挂载的D目录下,创建一个文件y.修改a文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev ~]# cd D</span><br><span class="line">[root@docker-dev D]# touch y</span><br><span class="line">[root@docker-dev ~]# echo &quot;hello from D&quot; &gt; D/a</span><br></pre></td></tr></table></figure><p>再次观察目录结构,发现新增或者修改的a和y文件出现在C目录下,A和B目录保持不变</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">A</span><br><span class="line">├── a</span><br><span class="line">└── x</span><br><span class="line">B</span><br><span class="line">├── a</span><br><span class="line">├── b</span><br><span class="line">└── x</span><br><span class="line">C</span><br><span class="line">├── a</span><br><span class="line">└── y</span><br><span class="line">D</span><br><span class="line">├── a</span><br><span class="line">├── b</span><br><span class="line">├── x</span><br><span class="line">└── y</span><br><span class="line">worker</span><br><span class="line">└── work</span><br></pre></td></tr></table></figure><p>查看A,B,C目录下的文件a的内容.发现A和B目录下的a文件内容不变,在挂载目录D下修改的a文件”出现”在C目录.</p><p>docker镜像就是使用了联合挂载的原理.镜像层类似于目录A和B,他们是不可写的,所有的写操作都发生在类似于目录C的可读写层..下面拿一个容器来举个例子</p><ul><li>启动一个容器</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev ~]# docker run -d --name c1  centos-demo sleep 999</span><br></pre></td></tr></table></figure><ul><li>查看该容器的存储信息.可以看到有2个底层(lowerDir层),以及一个init层.还有一个可读写的upperdir层</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&quot;GraphDriver&quot;: &#123;</span><br><span class="line">          &quot;Data&quot;: &#123;</span><br><span class="line">              &quot;LowerDir&quot;: &quot;/var/lib/docker/overlay2/e03a913bf2e0ed33012a912a5ae421e9d0ededc262c998656d828f8cd4a65e4c-init/diff:/var/lib/docker/overlay2/3c7dd67b43127ddc920230024b1c6c7ead18144cd7ed16d44b7093d9e73fb136/diff:/var/lib/docker/overlay2/53f22fea6b813c35a5356493a840fb550fe75593174bc192446224a9bd0dddbd/diff&quot;,</span><br><span class="line">              &quot;MergedDir&quot;: &quot;/var/lib/docker/overlay2/e03a913bf2e0ed33012a912a5ae421e9d0ededc262c998656d828f8cd4a65e4c/merged&quot;,</span><br><span class="line">              &quot;UpperDir&quot;: &quot;/var/lib/docker/overlay2/e03a913bf2e0ed33012a912a5ae421e9d0ededc262c998656d828f8cd4a65e4c/diff&quot;,</span><br><span class="line">              &quot;WorkDir&quot;: &quot;/var/lib/docker/overlay2/e03a913bf2e0ed33012a912a5ae421e9d0ededc262c998656d828f8cd4a65e4c/work&quot;</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;Name&quot;: &quot;overlay2&quot;</span><br></pre></td></tr></table></figure><p>为什么这里的lowerdir是2层呢? 因为该容器使用的centos-demo镜像就是一个2个layer组成的镜像.通过下面命令可以查看镜像的相关信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev ~]# docker inspect centos-demo</span><br><span class="line"> &quot;RootFS&quot;: &#123;</span><br><span class="line">            &quot;Type&quot;: &quot;layers&quot;,</span><br><span class="line">            &quot;Layers&quot;: [</span><br><span class="line">                &quot;sha256:174f5685490326fc0a1c0f5570b8663732189b327007e47ff13d2ca59673db02&quot;,</span><br><span class="line">                &quot;sha256:70d298037a86080efd38fbbd051a03c9708b92522cfb58aeef2ed21429ef9b22&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br></pre></td></tr></table></figure><p>该镜像的Dockerfile文件显示,该镜像只由两个指令组成: FROM和RUN.所以使用<code>docker build</code>命令编译成镜像后,该镜像只包含2个layer</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev ~]# cat centos/Dockerfile</span><br><span class="line">FROM centos:7</span><br><span class="line">RUN yum update -y \</span><br><span class="line">    &amp;&amp; yum install -y net-tools \</span><br><span class="line">       tcpdump \</span><br><span class="line">       bind-utils \</span><br><span class="line">    &amp;&amp; yum clean all \</span><br><span class="line">    &amp;&amp; rm -rf /var/cache/yum/*</span><br></pre></td></tr></table></figure><p>回到容器本身.从<code>docker inspect c1</code>命令的结果可以看到该容器的各个layer的信息.进入到<code>/var/lib/docker/overlay2/</code> 目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev ~]# cd /var/lib/docker/overlay2/</span><br></pre></td></tr></table></figure><p>下面这个子目录显示的就是<code>FROM centos:7</code> 指令中基础镜像<code>centos:7</code>的文件系统</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev overlay2]# ls 53f22fea6b813c35a5356493a840fb550fe75593174bc192446224a9bd0dddbd/diff</span><br><span class="line">anaconda-post.log  bin  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br></pre></td></tr></table></figure><p>我们来关注一下容器的可读写层.由于容器刚启动,我们没有对容器进行任何变更.所以容器里没有写入任何新数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev overlay2]# ll e03a913bf2e0ed33012a912a5ae421e9d0ededc262c998656d828f8cd4a65e4c/diff</span><br><span class="line">total 0</span><br><span class="line">[root@docker-dev overlay2]#</span><br></pre></td></tr></table></figure><p>在容器内部的<code>/tmp</code>目录下写入一个新的文件.内容为”hello this is c1”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev overlay2]# docker exec -it c1  bash</span><br><span class="line">[root@55771f4dbf23 /]# echo &quot;hello this is c1&quot; &gt; /tmp/c1.txt</span><br><span class="line">[root@55771f4dbf23 /]# cat /tmp/c1.txt</span><br><span class="line">hello this is c1</span><br></pre></td></tr></table></figure><p>再次查看宿主机上该容器的可读写层目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev overlay2]# ll e03a913bf2e0ed33012a912a5ae421e9d0ededc262c998656d828f8cd4a65e4c/diff/</span><br><span class="line">total 0</span><br><span class="line">drwxrwxrwt 2 root root 20 May  9 22:55 tmp</span><br><span class="line"></span><br><span class="line">[root@docker-dev overlay2]# cat e03a913bf2e0ed33012a912a5ae421e9d0ededc262c998656d828f8cd4a65e4c/diff/tmp/c1.txt</span><br><span class="line">hello this is c1</span><br></pre></td></tr></table></figure><p>从上面的例子中我们可以看到<strong>容器的 rootfs 由如下图所示的三部分组成：</strong></p><p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20210509225741148.png" alt="image-20210509225741148"></p><blockquote><p>图是盗来的,非本例子中的实际layer信息,但是大同小异</p></blockquote><p><strong>第一部分，只读层。</strong></p><p>它是这个容器的 rootfs 最下面的五层，对应的正是 <code>centos:7</code> 镜像的2层。可以看到，它们的挂载方式都是只读的</p><p><strong>第二部分，可读写层。</strong></p><p>它是这个容器的 rootfs 最上面的一层（e03a913bf2e0ed33），它的挂载方式为：rw，即 read write。在没有写入文件之前，这个目录是空的。而一旦在容器里做了写操作，你修改产生的内容就会以增量的方式出现在这个层中。</p><p>可是，你有没有想到这样一个问题：如果我现在要做的，是删除只读层里的一个文件呢？</p><p>为了实现这样的删除操作，overlay会在可读写层创建一个 whiteout 文件，把只读层里的文件“遮挡”起来。</p><p>比如，你要删除只读层里一个名叫 foo 的文件，那么这个删除操作实际上是在可读写层创建了一个名叫.wh.foo 的文件。这样，当这两个层被联合挂载之后，foo 文件就会被.wh.foo 文件“遮挡”起来，“消失”了。这个功能，就是“ro+wh”的挂载方式，即只读 +whiteout 的含义。我喜欢把 whiteout 形象地翻译为：“白障”。</p><p>所以，最上面这个可读写层的作用，就是专门用来存放你修改 rootfs 后产生的增量，无论是增、删、改，都发生在这里。而当我们使用完了这个被修改过的容器之后，还可以使用 docker commit 和 push 指令，保存这个被修改过的可读写层，并上传到 Docker Hub 上，供其他人使用；而与此同时，原先的只读层里的内容则不会有任何变化。这，就是增量 rootfs 的好处。</p><h4 id="init层"><a href="#init层" class="headerlink" title="init层"></a>init层</h4><p>它是一个以“-init”结尾的层，夹在只读层和读写层之间。Init 层是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等信息。</p><p>需要这样一层的原因是，这些文件本来属于只读的 Ubuntu 镜像的一部分，但是用户往往需要在启动容器时写入一些指定的值比如 hostname，所以就需要在可读写层对它们进行修改。</p><p>可是，这些修改往往只对当前的容器有效，我们并不希望执行 docker commit 时，把这些信息连同可读写层一起提交掉。</p><p>所以，Docker 做法是，在修改了这些文件之后，以一个单独的层挂载了出来。而用户执行 docker commit 只会提交可读写层，所以是不包含这些内容的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@docker-dev overlay2]# ll e03a913bf2e0ed33012a912a5ae421e9d0ededc262c998656d828f8cd4a65e4c-init</span><br><span class="line">total 8</span><br><span class="line">-rw------- 1 root root  0 May  9 22:40 committed</span><br><span class="line">drwxr-xr-x 4 root root 46 May  9 22:40 diff</span><br><span class="line">-rw-r--r-- 1 root root 26 May  9 22:40 link</span><br><span class="line">-rw-r--r-- 1 root root 57 May  9 22:40 lower</span><br><span class="line">drwx------ 3 root root 18 May  9 22:40 work</span><br><span class="line">[root@docker-dev overlay2]# ll e03a913bf2e0ed33012a912a5ae421e9d0ededc262c998656d828f8cd4a65e4c-init/diff</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x 4 root root 43 May  9 22:40 dev</span><br><span class="line">drwxr-xr-x 2 root root 66 May  9 22:40 etc</span><br><span class="line">[root@docker-dev overlay2]# ll e03a913bf2e0ed33012a912a5ae421e9d0ededc262c998656d828f8cd4a65e4c-init/diff/etc</span><br><span class="line">total 0</span><br><span class="line">-rwxr-xr-x 1 root root  0 May  9 22:40 hostname</span><br><span class="line">-rwxr-xr-x 1 root root  0 May  9 22:40 hosts</span><br><span class="line">lrwxrwxrwx 1 root root 12 May  9 22:40 mtab -&gt; /proc/mounts</span><br><span class="line">-rwxr-xr-x 1 root root  0 May  9 22:40 resolv.conf</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在今天的分享中，我着重介绍了 Linux 容器文件系统的实现方式。而这种机制，正是我们经常提到的容器镜像，也叫作：rootfs。它只是一个操作系统的所有文件和目录，并不包含内核，最多也就几百兆。而相比之下，传统虚拟机的镜像大多是一个磁盘的“快照”，磁盘有多大，镜像就至少有多大。</p><p>通过结合使用 Mount Namespace 和 rootfs，容器就能够为进程构建出一个完善的文件系统隔离环境。当然，这个功能的实现还必须感谢 chroot 和 pivot_root 这两个系统调用切换进程根目录的能力。</p><p>而在 rootfs 的基础上，Docker 公司创新性地提出了使用多个增量 rootfs 联合挂载一个完整 rootfs 的方案，这就是容器镜像中“层”的概念。</p><p>通过“分层镜像”的设计，以 Docker 镜像为核心，来自不同公司、不同团队的技术人员被紧密地联系在了一起。而且，由于容器镜像的操作是增量式的，这样每次镜像拉取、推送的内容，比原本多个完整的操作系统的大小要小得多；而共享层的存在，可以使得所有这些容器镜像需要的总空间，也比每个镜像的总和要小。这样就使得基于容器镜像的团队协作，要比基于动则几个 GB 的虚拟机磁盘镜像的协作要敏捷得多。</p><p>更重要的是，一旦这个镜像被发布，那么你在全世界的任何一个地方下载这个镜像，得到的内容都完全一致，可以完全复现这个镜像制作者当初的完整环境。这，就是容器技术“强一致性”的重要体现。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>Cgroup: <a href="https://coolshell.cn/articles/17049.html" target="_blank" rel="noopener">https://coolshell.cn/articles/17049.html</a> (这位大佬的很多文章都值得学习)<br>理解overlay2: <a href="https://www.cnblogs.com/jiangbo44/p/14056898.html" target="_blank" rel="noopener">https://www.cnblogs.com/jiangbo44/p/14056898.html</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Docker原理—-3-mount挂载&quot;&gt;&lt;a href=&quot;#Docker原理—-3-mount挂载&quot; class=&quot;headerlink&quot; title=&quot;Docker原理—-3.mount挂载&quot;&gt;&lt;/a&gt;Docker原理—-3.mount挂载&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;而正如我前面所说的，Namespace 的作用是“隔离”，它让应用进程只能看到该 Namespace 内的“世界”；而 Cgroups 的作用是“限制”，它给这个“世界”围上了一圈看不见的墙。这么一折腾，进程就真的被“装”在了一个与世隔绝的房间里，而这些房间就是 PaaS 项目赖以生存的应用“沙盒”。&lt;/p&gt;
&lt;p&gt;可是，还有一个问题不知道你有没有仔细思考过：这个房间四周虽然有了墙，但是如果容器进程低头一看地面，又是怎样一副景象呢？&lt;/p&gt;
&lt;p&gt;换句话说，&lt;strong&gt;容器里的进程看到的文件系统又是什么样子的呢？&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>prometheus监控mysql</title>
    <link href="https://jesse.top/2021/03/10/prometheus/prometheus%E7%9B%91%E6%8E%A7mysql/"/>
    <id>https://jesse.top/2021/03/10/prometheus/prometheus监控mysql/</id>
    <published>2021-03-09T23:59:58.000Z</published>
    <updated>2021-04-01T13:59:58.464Z</updated>
    
    <content type="html"><![CDATA[<h3 id="prometheus监控mysql"><a href="#prometheus监控mysql" class="headerlink" title="prometheus监控mysql"></a>prometheus监控mysql</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>本文使用的是官方的mysqld_exporter.github地址:<a href="https://github.com/prometheus/mysqld_exporter" target="_blank" rel="noopener">mysqld_exporter</a></p><blockquote><p>mysql版本需要在5.6版本或以上</p></blockquote><p>mysqld_exporter提供很多监控项(具体参考github项目介绍).如果需要开启一个监控项,在启动mysqld_exporter时,携带以下命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--collect.key</span><br></pre></td></tr></table></figure><p>如果需要关闭某个监控项.携带以下命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--no-collect.key</span><br></pre></td></tr></table></figure><blockquote><p>如果mysqld_exporter的版本小于0.10.0,命令有些变化,双横杠变成单横杠,使用-collect.key 或者-collect.key=True|false</p></blockquote><hr><h3 id="mysqld-exporter安装"><a href="#mysqld-exporter安装" class="headerlink" title="mysqld_exporter安装"></a>mysqld_exporter安装</h3><p>安装方式很简单,.下面是一个Ansible脚本以供参考</p><a id="more"></a><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">- hosts:</span> <span class="string">mysql-prod</span></span><br><span class="line"><span class="attr">  remote_user:</span> <span class="string">work</span></span><br><span class="line"><span class="attr">  become:</span> <span class="literal">yes</span></span><br><span class="line"><span class="attr">  gather_facts:</span> <span class="literal">no</span></span><br><span class="line"><span class="attr">  tasks:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">检查mysqld_exporter是否已经安装</span></span><br><span class="line"><span class="attr">      systemd:</span> </span><br><span class="line"><span class="attr">        name:</span> <span class="string">mysqld_exporter</span></span><br><span class="line"><span class="attr">        state:</span> <span class="string">started</span></span><br><span class="line"><span class="attr">      ignore_errors:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">      register:</span> <span class="string">result</span></span><br><span class="line">    </span><br><span class="line"><span class="attr">    - name:</span> <span class="string">安装mysqld_exporter</span></span><br><span class="line"><span class="attr">      unarchive:</span></span><br><span class="line"><span class="attr">        src:</span> <span class="attr">http://repo.doweidu.com/prometheus/mysqld_exporter-0.12.1.linux-amd64.tar.gz</span></span><br><span class="line"><span class="attr">        dest:</span> <span class="string">/home/work/</span></span><br><span class="line"><span class="attr">        remote_src:</span> <span class="literal">yes</span></span><br><span class="line"><span class="attr">      when:</span> <span class="string">result</span> <span class="string">is</span> <span class="string">failed</span> </span><br><span class="line">    </span><br><span class="line"><span class="attr">    - name:</span> <span class="string">重命名</span></span><br><span class="line"><span class="attr">      command:</span> <span class="string">mv</span> <span class="string">/home/work/mysqld_exporter-0.12.1.linux-amd64</span> <span class="string">/home/work/mysqld_exporter</span></span><br><span class="line"><span class="attr">      when:</span> <span class="string">result</span> <span class="string">is</span> <span class="string">failed</span> </span><br><span class="line">    </span><br><span class="line"><span class="attr">    - name:</span> <span class="string">拷贝到/usr/local下</span></span><br><span class="line"><span class="attr">      shell:</span> <span class="string">cp</span> <span class="bullet">-r</span> <span class="string">/home/work/mysqld_exporter</span> <span class="string">/usr/local/</span></span><br><span class="line"><span class="attr">      when:</span> <span class="string">result</span> <span class="string">is</span> <span class="string">failed</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">拷贝配置文件到/usr/local/mysqld_exporter下</span></span><br><span class="line"><span class="attr">      copy:</span> <span class="string">src=files/localhost_db.cnf</span> <span class="string">dest=/usr/local/mysqld_exporter/</span></span><br><span class="line"><span class="attr">      when:</span> <span class="string">result</span> <span class="string">is</span> <span class="string">failed</span></span><br><span class="line">    </span><br><span class="line"><span class="attr">    - name:</span> <span class="string">拷贝systemd文件</span></span><br><span class="line"><span class="attr">      copy:</span> <span class="string">src=files/mysqld_exporter.service</span> <span class="string">dest=/usr/lib/systemd/system/mysqld_exporter.service</span></span><br><span class="line"><span class="attr">      when:</span> <span class="string">result</span> <span class="string">is</span> <span class="string">failed</span> </span><br><span class="line"></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">启动mysqld_exporter</span></span><br><span class="line"><span class="attr">      systemd:</span> <span class="string">name=mysqld_exporter</span> <span class="string">state=started</span> <span class="string">enabled=yes</span></span><br><span class="line"><span class="attr">      when:</span> <span class="string">result</span> <span class="string">is</span> <span class="string">failed</span></span><br></pre></td></tr></table></figure><p>这里需要准备一个<code>localhost_db.cnf</code>配置文件.内容如下</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[client]</span></span><br><span class="line"><span class="string">host=127.0.0.1</span>  <span class="comment">#mysql服务器地址</span></span><br><span class="line"><span class="comment">#port=          #mysql端口,默认为3306,如果端口是默认3306,可以删掉这行</span></span><br><span class="line"><span class="string">user=zabbix</span>     <span class="comment">#mysql用于监控的账号</span></span><br><span class="line"><span class="string">password=xxxxxx</span>  <span class="comment">#密码</span></span><br></pre></td></tr></table></figure><p>mysqld_exporter启动脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=mysqld_exporter</span><br><span class="line">Documentation=https://prometheus.io/</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">User=root</span><br><span class="line">ExecStart=/usr/local/mysqld_exporter/mysqld_exporter --config.my-cnf=/usr/local/mysqld_exporter/localhost_db.cnf</span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>Ansible脚本执行完毕后,就可以通过<code>IP:9104</code>来访问mysql的Metrics</p><p>为了监控到主机的情况.此时还需要部署node_exporter客户端.关于Node_exporter我在另一篇笔记中再详细介绍</p><hr><h4 id="prometheus配置"><a href="#prometheus配置" class="headerlink" title="prometheus配置"></a>prometheus配置</h4><p>由于mysqld_exporter没有任何监控项能获取到mysql服务器的主机名.所以将数据展示到grafana时,只能通过IP去查看监控图表.这非常的不方便.比如下面截图中,当mysql服务器数量较多时,很难知道IP地址对应的是具体哪台服务器:</p><p><img src="https://img2.jesse.top/image-20201126103328877.png" alt="image-20201126103328877"></p><p>此时,就需要在prometheus配置文件中,将每个mysql服务器添加labels,给服务器打上主机名和组名的标签</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">- job_name: &apos;aliyun-mysql-exporter&apos;</span><br><span class="line">    static_configs:</span><br><span class="line">     - targets:</span><br><span class="line">         - &apos;10.111.200.176:9104&apos;</span><br><span class="line">       labels:</span><br><span class="line">         hostname: &apos;mg-goodscenter-mysql-master&apos;</span><br><span class="line">         group: &apos;mg-mysql&apos;</span><br><span class="line"></span><br><span class="line">     - targets: [&apos;10.111.50.39:9104&apos;]</span><br><span class="line">       labels:</span><br><span class="line">         hostname: &apos;mg-msg-mysql-master&apos;</span><br><span class="line">         group: &apos;mg-mysql&apos;</span><br><span class="line"></span><br><span class="line">......以下省略......</span><br></pre></td></tr></table></figure><hr><h4 id="grafana-dashboard"><a href="#grafana-dashboard" class="headerlink" title="grafana dashboard"></a>grafana dashboard</h4><p>下载dashboard: <a href="https://grafana.com/grafana/dashboards/7362" target="_blank" rel="noopener">https://grafana.com/grafana/dashboards/7362</a> </p><p>或者直接在grafana中添加7362的dashboard</p><p>原生的dashboard只有一个instance的变量.为了添加主机名,更好的区分和展示监控效果,需要做一些修改.</p><p><strong>1.定义变量:</strong></p><ul><li><p>job变量:</p><ul><li>name:job</li><li><p>label: job</p></li><li><p>query: label_values(mysql_up,job)</p></li></ul></li><li><p>group变量:</p><ul><li>name: group</li><li>label:主机组</li><li>Query:label_values(mysql_up{job=~”$job”}, group)</li></ul></li></ul><p><strong>2.修改变量</strong></p><p>将host变量修改为:</p><p>name: host</p><p>label: 主机名</p><p>query:label_values(mysql_up{job=~”$job”,group=~”$group”}, hostname)</p><p><strong>3.变量页面最终设置如下:</strong></p><p><img src="https://img2.jesse.top/image-20201126105202201.png" alt="image-20201126105202201"></p><p>接着,将当前的dashboard的json文件导出.复制下面的json文件</p><p><img src="https://img2.jesse.top/image-20201126105345163.png" alt="image-20201126105345163"></p><p>将文件中的<code>instance=~</code>全部替换为<code>hostname=~</code>.然后复制回去,点击<code>Save Changes</code></p><p>此时,可以通过主机组和主机名筛选具体的mysql服务器.</p><p><img src="https://img2.jesse.top/image-20210329171207919.png" alt="image-20210329171207919"></p><p><img src="https://img2.jesse.top/image-20210329171842731.png" alt="image-20210329171842731"></p><blockquote><p>别忘记保存dashboard</p></blockquote><hr><h4 id="rules告警规则"><a href="#rules告警规则" class="headerlink" title="rules告警规则"></a>rules告警规则</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br></pre></td><td class="code"><pre><span class="line">groups:</span><br><span class="line">- name: MySQLStatsAlert</span><br><span class="line">  rules:</span><br><span class="line">  - alert: MySQL is down</span><br><span class="line">    expr: mysql_up == 0</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; MySQL is down&quot;</span><br><span class="line">      description: &quot;MySQL database is down. This requires immediate action!&quot;</span><br><span class="line">  - alert: open files high</span><br><span class="line">    expr: mysql_global_status_innodb_num_open_files &gt; (mysql_global_variables_open_files_limit) * 0.75</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; open files high&quot;</span><br><span class="line">      description: &quot;Open files is high. Please consider increasing open_files_limit.&quot;</span><br><span class="line">  - alert: Read buffer size is bigger than max. allowed packet size</span><br><span class="line">    expr: mysql_global_variables_read_buffer_size &gt; mysql_global_variables_slave_max_allowed_packet</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Read buffer size is bigger than max. allowed packet size&quot;</span><br><span class="line">      description: &quot;Read buffer size (read_buffer_size) is bigger than max. allowed packet size (max_allowed_packet).This can break your replication.&quot;</span><br><span class="line">  - alert: Sort buffer possibly missconfigured</span><br><span class="line">    expr: mysql_global_variables_innodb_sort_buffer_size &lt;256*1024 or mysql_global_variables_read_buffer_size &gt; 4*1024*1024</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Sort buffer possibly missconfigured&quot;</span><br><span class="line">      description: &quot;Sort buffer size is either too big or too small. A good value for sort_buffer_size is between 256k and 4M.&quot;</span><br><span class="line">  - alert: Thread stack size is too small</span><br><span class="line">    expr: mysql_global_variables_thread_stack &lt;196608</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Thread stack size is too small&quot;</span><br><span class="line">      description: &quot;Thread stack size is too small. This can cause problems when you use Stored Language constructs for example. A typical is 256k for thread_stack_size.&quot;</span><br><span class="line">  - alert: Used more than 80% of max connections limited</span><br><span class="line">    expr: mysql_global_status_max_used_connections &gt; mysql_global_variables_max_connections * 0.8</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Used more than 80% of max connections limited&quot;</span><br><span class="line">      description: &quot;Used more than 80% of max connections limited&quot;</span><br><span class="line">  - alert: InnoDB Force Recovery is enabled</span><br><span class="line">    expr: mysql_global_variables_innodb_force_recovery != 0</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Force Recovery is enabled&quot;</span><br><span class="line">      description: &quot;InnoDB Force Recovery is enabled. This mode should be used for data recovery purposes only. It prohibits writing to the data.&quot;</span><br><span class="line">  - alert: InnoDB Log File size is too small</span><br><span class="line">    expr: mysql_global_variables_innodb_log_file_size &lt; 16777216</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Log File size is too small&quot;</span><br><span class="line">      description: &quot;The InnoDB Log File size is possibly too small. Choosing a small InnoDB Log File size can have significant performance impacts.&quot;</span><br><span class="line">  - alert: InnoDB Flush Log at Transaction Commit</span><br><span class="line">    expr: mysql_global_variables_innodb_flush_log_at_trx_commit != 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Flush Log at Transaction Commit&quot;</span><br><span class="line">      description: &quot;InnoDB Flush Log at Transaction Commit is set to a values != 1. This can lead to a loss of commited transactions in case of a power failure.&quot;</span><br><span class="line">  - alert: Table definition cache too small</span><br><span class="line">    expr: mysql_global_status_open_table_definitions &gt; mysql_global_variables_table_definition_cache</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Table definition cache too small&quot;</span><br><span class="line">      description: &quot;Your Table Definition Cache is possibly too small. If it is much too small this can have significant performance impacts!&quot;</span><br><span class="line">  - alert: Table open cache too small</span><br><span class="line">    expr: mysql_global_status_open_tables &gt;mysql_global_variables_table_open_cache * 99/100</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Table open cache too small&quot;</span><br><span class="line">      description: &quot;Your Table Open Cache is possibly too small (old name Table Cache). If it is much too small this can have significant performance impacts!&quot;</span><br><span class="line">  - alert: Thread stack size is possibly too small</span><br><span class="line">    expr: mysql_global_variables_thread_stack &lt; 262144</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Thread stack size is possibly too small&quot;</span><br><span class="line">      description: &quot;Thread stack size is possibly too small. This can cause problems when you use Stored Language constructs for example. A typical is 256k for thread_stack_size.&quot;</span><br><span class="line">  - alert: InnoDB Buffer Pool Instances is too small</span><br><span class="line">    expr: mysql_global_variables_innodb_buffer_pool_instances == 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Buffer Pool Instances is too small&quot;</span><br><span class="line">      description: &quot;If you are using MySQL 5.5 and higher you should use several InnoDB Buffer Pool Instances for performance reasons. Some rules are: InnoDB Buffer Pool Instance should be at least 1 Gbyte in size. InnoDB Buffer Pool Instances you can set equal to the number of cores of your machine.&quot;</span><br><span class="line">  - alert: InnoDB Plugin is enabled</span><br><span class="line">    expr: mysql_global_variables_ignore_builtin_innodb == 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Plugin is enabled&quot;</span><br><span class="line">      description: &quot;InnoDB Plugin is enabled&quot;</span><br><span class="line">  - alert: Binary Log is disabled</span><br><span class="line">    expr: mysql_global_variables_log_bin != 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Binary Log is disabled&quot;</span><br><span class="line">      description: &quot;Binary Log is disabled. This prohibits you to do Point in Time Recovery (PiTR).&quot;</span><br><span class="line">  - alert: Binlog Cache size too small</span><br><span class="line">    expr: mysql_global_variables_binlog_cache_size &lt; 1048576</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Binlog Cache size too small&quot;</span><br><span class="line">      description: &quot;Binlog Cache size is possibly to small. A value of 1 Mbyte or higher is OK.&quot;</span><br><span class="line">  - alert: Binlog Statement Cache size too small</span><br><span class="line">    expr: mysql_global_variables_binlog_stmt_cache_size &lt;1048576 and mysql_global_variables_binlog_stmt_cache_size &gt; 0</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Binlog Statement Cache size too small&quot;</span><br><span class="line">      description: &quot;Binlog Statement Cache size is possibly to small. A value of 1 Mbyte or higher is typically OK.&quot;</span><br><span class="line">  - alert: Binlog Transaction Cache size too small</span><br><span class="line">    expr: mysql_global_variables_binlog_cache_size  &lt;1048576</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Binlog Transaction Cache size too small&quot;</span><br><span class="line">      description: &quot;Binlog Transaction Cache size is possibly to small. A value of 1 Mbyte or higher is typically OK.&quot;</span><br><span class="line">  - alert: Sync Binlog is enabled</span><br><span class="line">    expr: mysql_global_variables_sync_binlog == 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Sync Binlog is enabled&quot;</span><br><span class="line">      description: &quot;Sync Binlog is enabled. This leads to higher data security but on the cost of write performance.&quot;</span><br><span class="line">  - alert: IO thread stopped</span><br><span class="line">    expr: mysql_slave_status_slave_io_running != 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; IO thread stopped&quot;</span><br><span class="line">      description: &quot;IO thread has stopped. This is usually because it cannot connect to the Master any more.&quot;</span><br><span class="line">  - alert: SQL thread stopped</span><br><span class="line">    expr: mysql_slave_status_slave_sql_running == 0</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; SQL thread stopped&quot;</span><br><span class="line">      description: &quot;SQL thread has stopped. This is usually because it cannot apply a SQL statement received from the master.&quot;</span><br><span class="line">  - alert: SQL thread stopped</span><br><span class="line">    expr: mysql_slave_status_slave_sql_running != 1</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: critical</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Sync Binlog is enabled&quot;</span><br><span class="line">      description: &quot;SQL thread has stopped. This is usually because it cannot apply a SQL statement received from the master.&quot;</span><br><span class="line">  - alert: Slave lagging behind Master</span><br><span class="line">    expr: rate(mysql_slave_status_seconds_behind_master[1m]) &gt;30</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Slave lagging behind Master&quot;</span><br><span class="line">      description: &quot;Slave is lagging behind Master. Please check if Slave threads are running and if there are some performance issues!&quot;</span><br><span class="line">  - alert: Slave is NOT read only(Please ignore this warning indicator.)</span><br><span class="line">    expr: mysql_global_variables_read_only != 0</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: page</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125; Slave is NOT read only&quot;</span><br><span class="line">      description: &quot;Slave is NOT set to read only. You can accidentally manipulate data on the slave and get inconsistencies...&quot;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;prometheus监控mysql&quot;&gt;&lt;a href=&quot;#prometheus监控mysql&quot; class=&quot;headerlink&quot; title=&quot;prometheus监控mysql&quot;&gt;&lt;/a&gt;prometheus监控mysql&lt;/h3&gt;&lt;h4 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h4&gt;&lt;p&gt;本文使用的是官方的mysqld_exporter.github地址:&lt;a href=&quot;https://github.com/prometheus/mysqld_exporter&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;mysqld_exporter&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;mysql版本需要在5.6版本或以上&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;mysqld_exporter提供很多监控项(具体参考github项目介绍).如果需要开启一个监控项,在启动mysqld_exporter时,携带以下命令:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;--collect.key&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果需要关闭某个监控项.携带以下命令:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;--no-collect.key&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;如果mysqld_exporter的版本小于0.10.0,命令有些变化,双横杠变成单横杠,使用-collect.key 或者-collect.key=True|false&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&quot;mysqld-exporter安装&quot;&gt;&lt;a href=&quot;#mysqld-exporter安装&quot; class=&quot;headerlink&quot; title=&quot;mysqld_exporter安装&quot;&gt;&lt;/a&gt;mysqld_exporter安装&lt;/h3&gt;&lt;p&gt;安装方式很简单,.下面是一个Ansible脚本以供参考&lt;/p&gt;
    
    </summary>
    
      <category term="prometheus" scheme="https://jesse.top/categories/prometheus/"/>
    
    
      <category term="prometheus" scheme="https://jesse.top/tags/prometheus/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes DaemonSet</title>
    <link href="https://jesse.top/2021/02/06/kubernetes/controller/daemonset/"/>
    <id>https://jesse.top/2021/02/06/kubernetes/controller/daemonset/</id>
    <published>2021-02-06T15:39:58.000Z</published>
    <updated>2021-04-01T13:41:29.050Z</updated>
    
    <content type="html"><![CDATA[<h2 id="DaemonSet"><a href="#DaemonSet" class="headerlink" title="DaemonSet"></a>DaemonSet</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>顾名思义,DaemonSet的主要作用是让你在kubernetes集群里运行一个Daemon Pod.所以,这个Pod有如下三个特征:</p><p>1.这个Pod运行在kubernetes集群的每一个节点(Node)上</p><p>2.每个节点只有一个这样的Pod实例</p><p>3.当有新的节点加入 Kubernetes 集群后，该 Pod 会自动地在新节点上被创建出来；而当旧节点被删除后，它上面的 Pod 也相应地会被回收掉.</p><p>这个机制听起来很简单，但 Daemon Pod 的意义确实是非常重要的。我随便给你列举几个例子：</p><ol><li>各种网络插件的 Agent 组件，都必须运行在每一个节点上，用来处理这个节点上的容器网络；</li><li>各种存储插件的 Agent 组件，也必须运行在每一个节点上，用来在这个节点上挂载远程存储目录，操作容器的 Volume 目录；</li><li>各种监控组件和日志组件，也必须运行在每一个节点上，负责这个节点上的监控信息和日志搜集。</li></ol><a id="more"></a><p>更重要的是，跟其他编排对象不一样，DaemonSet 开始运行的时机，很多时候比整个 Kubernetes 集群出现的时机都要早。</p><p>这个乍一听起来可能有点儿奇怪。但其实你来想一下：如果这个 DaemonSet 正是一个网络插件的 Agent 组件呢？</p><p>这个时候，整个 Kubernetes 集群里还没有可用的容器网络，所有 Worker 节点的状态都是 NotReady（NetworkReady=false）。这种情况下，普通的 Pod 肯定不能运行在这个集群上。所以，这也就意味着 DaemonSet 的设计，必须要有某种“过人之处”才行。</p><h3 id="DaemonSet工作原理"><a href="#DaemonSet工作原理" class="headerlink" title="DaemonSet工作原理"></a>DaemonSet工作原理</h3><p>为了弄清楚 DaemonSet 的工作原理，我们还是按照老规矩，先从它的 API 对象的定义说起。下面是一个Nginx的daemonset资源清单</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      name: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: nginx</span><br><span class="line">    spec:</span><br><span class="line">      tolerations:</span><br><span class="line">        - key: node-role.kubernetes.io/master</span><br><span class="line">          effect: NoSchedule</span><br><span class="line">      containers:</span><br><span class="line">        - name: nginx</span><br><span class="line">          image: nginx:1.19.6</span><br><span class="line">          resources:</span><br><span class="line">            limits:</span><br><span class="line">              memory: 200Mi</span><br><span class="line">            requests:</span><br><span class="line">              cpu: 100m</span><br><span class="line">              memory: 200Mi</span><br></pre></td></tr></table></figure><p>这个DaemonSet非常简单,管理一个Nginx镜像的Pod.可以看到DaemonSet和Deployment非常相似,只不过没有replicas字段.他也使用selector选择管理所有携带了name=Nginx标签的POD</p><h4 id="Daemonset创建Pod原理"><a href="#Daemonset创建Pod原理" class="headerlink" title="Daemonset创建Pod原理"></a>Daemonset创建Pod原理</h4><p>那么，<strong>DaemonSet 又是如何保证每个 Node 上有且只有一个被管理的 Pod 呢？</strong></p><p>显然，这是一个典型的“控制器模型”能够处理的问题。</p><p>DaemonSet Controller，首先从 Etcd 里获取所有的 Node 列表，然后遍历所有的 Node。这时，它就可以很容易地去检查，当前这个 Node 上是不是有一个携带了 name=fluentd-elasticsearch 标签的 Pod 在运行。</p><p>而检查的结果，可能有这么三种情况：</p><ol><li>没有这种 Pod，那么就意味着要在这个 Node 上创建这样一个 Pod；</li><li>有这种 Pod，但是数量大于 1，那就说明要把多余的 Pod 从这个 Node 上删除掉；</li><li>正好只有一个这种 Pod，那说明这个节点是正常的。</li></ol><p>其中，删除节点（Node）上多余的 Pod 非常简单，直接调用 Kubernetes API 就可以了。</p><p>但是，<strong>如何在指定的 Node 上创建新 Pod 呢？</strong></p><h5 id="1-利用nodeAffinity"><a href="#1-利用nodeAffinity" class="headerlink" title="1.利用nodeAffinity"></a>1.<strong>利用nodeAffinity</strong></h5><p>如果你已经熟悉了 Pod API 对象的话，那一定可以立刻说出答案：用 nodeSelector，选择 Node 的名字即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nodeSelector:</span><br><span class="line">    name: &lt;Node 名字 &gt;</span><br></pre></td></tr></table></figure><p>没错。不过，在 Kubernetes 项目里，nodeSelector 其实已经是一个将要被废弃的字段了。因为，现在有了一个新的、功能更完善的字段可以代替它，即：nodeAffinity。我来举个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: with-node-affinity</span><br><span class="line">spec:</span><br><span class="line">  affinity:</span><br><span class="line">    nodeAffinity:</span><br><span class="line">      requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">        nodeSelectorTerms:</span><br><span class="line">        - matchExpressions:</span><br><span class="line">          - key: metadata.name</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">            - node-geektime</span><br></pre></td></tr></table></figure><p>在这个 Pod 里，我声明了一个 spec.affinity 字段，然后定义了一个 nodeAffinity。</p><p>而在这里，我定义的 nodeAffinity 的含义是：</p><ol><li>requiredDuringSchedulingIgnoredDuringExecution：它的意思是说，这个 nodeAffinity 必须在每次调度的时候予以考虑。同时，这也意味着你可以设置在某些情况下不考虑这个 nodeAffinity；</li><li>这个 Pod，将来只允许运行在“<code>metadata.name</code>”是“node-geektime”的节点上。</li></ol><p>在这里，你应该注意到 nodeAffinity 的定义，可以支持更加丰富的语法，比如 operator: In（即：部分匹配；如果你定义 operator: Equal，就是完全匹配），这也正是 nodeAffinity 会取代 nodeSelector 的原因之一。</p><blockquote><p>其实在大多数时候，这些 Operator 语义没啥用处。所以说，在学习开源项目的时候，一定要学会抓住“主线”。不要顾此失彼。</p></blockquote><p>所以，<strong>我们的 DaemonSet Controller 会在创建 Pod 的时候，自动在这个 Pod 的 API 对象里，加上这样一个 nodeAffinity 定义</strong>。其中，需要绑定的节点名字，正是当前正在遍历的这个 Node。</p><p>当然，DaemonSet 并不需要修改用户提交的 YAML 文件里的 Pod 模板，而是在向 Kubernetes 发起请求之前，直接修改根据模板生成的 Pod 对象。这个思路，也正是我在前面讲解 Pod 对象时介绍过的。</p><p>创建刚才的Nginx的yaml清单:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl get pods -n kube-system -l name=nginx</span><br><span class="line">NAME          READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-b44l8   1/1     Running   0          66m</span><br><span class="line">nginx-cvkgz   1/1     Running   0          66m</span><br><span class="line">nginx-hldhl   1/1     Running   0          66m</span><br></pre></td></tr></table></figure><p>查看其中任意一个pod的yaml文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl get pods -n kube-system nginx-b44l8 -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">...略....</span><br><span class="line">spec:</span><br><span class="line">  affinity:</span><br><span class="line">    nodeAffinity:</span><br><span class="line">      requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">        nodeSelectorTerms:</span><br><span class="line">        - matchFields:</span><br><span class="line">          - key: metadata.name</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">            - k8s-node2</span><br><span class="line">...略....</span><br></pre></td></tr></table></figure><p>这个是DaemonSet自动为Pod打上的nodeaffinity节点亲和性属性,表示将该Pod调度到<code>k8s-node2</code>这个hostname的节点上.</p><blockquote><p>如果查看其它2个Nginx的Pod,他的nodeaffinity调度的节点名称自然也会不一样</p></blockquote><p><strong>通过nodeaffinity,DaemonSet就可以确保每个Pod都调度到不同的k8s节点.而不会将多个pod调度到同一个节点.但是如果需要确保Pod可以被调度到节点上,还需要利用另外一个和调度相关的字段:tolerations</strong></p><h5 id="2-利用tolerations"><a href="#2-利用tolerations" class="headerlink" title="2.利用tolerations"></a>2.利用tolerations</h5><p>tolerations(容忍度)这个字段意味着这个 Pod，会“容忍”（Toleration）某些 Node 的“污点”（Taint）。</p><p>而tolerations字段也是daemonset自动加上去的.还是执行上面的那条命令查看Pod的yaml清单文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tolerations:</span><br><span class="line">  - effect: NoSchedule</span><br><span class="line">    key: node-role.kubernetes.io/master</span><br><span class="line">  - effect: NoExecute</span><br><span class="line">    key: node.kubernetes.io/not-ready</span><br><span class="line">    operator: Exists</span><br><span class="line">  - effect: NoExecute</span><br><span class="line">    key: node.kubernetes.io/unreachable</span><br><span class="line">    operator: Exists</span><br><span class="line">  - effect: NoSchedule</span><br><span class="line">    key: node.kubernetes.io/disk-pressure</span><br><span class="line">    operator: Exists</span><br><span class="line">  - effect: NoSchedule</span><br><span class="line">    key: node.kubernetes.io/memory-pressure</span><br><span class="line">    operator: Exists</span><br><span class="line">  - effect: NoSchedule</span><br><span class="line">    key: node.kubernetes.io/pid-pressure</span><br><span class="line">    operator: Exists</span><br><span class="line">  - effect: NoSchedule</span><br><span class="line">    key: node.kubernetes.io/unschedulable</span><br><span class="line">    operator: Exists</span><br></pre></td></tr></table></figure><p>可以看到DaemonSet自动为这个Pod打上了很多容忍度,包括节点not-ready,unreachable,节点的磁盘,内存,pid的压力,以及哪怕节点被标记为<code>unschedulable</code> .就使得这些 Pod 可以忽略所有这些节点限制，继而保证每个节点上都会被调度一个 Pod。当然，如果这个节点有故障的话，这个 Pod 可能会启动失败，而 DaemonSet 则会始终尝试下去，直到 Pod 启动成功。</p><blockquote><p>而在正常情况下，被标记了 unschedulable“污点”的 Node，是不会有任何 Pod 被调度上去的（effect: NoSchedule）</p></blockquote><p>当然,你也可以在daemonset的资源清单里手动加上各种toleration污点容忍度.就像上面的例子:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tolerations:</span><br><span class="line">        - key: node-role.kubernetes.io/master</span><br><span class="line">          effect: NoSchedule</span><br></pre></td></tr></table></figure><p>因为在默认情况下，Kubernetes 集群不允许用户在 Master 节点部署 Pod。因为，Master 节点默认携带了一个叫作<code>node-role.kubernetes.io/master</code>的“污点”。所以，为了能在 Master 节点上部署 DaemonSet 的 Pod，我就必须让这个 Pod“容忍”这个“污点”。</p><p>这时，你应该可以猜到，我在前面介绍到的<strong>DaemonSet 的“过人之处”，其实就是依靠 Toleration 实现的。</strong></p><p>假如当前 DaemonSet 管理的，是一个网络插件的 Agent Pod，那么你就必须在这个 DaemonSet 的 YAML 文件里，给它的 Pod 模板加上一个能够“容忍”<code>node.kubernetes.io/network-unavailable</code>“污点”的 Toleration。正如下面这个例子所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: network-plugin-agent</span><br><span class="line">    spec:</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: node.kubernetes.io/network-unavailable</span><br><span class="line">        operator: Exists</span><br><span class="line">        effect: NoSchedule</span><br></pre></td></tr></table></figure><p>在 Kubernetes 项目中，当一个节点的网络插件尚未安装时，这个节点就会被自动加上名为<code>node.kubernetes.io/network-unavailable</code>的“污点”。</p><p><strong>而通过这样一个 Toleration，调度器在调度这个 Pod 的时候，就会忽略当前节点上的“污点”，从而成功地将网络插件的 Agent 组件调度到这台机器上启动起来。</strong></p><h3 id="DaemonSet的滚动更新"><a href="#DaemonSet的滚动更新" class="headerlink" title="DaemonSet的滚动更新"></a>DaemonSet的滚动更新</h3><p>通过命令查看daemonset的对象</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl get ds nginx -n kube-system</span><br><span class="line">NAME    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">nginx   3         3         3       3            3           &lt;none&gt;          3h14m</span><br></pre></td></tr></table></figure><p>就会发现 DaemonSet 和 Deployment 一样，也有 DESIRED、CURRENT 等多个状态字段。这也就意味着，DaemonSet 可以像 Deployment 那样，进行版本管理。这个版本，可以使用 kubectl rollout history 看到该daemonset的发布版本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl rollout history daemonset nginx -n kube-system</span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">1         &lt;none&gt;</span><br></pre></td></tr></table></figure><p>接下来将nginx的镜像版本升级到1.19.0.顺便加上–record参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl set image ds nginx nginx=nginx:1.19.0 -n kube-system --record</span><br></pre></td></tr></table></figure><p>观察升级过程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl rollout status ds nginx -n kube-system</span><br><span class="line">Waiting for daemon set &quot;nginx&quot; rollout to finish: 1 out of 3 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;nginx&quot; rollout to finish: 1 out of 3 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;nginx&quot; rollout to finish: 1 out of 3 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;nginx&quot; rollout to finish: 2 out of 3 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;nginx&quot; rollout to finish: 2 out of 3 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;nginx&quot; rollout to finish: 2 out of 3 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;nginx&quot; rollout to finish: 2 of 3 updated pods are available...</span><br><span class="line">daemon set &quot;nginx&quot; successfully rolled out</span><br></pre></td></tr></table></figure><p>在rollout history里就能看到滚动更新的记录:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl rollout history daemonset nginx -n kube-system</span><br><span class="line">daemonset.apps/nginx</span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">3         kubectl set image ds/nginx nginx=nginx:latest --record=true --namespace=kube-system</span><br><span class="line">4         &lt;none&gt;</span><br><span class="line">5         kubectl set image ds/nginx nginx=nginx:latest --record=true --namespace=kube-system</span><br><span class="line">6         kubectl set image ds nginx nginx=nginx:1.19.0 --namespace=kube-system --record=true</span><br></pre></td></tr></table></figure><p>通过后面的具体命令可以看到,这里我们是发布到了第6版.有了版本号，你也就可以像 Deployment 一样，将 DaemonSet 回滚到某个指定的历史版本了。</p><p>而我在前面的文章中讲解 Deployment 对象的时候，曾经提到过，Deployment 管理这些版本，靠的是“一个版本对应一个 ReplicaSet 对象”。可是，DaemonSet 控制器操作的直接就是 Pod，不可能有 ReplicaSet 这样的对象参与其中。<strong>那么，它的这些版本又是如何维护的呢？</strong></p><p>所谓，一切皆对象！</p><p>在 Kubernetes 项目中，任何你觉得需要记录下来的状态，都可以被用 API 对象的方式实现。当然，“版本”也不例外。</p><p>Kubernetes v1.7 之后添加了一个 API 对象，名叫<strong>ControllerRevision</strong>，专门用来记录某种 Controller 对象的版本。比如，你可以通过如下命令查看 fluentd-elasticsearch 对应的 ControllerRevision：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl get controllerrevision -n kube-system -l name=nginx</span><br><span class="line">NAME               CONTROLLER             REVISION   AGE</span><br><span class="line">nginx-66bcf54bbc   daemonset.apps/nginx   5          3h17m</span><br><span class="line">nginx-6fdb467d8c   daemonset.apps/nginx   4          3h35m</span><br><span class="line">nginx-757b664477   daemonset.apps/nginx   3          3h10m</span><br><span class="line">nginx-7ccc97dc9f   daemonset.apps/nginx   6          4m30s</span><br><span class="line">[root@k8s-master daemonset]$</span><br></pre></td></tr></table></figure><p>可以看到每个版本号(REVISION)对应一个controller.而如果你使用 kubectl describe 查看这个 ControllerRevision 对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl describe controllerrevision -n kube-system nginx-7ccc97dc9f</span><br><span class="line">Name:         nginx-7ccc97dc9f</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Labels:       controller-revision-hash=7ccc97dc9f</span><br><span class="line">              name=nginx</span><br><span class="line">Annotations:  deprecated.daemonset.template.generation: 6</span><br><span class="line">              kubernetes.io/change-cause: kubectl set image ds nginx nginx=nginx:1.19.0 --namespace=kube-system --record=true</span><br><span class="line">API Version:  apps/v1</span><br><span class="line">Data:</span><br><span class="line">  Spec:</span><br><span class="line">    Template:</span><br><span class="line">      $patch:  replace</span><br><span class="line">      Metadata:</span><br><span class="line">        Creation Timestamp:  &lt;nil&gt;</span><br><span class="line">        Labels:</span><br><span class="line">          Name:  nginx</span><br><span class="line">      Spec:</span><br><span class="line">        Containers:</span><br><span class="line">          Image:              nginx:1.19.0</span><br><span class="line">          Image Pull Policy:  IfNotPresent</span><br><span class="line">          Name:               nginx</span><br><span class="line">          Resources:</span><br><span class="line">            Limits:</span><br><span class="line">              Memory:  200Mi</span><br><span class="line">            Requests:</span><br><span class="line">              Cpu:                     100m</span><br><span class="line">              Memory:                  200Mi</span><br><span class="line">          Termination Message Path:    /dev/termination-log</span><br><span class="line">          Termination Message Policy:  File</span><br><span class="line">        Dns Policy:                    ClusterFirst</span><br><span class="line">        Restart Policy:                Always</span><br><span class="line">        Scheduler Name:                default-scheduler</span><br><span class="line">        Security Context:</span><br><span class="line">        Termination Grace Period Seconds:  30</span><br><span class="line">        Tolerations:</span><br><span class="line">          Effect:  NoSchedule</span><br><span class="line">          Key:     node-role.kubernetes.io/master</span><br><span class="line">Kind:              ControllerRevision</span><br><span class="line">Metadata:</span><br><span class="line">  Creation Timestamp:  2021-03-30T06:00:05Z</span><br><span class="line">  Owner References:</span><br><span class="line">    API Version:           apps/v1</span><br><span class="line">    Block Owner Deletion:  true</span><br><span class="line">    Controller:            true</span><br><span class="line">    Kind:                  DaemonSet</span><br><span class="line">    Name:                  nginx</span><br><span class="line">    UID:                   21e0bf42-536c-43c6-b961-f6cc51317eba</span><br><span class="line">  Resource Version:        142550</span><br><span class="line">  Self Link:               /apis/apps/v1/namespaces/kube-system/controllerrevisions/nginx-7ccc97dc9f</span><br><span class="line">  UID:                     27f9800a-bafa-40d6-9a02-ca11fc2824ce</span><br><span class="line">Revision:                  6</span><br><span class="line">Events:                    &lt;none&gt;</span><br></pre></td></tr></table></figure><p>就会看到，这个 ControllerRevision 对象，实际上是在 Data 字段保存了该版本对应的完整的 DaemonSet 的 API 对象。并且，在 Annotation 字段保存了创建这个对象所使用的 kubectl 命令。</p><p>接下来，我们可以尝试将这个 DaemonSet 回滚到 Revision=4 时的状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl rollout undo daemonset nginx --to-revision=4 -n kube-system</span><br><span class="line">daemonset.apps/nginx rolled back</span><br></pre></td></tr></table></figure><p>这个 kubectl rollout undo 操作，实际上相当于读取到了 Revision=4 的 ControllerRevision 对象保存的 Data 字段。而这个 Data 字段里保存的信息，就是 Revision=1 时这个 DaemonSet 的完整 API 对象。</p><p>所以，现在 DaemonSet Controller 就可以使用这个历史 API 对象，对现有的 DaemonSet 做一次 PATCH 操作（等价于执行一次 kubectl apply -f “旧的 DaemonSet 对象”），从而把这个 DaemonSet“更新”到一个旧版本。</p><p>这也是为什么，在执行完这次回滚完成后，你会发现，DaemonSet 的 Revision 并不会从 Revision=6 退回到 4，而是会增加成 Revision=7。这是因为，一个新的 ControllerRevision 被创建了出来。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl rollout history daemonset nginx -n kube-system</span><br><span class="line">daemonset.apps/nginx</span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">3         kubectl set image ds/nginx nginx=nginx:latest --record=true --namespace=kube-system</span><br><span class="line">5         kubectl set image ds/nginx nginx=nginx:latest --record=true --namespace=kube-system</span><br><span class="line">6         kubectl set image ds nginx nginx=nginx:1.19.0 --namespace=kube-system --record=true</span><br><span class="line">7         &lt;none&gt;</span><br></pre></td></tr></table></figure><h5 id="商榷之处"><a href="#商榷之处" class="headerlink" title="商榷之处:"></a>商榷之处:</h5><p>原文文档里说是<code>一个新的 ControllerRevision 被创建了出来</code> .但是经过实践发现,并没有创建一个新的revision=7的controllerrevision.而是仍然使用revision=4的controllerrevision,只不过将他的版本从4替代成了7..仔细对比下面回滚后的controllerrevision信息和回滚之前的信息可以发现这点:</p><h5 id="回滚到revision-4后-revision7出现了原本revision4的位置"><a href="#回滚到revision-4后-revision7出现了原本revision4的位置" class="headerlink" title="回滚到revision=4后,revision7出现了原本revision4的位置"></a>回滚到revision=4后,revision7出现了原本revision4的位置</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl get controllerrevision -n kube-system -l name=nginx</span><br><span class="line">NAME               CONTROLLER             REVISION   AGE</span><br><span class="line">nginx-66bcf54bbc   daemonset.apps/nginx   5          3h22m</span><br><span class="line">nginx-6fdb467d8c   daemonset.apps/nginx   7          3h40m</span><br><span class="line">nginx-757b664477   daemonset.apps/nginx   3          3h14m</span><br><span class="line">nginx-7ccc97dc9f   daemonset.apps/nginx   6          9m26s</span><br></pre></td></tr></table></figure><h5 id="下面这个是回滚到revision-4之前的controllervision信息-可以看到没有创建一个新的controllerrevision-而是原本revision-4的nginx-6fdb467d8c版本变更成了7"><a href="#下面这个是回滚到revision-4之前的controllervision信息-可以看到没有创建一个新的controllerrevision-而是原本revision-4的nginx-6fdb467d8c版本变更成了7" class="headerlink" title="下面这个是回滚到revision=4之前的controllervision信息.可以看到没有创建一个新的controllerrevision,而是原本revision=4的nginx-6fdb467d8c版本变更成了7"></a>下面这个是回滚到revision=4之前的controllervision信息.可以看到没有创建一个新的controllerrevision,而是原本revision=4的<code>nginx-6fdb467d8c</code>版本变更成了7</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master daemonset]$kubectl get controllerrevision -n kube-system -l name=nginx</span><br><span class="line">NAME               CONTROLLER             REVISION   AGE</span><br><span class="line">nginx-66bcf54bbc   daemonset.apps/nginx   5          3h17m</span><br><span class="line">nginx-6fdb467d8c   daemonset.apps/nginx   4          3h35m</span><br><span class="line">nginx-757b664477   daemonset.apps/nginx   3          3h10m</span><br><span class="line">nginx-7ccc97dc9f   daemonset.apps/nginx   6          4m30s</span><br><span class="line">[root@k8s-master daemonset]$</span><br></pre></td></tr></table></figure><p>这可能是作者使用的k8s集群版本(1.11)和我实践的版本(1.17.3)不同</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>相比于 Deployment，DaemonSet 只管理 Pod 对象，然后通过 nodeAffinity 和 Toleration 这两个调度器的小功能，保证了每个节点上有且只有一个 Pod。</p><p>与此同时，DaemonSet 使用 ControllerRevision，来保存和管理自己对应的“版本”。这种“面向 API 对象”的设计思路，大大简化了控制器本身的逻辑，也正是 Kubernetes 项目“声明式 API”的优势所在。</p><p>而且，相信聪明的你此时已经想到了，StatefulSet 也是直接控制 Pod 对象的，那么它是不是也在使用 ControllerRevision 进行版本管理呢？</p><p>没错。在 Kubernetes 项目里，ControllerRevision 其实是一个通用的版本管理对象。这样，Kubernetes 项目就巧妙地避免了每种控制器都要维护一套冗余的代码和逻辑的问题。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料:"></a>参考资料:</h3><p>张磊—&lt;深入剖析Kubernetes&gt;: 容器化守护进程的意义：DaemonSet</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;DaemonSet&quot;&gt;&lt;a href=&quot;#DaemonSet&quot; class=&quot;headerlink&quot; title=&quot;DaemonSet&quot;&gt;&lt;/a&gt;DaemonSet&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;顾名思义,DaemonSet的主要作用是让你在kubernetes集群里运行一个Daemon Pod.所以,这个Pod有如下三个特征:&lt;/p&gt;
&lt;p&gt;1.这个Pod运行在kubernetes集群的每一个节点(Node)上&lt;/p&gt;
&lt;p&gt;2.每个节点只有一个这样的Pod实例&lt;/p&gt;
&lt;p&gt;3.当有新的节点加入 Kubernetes 集群后，该 Pod 会自动地在新节点上被创建出来；而当旧节点被删除后，它上面的 Pod 也相应地会被回收掉.&lt;/p&gt;
&lt;p&gt;这个机制听起来很简单，但 Daemon Pod 的意义确实是非常重要的。我随便给你列举几个例子：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;各种网络插件的 Agent 组件，都必须运行在每一个节点上，用来处理这个节点上的容器网络；&lt;/li&gt;
&lt;li&gt;各种存储插件的 Agent 组件，也必须运行在每一个节点上，用来在这个节点上挂载远程存储目录，操作容器的 Volume 目录；&lt;/li&gt;
&lt;li&gt;各种监控组件和日志组件，也必须运行在每一个节点上，负责这个节点上的监控信息和日志搜集。&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
      <category term="controller" scheme="https://jesse.top/categories/kubernetes/controller/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes DNS</title>
    <link href="https://jesse.top/2021/02/06/kubernetes/service/DNS/"/>
    <id>https://jesse.top/2021/02/06/kubernetes/service/DNS/</id>
    <published>2021-02-06T15:39:58.000Z</published>
    <updated>2021-02-06T15:40:39.583Z</updated>
    
    <content type="html"><![CDATA[<h2 id="DNS介绍"><a href="#DNS介绍" class="headerlink" title="DNS介绍"></a>DNS介绍</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>kubernets的所有资源.包括Service,Pod都有生命周期,会频繁的销毁和创建.这些资源的IP地址也会随之动态变化.所以Kubernetes使用DNS实现通过资源名解析IP地址.</p><h3 id="DNS服务器"><a href="#DNS服务器" class="headerlink" title="DNS服务器"></a>DNS服务器</h3><p>Kubernetes集群安装了默认的Core-dns组件(通过Pod方式运行).以及kube-dns的service.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl get pods -n kube-system | grep dns</span><br><span class="line">coredns-7f9c544f75-9sh28                   1/1     Running   2          324d</span><br><span class="line">coredns-7f9c544f75-jgmqq                   1/1     Running   2          324d</span><br><span class="line"></span><br><span class="line">#下方这个10.96.0.10就是kubernetes集群的内部DNS服务器</span><br><span class="line">[root@k8s-master ~]$kubectl get svc -n kube-system | grep dns</span><br><span class="line">kube-dns         ClusterIP   10.96.0.10     &lt;none&gt;        53/UDP,53/TCP,9153/TCP   324d</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="pod容器内部dns解析"><a href="#pod容器内部dns解析" class="headerlink" title="pod容器内部dns解析"></a>pod容器内部dns解析</h3><p>创建一个临时的pod容器,测试DNS解析效果.下面的命令临时运行了一个busybox的镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl run -it dns-test --rm --image=busybox:1.28.4 -- sh</span><br></pre></td></tr></table></figure><blockquote><p>不要使用latest版本的镜像,其dns解析有问题.最好使用1.28.4版本的</p></blockquote><p>下方是Pod容器的内部dns服务器信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/ # cat /etc/resolv.conf</span><br><span class="line">nameserver 10.96.0.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local localdomain</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure><p><code>resolv.conf</code> 配置文件说明</p><p><strong>nameserver</strong>：指明DNS服务器地址.也就是上文提到的<code>kube-dns</code> 的service</p><p><strong>search</strong>：当原始域名不能被DNS解析时，resolver会将该域名加上search指定的参数，重新请求DNS，直到被正确解析或试完search指定的列表为止 options：dns配置 </p><p><strong>ndots:5</strong>：所有DNS查询中，如果“.”的个数少于5个，则会根据search中配置的列表依次在对应域中先进行搜索，如果没有返回，则最后再直接查询域名本身</p><p>为了了解<code>search</code>和<code>ndots</code> 的概念,我们先要了解FQDN的概念.<code>FQDN(Fully qualified domain name)</code>即完整域名。一般来说如果一个域名以<code>.</code>结束，就表示一个完整域名。比如<code>www.abc.xyz.</code>就是一个<code>FQDN</code>，而<code>www.abc.xyz</code>则不是<code>FQDN</code>。了解了这个概念之后我们就来看<code>search</code>和<code>options ndots</code>。</p><p>如果一个域名是<code>FQDN</code>，那么这个域名会被转发给DNS服务器进行解析。如果域名不是<code>FQDN</code>，那么这个域名会到<code>search</code>搜索解析，还是通过一个例子说明，比如访问<code>abc.xyz</code>这个域名，因为它并不是一个<code>FQDN</code>，所以它会和<code>search</code>域中的值进行组合而变成一个<code>FQDN</code>，以上文的<code>resolv.conf</code>为例，这域名会这样组合：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">abx.xyz.default.svc.cluster.local.</span><br><span class="line">abc.xyz.svc.cluster.local.</span><br><span class="line">abc.xyz.cluster.local.</span><br></pre></td></tr></table></figure><p>然后这些域名先被<code>kube-DNS</code>解析，如果没有解析成功再由宿主机的<code>DNS</code>服务器进行解析。</p><p>而<code>ndots</code>是用来表示一个域名中<code>.</code>的个数在不小于该值的情况下会被认为是一个<code>FQDN</code>。简单说这个属性用来判断一个不是以<code>.</code>结束的域名在什么条件下会被认定为是一个<code>FQDN</code>.上面的示例中ndots为5,也就是说如果一个域名中<code>.</code>的数量大于等于5，即使域名不是以<code>.</code>结尾，也会被认定为是一个<code>FQDN</code>。比如：域名是<code>abc.xyz.xxx.yyy.zzz.aaa</code>这个域名就是<code>FQDN</code>.</p><p>之所以会有<code>search</code>域主要还是为了方便k8s内部服务之间的访问。比如：k8s在同一个<code>namespace</code>下是可以直接通过服务名称进行访问的，其原理就是会在<code>search</code>域查找，比如上面的<code>resolv.conf</code>中<code>jplat、oms-dev</code>着两个其实都是这两个pod所在的<code>namespace</code>的名称。所以通过服务名称访问的时候，会和<code>search</code>域进行组合，这样最终域名会组合成<code>servicename.namespace.svc.cluster.local</code>。而如果是跨<code>namespace</code>访问，则可以通过<code>servicename.namespace</code>这样的形式，在通过和<code>search</code>域组合，依然可以得到<code>servicename.namespace.svc.cluster.local</code>。</p><h3 id="DNS解析"><a href="#DNS解析" class="headerlink" title="DNS解析"></a>DNS解析</h3><h4 id="解析对象"><a href="#解析对象" class="headerlink" title="解析对象"></a>解析对象</h4><p>Kubernetes集群中的每个Service资源都会被指派一个DNS名称.客户端Pod的DNS搜索列表默认是搜索自己的<code>namespace</code> 名称空间内的资源.</p><p>例如上文的<code>resolv.conf</code> 文件内的search搜索列表为<code>search default.svc.cluster.local svc.cluster.local cluster.local localdomain</code> .此时Pod可以直接搜索<code>default</code> 名称空间下的所有Service:</p><p>例如.使用上面的临时busybox容器解析<code>my-svc</code>的Service</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/ # nslookup my-svc</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      my-svc</span><br><span class="line">Address 1: 10.96.51.58 my-svc.default.svc.cluster.local</span><br></pre></td></tr></table></figure><p>上面的IP地址<code>10.96.51.58</code> 表示成功解析到该Service的IP.<code>my-svc.default.svc.cluster.local</code>这个就是该Service的FQDN完全限定域名.</p><p>其中:</p><p><strong><code>default</code></strong> —表示名称空间,我们的名称空间名字就是默认的default</p><p><strong><code>svc</code></strong> —————-表示资源类型,这里是Service</p><p><strong><code>cluster.local</code></strong> –k8s集群域名</p><p>也可以解析其他名称空间内的资源,比如解析<code>kube-system</code> 名称空间下的DNS服务器的Service.(DNS服务器本身也会被指定一个DNS名称).就可以通过<code>&lt;svc-name&gt;.&lt;namespace-name&gt;</code> 实现.比如下面解析<code>kube-system</code>名称空间下的<code>kube-dns</code>的Service</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/ # nslookup kube-dns.kube-system</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      kube-dns.kube-system</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br></pre></td></tr></table></figure><p>实际上DNS解析的是完全FQDN域名,只不过后面一部分内容<code>default.svc.cluster.local</code> 可以省略罢了.默认就是解析当前名称空间下的资源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">/ # nslookup my-svc</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      my-svc</span><br><span class="line">Address 1: 10.96.51.58 my-svc.default.svc.cluster.local</span><br><span class="line"></span><br><span class="line">#等同于:</span><br><span class="line">/ # nslookup my-svc.default.svc.cluster.local</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      my-svc.default.svc.cluster.local</span><br><span class="line">Address 1: 10.96.51.58 my-svc.default.svc.cluster.local</span><br></pre></td></tr></table></figure><p>在kubernetes官网中也提到:</p><p>假设在 Kubernetes 集群的名字空间 <code>bar</code> 中，定义了一个服务 <code>foo</code>。 运行在名字空间 <code>bar</code> 中的 Pod 可以简单地通过 DNS 查询 <code>foo</code> 来找到该服务。 运行在名字空间 <code>quux</code> 中的 Pod 可以通过 DNS 查询 <code>foo.bar</code> 找到该服务。</p><h4 id="Service-A记录"><a href="#Service-A记录" class="headerlink" title="Service A记录"></a>Service A记录</h4><p>对于普通的Service资源.会以<code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local</code>这种形式被分配一个DNS A记录.也就是上文中的<code>my-svc</code>的<code>10.96.51.58</code>这个IP地址.</p><p>如果是对于无头服务(headless service).这种service没有IP.但是也会以上面的形式被指派一个DNS的A记录.只不过这种记录和普通Service不同,而是被解析成对应服务的POD集合的Pod的IP.客户端使用标准的负载均衡策略从这组Pod中进行选择.</p><p>例如下面创建一个headless的svc.和普通svc的区别在于<code>clusterIP的值为None</code>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$cat deployment-kubia-v1.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: hsq1</span><br><span class="line">  labels:</span><br><span class="line">     app: hsq-openapi</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx</span><br><span class="line">      image: nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: hsq2</span><br><span class="line">  labels:</span><br><span class="line">     app: hsq-openapi</span><br><span class="line">spec:</span><br><span class="line">    containers:</span><br><span class="line">       - name: nginx</span><br><span class="line">         image: nginx</span><br><span class="line">---</span><br><span class="line">#一个yaml文件可以定义多种资源,中间用---隔开</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: hsq-openapi</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: hsq-openapi</span><br><span class="line">  clusterIP: None</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br></pre></td></tr></table></figure><blockquote><p>headless服务一般用于statefulset资源.不能用于deployment控制器</p></blockquote><p>创建该文件后查看<code>hsq-openapi</code>service:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl describe svc hsq-openapi</span><br><span class="line">Name:              hsq-openapi</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       kubectl.kubernetes.io/last-applied-configuration:</span><br><span class="line">                     &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;hsq-openapi&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;spec&quot;:&#123;&quot;clusterIP&quot;:&quot;None&quot;,&quot;p...</span><br><span class="line">Selector:          app=hsq-openapi</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                None</span><br><span class="line">Port:              &lt;unset&gt;  80/TCP</span><br><span class="line">TargetPort:        80/TCP</span><br><span class="line">Endpoints:         10.100.36.66:80,10.100.36.69:80  #这里是后端pod列表</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br></pre></td></tr></table></figure><p><strong>headless类型服务的DNS解析</strong></p><p>仍然使用上文中的busybox测试容器.解析<code>hsq-openapi</code> service 的A记录.可以看到解析的结果返回了2个pod的IP地址列表.对于这种类型的service.和普通的service不同.他解析出来的是POD的ip地址列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/ # nslookup hsq-openapi</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      hsq-openapi</span><br><span class="line">Address 1: 10.100.36.69 10-100-36-69.hsq-openapi.default.svc.cluster.local</span><br><span class="line">Address 2: 10.100.36.66 10-100-36-66.hsq-openapi.default.svc.cluster.local</span><br><span class="line">/ # ???</span><br></pre></td></tr></table></figure><hr><h4 id="Pod的A记录"><a href="#Pod的A记录" class="headerlink" title="Pod的A记录"></a>Pod的A记录</h4><p>一般而言,Pod会对应如下DNS名字解析: <code>pod-ip-address.&lt;namespace-name&gt;.pod.cluster.local</code> 例如对于上面例子中的iP为<code>10.100.36.69</code> 的Pod.对应的DNS名称为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/ # nslookup 10-100-36-69.default.pod.cluster.local  #DNS名称</span><br><span class="line">Server:    10.96.0.10</span><br><span class="line">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      10-100-36-69.default.pod.cluster.local</span><br><span class="line">Address 1: 10.100.36.69 10-100-36-69.hsq-openapi.default.svc.cluster.local</span><br></pre></td></tr></table></figure><h3 id="k8s默认的DNS策略"><a href="#k8s默认的DNS策略" class="headerlink" title="k8s默认的DNS策略"></a>k8s默认的DNS策略</h3><p>k8s提供了5种DNS策略，如下：</p><ul><li><code>Default</code>: Pod 从运行所在的节点继承名称解析配置。</li><li><code>ClusterFirst</code>: 与配置的集群域后缀不匹配的任何 DNS 查询（例如 “<a href="https://links.jianshu.com/go?to=http%3A%2F%2Fwww.kubernetes.io" target="_blank" rel="noopener">www.kubernetes.io</a>”） 都将转发到从节点继承的上游名称服务器。集群管理员可能配置了额外的存根域和上游 DNS 服务器。</li><li><code>ClusterFirstWithHostNet</code>：对于以 hostNetwork 方式运行的 Pod，应显式设置其 DNS 策略 <code>ClusterFirstWithHostNet</code>。</li><li><code>None</code>: 此设置允许 Pod 忽略 Kubernetes 环境中的 DNS 设置。Pod 会使用其 <code>dnsConfig</code> 字段 所提供的 DNS 设置。</li></ul><p>k8s默认使用的DNS策略是<code>ClusterFirst</code>，这点需要注意，也就是说域名解析会优先使用集群的DNS（<code>kube-DNS</code>）进行查询，如果k8s的DNS解析失败，会转发到宿主机的DNS进行解析。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;DNS介绍&quot;&gt;&lt;a href=&quot;#DNS介绍&quot; class=&quot;headerlink&quot; title=&quot;DNS介绍&quot;&gt;&lt;/a&gt;DNS介绍&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;kubernets的所有资源.包括Service,Pod都有生命周期,会频繁的销毁和创建.这些资源的IP地址也会随之动态变化.所以Kubernetes使用DNS实现通过资源名解析IP地址.&lt;/p&gt;
&lt;h3 id=&quot;DNS服务器&quot;&gt;&lt;a href=&quot;#DNS服务器&quot; class=&quot;headerlink&quot; title=&quot;DNS服务器&quot;&gt;&lt;/a&gt;DNS服务器&lt;/h3&gt;&lt;p&gt;Kubernetes集群安装了默认的Core-dns组件(通过Pod方式运行).以及kube-dns的service.&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[root@k8s-master ~]$kubectl get pods -n kube-system | grep dns&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;coredns-7f9c544f75-9sh28                   1/1     Running   2          324d&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;coredns-7f9c544f75-jgmqq                   1/1     Running   2          324d&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#下方这个10.96.0.10就是kubernetes集群的内部DNS服务器&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[root@k8s-master ~]$kubectl get svc -n kube-system | grep dns&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;kube-dns         ClusterIP   10.96.0.10     &amp;lt;none&amp;gt;        53/UDP,53/TCP,9153/TCP   324d&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
      <category term="Service" scheme="https://jesse.top/categories/kubernetes/Service/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Calico</title>
    <link href="https://jesse.top/2021/02/06/kubernetes/network/kubernetes%20Calico/"/>
    <id>https://jesse.top/2021/02/06/kubernetes/network/kubernetes Calico/</id>
    <published>2021-02-06T15:39:58.000Z</published>
    <updated>2021-02-06T15:56:37.275Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kubernetes-Calico"><a href="#Kubernetes-Calico" class="headerlink" title="Kubernetes Calico"></a>Kubernetes Calico</h2><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h3><p>​    Calico是一个非常流行的Kubernetes网络插件和解决方案.Calico是一个开源虚拟化网络方案，用于为云原生应用实现互联及策略控制。与Flannel相比，Calico的一个显著优势是对网络策略（network policy）的支持，它允许用户动态定义ACL规则控制进出容器的数据报文，实现为Pod间的通信按需施加安全策略。事实上，Calico可以整合进大多数主流的编排系统，如Kubernetes、Apache Mesos、Docker和OpenStack等。</p><p>​    Calico本身是一个三层的虚拟网络方案，它将每个节点都当作路由器（router），将每个节点的容器都当作是“节点路由器”的一个终端并为其分配一个IP地址，各节点路由器通过BGP（Border Gateway Protocol）学习生成路由规则，从而将不同节点上的容器连接起来。因此，Calico方案其实是一个纯三层的解决方案，通过每个节点协议栈的三层（网络层）确保容器之间的连通性，这摆脱了flannel host-gw类型的所有节点必须位于同一二层网络的限制，从而极大地扩展了网络规模和网络边界。</p><a id="more"></a><p>​    Calico利用Linux内核在每一个计算节点上实现了一个高效的vRouter（虚拟路由器）进行报文转发，而每个vRouter都通过BGP负责把自身所属的节点上运行的Pod资源的IP地址信息基于节点的agent程序（Felix）直接由vRouter生成路由规则向整个Calico网络内进行传播.</p><p>​    Calico承载的各Pod资源直接通过vRouter经由基础网络进行互联，它非叠加、无隧道、不使用VRF表，也不依赖于NAT，因此每个工作负载都可以直接配置使用公网IP接入互联网，当然，也可以按需使用网络策略控制它的网络连通性。</p><p>​    Calico官网介绍: projectcaclico.org</p><h3 id="2-重要特性"><a href="#2-重要特性" class="headerlink" title="2.重要特性"></a>2.重要特性</h3><h4 id="2-1-经IP路由直连"><a href="#2-1-经IP路由直连" class="headerlink" title="2.1 经IP路由直连"></a>2.1 经IP路由直连</h4><p>Calico中，Pod收发的IP报文由所在节点的Linux内核路由表负责转发，并通过iptables规则实现其安全功能。某Pod对象发送报文时，Calico应确保节点总是作为下一跳MAC地址返回，不管工作负载本身可能配置什么路由，而发往某Pod对象的报文，其最后一个IP跃点就是Pod所在的节点，也就是说，报文的最后一程即由节点送往目标Pod对象，如下图所示。</p><p><img src="https://img2.jesse.top/image-20210206163936241.png" alt="image-20210206163936241"></p><p>需为某Pod对象提供连接时，系统上的专用插件（如Kubernetes的CNI）负责将需求通知给Calico Agent。收到消息后，Calico Agent会为每个工作负载添加直接路径信息到工作负载的TAP设备（如veth）。而运行于当前节点的BGP客户端监控到此类消息后会调用路由reflector向工作于其他节点的BGP客户端进行通告。</p><h4 id="2-2-简单、高效、易扩展"><a href="#2-2-简单、高效、易扩展" class="headerlink" title="2.2 简单、高效、易扩展"></a>2.2 简单、高效、易扩展</h4><p>Calico未使用额外的报文封装和解封装，从而简化了网络拓扑，这也是Calico高性能、易扩展的关键因素。毕竟，小的报文减少了报文分片的可能性，而且较少的封装和解封装操作也降低了对CPU的占用。此外，较少的封装也易于实现报文分析，易于进行故障排查。</p><p>创建、移动或删除Pod对象时，相关路由信息的通告速度也是影响其扩展性的一个重要因素。Calico出色的扩展性缘于与互联网架构设计原则别无二致的方式，它们都使用了BGP作为控制平面。BGP以高效管理百万级的路由设备而闻名于世，Calico自然可以游刃有余地适配大型IDC网络规模。另外，由于Calico各工作负载使用基IP直接进行互联，因此它还支持多个跨地域的IDC之间进行协同。</p><h3 id="3-Calico系统架构"><a href="#3-Calico系统架构" class="headerlink" title="3.Calico系统架构"></a>3.Calico系统架构</h3><p><img src="https://img2.jesse.top/1060878-20190413152300545-538840176.png" alt="img"></p><p>各组件介绍如下:</p><ul><li><p><strong>Felix</strong>：Calico Agent，运行于每个节点。主要负责网络接口管理和监听、路由、ARP 管理、ACL 管理和同步、状态上报等。</p></li><li><p><strong>etcd</strong>：分布式键值存储，主要负责网络元数据一致性，确保Calico网络状态的准确性，可以与kubernetes共用；</p></li><li><p><strong>BGP Client（BIRD）</strong>：Calico 为每一台 Host 部署一个 BGP Client，使用 BIRD 实现，BIRD 是一个单独的持续发展的项目，实现了众多动态路由协议比如 BGP、OSPF、RIP 等。在 Calico 的角色是监听 Host 上由 Felix 注入的路由信息，然后通过 BGP 协议广播告诉剩余 Host 节点，从而实现网络互通。</p></li><li><strong>BGP Route Reflector</strong>：在大型网络规模中，如果仅仅使用 BGP client 形成 mesh 全网互联的方案就会导致规模限制，因为所有节点之间俩俩互联，需要 N^2 个连接，为了解决这个规模问题，可以采用 BGP 的 Router Reflector 的方法，使所有 BGP Client 仅与特定 RR 节点互联并做路由同步，从而大大减少连接数。</li></ul><h4 id="3-1-Felix"><a href="#3-1-Felix" class="headerlink" title="3.1 Felix"></a>3.1 Felix</h4><p>​    Felix运行于各节点的用于支持端点（VM或Container）构建的守护进程，它负责生成路由和ACL，以及其他任何由节点用到的信息，从而为各端点构建连接机制。Felix在各编排系统中主要负责以下任务。</p><p>​    首先是接口管理（Interface Management）功能，负责为接口生成必要的信息并送往内核，以确保内核能够正确处理各端点的流量，尤其是要确保各节点能够响应目标MAC为当前节点上各工作负载的MAC地址的ARP请求，以及为其管理的接口打开转发功能。另外，它还要监控各接口的变动以确保规则能够得到正确的应用。</p><p>​    其次是路由规划（Route Programming）功能，其负责为当前节点运行的各端点在内核FIB（Forwarding Information Base）中生成路由信息，以保证到达当前节点的报文可正确转发给端点。</p><p>​    再次是ACL规划（ACL Programming）功能，负责在Linux内核中生成ACL，用于实现仅放行端点间的合法流量，并确保流量不能绕过Calico的安全措施。</p><p>​    最后是状态报告（State Reporting）功能，负责提供网络健康状态的相关数据，尤其是报告由其管理的节点上的错误和问题。这些报告数据会存储于etcd，供其他组件或网络管理员使用。</p><h4 id="3-2-编排系统插件"><a href="#3-2-编排系统插件" class="headerlink" title="3.2 编排系统插件"></a>3.2 编排系统插件</h4><p>​    编排系统插件（Orchestrator Plugin）依赖于编排系统自身的实现，故此并不存在一个固定的插件以代表此组件。编排系统插件的主要功能是将Calico整合进系统中，并让管理员和用户能够使用Calico的网络功能。它主要负责完成API的转换和反馈输出。</p><p>​    编排系统通常有其自身的网络管理API，网络插件需要负责将对这些API的调用转为Calico的数据模型并存储于Calico的存储系统中。如果有必要，网络插件还要将Calico系统的信息反馈给编排系统，如Felix的存活状态，网络发生错误时设定相应的端点为故障等。</p><h4 id="3-3-etcd存储系统"><a href="#3-3-etcd存储系统" class="headerlink" title="3.3 etcd存储系统"></a>3.3 etcd存储系统</h4><p>​    Calico使用etcd完成组件间的通信，并以之作为一个持久数据存储系统。根据编排系统的不同，etcd所扮演角色的重要性也因之而异，但它贯穿了整个Calico部署全程，并被分为两类主机：核心集群和代理（proxy）。在每个运行着Felix或编排系统插件的主机上都应该运行一个etcd代理以降低etcd集群和集群边缘节点的压力。此模式中，每个运行着插件的节点都会运行着etcd集群的一个成员节点。</p><p>​    etcd是一个分布式、强一致、具有容错功能的存储系统，这一点有助于将Calico网络实现为一个状态确切的系统：要么正常，要么发生故障。另外，分布式存储易于通过扩展应对访问压力的提升，而避免成为系统瓶颈。另外，etcd也是Calico各组件的通信总线，可用于确保让非etcd组件在键空间（keyspace）中监控某些特定的键，以确保它们能够看到所做的任何更改，从而使它们能够及时地响应这些更改。</p><h4 id="3-4-BGP客户端-BIRD"><a href="#3-4-BGP客户端-BIRD" class="headerlink" title="3.4 BGP客户端(BIRD)"></a>3.4 BGP客户端(BIRD)</h4><p>​    Calico要求在每个运行着Felix的节点上同时还要运行一个BGP客户端，负责将Felix生成的路由信息载入内核并通告到整个IDC。在Calico语境中，此组件是通用的BIRD，因此任何BGP客户端（如GoBGP等）都可以从内核中提取路由并对其分发对于它们来说都适合的角色。</p><p>​    BGP客户端的核心功能就是路由分发，在Felix插入路由信息至内核FIB中时，BGP客户端会捕获这些信息并将其分发至其他节点，从而确保了流量的高效路由。</p><h4 id="3-5-BGP路由反射器-BRID"><a href="#3-5-BGP路由反射器-BRID" class="headerlink" title="3.5 BGP路由反射器(BRID)"></a>3.5 BGP路由反射器(BRID)</h4><p>​    在大规模的部署场景中，简易版的BGP客户端易于成为性能瓶颈，因为它要求每个BGP客户端都必须连接至其同一网络中的其他所有BGP客户端以传递路由信息，一个有着N个节点的部署环境中，其存在网络连接的数量为N的二次方，随着N值的逐渐增大，其连接复杂度会急剧上升。因而在较大规模的部署场景中，Calico应该选择部署一个BGP路由反射器，它是由BGP客户端连接的中心点，BGP的点到点通信也就因此转化为与中心点的单路通信模型，如图11-18所示。出于冗余之需，生产实践中应该部署多个BGP路由反射器。对于Calico来说，BGP客户端程序除了作为客户端使用之外，还可以配置成路由反射器。</p><h3 id="4-Calico网络工作模式"><a href="#4-Calico网络工作模式" class="headerlink" title="4.Calico网络工作模式"></a>4.Calico网络工作模式</h3><h4 id="4-1-BGP模式"><a href="#4-1-BGP模式" class="headerlink" title="4.1 BGP模式"></a>4.1 BGP模式</h4><p>边界网关协议（Border Gateway Protocol, BGP）是互联网上一个核心的去中心化自治路由协议，它通过维护IP路由表或“前缀”表来实现自治系统（AS）之间的可达性，属于矢量路由协议。不过，考虑到并非所有的网络都能支持BGP，以及Calico控制平面的设计要求物理网络必须是二层网络，以确保vRouter间均直接可达，路由不能够将物理设备当作下一跳等原因，为了支持三层网络</p><h4 id="4-2-IPIP模式"><a href="#4-2-IPIP模式" class="headerlink" title="4.2 IPIP模式"></a>4.2 IPIP模式</h4><p>​    BGP模式要求Kubernetes的所有物理节点网络必须是二层网络.为了支持三层网络，Calico还推出了IP-in-IP叠加的模型，它也使用Overlay的方式来传输数据。IPIP的包头非常小，而且也是内置在内核中，因此理论上它的速度要比VxLAN快一点，但安全性更差。Calico 3.x的默认配置使用的是IPIP类型的传输方案而非BGP。</p><p>​    工作于IPIP模式的Calico会在每个节点上创建一个tunl0接口（TUN类型虚拟设备）用于封装三层隧道报文。节点上创建的每一个Pod资源，都会由Calico自动创建一对虚拟以太网接口（TAP类型的虚拟设备），其中一个附加于Pod的网络名称空间，另一个（名称以cali为前缀后跟随机字串）留置在节点的根网络名称空间，并经由tunl0封装或解封三层隧道报文。Calico IPIP模式如下图所示。</p><p><img src="https://img2.jesse.top/image-20210206165304293.png" alt="image-20210206165304293"></p><h3 id="5-Calico-网络通信方式"><a href="#5-Calico-网络通信方式" class="headerlink" title="5. Calico 网络通信方式"></a>5. Calico 网络通信方式</h3><h4 id="5-1-Calico网络环境介绍"><a href="#5-1-Calico网络环境介绍" class="headerlink" title="5.1 Calico网络环境介绍"></a>5.1 Calico网络环境介绍</h4><p>当前k8s集群使用的是v1.17.3的版本.有2个node节点.IP地址分别如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl get nodes -o wide | awk &apos;&#123;print $1,$6&#125;&apos; | sed 1,2d</span><br><span class="line">k8s-node1 172.16.20.252</span><br><span class="line">k8s-node2 172.16.20.253</span><br></pre></td></tr></table></figure><p>每个node节点都启动一个<code>tunl0</code> 的虚拟路由器.和许多<code>calixxx</code> 开头的虚拟网卡设备</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-node1 ~]# ifconfig</span><br><span class="line">cali42b086c8543: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1440</span><br><span class="line">        inet6 fe80::ecee:eeff:feee:eeee  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether ee:ee:ee:ee:ee:ee  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 13335563  bytes 928478769 (885.4 MiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 13335563  bytes 928478769 (885.4 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">tunl0: flags=193&lt;UP,RUNNING,NOARP&gt;  mtu 1440</span><br><span class="line">        inet 10.100.36.64  netmask 255.255.255.255</span><br><span class="line">        tunnel   txqueuelen 1000  (IPIP Tunnel)  #默认是IPIP模式</span><br><span class="line">        RX packets 3978810  bytes 345003038 (329.0 MiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 3674392  bytes 613045453 (584.6 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions</span><br></pre></td></tr></table></figure><p>Calico的CNI插件会为每个容器设置一个veth pair设备，然后把另一端接入到宿主机网络空间，由于没有网桥，CNI插件还需要在宿主机上为每个容器的veth pair设备配置一条路由规则，用于接收传入的IP包.</p><p>了这样的veth pair设备以后，容器发出的IP包就会通过veth pair设备到达宿主机，这些路由规则都是Felix维护配置的，而路由信息则是calico bird组件基于BGP分发而来。Calico实际上是将集群里所有的节点都当做边界路由器来处理，他们一起组成了一个全互联的网络，彼此之间通过BGP交换路由，这些节点我们叫做BGP Peer。</p><p>为了下面试验Calico的网络工作.当前集群使用daemonSet控制器运行了2个<code>busybox:1.28.4</code> 镜像的容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl get pods -o wide</span><br><span class="line">NAME            READY   STATUS    RESTARTS   AGE    IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">busybox-g5rkr   1/1     Running   0          130m   10.100.36.103    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">busybox-zdwsc   1/1     Running   0          130m   10.100.169.176   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>在<code>k8s-node1</code>节点上可以看到两条相关路由</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">10.100.36.103   0.0.0.0         255.255.255.255 UH    0      0        0 cali96df9f67b52</span><br><span class="line">10.100.169.128  172.16.20.253   255.255.255.192 UG    0      0        0 tunl0</span><br></pre></td></tr></table></figure><p>第一条路由是访问该节点下的Busybox容器.它的下一跳是<code>calixxxx</code>开头的虚拟网卡.这种通信方式和docker的Bridge网桥模式其实并没有任何区别.</p><p>第二条路由的目的网络是10.100.169.128,子网掩码是255.255.255.192.它代表了IP范围为10.100.169.128-190的地址.而运行于另外一个节点下的<code>busybox-zdwsc</code>Pod的IP地址就位于这个范围之内.所以这条路由可以使node1节点借助于tunl0可以直接和node2节点下的pod进行通信.</p><blockquote><p>在<code>k8s-node2</code> 服务器可以看到类似的这2条路由</p></blockquote><h4 id="5-2-Calico网络模型解密"><a href="#5-2-Calico网络模型解密" class="headerlink" title="5.2 Calico网络模型解密"></a>5.2 Calico网络模型解密</h4><p>登录<code>k8s-node1</code>节点下的Pod容器内部.查看Pod容器的IP地址,以及路由条目.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl exec -it busybox-g5rkr -- sh</span><br><span class="line">/ # ifconfig</span><br><span class="line">eth0      Link encap:Ethernet  HWaddr 4A:7C:E7:FA:4B:CC</span><br><span class="line">          inet addr:10.100.36.103  Bcast:0.0.0.0  Mask:255.255.255.255</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1440  Metric:1</span><br><span class="line">          RX packets:14 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0</span><br><span class="line">          RX bytes:1322 (1.2 KiB)  TX bytes:426 (426.0 B)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">/ # route -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">0.0.0.0         169.254.1.1     0.0.0.0         UG    0      0        0 eth0</span><br><span class="line">169.254.1.1     0.0.0.0         255.255.255.255 UH    0      0        0 eth0</span><br></pre></td></tr></table></figure><p>通过<code>k8s-node</code>节点上的下面的路由条目,我们可以知道节点主机和Pod容器的IP地址<code>10.100.36.103</code>通信使用的是<code>cali96df9f67b52</code>这个虚拟网卡</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10.100.36.103   0.0.0.0         255.255.255.255 UH    0      0        0 cali96df9f67b52</span><br></pre></td></tr></table></figure><p>路由条目显示<code>169.254.1.1</code> 是Pod容器的默认网关.但是有网络常识的我们都知道这个IP是个保留的IP地址,不存在于互联网或者任何设备中.那Pod如何和网关通信呢?</p><p>回顾一下网络课程,我们知道任何网络设备和网关设备都是在一个二层局域网中,而二层数据链路层使用MAC地址进行通信,不需要双方的IP地址信息.通信方(这里是Pod容器)会通过ARP协议获取网关的MAC地址,然后通过MAC地址将数据包发送给网关..也就是说网络设备不关心对方的IP是否可达,只要能找到对应的MAC地址就可以.</p><p>通过<code>ip neigh</code>命令查看Pod容器的ARP缓存</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/ # ip neigh</span><br><span class="line">169.254.1.1 dev eth0 lladdr ee:ee:ee:ee:ee:ee ref 1 used 0/0/0 probes 4 REACHABLE</span><br></pre></td></tr></table></figure><blockquote><p>如果是新的Pod容器可能无法获得ARP缓存,此时只需要随便发生一个网络交互(例如ping百度)即可</p></blockquote><p>这个MAC地址(ee:ee:ee:ee:ee:ee)也是Calico的虚拟<code>cali96df9f67b52</code>网卡的虚拟MAC地址.下放是宿主机网卡信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cali96df9f67b52: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1440</span><br><span class="line">        inet6 fe80::ecee:eeff:feee:eeee  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether ee:ee:ee:ee:ee:ee  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure><p>所有虚拟网卡默认开启了ARP代理协议</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-node1 ~]# cat /proc/sys/net/ipv4/conf/cali96df9f67b52/proxy_arp</span><br><span class="line">1</span><br></pre></td></tr></table></figure><p>所以Calico 通过一个巧妙的方法将 Pod 的所有流量引导到一个特殊的网关 169.254.1.1，从而引流到主机的 calixxx 网络设备上，最终将二三层流量全部转换成三层流量来转发。</p><h3 id="6-Calico-IPIP网络模式"><a href="#6-Calico-IPIP网络模式" class="headerlink" title="6.Calico IPIP网络模式"></a>6.Calico IPIP网络模式</h3><p>登录<code>busybox-g5rkr</code>Pod容器内部.ping位于另外一台<code>k8s-node2</code> 下的<code>busybox-zdwsc</code>Pod容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl get pods -o wide</span><br><span class="line">NAME            READY   STATUS    RESTARTS   AGE    IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">busybox-g5rkr   1/1     Running   0          130m   10.100.36.103    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">busybox-zdwsc   1/1     Running   0          130m   10.100.169.176   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>两个Pod之前可以直接访问对方的IP地址.而不需要像Docker容器那样暴露端口,然后利用对方宿主机的IP进行通信</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl exec -it busybox-g5rkr -- sh</span><br><span class="line">/ # ifconfig</span><br><span class="line">eth0      Link encap:Ethernet  HWaddr 4A:7C:E7:FA:4B:CC</span><br><span class="line">          inet addr:10.100.36.103  Bcast:0.0.0.0  Mask:255.255.255.255</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1440  Metric:1</span><br><span class="line">          RX packets:14 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:6 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0</span><br><span class="line">          RX bytes:1322 (1.2 KiB)  TX bytes:426 (426.0 B)</span><br><span class="line"></span><br><span class="line">/ # ping 10.100.169.176</span><br><span class="line">PING 10.100.169.176 (10.100.169.176): 56 data bytes</span><br><span class="line">64 bytes from 10.100.169.176: seq=0 ttl=62 time=0.622 ms</span><br><span class="line">64 bytes from 10.100.169.176: seq=1 ttl=62 time=0.552 ms</span><br><span class="line">64 bytes from 10.100.169.176: seq=2 ttl=62 time=0.597 ms</span><br></pre></td></tr></table></figure><p>在<code>k8s-node2</code> 节点抓包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-node2 ~]# tcpdump -i ens192 -nn  -w imcp.cap</span><br></pre></td></tr></table></figure><p>用wireshark软件打开抓包文件.发现如下ICMP的报文</p><p><img src="https://img2.jesse.top/image-20210206220720374.png" alt="image-20210206220720374"></p><p>可以看到每个数据报文共有两个IP网络层,内层是Pod容器之间的IP网络报文,外层是宿主机节点的网络报文(2个node节点).之所以要这样做是因为tunl0是一个隧道端点设备，在数据到达时要加上一层封装，便于发送到对端隧道设备中。 </p><p>Pod间的通信经由IPIP的三层隧道转发,相比较VxLAN的二层隧道来说，IPIP隧道的开销较小，但其安全性也更差一些。</p><p>IPIP的通信方式如下:</p><p><img src="https://img2.jesse.top/1060878-20190415165144848-1984358878.png" alt="img"></p><h4 id="6-1-Pod和Service网络通信"><a href="#6-1-Pod和Service网络通信" class="headerlink" title="6.1 Pod和Service网络通信"></a>6.1 Pod和Service网络通信</h4><p>经过测试.在k8s集群内部物理节点和pod容器内部访问Service的http服务.仍然使用的是Ipip通信模式.</p><p>下面是在容器内部通过Service访问busybox pod容器的http服务的抓包报文</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl exec -it busybox-6hnvc -- sh</span><br><span class="line">/ # curl http://10.96.166.242</span><br><span class="line">sh: curl: not found</span><br><span class="line">/ # wget -O - -q http://10.96.166.242</span><br><span class="line">wget: server returned error: HTTP/1.0 404 Not Found</span><br><span class="line">/ # wget -O - -q http://10.96.166.242</span><br><span class="line">wget: server returned error: HTTP/1.0 404 Not Found</span><br></pre></td></tr></table></figure><p><img src="https://img2.jesse.top/image-20210206222911793.png" alt="image-20210206222911793"></p><h3 id="7-BGP网络模式"><a href="#7-BGP网络模式" class="headerlink" title="7. BGP网络模式"></a>7. BGP网络模式</h3><p>Calico网络部署时,默认安装就是IPIP网络.通过修改calico.yaml部署文件中的<code>CALICO_IPV4POOL_IPIP</code> 值修改成<code>off</code> 就切换到BGP网络模式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Enable IPIP</span><br><span class="line">- name: CALICO_IPV4POOL_IPIP</span><br><span class="line">  value: &quot;Always&quot;  #改成Off</span><br></pre></td></tr></table></figure><p>重新部署calico</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl apply -f calico-3.10.2.yaml</span><br></pre></td></tr></table></figure><p>然后关闭ipipMode.把ipipMode从Always修改成为Never</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master1 target]# kubectl edit ippool</span><br><span class="line"></span><br><span class="line">  ipipMode: Never</span><br></pre></td></tr></table></figure><h4 id="7-1-和Ipip的区别"><a href="#7-1-和Ipip的区别" class="headerlink" title="7.1 和Ipip的区别"></a>7.1 和Ipip的区别</h4><p>BGP网络相比较IPIP网络，最大的不同之处就是没有了隧道设备 tunl0。 前面介绍过IPIP网络pod之间的流量发送tunl0，然后tunl0发送对端设备。BGP网络中，pod之间的流量直接从网卡发送目的地，减少了tunl0这个环节。</p><h4 id="7-2-通信方式"><a href="#7-2-通信方式" class="headerlink" title="7.2 通信方式"></a>7.2 通信方式</h4><p>删除原来的pod.重新启动新的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$kubectl create -f deployment-kubia-v1.yaml</span><br><span class="line">daemonset.apps/busybox created</span><br><span class="line">service/busybox created</span><br><span class="line">[root@k8s-master ~]$kubectl get pods -o wide</span><br><span class="line">NAME            READY   STATUS    RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">busybox-bd566   1/1     Running   0          16s   10.100.36.97     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">busybox-fntv9   1/1     Running   0          16s   10.100.169.129   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>再次查看路由表.发现节点和pod容器通信直接通过宿主机的物理网卡,而不是tunl0设备了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]$route -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">0.0.0.0         172.16.20.254   0.0.0.0         UG    100    0        0 ens192</span><br><span class="line">10.100.36.64    172.16.20.252   255.255.255.192 UG    0      0        0 ens192</span><br><span class="line">10.100.169.128  172.16.20.253   255.255.255.192 UG    0      0        0 ens192</span><br></pre></td></tr></table></figure><p>此时,再次2个Pod容器互ping抓包分析.发现两个Pod像物理机一样直接通信,而不需要进行任何数据包封装和解封装.并且数据报文的MAC地址也是node1和node2物理网卡的MAC地址</p><p><img src="https://img2.jesse.top/image-20210206224938109.png" alt="image-20210206224938109"></p><p>BGP的网络连接方式:</p><p><img src="https://img2.jesse.top/1060878-20190415165320714-135136611.png" alt="img"></p><h3 id="8-BGP和ipip网络模式对比"><a href="#8-BGP和ipip网络模式对比" class="headerlink" title="8. BGP和ipip网络模式对比"></a>8. BGP和ipip网络模式对比</h3><ul><li><p><strong>IPIP</strong>:</p><p>特点: tunl0封装数据.形成隧道.所有Pod和pod.pod和节点之间进行三层网络传输</p><p>优点: 适用所有网络类型.能够解决跨网段的路由问题.</p></li><li><p><strong>BGP</strong>:</p><p>特点: 适用BGP路由导向流量</p><p>优点: Pod之间直接通信.省去了隧道,封装,解封装等任何中间环节,传输效率非常高.</p><p>缺点: 需要确保所有物理节点在同一个二层网络,否则Pod无法跨节点网段通信</p></li></ul><h3 id="9-Calico网络优化"><a href="#9-Calico网络优化" class="headerlink" title="9. Calico网络优化"></a>9. Calico网络优化</h3><h4 id="9-1-MTU"><a href="#9-1-MTU" class="headerlink" title="9.1 MTU"></a>9.1 MTU</h4><p>Calico 的IPIP网络模型下tunl0接口的MTU默认为1440，这种设置主要是为适配Google的GCE环境，在非GCE的物理环境中，其最佳值为1480。因此，对于非GCE环境的部署，建议将配置清单calico.yaml下载至本地修改后，再将其应用到集群中。要修改的内容是DaemonSet资源calico-node的Pod模板，将容器calico-node的环境变量“FELIX_INPUTMTU”的值修改为1480即可</p><blockquote><p>因为IPIP多了一层IP报文封装,而IP报文头部一般是20个字节.所以MUT的值应该是最大1500-20.</p></blockquote><h4 id="9-2-Calico-typha"><a href="#9-2-Calico-typha" class="headerlink" title="9.2 Calico-typha"></a>9.2 Calico-typha</h4><p>对于50个节点以上规模的集群来说，所有Calico节点均基于Kubernetes API存取数据会为API Server带来不小的通信压力，这就应该使用calico-typha进程将所有Calico的通信集中起来与API Server进行统一交互。calico-typha以Pod资源的形式托管运行于Kubernetes系统之上，启用的方法为下载前面步骤中用到的Calico的部署清单文件至本地，修改其calico-typha的Pod资源副本数量为所期望的值并重新应用配置清单即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: calico-typha</span><br><span class="line">  ...</span><br><span class="line">spec:</span><br><span class="line">  ...</span><br><span class="line">  replicas: &lt;number of replicas&gt;</span><br></pre></td></tr></table></figure><p>每个calico-typha Pod资源可承载100到200个Calico节点的连接请求，最多不要超过200个。另外，整个集群中的calico-typha的Pod资源总数尽量不要超过20个。</p><h4 id="9-3-BGP路由模型"><a href="#9-3-BGP路由模型" class="headerlink" title="9.3 BGP路由模型"></a>9.3 BGP路由模型</h4><p>默认情况下，Calico的BGP网络工作于点对点的网格（node-to-node mesh）模型，它仅适用于较小规模的集群环境。中级集群环境应该使用全局对等BGP模型（Global BGP peers），以在同一二层网络中使用一个或一组BGP反射器构建BGP网络环境。而大型集群环境需要使用每节点对等BGP模型（Per-node BGP peers），即分布式BGP反射器模型，一个典型的用法是将每个节点都配置为自带BGP反射器接入机架顶部交换机上的路由反射器。</p><h4 id="9-4-使用BGP而非IPIP"><a href="#9-4-使用BGP而非IPIP" class="headerlink" title="9.4 使用BGP而非IPIP"></a>9.4 使用BGP而非IPIP</h4><p>事实上，仅在那些不支持用户自定义BGP配置的网络中才需要使用IPIP的隧道通信类型。如果有一个自主可控的网络环境且部署规模较大时，可以考虑启用BGP的通信类型降低网络开销以提升传输性能，并且应该部署BGP反射器来提高路由学习效率。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>Calico官网: <a href="http://www.projectcalico.org" target="_blank" rel="noopener">www.projectcalico.org</a></p><p>k8s网络之Calico网络: <a href="https://www.cnblogs.com/goldsunshine/p/10701242.html#mxAMjXzT" target="_blank" rel="noopener">https://www.cnblogs.com/goldsunshine/p/10701242.html#mxAMjXzT</a></p><p>kubernetes容器网络: <a href="https://tech.ipalfish.com/blog/2020/03/06/kubernetes_container_network/" target="_blank" rel="noopener">https://tech.ipalfish.com/blog/2020/03/06/kubernetes_container_network/</a> (伴鱼团队)</p><p>&lt;kubernetes进阶实战&gt; 11.4 马永亮</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kubernetes-Calico&quot;&gt;&lt;a href=&quot;#Kubernetes-Calico&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes Calico&quot;&gt;&lt;/a&gt;Kubernetes Calico&lt;/h2&gt;&lt;h3 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1.简介&quot;&gt;&lt;/a&gt;1.简介&lt;/h3&gt;&lt;p&gt;​    Calico是一个非常流行的Kubernetes网络插件和解决方案.Calico是一个开源虚拟化网络方案，用于为云原生应用实现互联及策略控制。与Flannel相比，Calico的一个显著优势是对网络策略（network policy）的支持，它允许用户动态定义ACL规则控制进出容器的数据报文，实现为Pod间的通信按需施加安全策略。事实上，Calico可以整合进大多数主流的编排系统，如Kubernetes、Apache Mesos、Docker和OpenStack等。&lt;/p&gt;
&lt;p&gt;​    Calico本身是一个三层的虚拟网络方案，它将每个节点都当作路由器（router），将每个节点的容器都当作是“节点路由器”的一个终端并为其分配一个IP地址，各节点路由器通过BGP（Border Gateway Protocol）学习生成路由规则，从而将不同节点上的容器连接起来。因此，Calico方案其实是一个纯三层的解决方案，通过每个节点协议栈的三层（网络层）确保容器之间的连通性，这摆脱了flannel host-gw类型的所有节点必须位于同一二层网络的限制，从而极大地扩展了网络规模和网络边界。&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
      <category term="network" scheme="https://jesse.top/categories/kubernetes/network/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes pod原理</title>
    <link href="https://jesse.top/2021/02/06/kubernetes/pod/kubernetes--pod%E5%8E%9F%E7%90%86/"/>
    <id>https://jesse.top/2021/02/06/kubernetes/pod/kubernetes--pod原理/</id>
    <published>2021-02-06T15:39:58.000Z</published>
    <updated>2021-04-01T13:58:37.072Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kubernetes—-pod原理"><a href="#Kubernetes—-pod原理" class="headerlink" title="Kubernetes—-pod原理"></a>Kubernetes—-pod原理</h2><h3 id="开篇"><a href="#开篇" class="headerlink" title="开篇"></a>开篇</h3><p>Pod是kubernetes项目中最小的API对象,是原子调度单位.我们之前学习过很多Linux容器,Docker方面的知识.那Kubernetes为什么不使用容器作为调度单位,而是要将容器封装成一个Pod?</p><p>要探讨这个问题,我们需要深入研究一下Kubernetes的设计思想和工作原理.</p><h3 id="容器本质"><a href="#容器本质" class="headerlink" title="容器本质"></a>容器本质</h3><p>通过之前Docker的原理学习,我们知道容器的本质到底是什么? <strong>容器的本质是进程</strong>.<strong>容器的镜像就像是这个进程的安装包.</strong>一键启动这个镜像,就相当于用这个安装包启动了一个进程(PID为1).那么Kubernetes呢?</p><p>Kubernetes就是操作系统! 负责所有容器的编排和管理</p><p>但是,在一个操作系统里,进程并不是孤苦伶仃的单独运行的,而是以进程组的方式,多个进程同时在一起运行.这些进程存在着”进程和进程组”的关系,他们之间有非常密切的写作关系,使得他们必须部署在同一台机器上.</p><p>由于受限于容器的”单进程模型”.一个进程组下的不同进程可能需要制作成多个不同的容器,而这多个容器在传统的调度工作中(比如像Docker Swarm,Mesos)都没有被妥善处理好,在进程组的调度上要么无法保障一个进程组的多个容器无法调度到同一个节点,要么调度的效率和性能的问题.</p><p>可在Kubernetes里,这个问题通过Pod完美解决了.Pod是kubernetes的原子调度单位,这就意味着Kubernetes是统一按照Pod而非单个容器的资源需求计算的.所以可以将多个容器部署在同一个Pod里,这些容器共享同一个Pod的网络名称,进程间通信,IP地址,共享卷等.而Kubernetes在调度时,会将他们作为一个整体,而非单个容器进程.</p><p>像这样容器间的紧密协作，我们可以称为“超亲密关系”。这些具有“超亲密关系”容器的典型特征包括但不限于：互相之间会发生直接的文件交换、使用 localhost 或者 Socket 文件进行本地通信、会发生非常频繁的远程调用、需要共享某些 Linux Namespace（比如，一个容器要加入另一个容器的 Network Namespace）等等。</p><a id="more"></a><p>这也就意味着，并不是所有有“关系”的容器都属于同一个 Pod。比如，PHP 应用容器和 MySQL 虽然会发生访问关系，但并没有必要、也不应该部署在同一台机器上，它们更适合做成两个 Pod。</p><p>不过，相信此时你可能会有<strong>第二个疑问：</strong></p><p>对于初学者来说，一般都是先学会了用 Docker 这种单容器的工具，才会开始接触 Pod。</p><p>而如果 Pod 的设计只是出于调度上的考虑，那么 Kubernetes 项目似乎完全没有必要非得把 Pod 作为“一等公民”吧？这不是故意增加用户的学习门槛吗？</p><p>为了理解这一层含义，我就必须先给你介绍一下Pod 的实现原理。</p><h3 id="POD实现原理"><a href="#POD实现原理" class="headerlink" title="POD实现原理"></a>POD实现原理</h3><ul><li><strong>Pod只是一个逻辑的概念</strong></li></ul><p>也就是说，Kubernetes 真正处理的，还是宿主机操作系统上 Linux 容器的 Namespace 和 Cgroups，而并不存在一个所谓的 Pod 的边界或者隔离环境。</p><p>那么，Pod 又是怎么被“创建”出来的呢？</p><p>答案是：Pod，其实是一组共享了某些资源的容器。</p><p>具体的说：<strong>Pod 里的所有容器，共享的是同一个 Network Namespace，并且可以声明共享同一个 Volume。</strong></p><p>那这么来看的话，一个有 A、B 两个容器的 Pod，不就是等同于一个容器（容器 A）共享另外一个容器（容器 B）的网络和 Volume 的玩儿法么？</p><p>这好像通过 docker run –net –volumes-from 这样的命令就能实现嘛，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --net=B --volumes-from=B --name=A image-A ...</span><br></pre></td></tr></table></figure><p>但是，你有没有考虑过，如果真这样做的话，容器 B 就必须比容器 A 先启动，这样一个 Pod 里的多个容器就不是对等关系，而是拓扑关系了。</p><p>所以，在 Kubernetes 项目里，Pod 的实现需要使用一个中间容器，这个容器叫作 Infra 容器。在这个 Pod 中，Infra 容器永远都是第一个被创建的容器，而其他用户定义的容器，则通过 Join Network Namespace 的方式，与 Infra 容器关联在一起。这样的组织关系，可以用下面这样一个示意图来表达：</p><p><img src="https://img2.jesse.top/image-20210328112238279.png" alt="image-20210328112238279"></p><p>如上图所示，这个 Pod 里有两个用户容器 A 和 B，还有一个 Infra 容器。很容易理解，在 Kubernetes 项目里，Infra 容器一定要占用极少的资源，所以它使用的是一个非常特殊的镜像，叫作：<code>k8s.gcr.io/pause</code>。这个镜像是一个用汇编语言编写的、永远处于“暂停”状态的容器，解压后的大小也只有 100~200 KB 左右。</p><p>而在 Infra 容器“Hold 住”Network Namespace 后，用户容器就可以加入到 Infra 容器的 Network Namespace 当中了。所以，如果你查看这些容器在宿主机上的 Namespace 文件（这个 Namespace 文件的路径，我已经在前面的内容中介绍过），它们指向的值一定是完全一样的。</p><p>这也就意味着，对于 Pod 里的容器 A 和容器 B 来说：</p><ul><li>它们可以直接使用 localhost 进行通信；</li><li>它们看到的网络设备跟 Infra 容器看到的完全一样；</li><li>一个 Pod 只有一个 IP 地址，也就是这个 Pod 的 Network Namespace 对应的 IP 地址；</li><li>当然，其他的所有网络资源，都是一个 Pod 一份，并且被该 Pod 中的所有容器共享；</li><li>Pod 的生命周期只跟 Infra 容器一致，而与容器 A 和 B 无关。</li></ul><h3 id="容器设计模式"><a href="#容器设计模式" class="headerlink" title="容器设计模式"></a>容器设计模式</h3><p>Pod 这种“超亲密关系”容器的设计思想，实际上就是希望，当用户想在一个容器里跑多个功能并不相关的应用时，应该优先考虑它们是不是更应该被描述成一个 Pod 里的多个容器。</p><p>为了能够掌握这种思考方式，你就应该尽量尝试使用它来描述一些用单个容器难以解决的问题。</p><p>一个典型的例子就是容器的日志收集</p><p>比如，我现在有一个应用，需要不断地把日志文件输出到容器的 /var/log 目录中。</p><p>这时，我就可以把一个 Pod 里的 Volume 挂载到应用容器的 /var/log 目录上。</p><p>然后，我在这个 Pod 里同时运行一个日志收集客户端 容器，它也声明挂载同一个 Volume 到自己的 /var/log 目录上。</p><p>像这样，我们就用一种“组合”方式,解决了主容器和日志收集容器之间的耦合关系,而日志收集容器我们一般也称之为辅助容器.实际上，这个所谓的“组合”操作，正是容器设计模式里最常用的一种模式，它的名字叫：sidecar。顾名思义，sidecar 指的就是我们可以在一个 Pod 中，启动一个辅助容器，来完成一些独立于主进程（主容器）之外的工作。</p><p>这样，接下来 sidecar 容器就只需要做一件事儿，那就是不断地从自己的 /var/log 目录里读取日志文件，转发到 MongoDB 或者 Elasticsearch 中存储起来。这样，一个最基本的日志收集工作就完成了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kubernetes—-pod原理&quot;&gt;&lt;a href=&quot;#Kubernetes—-pod原理&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes—-pod原理&quot;&gt;&lt;/a&gt;Kubernetes—-pod原理&lt;/h2&gt;&lt;h3 id=&quot;开篇&quot;&gt;&lt;a href=&quot;#开篇&quot; class=&quot;headerlink&quot; title=&quot;开篇&quot;&gt;&lt;/a&gt;开篇&lt;/h3&gt;&lt;p&gt;Pod是kubernetes项目中最小的API对象,是原子调度单位.我们之前学习过很多Linux容器,Docker方面的知识.那Kubernetes为什么不使用容器作为调度单位,而是要将容器封装成一个Pod?&lt;/p&gt;
&lt;p&gt;要探讨这个问题,我们需要深入研究一下Kubernetes的设计思想和工作原理.&lt;/p&gt;
&lt;h3 id=&quot;容器本质&quot;&gt;&lt;a href=&quot;#容器本质&quot; class=&quot;headerlink&quot; title=&quot;容器本质&quot;&gt;&lt;/a&gt;容器本质&lt;/h3&gt;&lt;p&gt;通过之前Docker的原理学习,我们知道容器的本质到底是什么? &lt;strong&gt;容器的本质是进程&lt;/strong&gt;.&lt;strong&gt;容器的镜像就像是这个进程的安装包.&lt;/strong&gt;一键启动这个镜像,就相当于用这个安装包启动了一个进程(PID为1).那么Kubernetes呢?&lt;/p&gt;
&lt;p&gt;Kubernetes就是操作系统! 负责所有容器的编排和管理&lt;/p&gt;
&lt;p&gt;但是,在一个操作系统里,进程并不是孤苦伶仃的单独运行的,而是以进程组的方式,多个进程同时在一起运行.这些进程存在着”进程和进程组”的关系,他们之间有非常密切的写作关系,使得他们必须部署在同一台机器上.&lt;/p&gt;
&lt;p&gt;由于受限于容器的”单进程模型”.一个进程组下的不同进程可能需要制作成多个不同的容器,而这多个容器在传统的调度工作中(比如像Docker Swarm,Mesos)都没有被妥善处理好,在进程组的调度上要么无法保障一个进程组的多个容器无法调度到同一个节点,要么调度的效率和性能的问题.&lt;/p&gt;
&lt;p&gt;可在Kubernetes里,这个问题通过Pod完美解决了.Pod是kubernetes的原子调度单位,这就意味着Kubernetes是统一按照Pod而非单个容器的资源需求计算的.所以可以将多个容器部署在同一个Pod里,这些容器共享同一个Pod的网络名称,进程间通信,IP地址,共享卷等.而Kubernetes在调度时,会将他们作为一个整体,而非单个容器进程.&lt;/p&gt;
&lt;p&gt;像这样容器间的紧密协作，我们可以称为“超亲密关系”。这些具有“超亲密关系”容器的典型特征包括但不限于：互相之间会发生直接的文件交换、使用 localhost 或者 Socket 文件进行本地通信、会发生非常频繁的远程调用、需要共享某些 Linux Namespace（比如，一个容器要加入另一个容器的 Network Namespace）等等。&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
      <category term="pod" scheme="https://jesse.top/categories/kubernetes/pod/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 网络</title>
    <link href="https://jesse.top/2021/02/06/kubernetes/network/kubernetes%20network/"/>
    <id>https://jesse.top/2021/02/06/kubernetes/network/kubernetes network/</id>
    <published>2021-02-06T15:39:58.000Z</published>
    <updated>2021-02-06T15:48:07.531Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kubernetes-网络"><a href="#kubernetes-网络" class="headerlink" title="kubernetes 网络"></a>kubernetes 网络</h2><h3 id="1-开篇"><a href="#1-开篇" class="headerlink" title="1.开篇"></a>1.开篇</h3><p>​    Docker容器诞生以来,,如何确定合适的网络方案是亟待解决的难题之一.在日趋复杂的业务场景下,网络的复杂性也呈几何级数上升.本篇首先回顾了Docker容器的网络通信,然后介绍Kubernertes的网络模型.在Kubernetes集群中,IP地址的分配对象是以Pod为单位,而非容器.同一个Pod内的所有容器共享同一个网络名称空间</p><a id="more"></a><h3 id="2-容器网络基础"><a href="#2-容器网络基础" class="headerlink" title="2 容器网络基础"></a>2 容器网络基础</h3><p>​    一个Linux容器的网络栈是被隔离在它自己的Network Namespace中，Network Namespace包括了：网卡（Network Interface），回环设备（Lookback Device），路由表（Routing Table）和iptables规则，对于服务进程来讲这些就构建了它发起请求和相应的基本环境。而要实现一个容器网络，离不开以下Linux网络功能：</p><ul><li>网络命名空间：将独立的网络协议栈隔离到不同的命令空间中，彼此间无法通信</li><li>Veth Pair：Veth设备对的引入是为了实现在不同网络命名空间的通信，总是以两张虚拟网卡（veth peer）的形式成对出现的。并且，从其中一端发出的数据，总是能在另外一端收到</li><li>Iptables/Netfilter：Netfilter负责在内核中执行各种挂接的规则（过滤、修改、丢弃等），运行在内核中；Iptables模式是在用户模式下运行的进程，负责协助维护内核中Netfilter的各种规则表；通过二者的配合来实现整个Linux网络协议栈中灵活的数据包处理机制</li><li>网桥：网桥是一个二层网络虚拟设备，类似交换机，主要功能是通过学习而来的Mac地址将数据帧转发到网桥的不同端口上</li><li>路由: Linux系统包含一个完整的路由功能，当IP层在处理数据发送或转发的时候，会使用路由表来决定发往哪里</li></ul><h3 id="2-Docker容器网络模型"><a href="#2-Docker容器网络模型" class="headerlink" title="2.Docker容器网络模型"></a>2.Docker容器网络模型</h3><h4 id="2-1-同节点容器通信"><a href="#2-1-同节点容器通信" class="headerlink" title="2.1 同节点容器通信"></a>2.1 同节点容器通信</h4><p>​    Docker容器网络的原始模型主要用到的就是Bridge桥接网络.Docker守护进程首次启动时,会在当前宿主机节点创建一个名为<code>docker0</code> 的虚拟网桥设备.并默认配置其使用172.17.0.0/16的网络.</p><blockquote><p>Host和Container网络模型使用场景非常少.不再费篇幅介绍 </p></blockquote><p>​    并且为该主机节点上的每一个容器分配一个虚拟的以<code>vethxxx</code> 开头的虚拟网卡.从而使得同一节点下的所有容器都可以在二层网络模式下.利用<code>docker0</code> 虚拟网桥实现容器和容器之间,容器和宿主机节点之间的网络通信.</p><p>​    同一宿主机节点下的容器网络通信方式如下</p><p><img src="https://img2.jesse.top/image-20210206153900657.png" alt="image-20210206153900657"></p><h4 id="2-2-不同节点容器通信"><a href="#2-2-不同节点容器通信" class="headerlink" title="2.2 不同节点容器通信"></a>2.2 不同节点容器通信</h4><p>​    以上是同节点上的容器通信方式.对于不同节点的容器之间进行通信,Docker则无能为力.因为每个节点的docker0网桥分配的虚拟IP都是同一网段,所以不同宿主机节点上的容器可能使用的是同一个IP地址,双方并不清楚对方容器具体在哪台节点.</p><p>​    解决此问题的方式是NAT.所有容器均会被NAT隐藏在节点网络之内.他们发往Docker主机外部的所有流量都会SNAT后出去,容器若要接入Docker主机外部的流量,则需要事先将网络端口暴露到宿主机的端口..然后对方容器的流量达到宿主机后再执行DNAT转发给目的容器.</p><p>不同宿主机节点下的容器网络通信方式如下</p><p><img src="https://img2.jesse.top/image-20210206155556803.png" alt="image-20210206155556803"></p><p>这种解决方式在网络规模庞大的时候兼职就是个灾难.转发效率非常低下,宿主机上端口变成一种稀缺资源.</p><h3 id="3-Kubernetes网络模型"><a href="#3-Kubernetes网络模型" class="headerlink" title="3. Kubernetes网络模型"></a>3. Kubernetes网络模型</h3><p>Kubernetes的网络模型主要用于解决四类通信需求:</p><h4 id="1-容器间通信"><a href="#1-容器间通信" class="headerlink" title="(1) 容器间通信"></a>(1) 容器间通信</h4><p>​    Pod对象内的各容器共享同一个网络名称空间.所有运行于同一个Pod内部的容器与同一主机上的多个进程类似.彼此之间可以通过<code>localhost</code> 或者<code>lo</code> 回环接口进行通信.</p><p>例如下图3-1所示,每个节点上的Container1和container2容器在一个Pod内部,共享同一个IP地址和网络接口</p><p><img src="https://img2.jesse.top/image-20210206160446993.png" alt="image-20210206160446993"></p><p>​                                                                                                                图 3-1 Pod网络</p><h4 id="2-Pod间通信"><a href="#2-Pod间通信" class="headerlink" title="(2) Pod间通信"></a>(2) Pod间通信</h4><p>​    Kubernertes要求每个Pod对象需要运行于同一个平面网络中,并且都拥有一个集群内全局唯一的IP地址,可以直接于其他Pod通信.例如上图3-1中的Pod P和Pod Q之间通信.另外，运行Pod的各节点也会通过桥接设备等持有此平面网络中的一个IP地址，如图3-1中的cbr0接口，这就意味着Node到Pod间的通信也可在此网络上直接进行。因此，Pod间的通信或Pod到Node间的通信比较类似于同一IP网络中主机间进行的通信。</p><h4 id="3-Service与Pod间通信"><a href="#3-Service与Pod间通信" class="headerlink" title="(3) Service与Pod间通信"></a>(3) Service与Pod间通信</h4><p>​    Service资源的专用网络也称为集群网络（Cluster Network），需要在启动kube-apiserver时经由“–service-cluster-ip-range”选项进行指定，如10.96.0.0/12，而每个Service对象在此网络中均拥一个称为Cluster-IP的固定地址。管理员或用户对Service对象的创建或更改操作由API Server存储完成后触发各节点上的kube-proxy，并根据代理模式的不同将其定义为相应节点上的iptables规则或ipvs规则，借此完成从Service的Cluster-IP与Pod-IP之间的报文转发，如图3-2所示。</p><p><img src="https://img2.jesse.top/image-20210206161001841.png" alt="image-20210206161001841"></p><p>​                                                                                                         图 3-2 Service和Pod</p><h4 id="4-集群外部到Pod对象之间的通信"><a href="#4-集群外部到Pod对象之间的通信" class="headerlink" title="(4) 集群外部到Pod对象之间的通信"></a>(4) 集群外部到Pod对象之间的通信</h4><p>​    将集群外部的流量引入到Pod对象的方式有受限于Pod所在的工作节点范围的节点端口（nodePort）和主机网络（hostNetwork）两种，以及工作于集群级别的NodePort或LoadBalancer类型的Service对象。不过，即便是四层代理的模式也要经由两级转发才能到达目标Pod资源：请求流量首先到达外部负载均衡器，由其调度至某个工作节点之上，而后再由工作节点的netfilter（kube-proxy）组件上的规则（iptables或ipvs）调度至某个目标Pod对象。</p><h3 id="4-Kubernetes-CNI插件"><a href="#4-Kubernetes-CNI插件" class="headerlink" title="4. Kubernetes CNI插件"></a>4. Kubernetes CNI插件</h3><p>​    Kubernetes设计了以上四种网络模型.但是Kubernetes自己并不负责网络具体工作,而是交给的了第三方网络插件.为了规范以及兼容各种解决方案.CoreOS和Google联合制定了CNI（Container Network Interface）标准，旨在定义容器网络模型规范。它连接了两个组件：容器管理系统和网络插件。它们之间通过JSON格式的文件进行通信，以实现容器的网络功能.具体的网络工作均由插件来实现，包括创建容器netns、关联网络接口到对应的netns以及为网络接口分配IP等。</p><p>CNI的基本思想是:容器运行时环境在创建容器时，先创建好网络名称空间（netns），然后调用CNI插件为这个netns配置网络，而后再启动容器内的进程。</p><p>Kubernetes要求网络插件需要满足以下基本原则:</p><ul><li>Pod无论运行在任何节点都可以互相直接通信，而不需要借助NAT地址转换实现。</li><li>Node与Pod可以互相通信，在不限制的前提下，Pod可以访问任意网络。</li><li>Pod拥有独立的网络栈，Pod看到自己的地址和外部看见的地址应该是一样的，并且同个Pod内所有的容器共享同个网络栈。 </li></ul><p>CNI本身只是规范，付诸生产还需要有特定的实现。目前，CNI提供的插件分为三类：main、meta和ipam。main一类的插件主要在于实现某种特定的网络功能，例如loopback、bridge、macvlan和ipvlan等；meta一类的插件自身并不提供任何网络实现，而是用于调用其他插件，例如调用flannel；ipam仅用于分配IP地址，而不提供网络实现。</p><p>CNI具有很强的扩展性和灵活性，例如，如果用户对某个插件具有额外的需求，则可以通过输入中的args和环境变量CNI_ARGS进行传递，然后在插件中实现自定义的功能，这大大增加了它的扩展性。另外，CNI插件将main和ipam分开，赋予了用户自由组合它们的机制，甚至一个CNI插件也可以直接调用另外一个CNI插件。CNI目前已经是Kubernetes当前推荐的网络方案。常见的CNI网络插件包含如下这些主流的项目.</p><ul><li><p><strong>Flannel</strong>.</p><p>一个为Kubernetes提供叠加网络的网络插件，它基于Linux TUN/TAP，使用UDP封装IP报文来创建叠加网络，并借助etcd维护网络的分配情况。</p></li><li><p><strong>Calico</strong></p><p>一个基于BGP的三层网络插件，并且也支持网络策略来实现网络的访问控制；它在每台机器上运行一个vRouter，利用Linux内核来转发网络数据包，并借助iptables实现防火墙等功能。</p></li><li><p><strong>Weave Net</strong>：</p><p>Weave Net是一个多主机容器的网络方案，支持去中心化的控制平面，在各个host上的wRouter间建立Full Mesh的TCP连接，并通过Gossip来同步控制信息。</p></li></ul><h3 id="5-Kubernetes-CNI网络通信"><a href="#5-Kubernetes-CNI网络通信" class="headerlink" title="5. Kubernetes CNI网络通信"></a>5. Kubernetes CNI网络通信</h3><p>实际上CNI的容器网络通信流程跟前面的基础网络一样，只是CNI维护了一个单独的网桥来代替 docker0。这个网桥的名字就叫作：CNI 网桥，它在宿主机上的设备名称默认是：cni0。cni的设计思想，就是：Kubernetes在启动Infra容器之后，就可以直接调用CNI网络插件，为这个Infra容器的Network Namespace，配置符合预期的网络栈。</p><p>CNI插件三种网络实现模式：</p><p><img src="https://img2.jesse.top/cni-network.png" alt="img"></p><ul><li>overlay 模式是基于隧道技术实现的，整个容器网络和主机网络独立，容器之间跨主机通信时将整个容器网络封装到底层网络中，然后到达目标机器后再解封装传递到目标容器。不依赖与底层网络的实现。实现的插件有flannel(UDP、vxlan)、calico(IPIP)等等</li><li>三层路由模式中容器和主机也属于不通的网段，他们容器互通主要是基于路由表打通，无需在主机之间建立隧道封包。但是限制条件必须依赖大二层同个局域网内。实现的插件有flannel(host-gw)、calico(BGP)等等</li><li>underlay网络是底层网络，负责互联互通。 容器网络和主机网络依然分属不同的网段，但是彼此处于同一层网络，处于相同的地位。整个网络三层互通，没有大二层的限制，但是需要强依赖底层网络的实现支持.实现的插件有calico(BGP)等等</li></ul><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>kubernetes容器网络: <a href="https://tech.ipalfish.com/blog/2020/03/06/kubernetes_container_network/" target="_blank" rel="noopener">https://tech.ipalfish.com/blog/2020/03/06/kubernetes_container_network/</a> (伴鱼团队)</p><p>&lt;kubernetes进阶实战&gt; 11.1 马永亮</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kubernetes-网络&quot;&gt;&lt;a href=&quot;#kubernetes-网络&quot; class=&quot;headerlink&quot; title=&quot;kubernetes 网络&quot;&gt;&lt;/a&gt;kubernetes 网络&lt;/h2&gt;&lt;h3 id=&quot;1-开篇&quot;&gt;&lt;a href=&quot;#1-开篇&quot; class=&quot;headerlink&quot; title=&quot;1.开篇&quot;&gt;&lt;/a&gt;1.开篇&lt;/h3&gt;&lt;p&gt;​    Docker容器诞生以来,,如何确定合适的网络方案是亟待解决的难题之一.在日趋复杂的业务场景下,网络的复杂性也呈几何级数上升.本篇首先回顾了Docker容器的网络通信,然后介绍Kubernertes的网络模型.在Kubernetes集群中,IP地址的分配对象是以Pod为单位,而非容器.同一个Pod内的所有容器共享同一个网络名称空间&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
      <category term="network" scheme="https://jesse.top/categories/kubernetes/network/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Kong自定义配置Nginx</title>
    <link href="https://jesse.top/2021/01/19/Linux-Web/Kong%E8%87%AA%E5%AE%9A%E4%B9%89%E9%85%8D%E7%BD%AENginx/"/>
    <id>https://jesse.top/2021/01/19/Linux-Web/Kong自定义配置Nginx/</id>
    <published>2021-01-19T03:59:58.000Z</published>
    <updated>2021-02-06T16:04:56.932Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kong自定义配置Nginx"><a href="#Kong自定义配置Nginx" class="headerlink" title="Kong自定义配置Nginx"></a>Kong自定义配置Nginx</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>Kong是基于Nginx实现代理转发.官方的 <code>nginx.conf</code> 配置文件过于简单.如果需要优化nginx的性能,就需要修改默认的nginx配置文件,或者重新自定义一个nginx配置文件.</p><p>具体方法可以参考官方文档: <a href="https://docs.konghq.com/2.2.x/configuration/#environment-variables" target="_blank" rel="noopener">https://docs.konghq.com/2.2.x/configuration/#environment-variables</a></p><p>下面介绍2种方式自定义nginx的配置</p><h3 id="通过环境变量注入"><a href="#通过环境变量注入" class="headerlink" title="通过环境变量注入"></a>通过环境变量注入</h3><p>Kong服务启动时会每次都新建一个新的nginx配置文件.可以通过将nginx指令注入到 <code>kong.conf</code> 配置文件中从而配置到这个新的nginx配置文件</p><a id="more"></a> <h4 id="注入Nginx单个指令"><a href="#注入Nginx单个指令" class="headerlink" title="注入Nginx单个指令"></a>注入Nginx单个指令</h4><p>注入到Kong的环境变量一般包含下面2种前缀.前缀名不同代表注入的nginx指令作用在不同的作用域下.Kong会将环境变量的前缀去掉,然后将环境变量的后面部分注入到nginx.</p><ul><li><code>nginx_http_</code> 该前缀环境变量会被注入到Nginx的http代码块</li><li><code>nginx_proxy_</code> 该前缀会被注入到nginx的server代码块</li></ul><p>例如.如果注入以下环境变量到 <code>kong.conf</code> 配置文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nginx_proxy_large_client_header_buffers=16 128k</span><br></pre></td></tr></table></figure><p>Kong会将以下环境变量注入到Nginx配置文件的代理 <code>server</code> 块中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">large_client_header_buffers 16 128k;</span><br></pre></td></tr></table></figure><p>下面的环境变量,会被注入到nginx的http块中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export KONG_NGINX_HTTP_OUTPUT_BUFFERS=&quot;4 64k&quot;</span><br><span class="line"></span><br><span class="line">#注入以下Nginx指令</span><br><span class="line">output_buffers 4 64k;</span><br></pre></td></tr></table></figure><blockquote><p>还有一种前缀 <code>Nginx_admin_</code> 这个作用在kong的admin api,所以用的较少</p></blockquote><h4 id="注入Nginx代码块"><a href="#注入Nginx代码块" class="headerlink" title="注入Nginx代码块"></a>注入Nginx代码块</h4><p>对于一些复杂的配置场景,比如需要将整个server代码块添加到Nginx配置文件.可以使用上面的环境变量注入的方式,注入一个 <code>include</code> 指令到Nginx配置文件.</p><p>例如下面这个nginx的server代码块文件.假如该文件名为 <code>my-server.conf</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># custom server</span><br><span class="line">server &#123;</span><br><span class="line">  listen 2112;</span><br><span class="line">  location / &#123;</span><br><span class="line">    # ...more settings...</span><br><span class="line">    return 200;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以通过下面的方式添加到 <code>kong.conf</code> 配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nginx_http_include = /path/to/your/my-server.conf</span><br></pre></td></tr></table></figure><p>或者通过环境变量方式注入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export KONG_NGINX_HTTP_INCLUDE=&quot;/path/to/your/my-server.conf&quot;</span><br></pre></td></tr></table></figure><p>这样当Kong启动后,server代码块会被添加到Nginx的配置文件.</p><blockquote><p>这里也可以使用相对路径来注入一个server代码块的配置文件,但是配置文件需要在 <code>kong.conf</code> 配置文件的prefix路径之下.或者kong启动时候通过 <code>-p</code> 参数自定义的prefix路径之下</p></blockquote><h3 id="自定义Nginx模板"><a href="#自定义Nginx模板" class="headerlink" title="自定义Nginx模板"></a>自定义Nginx模板</h3><p>kong在启动的时候会根据 <code>/usr/local/share/lua/5.1/kong/templates/nginx.lua</code>和 <code>/usr/local/share/lua/5.1/kong/templates/nginx_kong.lua</code> 这2个lua模板来自动生成nginx的配置文件.当Kong启动后会自动在prefix路径下生成 <code>nginx.conf</code> 和 <code>nginx-kong.conf</code> .前者是Nginx的主配置文件,然后通过include方式引入了 <code>nginx_kong.conf</code> </p><p>当kong启动后,会产生下面2个文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/kong</span><br><span class="line">    - nginx.conf</span><br><span class="line">    - nginx-kong.conf</span><br></pre></td></tr></table></figure><blockquote><p>在 <a href="https://github.com/kong/kong/tree/master/kong/templates下也存放了kong的默认模板文件" target="_blank" rel="noopener">https://github.com/kong/kong/tree/master/kong/templates下也存放了kong的默认模板文件</a>.</p></blockquote><p>所以在 <code>usr/local/kong</code> 目录下直接修改 <code>Nginx.conf</code> 配置文件无法永久生效.当kong重启时,配置文件会被默认的Lua目标所覆盖和替代</p><p>如果一定要自定义nginx配置文件.可以自定义nginx的模板文件来替代 <code>Nginx.lua</code> .然后在该模板文件里引入 <code>nginx-kong.conf</code> </p><h4 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h4><ol><li>拷贝 <code>nginx.conf</code> 配置文件为  <code>nginx.conf.template</code> </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /usr/local/kong/nginx.conf nginx.conf.template</span><br></pre></td></tr></table></figure><ol><li>自定义配置 <code>nginx.conf.template</code> .例如下面是我的配置文件内容</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">pid pids/nginx.pid;</span><br><span class="line">error_log logs/error.log $&#123;&#123;LOG_LEVEL&#125;&#125;; </span><br><span class="line"></span><br><span class="line"># injected nginx_main_* directives</span><br><span class="line">daemon off;</span><br><span class="line">worker_processes auto;</span><br><span class="line">worker_rlimit_nofile 204800;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    # injected nginx_events_* directives</span><br><span class="line">    multi_accept on;</span><br><span class="line">    use epoll;</span><br><span class="line">    worker_connections  204800;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line">    sendfile            on;</span><br><span class="line">    tcp_nopush          on;</span><br><span class="line">    tcp_nodelay         on;</span><br><span class="line">    include &apos;nginx-kong.conf&apos;;</span><br><span class="line"></span><br><span class="line">    keepalive_timeout  60;</span><br><span class="line">    keepalive_requests 1024;</span><br><span class="line">    client_header_buffer_size 4k;</span><br><span class="line">    large_client_header_buffers 4 32k;</span><br><span class="line"></span><br><span class="line">    types_hash_max_size 2048;</span><br><span class="line">    client_body_timeout 180;</span><br><span class="line">    client_header_timeout 10;</span><br><span class="line">    send_timeout 240;</span><br><span class="line"></span><br><span class="line">    proxy_connect_timeout   1000ms;</span><br><span class="line">    proxy_send_timeout      5000ms;</span><br><span class="line">    proxy_read_timeout      5000ms;</span><br><span class="line">    proxy_buffers           64 8k;</span><br><span class="line">    proxy_busy_buffers_size    128k;</span><br><span class="line">    proxy_temp_file_write_size 64k;</span><br><span class="line">    proxy_redirect off;</span><br><span class="line">    proxy_next_upstream off;</span><br><span class="line">    </span><br><span class="line">    gzip on;</span><br><span class="line">    gzip_min_length 1k;</span><br><span class="line">    gzip_buffers 4 16k;</span><br><span class="line">    gzip_http_version 1.0;</span><br><span class="line">    gzip_comp_level 2;</span><br><span class="line">    gzip_types text/plain application/x-javascript text/css application/xml;</span><br><span class="line">    gzip_vary on;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>注意该配置文件内的指令不能和 <code>nginx-kong.conf</code> 配置文件有同名或者冲突.否则kong无法启动</p></blockquote><ol><li>重新启动Kong.使用下面的参数指定自定义的Nginx模板文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kong start -c /etc/kong/kong.conf --nginx-conf nginx.conf.template</span><br></pre></td></tr></table></figure><h4 id=""><a href="#" class="headerlink" title=" "></a> </h4><h4 id="docker运行kong"><a href="#docker运行kong" class="headerlink" title="docker运行kong"></a>docker运行kong</h4><p>如果是docker方式运行.可以使用 <code>Dockerfile</code> 自定义kong镜像</p><p>以下是Dockerfile文件内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FROM kong:2.2.0</span><br><span class="line">COPY nginx.conf.template /usr/local/kong/nginx.conf.template</span><br><span class="line">CMD kong start --nginx-conf /usr/local/kong/nginx.conf.template</span><br></pre></td></tr></table></figure><p>编译docker镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t dwd-kong:2.2.0 .</span><br></pre></td></tr></table></figure><p>重启运行docker.但是要先在kong容器运行 <code>kong migrations up</code> 和 <code>kong migrations finish</code> 命令.所以 <code>docker-compose.yml</code> 配置文件内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">kong:</span><br><span class="line">    image: hub.doweidu.com/beta/dwd-kong:2.2.0</span><br><span class="line">    container_name: kong</span><br><span class="line">    hostname: kong</span><br><span class="line">    environment:</span><br><span class="line">      - KONG_DATABASE=postgres</span><br><span class="line">      - KONG_PG_HOST=kong-database</span><br><span class="line">      - KONG_PROXY_ACCESS_LOG=/var/log/kong/access.log</span><br><span class="line">      - KONG_ADMIN_ACCESS_LOG=/var/log/kong/admin_access.log</span><br><span class="line">      - KONG_PROXY_ERROR_LOG=/var/log/kong/error.log</span><br><span class="line">      - KONG_ADMIN_ERROR_LOG=/var/log/kong/admin_error.log</span><br><span class="line">      - KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl</span><br><span class="line">      - KONG_TRUSTED_IPS=0.0.0.0/0,::/0</span><br><span class="line">      - KONG_REAL_IP_HEADER=X-Forwarded-For</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/logs/kong:/var/log/kong</span><br><span class="line">      - /etc/localtime:/etc/localtime</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8000:8000&quot;</span><br><span class="line">      - &quot;8443:8443&quot;</span><br><span class="line">      - &quot;8001:8001&quot;</span><br><span class="line">      - &quot;8444:8444&quot;</span><br><span class="line">    expose:</span><br><span class="line">      - &quot;8000&quot;</span><br><span class="line">      - &quot;8443&quot;</span><br><span class="line">      - &quot;8001&quot;</span><br><span class="line">      - &quot;8444&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - dev-net</span><br><span class="line">    restart: always</span><br><span class="line">    depends_on:</span><br><span class="line">        - kong-database</span><br><span class="line">        - kong-migration</span><br><span class="line">        - kong-migration-finish</span><br><span class="line">        </span><br><span class="line">kong-migration:</span><br><span class="line">    image: hub.doweidu.com/beta/dwd-kong:2.2.0</span><br><span class="line">   # command: &quot;kong migrations bootstrap&quot;</span><br><span class="line">    command: &quot;kong migrations up&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - dev-net</span><br><span class="line">    restart: on-failure</span><br><span class="line">    environment:</span><br><span class="line">      KONG_PG_HOST: kong-database</span><br><span class="line">    depends_on:</span><br><span class="line">      - kong-database</span><br><span class="line"></span><br><span class="line">  kong-migration-finish:</span><br><span class="line">    image: hub.doweidu.com/beta/dwd-kong:2.2.0</span><br><span class="line">   # command: &quot;kong migrations bootstrap&quot;</span><br><span class="line">    command: &quot;kong migrations finish&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - dev-net</span><br><span class="line">    restart: on-failure</span><br><span class="line">    environment:</span><br><span class="line">      KONG_PG_HOST: kong-database</span><br><span class="line">    depends_on:</span><br><span class="line">      - kong-database</span><br><span class="line">      - kong-migration</span><br></pre></td></tr></table></figure><p>启动容器后.可以查看配置文件是否生效:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[work@docker docker-compose]$docker exec kong cat /usr/local/kong/nginx.conf</span><br><span class="line">pid pids/nginx.pid;</span><br><span class="line">error_log logs/error.log notice;</span><br><span class="line"></span><br><span class="line"># injected nginx_main_* directives</span><br><span class="line">daemon off;</span><br><span class="line">worker_processes auto;</span><br><span class="line">worker_rlimit_nofile 204800;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    # injected nginx_events_* directives</span><br><span class="line">    multi_accept on;</span><br><span class="line">    use epoll;</span><br><span class="line">    worker_connections  204800;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line">    sendfile            on;</span><br><span class="line">    tcp_nopush          on;</span><br><span class="line">    tcp_nodelay         on;</span><br><span class="line">    include &apos;nginx-kong.conf&apos;;</span><br><span class="line"> .......略...........</span><br></pre></td></tr></table></figure><p>如此,便实现了自定义kong的nginx配置文件,这在大并发场景中可能需要优化nginx的转发性能.如果是小规模场景中,可以使用Kong的默认的Nginx配置文件即可.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kong自定义配置Nginx&quot;&gt;&lt;a href=&quot;#Kong自定义配置Nginx&quot; class=&quot;headerlink&quot; title=&quot;Kong自定义配置Nginx&quot;&gt;&lt;/a&gt;Kong自定义配置Nginx&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;Kong是基于Nginx实现代理转发.官方的 &lt;code&gt;nginx.conf&lt;/code&gt; 配置文件过于简单.如果需要优化nginx的性能,就需要修改默认的nginx配置文件,或者重新自定义一个nginx配置文件.&lt;/p&gt;
&lt;p&gt;具体方法可以参考官方文档: &lt;a href=&quot;https://docs.konghq.com/2.2.x/configuration/#environment-variables&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://docs.konghq.com/2.2.x/configuration/#environment-variables&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;下面介绍2种方式自定义nginx的配置&lt;/p&gt;
&lt;h3 id=&quot;通过环境变量注入&quot;&gt;&lt;a href=&quot;#通过环境变量注入&quot; class=&quot;headerlink&quot; title=&quot;通过环境变量注入&quot;&gt;&lt;/a&gt;通过环境变量注入&lt;/h3&gt;&lt;p&gt;Kong服务启动时会每次都新建一个新的nginx配置文件.可以通过将nginx指令注入到 &lt;code&gt;kong.conf&lt;/code&gt; 配置文件中从而配置到这个新的nginx配置文件&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>Kong实现限流</title>
    <link href="https://jesse.top/2021/01/19/Linux-Web/Kong%20Rate%20Limiting%E9%99%90%E6%B5%81%E6%8F%92%E4%BB%B6/"/>
    <id>https://jesse.top/2021/01/19/Linux-Web/Kong Rate Limiting限流插件/</id>
    <published>2021-01-19T03:59:58.000Z</published>
    <updated>2021-01-19T14:39:47.543Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kong实现限流"><a href="#Kong实现限流" class="headerlink" title="Kong实现限流"></a>Kong实现限流</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>近期发现公司某个业务对外的openapi接口的/merchantapi路径异常调用非常频繁.公司的第三方商户需要通过这个路径来调用ERP接口,但是经常发生被恶意刷接口的情况,导致公司的业务服务器资源使用率飙升,面临很大的宕机风险和隐患.</p><p>目前外部客户端访问公司业务仍然是阿里云SLB—–Nginx—php-fpm的架构.由于Nginx的限流能力并不出色,特别是针对具体path路径的限流.所以,引入了Kong api网关</p><h3 id="Rate-Limiting限流插件介绍"><a href="#Rate-Limiting限流插件介绍" class="headerlink" title="Rate Limiting限流插件介绍"></a>Rate Limiting限流插件介绍</h3><p>Rate Limiting是Kong社区版就已经自带的官方流量控制插件.详细信息可以参考Kong官网介绍. <a href="https://docs.konghq.com/hub/kong-inc/rate-limiting/" target="_blank" rel="noopener">https://docs.konghq.com/hub/kong-inc/rate-limiting/</a></p><p>它可以针对<code>consumer</code> ,<code>credential</code> ,<code>ip</code> ,<code>service</code>,<code>path</code>,<code>header</code> 等多种维度来进行限流.流量控制的精准度也有多种方式可以参考,比如可以做到秒级,分钟级,小时级等限流控制.</p><h4 id="响应客户端头部信息"><a href="#响应客户端头部信息" class="headerlink" title="响应客户端头部信息"></a>响应客户端头部信息</h4><p>当启用这个插件后.Kong会响应客户端一些额外的头部信息,告诉客户端限流信息.例如下面是Kong响应给客户端的header信息,告诉客户端当前的限流策略是10r/s</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RateLimit-Limit: 10</span><br><span class="line">RateLimit-Remaining: 0</span><br><span class="line">RateLimit-Reset: 1</span><br><span class="line"></span><br><span class="line">X-Kong-Response-Latency: 1</span><br><span class="line">X-RateLimit-Limit-Second: 10</span><br><span class="line">X-RateLimit-Remaining-Second: 0</span><br></pre></td></tr></table></figure><p>如果客户端的访问请求超过限流的阈值,Kong会返回status<code>429</code>的状态码以及下面的错误信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; &quot;message&quot;: &quot;API rate limit exceeded&quot; &#125;</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="限流策略对Kong性能影响"><a href="#限流策略对Kong性能影响" class="headerlink" title="限流策略对Kong性能影响"></a>限流策略对Kong性能影响</h4><p>Rate limiting插件支持3种限流策略.</p><p><code>cluster</code> 集群策略.Kong的数据库会维护一个计数器,并且在所有的Kong集群内每个节点共享这个计数器.如果计数器触发限流上线,所有的Kong节点都拒绝客户端的转发.这就意味着每个节点接收到客户端的请求,都会对数据库进行读写操作.</p><p><code>redis</code> redis策略和<code>cluster</code> 相似,唯一不同的是,计数器是存储在redis数据中.并且在集群内所有节点共享.</p><p><code>local</code> 本地策略.计数器保存在Kong节点服务器本地内存缓冲区.并且计数器只对该节点有效.这意味着<code>local</code>策略有最好的性能表现.但是由于计数器存储在本地.所以限流的精度没有<code>redis</code>和<code>cluster</code> 准确.并且会影响Kong节点服务器弹性扩容(比如限流设置30r/s,Kong集群从2个节点扩容到4个节点.限流就从60r/s变成了120r/s.此时需要手动将限流设置从30r/s降低到15r/s)</p><blockquote><p>或者,可以在Kong前面配置一个hash转发策略的负载均衡,将同一个外部客户端的请求代理到同一个节点.这样local策略的精确度可以提升,并且kong节点的弹性扩容不会影响限流效果</p></blockquote><p>下面是3种限流策略的对比表</p><table><thead><tr><th>policy</th><th>describe</th><th>pros</th><th>cons</th></tr></thead><tbody><tr><td>cluster</td><td>集群策略</td><td>限流精准度高,不需要第三方组件支持</td><td>对Kong性能影响比较大</td></tr><tr><td>redis</td><td>redis策略</td><td>限流精准度高,对Kong性能影响较低</td><td>需要额外的redis服务</td></tr><tr><td>local</td><td>本地策略</td><td>对Kong性能影响最低</td><td>精准度比较差,Kong节点扩容和缩容需要手动调整限流速率</td></tr></tbody></table><p>下面是以上集群策略的使用场景:</p><ul><li>如果对流量精确度要求非常高.比如金融,交易等.那么适合redis或者cluster的限流策略</li><li>如果是为了保护后端服务,避免大流量带来的服务器过载.那么适合local限流策略,这种场景对限流的精度要求不高</li></ul><h3 id="针对客户端IP限流"><a href="#针对客户端IP限流" class="headerlink" title="针对客户端IP限流"></a>针对客户端IP限流</h3><p>我们场景中针对客户端IP进行限流.但是由于Kong是在SLB或者Nginx的负载均衡后面,所以默认情况下,Kong采用的IP是上一级负载均衡器的IP.此时就需要将客户端的真实IP传递到Kong,并且使用该IP作为<code>remote_ip</code> .实现方法如下:</p><ul><li>虚拟机运行Kong</li></ul><p>针对rpm包或者其他方式安装的Kong服务,可以修改默认的<code>/etc/kong/kong.conf</code> 配置文件.加入下面2行配置信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trusted_ips = 0.0.0.0/0,::/0</span><br><span class="line">real_ip_header = X-Forwarded-For</span><br></pre></td></tr></table></figure><p>重载kong配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kong reload</span><br></pre></td></tr></table></figure><ul><li>docker容器方式运行kong</li></ul><p>针对docker容器方式运行的Kong,修改配置文件不方便,此时可以通过变量注入的方式自定义配置<code>kong.conf</code> 配置文件.还可以通过这种方式注入nginx自定义配置,具体可以参考官方的文档介绍:<a href="https://docs.konghq.com/2.2.x/configuration/#environment-variables" target="_blank" rel="noopener">environment-variables</a></p><p>例如,上面的2行配置内容可以通过在配置参数前面加<code>KONG_</code>以及大写的参数名的方式注入环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">KONG_TRUSTED_IPS=0.0.0.0/0,::/0</span><br><span class="line">KONG_REAL_IP_HEADER=X-Forwarded-For</span><br></pre></td></tr></table></figure><p>修改Kong的<code>docker-compose</code>文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">kong:</span><br><span class="line">    image: dwd-kong:2.2.0  #自定义kong镜像.也可以使用官方的docker镜像</span><br><span class="line">    container_name: kong</span><br><span class="line">    hostname: kong</span><br><span class="line">    environment:</span><br><span class="line">      - KONG_DATABASE=postgres</span><br><span class="line">      - KONG_PG_HOST=kong-database</span><br><span class="line">      - KONG_PROXY_ACCESS_LOG=/var/log/kong/access.log</span><br><span class="line">      - KONG_ADMIN_ACCESS_LOG=/var/log/kong/admin_access.log</span><br><span class="line">      - KONG_PROXY_ERROR_LOG=/var/log/kong/error.log</span><br><span class="line">      - KONG_ADMIN_ERROR_LOG=/var/log/kong/admin_error.log</span><br><span class="line">      - KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl</span><br><span class="line">      - KONG_TRUSTED_IPS=0.0.0.0/0,::/0       #增加这两行</span><br><span class="line">      - KONG_REAL_IP_HEADER=X-Forwarded-For   #增加这两行</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/logs/kong:/var/log/kong</span><br><span class="line">      - /etc/localtime:/etc/localtime</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8000:8000&quot;</span><br><span class="line">      - &quot;8443:8443&quot;</span><br><span class="line">      - &quot;8001:8001&quot;</span><br><span class="line">      - &quot;8444:8444&quot;</span><br><span class="line">    expose:</span><br><span class="line">      - &quot;8000&quot;</span><br><span class="line">      - &quot;8443&quot;</span><br><span class="line">      - &quot;8001&quot;</span><br><span class="line">      - &quot;8444&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - dev-net</span><br><span class="line">    restart: always</span><br><span class="line">    depends_on:</span><br><span class="line">        - kong-database</span><br><span class="line">        - kong-migration</span><br><span class="line">        - kong-migration-finish</span><br></pre></td></tr></table></figure><h3 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h3><p>Rate Limiting插件由Kong默认提供,所以无需自行安装.由于是针对<code>/merchantapi</code> 这个借口进行限流,所以只需配置该route,并且将插件应用到这个route下即可.由于我日常使用的python进行Kong的配置,所以这里只列出我的python配置文件中相关配置.不演示具体配置了.</p><blockquote><p>使用kong的dashboard也可以很方便的实现配置</p></blockquote><ul><li>配置service</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hsq_openapi_dev = &#123; &quot;name&quot;: &quot;hsq_openapi_dev&quot;,</span><br><span class="line">            &quot;host&quot;: &quot;kong.devapi.hsq.net&quot;,</span><br><span class="line">            &quot;port&quot;: 80,</span><br><span class="line">            &quot;protocol&quot;: &quot;http&quot;,</span><br><span class="line">            &quot;path&quot;: &quot;/&quot;</span><br><span class="line">          &#125;</span><br></pre></td></tr></table></figure><ul><li>配置route</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#默认转发路由</span><br><span class="line">hsq_openapi_dev =   &#123;  &quot;service_name&quot;:&quot;hsq_openapi_dev&quot;,</span><br><span class="line">                &quot;data&quot;:&#123;</span><br><span class="line">                &quot;name&quot;: &quot;hsq-openapi-dev&quot;,</span><br><span class="line">                &quot;hosts&quot;: &quot;m.devapi.hsq.net&quot;,</span><br><span class="line">                &quot;strip_path&quot;: &quot;false&quot;,</span><br><span class="line">                &quot;protocols&quot;: [&quot;http&quot;, &quot;https&quot;],</span><br><span class="line">                &quot;paths&quot;: &quot;/&quot;,</span><br><span class="line">                &quot;methods&quot;: [&quot;GET&quot;, &quot;POST&quot;,&quot;PUT&quot;,&quot;OPTIONS&quot;,&quot;DELETE&quot;]&#125;</span><br><span class="line">             &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#限流接口转发路由</span><br><span class="line">hsq_merchant_api_limit = &#123;  &quot;service_name&quot;:&quot;hsq_openapi_dev&quot;,</span><br><span class="line">                    &quot;data&quot;:&#123;</span><br><span class="line">                    &quot;name&quot;: &quot;hsq_merchant_api_limit&quot;,</span><br><span class="line">                    &quot;hosts&quot;: &quot;m.devapi.hsq.net&quot;,</span><br><span class="line">                    &quot;strip_path&quot;: &quot;false&quot;,</span><br><span class="line">                    &quot;protocols&quot;: [&quot;http&quot;, &quot;https&quot;],</span><br><span class="line">                    &quot;paths&quot;: &quot;/merchantapi&quot;,</span><br><span class="line">                    &quot;methods&quot;: [&quot;GET&quot;, &quot;POST&quot;,&quot;PUT&quot;,&quot;OPTIONS&quot;,&quot;DELETE&quot;]&#125;</span><br><span class="line">                 &#125;</span><br></pre></td></tr></table></figure><ul><li>配置rate-limiting插件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hsq_merchantapi_limit = &#123; &quot;route_name&quot;: &quot;hsq_merchant_api_limit&quot;,  #关联到上面的route.表示该插件作用在route级别</span><br><span class="line">         &quot;data&quot;: &#123;</span><br><span class="line">         &quot;name&quot;: &quot;rate-limiting&quot;, #插件名称</span><br><span class="line">         &quot;config.second&quot;: 10, # 限流.每秒10个请求</span><br><span class="line">         &quot;config.policy&quot;: &quot;local&quot;, #限流策略</span><br><span class="line">         &quot;config.limit_by&quot;: &quot;ip&quot;    #针对客户端IP限流</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>运行python脚本,配置Kong</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~/Desktop/kong-python   master ●✚  python3 kong.py</span><br><span class="line">请输入你要配置的Kong的环境名称,例如:dev,beta,prod:&gt;&gt;&gt;dev</span><br><span class="line">正在创建service:hsq_openapi_dev</span><br><span class="line">service:hsq_openapi_dev创建成功</span><br><span class="line">开始创建routes:hsq-openapi-dev</span><br><span class="line">routes路由hsq-openapi-dev创建成功</span><br><span class="line">开始创建routes:hsq_merchant_api_limit</span><br><span class="line">routes路由hsq_merchant_api_limit创建成功</span><br><span class="line">plugins:rate-limiting创建成功.绑定在route路由:hsq_merchant_api_limit中</span><br></pre></td></tr></table></figure><h3 id="压测效果"><a href="#压测效果" class="headerlink" title="压测效果"></a>压测效果</h3><p>为了验证插件效果,这里使用<code>ab</code> 这个简单的压测工具进行测试.</p><p>1.开启一个终端,执行下面的命令.压测命令运行了1.18秒,只有20个请求成功响应,其余80个请求失败.这恰好符合了rate-limiting插件每秒10个请求的限流策略</p><blockquote><p>由于是在dev环境,所有只有一个Kong节点.如果外部流量负载均衡分发到Kong集群的所有节点,那么总体的限流应该是:Kong节点数量x限流数量</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">ab -n 100 -c 10 https://m.devapi.hsq.net/merchantapi</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">Document Path:          /merchantapi</span><br><span class="line">Document Length:        122 bytes</span><br><span class="line"></span><br><span class="line">Concurrency Level:      10</span><br><span class="line">Time taken for tests:   1.180 seconds</span><br><span class="line">Complete requests:      100         #总共100个请求</span><br><span class="line">Failed requests:        80          #失败了80个</span><br><span class="line">   (Connect: 0, Receive: 0, Length: 80, Exceptions: 0)</span><br><span class="line">Non-2xx responses:      80</span><br><span class="line">Total transferred:      41888 bytes</span><br><span class="line">HTML transferred:       5720 bytes</span><br><span class="line">Requests per second:    84.71 [#/sec] (mean)</span><br><span class="line">Time per request:       118.044 [ms] (mean)</span><br><span class="line">Time per request:       11.804 [ms] (mean, across all concurrent requests)</span><br><span class="line">Transfer rate:          34.65 [Kbytes/sec] received</span><br><span class="line">......</span><br></pre></td></tr></table></figure><ol start="2"><li>将请求继续增大,同时使用curl和浏览器访问该域名.发现请求被拒绝</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl https://m.devapi.hsq.net/merchantapi</span><br><span class="line">&#123;</span><br><span class="line">  &quot;message&quot;:&quot;API rate limit exceeded&quot;</span><br><span class="line">&#125;%</span><br></pre></td></tr></table></figure><p><img src="https://img2.jesse.top/20210119164349.png" alt=""></p><ol start="3"><li>在压测的同时,使用另外一个客户端来同时访问该接口,可以正常访问</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.99.1 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 200 122 &quot;-&quot; &quot;curl/7.29.0&quot;   #其他客户端仍然可以正常访问</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kong实现限流&quot;&gt;&lt;a href=&quot;#Kong实现限流&quot; class=&quot;headerlink&quot; title=&quot;Kong实现限流&quot;&gt;&lt;/a&gt;Kong实现限流&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;近期发现公司某个业务对外的openapi接口的/merchantapi路径异常调用非常频繁.公司的第三方商户需要通过这个路径来调用ERP接口,但是经常发生被恶意刷接口的情况,导致公司的业务服务器资源使用率飙升,面临很大的宕机风险和隐患.&lt;/p&gt;
&lt;p&gt;目前外部客户端访问公司业务仍然是阿里云SLB—–Nginx—php-fpm的架构.由于Nginx的限流能力并不出色,特别是针对具体path路径的限流.所以,引入了Kong api网关&lt;/p&gt;
&lt;h3 id=&quot;Rate-Limiting限流插件介绍&quot;&gt;&lt;a href=&quot;#Rate-Limiting限流插件介绍&quot; class=&quot;headerlink&quot; title=&quot;Rate Limiting限流插件介绍&quot;&gt;&lt;/a&gt;Rate Limiting限流插件介绍&lt;/h3&gt;&lt;p&gt;Rate Limiting是Kong社区版就已经自带的官方流量控制插件.详细信息可以参考Kong官网介绍. &lt;a href=&quot;https://docs.konghq.com/hub/kong-inc/rate-limiting/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://docs.konghq.com/hub/kong-inc/rate-limiting/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;它可以针对&lt;code&gt;consumer&lt;/code&gt; ,&lt;code&gt;credential&lt;/code&gt; ,&lt;code&gt;ip&lt;/code&gt; ,&lt;code&gt;service&lt;/code&gt;,&lt;code&gt;path&lt;/code&gt;,&lt;code&gt;header&lt;/code&gt; 等多种维度来进行限流.流量控制的精准度也有多种方式可以参考,比如可以做到秒级,分钟级,小时级等限流控制.&lt;/p&gt;
&lt;h4 id=&quot;响应客户端头部信息&quot;&gt;&lt;a href=&quot;#响应客户端头部信息&quot; class=&quot;headerlink&quot; title=&quot;响应客户端头部信息&quot;&gt;&lt;/a&gt;响应客户端头部信息&lt;/h4&gt;&lt;p&gt;当启用这个插件后.Kong会响应客户端一些额外的头部信息,告诉客户端限流信息.例如下面是Kong响应给客户端的header信息,告诉客户端当前的限流策略是10r/s&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;RateLimit-Limit: 10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;RateLimit-Remaining: 0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;RateLimit-Reset: 1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X-Kong-Response-Latency: 1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X-RateLimit-Limit-Second: 10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X-RateLimit-Remaining-Second: 0&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果客户端的访问请求超过限流的阈值,Kong会返回status&lt;code&gt;429&lt;/code&gt;的状态码以及下面的错误信息&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123; &amp;quot;message&amp;quot;: &amp;quot;API rate limit exceeded&amp;quot; &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>Prometheus Alertmanager 告警路由</title>
    <link href="https://jesse.top/2021/01/10/prometheus/Prometheus%20alertmanager%20%E5%91%8A%E8%AD%A6%E8%B7%AF%E7%94%B1/"/>
    <id>https://jesse.top/2021/01/10/prometheus/Prometheus alertmanager 告警路由/</id>
    <published>2021-01-09T23:59:58.000Z</published>
    <updated>2021-04-01T14:00:07.210Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Prometheus-Alertmanager-告警路由"><a href="#Prometheus-Alertmanager-告警路由" class="headerlink" title="Prometheus Alertmanager 告警路由"></a>Prometheus Alertmanager 告警路由</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>公司目前Prometheus监控了IDC数据中心的主机,中间,站点等,同时也监控了阿里云线上的rabbitmq,mysql,kong(所有资源都是ECS自己搭建的,非阿里云的saas服务)</p><p>prometheus使用了第三方的钉钉监控插件(prometheus-webhook-dingtalk),github地址: <a href="https://github.com/timonwong/prometheus-webhook-dingtalk" target="_blank" rel="noopener">https://github.com/timonwong/prometheus-webhook-dingtalk</a> </p><p>Prometheus通过alertmanager将告警信息发送到钉钉机器人</p><hr><h3 id="告警路由"><a href="#告警路由" class="headerlink" title="告警路由"></a>告警路由</h3><p>我们需要将阿里云的线上中间件监控告警发送到阿里云的钉钉群.IDC资源的监控告警发送到IDC的钉钉群,不同的钉钉群面对的人群也不同.方便监控告警信息的分类和管理.</p><p>幸好,alertmanager天生支持告警路由的功能,将不同的告警信息发送给不同的receiver接收人</p><hr><h3 id="alertmanager概念介绍"><a href="#alertmanager概念介绍" class="headerlink" title="alertmanager概念介绍"></a>alertmanager概念介绍</h3><p>Prometheus本身并不提供告警功能,所有告警信息都是发送给Alertmanager处理.Alertmanager接收到告警信息后负责将它们分组,抑制,静默,然后路由到相关接收者.</p><h5 id="Grouping分组"><a href="#Grouping分组" class="headerlink" title="Grouping分组"></a>Grouping分组</h5><p>分组功能将多个同一类型的告警合并一起后发送,这在某个服务发生故障从而影响其他几十,上百个相关依赖性的服务时非常有用,可以有效避免告警信息轰炸.例如当网络出现问题时,可能该网络下的数百个服务都出现访问故障,结果数以百计的告警被发送给Alertmanager.此时Alertmanager将同类型的服务合并到一起仅仅使用单条告警通知发送给接收者</p><a id="more"></a><h5 id="Inhibition抑制"><a href="#Inhibition抑制" class="headerlink" title="Inhibition抑制"></a>Inhibition抑制</h5><p>抑止是指如果某个告警已经触发,那么抑止其他有关该服务的告警消息.</p><p>例如如果某个集群A不可达,已经触发了告警.那么其他B,C,D等集群和服务发出的A集群不可达的告警通知将被Alertmanager抑止.告警抑制机制可以防止数百上千的重复故障告警</p><h5 id="Silences静默"><a href="#Silences静默" class="headerlink" title="Silences静默"></a>Silences静默</h5><p>Silence静默配置的作用类似于Zabbix中的Maintenance维护功能，可以配置一个时间区间和相关规则，符合该配置的事件将不会进行告警。比如明确凌晨会暂停服务，这个时候就可以提前设置好静默规则，减少不必要的告警骚扰。Prometheus的Silence规则只需要通过AlertManager的Web界面就可以完成，不需要配置文件</p><hr><h3 id="Alertmanager主要处理流程"><a href="#Alertmanager主要处理流程" class="headerlink" title="Alertmanager主要处理流程"></a>Alertmanager主要处理流程</h3><p>处理流程:<br><strong>1.</strong> 接收到Alert，根据labels判断属于哪些Route（可存在多个Route，一个Route有多个Group，一个Group有多个Alert）。<br><strong>2.</strong> 将Alert分配到Group中，没有则新建Group。<br><strong>3.</strong> 新的Group等待group_wait指定的时间（等待时可能收到同一Group的Alert），根据resolve_timeout判断Alert是否解决，然后发送通知。<br><strong>4.</strong> 已有的Group等待group_interval指定的时间，判断Alert是否解决，当上次发送通知到现在的间隔大于repeat_interval或者Group有更新时会发送通知。</p><hr><h3 id="Alertmanager配置文件"><a href="#Alertmanager配置文件" class="headerlink" title="Alertmanager配置文件"></a>Alertmanager配置文件</h3><p>Alertmanager可以通过命令行配置和yaml配置文件配置.<code>./alertmanager -h</code> 可以打印出所有的命令行配置选项.这里主要介绍<code>alertmanager.yaml</code>这个配置文件的相关配置</p><p><code>alertmanager.yaml</code> 配置文件主要字段有如下几个:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">global:</span><br><span class="line">   #顶级路由,在route下可以定义routes子路由树</span><br><span class="line">   route:</span><br><span class="line">   </span><br><span class="line">   #告警接收者,如果有多个routes,那么需要在receivers下定义多个接收者</span><br><span class="line">   receivers:</span><br><span class="line">   </span><br><span class="line">   #告警抑制配置</span><br><span class="line">   inhibit_rules:</span><br></pre></td></tr></table></figure><h5 id="route"><a href="#route" class="headerlink" title="route"></a>route</h5><p><code>route</code> 字段定义路由树的节点,以及子节点的相关配置.子节点可以从父节点继承所有配置参数.</p><p>每个告警进入到顶级配置的route.该route必须是一个默认路由,匹配所有Prometheus的告警规则.然后去遍历所有子路由.如果<code>continue</code> 设置为<code>false</code> ,在匹配到第一个子路由(routes)后就停止继续匹配,并且交给子路由的receiver发出告警.如果<code>continue</code> 设置为<code>true</code> 则继续与后续的其他子路由(routes)匹配.如果某条告警信息不匹配任何子路由,或者当前没有配置任何子路由,则交给默认的顶级route处理.</p><p>下面是一个route路由配置的案例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">route:</span><br><span class="line">  resolve_timeout: 5m #一条告警消息发出后,如果在多长时间内没有再次告警,则认为该故障已经解除,发送告警恢复消息</span><br><span class="line">  group_by: [&apos;alertname&apos;]  #根据告警规则名称分组 也就是rule文件的顶部定义的名称.还可以根据标签label或者job来分组</span><br><span class="line">  group_wait: 30s #一组警报消息到达后等待多少秒发送,这允许在这期间收集更多同组警报消息</span><br><span class="line">  group_interval: 5m #发送某组的第一个警告信息后,等待多久继续发送改组新的警告消息</span><br><span class="line">  repeat_interval: 4h #如果告警信息已经成功发送,等待多久重新发送</span><br><span class="line">  receiver: &apos;default&apos; #顶级route的默认接收者,所有未匹配到子路由的消息都发送到该receiver</span><br><span class="line">  routes: #子路由配置.在子路由下默认继承上面的group_by,group_wait等配置,也可以重写</span><br><span class="line">    - receiver: &quot;aliyun&quot; #定义子路由的接收者</span><br><span class="line">      match_re: #匹配告警信息,可以通过Match固定匹配,也可以通过match_re正则匹配</span><br><span class="line">        service: rabbitmq|mysql #只要是service这个lable标签,值为rabbitmq或者mysql的告警信息都使用该子路由处理</span><br><span class="line">    - receiver: &apos;frontend-pager&apos;</span><br><span class="line">    group_by: [product, environment] #对标签名为product和environment的告警信息作为一组</span><br><span class="line">    match:</span><br><span class="line">      team: frontend</span><br></pre></td></tr></table></figure><h5 id="receiver"><a href="#receiver" class="headerlink" title="receiver"></a>receiver</h5><p>该配置定义了一个或者多个告警消息接收器.Alertmanager并不会主动联系receiver,而是需要第三方webhook插件实现告警接收.我们这里使用的是钉钉告警插件.关于钉钉告警插件的配置在后文会有详细介绍.</p><p>有多少个子route,就对应多少个receiver.(当然也可以多个子route对应同一个receiver).receiver定义了告警消息接受地址</p><p>下面是receiver的配置,定义了2个reciever,对应上面的route.<code>aliyun</code>的receiver用来接收rabbitmq,mysql的告警通知,<code>defualt</code> 用来接收其他所有的告警消息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">      </span><br><span class="line"># 定义接收者信息 </span><br><span class="line">receivers:</span><br><span class="line">- name: &apos;default&apos;</span><br><span class="line">  webhook_configs: #第三方插件的webhook地址</span><br><span class="line">  - url: &apos;http://localhost:8060/dingtalk/default/send&apos; #这里使用的是我本地运行的钉钉插件的发送告警消息</span><br><span class="line">- name: &apos;aliyun&apos;</span><br><span class="line">  webhook_configs:</span><br><span class="line">  - url: &apos;http://localhost:8060/dingtalk/aliyun/send&apos;</span><br></pre></td></tr></table></figure><h5 id="inhibit-rules"><a href="#inhibit-rules" class="headerlink" title="inhibit_rules"></a>inhibit_rules</h5><p>在Alertmanager配置文件中,使用<code>inhibit_rules</code> 定义一组告警的抑制规则.当已经发送的告警通知匹配到target_match和target_match_re规则，当有新的告警规则如果满足source_match或者定义的匹配规则，并且以发送的告警与新产生的告警中equal定义的标签完全相同，则启动抑制机制，新的告警不会发送。</p><p>上面这段概念理解起来比较拗口,使用下面的配置作为一个案例解读:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- source_match:</span><br><span class="line">    alertname: NodeDown</span><br><span class="line">  target_match_reg:</span><br><span class="line">    severity: ~&quot;middle|low&quot;</span><br><span class="line">  equal:</span><br><span class="line">    - node</span><br></pre></td></tr></table></figure><p>当接收到一个lable名称为<code>alertname</code> ,值为<code>NodeDown</code> 的告警.并且为该告警发送了一个通知:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;alertname=&quot;NodeDown&quot;,node=&quot;x.x.x.x&quot;,...&#125; time annotation</span><br></pre></td></tr></table></figure><p>那么Alertmanager就会创建一条抑制规则:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;node=&quot;x.x.x.x&quot;,serverity=~&quot;middle|low&quot;&#125;</span><br></pre></td></tr></table></figure><p>如果新的告警满足severity=~”middle|low”,并且node标签相等(也就是equal的作用).那么该告警就会被抑制..例如该主机上的mysqldown的告警消息就不会被发送</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;alertname=&quot;MysqlDown&quot;,node=&quot;x.x.x.x&quot;,serverity=&quot;middle&quot;,...&#125; time annotation</span><br></pre></td></tr></table></figure><p>这也是我们期望看到的,因为当我们收到了某个主机节点Down的告警通知,那么该主机上的所有服务不可用的告警消息不应该再次发送.</p><hr><h3 id="钉钉插件"><a href="#钉钉插件" class="headerlink" title="钉钉插件"></a>钉钉插件</h3><p>目前使用的是prometheus-webhook-dingtalk钉钉告警插件.在github上可以直接下载二进制文件运行.默认监听在8060端口.使用<code>./prometheus-webhook-dingtalk -h</code>可以指定自定义配置文件和监听端口</p><p>该插件提供了一下路由供Alertmanager的webhook_configs使用</p><p><code>/dingtalk/&lt;profile&gt;/send</code> </p><p>这里的profile需要在插件启动时<code>-ding.profile</code> 中指定相应的名称.为了支持多个receiver,同时往多个钉钉自定义机器人发送告警消息,该插件可以指定多个<code>-ding.profile</code> 参数,从而指定多个钉钉机器人的地址.例如下面的prometheus-webhook-dingtalk启动配置文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[work@172 prometheus-webhook-dingtalk]$ systemctl cat prometheus-webhook-dingtalk</span><br><span class="line"># /usr/lib/systemd/system/prometheus-webhook-dingtalk.service</span><br><span class="line">[Unit]</span><br><span class="line">Description=prometheus-webhook-dingtalk</span><br><span class="line">After=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Restart=on-failure</span><br><span class="line">user=root</span><br><span class="line">#定义default和aliyun2个钉钉机器人地址,用来实现告警路由,将不同的告警消息发送到不同的钉钉群</span><br><span class="line">ExecStart=/data/prometheus-webhook-dingtalk/prometheus-webhook-dingtalk \</span><br><span class="line">          --ding.profile=default=https://oapi.dingtalk.com/robot/send?access_token=c35575a65532bc15b0ff50cfad1xxxxx \</span><br><span class="line">          --ding.profile=aliyun=https://oapi.dingtalk.com/robot/send?access_token=f064adad17bc66ad30d94c7c9cxxxxxxx</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>上面定义了2个profile:<code>aliyun</code>和<code>default</code> 对应了<code>alertmanager.yaml</code>配置文件中的不同的receiver配置.需要注意的是不同的profile,它供alertmanager调用的地址也是不同的.比如:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">default的profile的地址为: http://localhost:8060/dingtalk/default/send</span><br></pre></td></tr></table></figure><p>经过测试下来,不同的监控对象告警信息发送到不同的钉钉群组,方便相关的团队和人员第一时间接收和处理</p><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p><a href="https://prometheus.io/docs/alerting/latest/configuration/" target="_blank" rel="noopener">https://prometheus.io/docs/alerting/latest/configuration/</a>   #alertmanager 官方介绍</p><p><a href="https://www.kancloud.cn/pshizhsysu/prometheus/1803807" target="_blank" rel="noopener">https://www.kancloud.cn/pshizhsysu/prometheus/1803807</a> #Alertmanager介绍</p><p><a href="https://www.pianshen.com/article/410280293/" target="_blank" rel="noopener">https://www.pianshen.com/article/410280293/</a> #钉钉插件介绍</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Prometheus-Alertmanager-告警路由&quot;&gt;&lt;a href=&quot;#Prometheus-Alertmanager-告警路由&quot; class=&quot;headerlink&quot; title=&quot;Prometheus Alertmanager 告警路由&quot;&gt;&lt;/a&gt;Prometheus Alertmanager 告警路由&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;公司目前Prometheus监控了IDC数据中心的主机,中间,站点等,同时也监控了阿里云线上的rabbitmq,mysql,kong(所有资源都是ECS自己搭建的,非阿里云的saas服务)&lt;/p&gt;
&lt;p&gt;prometheus使用了第三方的钉钉监控插件(prometheus-webhook-dingtalk),github地址: &lt;a href=&quot;https://github.com/timonwong/prometheus-webhook-dingtalk&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/timonwong/prometheus-webhook-dingtalk&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;Prometheus通过alertmanager将告警信息发送到钉钉机器人&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;告警路由&quot;&gt;&lt;a href=&quot;#告警路由&quot; class=&quot;headerlink&quot; title=&quot;告警路由&quot;&gt;&lt;/a&gt;告警路由&lt;/h3&gt;&lt;p&gt;我们需要将阿里云的线上中间件监控告警发送到阿里云的钉钉群.IDC资源的监控告警发送到IDC的钉钉群,不同的钉钉群面对的人群也不同.方便监控告警信息的分类和管理.&lt;/p&gt;
&lt;p&gt;幸好,alertmanager天生支持告警路由的功能,将不同的告警信息发送给不同的receiver接收人&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;alertmanager概念介绍&quot;&gt;&lt;a href=&quot;#alertmanager概念介绍&quot; class=&quot;headerlink&quot; title=&quot;alertmanager概念介绍&quot;&gt;&lt;/a&gt;alertmanager概念介绍&lt;/h3&gt;&lt;p&gt;Prometheus本身并不提供告警功能,所有告警信息都是发送给Alertmanager处理.Alertmanager接收到告警信息后负责将它们分组,抑制,静默,然后路由到相关接收者.&lt;/p&gt;
&lt;h5 id=&quot;Grouping分组&quot;&gt;&lt;a href=&quot;#Grouping分组&quot; class=&quot;headerlink&quot; title=&quot;Grouping分组&quot;&gt;&lt;/a&gt;Grouping分组&lt;/h5&gt;&lt;p&gt;分组功能将多个同一类型的告警合并一起后发送,这在某个服务发生故障从而影响其他几十,上百个相关依赖性的服务时非常有用,可以有效避免告警信息轰炸.例如当网络出现问题时,可能该网络下的数百个服务都出现访问故障,结果数以百计的告警被发送给Alertmanager.此时Alertmanager将同类型的服务合并到一起仅仅使用单条告警通知发送给接收者&lt;/p&gt;
    
    </summary>
    
      <category term="prometheus" scheme="https://jesse.top/categories/prometheus/"/>
    
    
      <category term="prometheus" scheme="https://jesse.top/tags/prometheus/"/>
    
  </entry>
  
  <entry>
    <title>kafka-4.2分区管理</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/4.2%E5%88%86%E5%8C%BA%E7%AE%A1%E7%90%86/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/4-主题和分区/4.2分区管理/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:54:58.225Z</updated>
    
    <content type="html"><![CDATA[<h3 id="4-2-1-优选副本的选举"><a href="#4-2-1-优选副本的选举" class="headerlink" title="4.2.1 优选副本的选举"></a>4.2.1 优选副本的选举</h3><h4 id="4-2-1-1-什么是优先副本"><a href="#4-2-1-1-什么是优先副本" class="headerlink" title="4.2.1.1 什么是优先副本"></a>4.2.1.1 什么是优先副本</h4><p>分区使用多副本机制来提升可靠性,但是只有leader副本对外提供读写服务.而follower副本只负责在内部进行消息的同步.如果一个分区的leader副本不可用,那么就意味着整个分区变得不可用.此时就需要从剩余的follower副本中挑选一个新的leader副本继续对外提供服务.</p><blockquote><p>broker节点中的Leader副本个数决定了这个节点负载的高低</p></blockquote><p>在创建主题的时候,主题的分区和副本会尽可能的均匀分布在kafka集群的各个broker节点.对应的Leader副本的分配也比较均匀.例如下面的 <code>topic-demo</code> 主题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 152,153,154</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 152,153,154</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 152,153,154</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 152,153,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><a id="more"></a> <p>可以看到,leader副本均匀分布在所有的broker节点.另外,同一个分区,在同一台broker节点只能存在一个副本.所以leader副本所在的broker节点叫做分区的leader节点.而follower副本所在的broker节点叫做分区的follower节点.</p><p>可以想象的是,随着时间的推移,kafka集群中不可避免的出现节点宕机或者崩溃的情况.当分区的Leader节点发生故障时,其中一个follower节点就会成为新的Leader节点.这样导致集群中的节点之间负载不均衡,从而影响kafka整个集群的稳定性和健壮性.</p><p>即使原来的Leader节点恢复后,加入到集群时,也只能成为一个新的follower节点,而不会自动”抢班夺权”变成leader.</p><p>例如刚才的 <code>topic-demo</code> 分区重启152节点后,leader分布如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 153 Replicas: 152,153,154   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 154 Replicas: 152,154,153   Isr: 153,154,152</span><br></pre></td></tr></table></figure><p>尽管kafka非常均匀的将leader副本分布在其他另外2个几点.但是此时152节点的负载几乎为零.</p><p>为了有效的治理负载失衡的情况,kafka引入了<strong>优先副本(preferred replica)</strong>的概念.所谓的优先副本就是在AR集合列表中的第一个副本为优先副本,理想情况下优先副本就是该分区的leader副本.所以也可以称之为 <code>preferred leader</code> .</p><p>比如上面的例子中,分区0的AR集合(Replicas)是[152,153,154].那么分区0的优先副本就是152.<strong>Kafka会确保所有主题的优先副本均匀分布.这样就保证了所有分区的leader均衡分布.</strong></p><p><strong></strong></p><h4 id="4-2-1-2-优先副本选举"><a href="#4-2-1-2-优先副本选举" class="headerlink" title="4.2.1.2 优先副本选举"></a>4.2.1.2 优先副本选举</h4><p>所谓的优先副本选举是指通过一定的方式促使优先副本选举为Leader副本,促进集群的负载均衡.这一行为也称之为”分区平衡”.</p><p>kafka broker端(server.properties配置文件)有个 <code>auto.leader.rebalance.enble</code> 参数.默认为true.也就是分区自动平衡功能.Kafka会启动一个定时任务,轮询所有的broker节点,自动执行优先副本选举动作.</p><p>不过在生产环境中建议将该配置设置为 <code>false</code> .因为kafka自动平衡分区可能在某些关键高分期时刻引起负面性能问题.也有可能引起客户端的阻塞.为了防止出现此类情况,建议针对副本不均衡的问题进行相应监控和告警,然后在合适的时间通过手动来执行分区平衡.</p><p>Kafka中的 <code>kafka-preferred-replica-election.sh</code> 脚本提供了对分区leader副本进行重新平衡的功能.优先副本选举过程是一个安全的过程,kafka客户端会自动感知leader副本的变更.</p><p>命令用法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181</span><br></pre></td></tr></table></figure><p>但是这样一来会对kafka集群的所有主题和分区都执行一遍优先副本的选举操作.如果集群中包含大量的分区,那么可能选举会失败,并且会对性能造成一定的应用.比较建议的是使用 <code>path-to-json-file</code> 参数来小批量的对部分指定的主题分区进行优先副本的选举操作.该参数指定一个JSON文件,这个JSON文件保存需要执行优先副本选举的分区清单.</p><p>举个例子,对上面的 <code>topic-demo</code> 分区进行优先副本选举操作.先创建一个JSON文件,文件名可以任意:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">  &quot;partitions&quot;: [</span><br><span class="line">    &#123; </span><br><span class="line">        &quot;partition&quot;:0,</span><br><span class="line">        &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;partition&quot;:1,</span><br><span class="line">        &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;   &quot;partition&quot;:2,</span><br><span class="line">            &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;   &quot;partition&quot;:3,</span><br><span class="line">            &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;</span><br><span class="line">   ]</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>将上述内容保存为 <code>election.json</code> 文件.然后执行下列命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-preferred-replica-election.sh --zookeeper localhost:2181 --path-to-json-file ~/election.json</span><br><span class="line"> </span><br><span class="line">Created preferred replica election path with &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:0&#125;,&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:1&#125;,&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:2&#125;,&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:3&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic-demo,0], [topic-demo,1], [topic-demo,2], [topic-demo,3])</span><br></pre></td></tr></table></figure><p>提示优先副本选举成功.下列结果显示leader副本已经均衡分配到所有Broker节点了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 153,154,152</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>在实际生产环境中,建议使用这种方式来分批的执行优先副本选举操作.杜绝直接粗暴的进行所有分区的优先副本选举.另外,这类操作也应该需要避开业务高峰期,以免对性能造成负面影响,或者出现意外故障.</p><h3 id="4-2-2-分区重分配"><a href="#4-2-2-分区重分配" class="headerlink" title="4.2.2 分区重分配"></a>4.2.2 分区重分配</h3><p>当集群中一个Broker节点宕机,该节点的所有副本都处于丢失状态.kafka并不会自动将这些失效的分区副本自动迁移到集群其他broker节点.另外当集群中新增一台Broker节点时,只有新创建的主题分区才能被分配到这个节点上,而之前的主题分区并不会自动的加入到新节点(因为在创建时,并没有这个节点).这就导致新节点负载和原有节点负载之间严重不均衡.</p><p>为了解决这些问题,需要让分区副本再次进行合理的分配.也就是所谓的分区重分配.kafka提供了 <code>kafka-reassign-paritions.sh</code> 脚本执行分区重分配的工作.可以在集群节点失效或者扩容时使用.使用需要3个步骤:</p><ul><li>创建一个包含主题清单的JSON文件</li><li>根据主题清单和Broker节点清单生成一份重分配方案</li><li>执行具体重分配工作</li></ul><blockquote><p>要执行分区重分配,前提是broker节点清单数量要大于或者等于副本因子数量,否则会报错</p><p>Partitions reassignment failed due to replication factor: 3 larger than available brokers: 2</p></blockquote><p>下面创建一个4分区,2个副本因子的主题 <code>topic-reassign</code> 举例.假定要将152这个broker节点下线.下线之前需要将该节点上的分区副本迁移出去.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-reassign --replication-factor 2 --partitions 4</span><br><span class="line">Created topic &quot;topic-reassign&quot;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$  kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,153   Isr: 152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>第一步,创建一个JSON文件(文件名假定为reassign.json).文件内容是主题清单:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">    &quot;topics&quot;:[</span><br><span class="line">      &#123; </span><br><span class="line">                &quot;topic&quot;:&quot;topic-reassign&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;version&quot;:1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第二步,根据这个JSON文件和指定要分配的broker节点列表生成一份候选重分配方案:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --generate --topics-to-move-json-file ~/reassign.json --broker-list 153,154</span><br><span class="line"> </span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[153,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,153]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file project.json</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[153,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,153]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>在上面的例子中有以下几个参数:</p><p><code>--zookeeper</code> 这个参数已经非常熟悉了</p><p><code>--generate</code> 指令类型参数,类似于kafka-topics.sh脚本中的 <code>--create</code> , <code>--list</code> . <code>--describe</code> 等</p><p><code>--topics-to-move-json-file</code> 指定主题清单文件路径</p><p><code>--broker-list</code> 指定要分配的broker节点列表</p><p>上面的例子中打印了2个JSON格式内容:</p><p><code>Current partition replica assignment</code> 表示目前的分区副本分配情况,在执行分区重分配前最好备份这个内容,以便后续回滚操作</p><p><code>Proposed partition reassignment configuration</code> 表示候选重分配方案.这里只是一个方案,并没有真正执行.</p><p>将第二个Json内容格式化输出后,我们发现这个方案正如我们计划的那样,将该主题的所有分区下的AR副本集合分配到153和154节点,所有副本已经从即将要下线的152节点迁移走.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;:1,</span><br><span class="line">    &quot;partitions&quot;:[</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:1,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                153</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:0,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                153,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:3,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                153</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:2,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                153,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第三步,将 <code>Proposed partition reassignment configuration</code> JSON文件内容保存在一个文件中(假定为project.json).然后执行具体的重分配的动作,命令如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file project.json</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[153,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,153]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><blockquote><p>这里仍然打印了之前的副本分配方案,并且提示保存到JSON文件,以便回滚</p></blockquote><p>这里使用了2个不同的命令参数:</p><ul><li><code>--execute</code> 指令类型参数,执行重分配动作</li><li><code>--reassignment-json-file</code> 指定重分配方案文件路径</li></ul><p>再次查看 <code>topic-reassign</code> 主题分区副本分配情况,所有的副本都从152迁移出去,此时该节点可以顺利下线</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 153 Replicas: 154,153   Isr: 153,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>当然,我们也可以直接编写第二个JSON文件来自定义重分配方案,这样就不需要执行上面的第一步和第二步操作了.</p><p>分区重分配的基本原理是为每个分区添加新副本(增加副本数量),新副本会从leader副本复制所有的数据.复制完成后,控制器将旧副本从副本清单里移除.(恢复成原来的副本数量).</p><blockquote><p>所以,分区重分配需要确保有足够的空间,并且避免在业务高峰期操作</p></blockquote><p>从刚才的主题分区结果可以看到,大部分的分区leader副本都集中在153这个broker节点.这样负载非常不均衡,我们可以继续借助 <code>kafka-preferred-replica-election.sh</code> 脚本执行一次优先副本选举.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-preferred-replica-election.sh --zookeeper localhost:2181 --path-to-json-file election.json</span><br><span class="line"></span><br><span class="line">Created preferred replica election path with &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic-reassign,0], [topic-reassign,1], [topic-reassign,2], [topic-reassign,3])</span><br><span class="line"></span><br><span class="line">#选举完成后,副本分配均衡</span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 154 Replicas: 154,153   Isr: 153,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><blockquote><p>和优先副本选举一样,分区重分配对集群的性能有很大的影响.需要占用额外的磁盘,网络IO等资源.在生产环境执行操作时应该分批次执行.</p></blockquote><h3 id="4-2-3-复制限流"><a href="#4-2-3-复制限流" class="headerlink" title="4.2.3 复制限流"></a>4.2.3 复制限流</h3><p>我们了解分区重分配的本质在于数据复制,先增加新副本,进行数据同步,然后删除旧副本.如果副本数据量太大必然会占用很多额外的资源,从而影响集群整体性能.kafka有限流机制,可以对副本之间的复制流量进行限制.</p><p>副本复制限流有2种实现方式:</p><ul><li><code>kafka-config.sh</code> </li><li><code>kafka-reassign-partitions.sh</code> </li></ul><p>前者的实现方式有点繁琐,这里介绍后者的使用方式.</p><p><code>kafka-reassign-partitions.sh</code> 的实现方式非常简单,只需要一个throttle参数即可.例如上面的例子中副本都在153和154节点,现在继续使用分区重分配,让副本从153节点迁移到152节点.但是这次使用限流工具</p><p>首先,修改 <code>project.json</code> 文件,将153替换成152</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &apos;s/153/152/g&apos; project.json</span><br></pre></td></tr></table></figure><p>然后,执行分区重分配,这里使用 <code>--throttle</code> 参数,指定一个限流速度(单位是B/s)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file project.json --throttle 10</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,153]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[154,153]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[153,154]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value.</span><br><span class="line">The throttle limit was set to 10 B/s</span><br><span class="line">Successfully started reassignment of partitions.</span><br></pre></td></tr></table></figure><p>上面的示例输出中提示以下3点信息:</p><ol><li>需要周期性的使用 <code>--verify</code> 参数来周期性的查看副本复制进度,直到分区重分配完成,也就是说需要显示的使用这种方式确保分区重分配完成后解除限流的设置</li><li>限流的速度为10B/s</li><li>如果想要修改限流速度,重复此条执行命令,修改throttle的值即可</li></ol><p>接下来使用 <code>--verify</code> 参数查看复制进度.下面的示例显示复制已经完成,并且限流已被解除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --verify --reassignment-json-file project.json</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition [topic-reassign,1] completed successfully</span><br><span class="line">Reassignment of partition [topic-reassign,0] completed successfully</span><br><span class="line">Reassignment of partition [topic-reassign,3] completed successfully</span><br><span class="line">Reassignment of partition [topic-reassign,2] completed successfully</span><br><span class="line">Throttle was removed.</span><br></pre></td></tr></table></figure><p>此时153的副本已经被移除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 152 Replicas: 152,154   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 154 Replicas: 154,152   Isr: 154,152</span><br></pre></td></tr></table></figure><h3 id="4-2-4-修改副本因子"><a href="#4-2-4-修改副本因子" class="headerlink" title="4.2.4 修改副本因子"></a>4.2.4 修改副本因子</h3><p>上面的例子中分区重分配,将副本从一个broker节点中移除,此时kafka集群的broker节点数量只剩下2个.副本因子也只有2个.这里有个问题,此时153节点重启,或者新增broker节点后,如何将新增的broker节点加入进群,扩展副本数量呢?或者还有一种情况,当创建主题和分区后,想要修改副本因子呢?</p><p><code>kafka-reassign-parition.sh</code> 脚本同样实现了修改副本因子的功能..仔细观察一下分区重分配案例中的 <code>project.json</code> 文件内容:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ cat project.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;:1,</span><br><span class="line">    &quot;partitions&quot;:[</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:1,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                152</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:0,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                152,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:3,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                152</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:2,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                152,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>json文件中的副本集合(replicas)都是2个副本,我们可以很简单的添加一个副本.比如对于分区0而言,可以将153节点添加进去.(其他分区也是如此)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:1,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                152,</span><br><span class="line">                153</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br></pre></td></tr></table></figure><p>执行 <code>kafka-reassign-partition.sh</code> 脚本,执行命令的方法和参数和分区重分片几乎一致:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file add.json</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[152,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,154]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>查看副本分配情况.副本数量已经增加到了3个</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,154,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 154 Replicas: 153,154,152   Isr: 154,152,153</span><br></pre></td></tr></table></figure><blockquote><p>虽然副本因子增加到3个,但是Leader还是没有分配到新的153这个broker节点.此时可以通过优先副本选举重新分配</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-preferred-replica-election.sh --zookeeper localhost:2181 --path-to-json-file election.json</span><br><span class="line">Created preferred replica election path with &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic-reassign,0], [topic-reassign,1], [topic-reassign,2], [topic-reassign,3])</span><br><span class="line"></span><br><span class="line">#再次查看topic-reassign主题,分区副本分配均衡</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,154,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 153 Replicas: 153,154,152   Isr: 154,152,153</span><br></pre></td></tr></table></figure><p>重点: <strong>与修改分区数量不同,副本数还可以减少</strong>,修改方法和命令几乎一样,只需要编辑JSON配置文件即可.这里就不再演示</p><h3 id="4-2-5-如何选择合适的分区数量"><a href="#4-2-5-如何选择合适的分区数量" class="headerlink" title="4.2.5 如何选择合适的分区数量"></a>4.2.5 如何选择合适的分区数量</h3><p>如何选择合适的分区数量是需要经常面对的问题,但是这个问题似乎并没有权威的标准答案,需要根据实际的业务场景,硬件资源,应用软件,负载等情况做具体考量.这一章节主要介绍与本问题相关的一些决策因素,以供参考</p><h4 id="4-2-5-1-性能测试工具"><a href="#4-2-5-1-性能测试工具" class="headerlink" title="4.2.5.1 性能测试工具"></a>4.2.5.1 性能测试工具</h4><p>在生产环境中设定分区数量需要考虑性能因素.所以性能测试工具必不可少,kafka本身提供了用于生产者性能测试的 <code>kafka-producer-pref-test.sh</code> 脚本和用于消费者性能测试的 <code>kafka-consumer-perf-test.sh</code>脚本</p><p>首先创建一个用于测试的分区为1,副本为1的 <code>topic-1</code> 的主题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Topic:topic-1   PartitionCount:1    ReplicationFactor:1 Configs:</span><br><span class="line">    Topic: topic-1  Partition: 0    Leader: 153 Replicas: 153   Isr: 153</span><br></pre></td></tr></table></figure><p>其次.我们往这个主题发送100万条消息,并且每条消息大小为1024B,生产者对应的acks参数为1:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-producer-perf-test.sh --topic topic-1 --num-records 1000000 --record-size 1024 --throughput -1 --producer-props bootstrap.servers=localhost:9092 acks=1</span><br><span class="line">76666 records sent, 15333.2 records/sec (14.97 MB/sec), 1517.5 ms avg latency, 2061.0 max latency.</span><br><span class="line">119400 records sent, 23880.0 records/sec (23.32 MB/sec), 1353.6 ms avg latency, 1631.0 max latency.</span><br><span class="line">124560 records sent, 24912.0 records/sec (24.33 MB/sec), 1231.2 ms avg latency, 1375.0 max latency.</span><br><span class="line">146520 records sent, 29304.0 records/sec (28.62 MB/sec), 1066.6 ms avg latency, 1146.0 max latency.</span><br><span class="line">156795 records sent, 31359.0 records/sec (30.62 MB/sec), 972.3 ms avg latency, 1051.0 max latency.</span><br><span class="line">133365 records sent, 26673.0 records/sec (26.05 MB/sec), 1141.1 ms avg latency, 1322.0 max latency.</span><br><span class="line">159945 records sent, 31989.0 records/sec (31.24 MB/sec), 964.3 ms avg latency, 1178.0 max latency.</span><br><span class="line">1000000 records sent, 26148.576210 records/sec (25.54 MB/sec), 1143.54 ms avg latency, 2061.00 ms max latency, 1114 ms 50th, 1654 ms 95th, 1869 ms 99th, 2036 ms 99.9th.</span><br></pre></td></tr></table></figure><p>示例中使用了多个参数:</p><p><code>num-records</code>:  指定发送消息的总条数</p><p><code>record-size</code>: 设置每条消息的字节数</p><p><code>throughtput</code> : 限流控制,-1表示不限流,大于0表示限流值</p><p><code>producer-props</code> : 指定生产者的配置,可以同时指定多组配置</p><p>除此之外还有其他参数,比如 <code>print-metrics</code> 在测试完成之后,打印很多指标信息.有兴趣可以执行 <code>--help</code> 查看更多参数信息.</p><p>回过头再看看上面示例中的压测结果信息,以第一条和最后一条为例:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">76666 records sent, 15333.2 records/sec (14.97 MB/sec), 1517.5 ms avg latency, 2061.0 max latency.</span><br><span class="line">1114 ms 50th, 1654 ms 95th, 1869 ms 99th, 2036 ms 99.9th</span><br></pre></td></tr></table></figure><p><strong>records sent: 表示发送的消息综述</strong></p><p><strong>records/sec: 吞吐量,表示每秒发送的消息数量</strong></p><p><strong>MB/sec: 吞吐量,表示每秒发送的消息大小</strong></p><p><strong>avg latency: 表示消息处理的平均耗时</strong></p><p><strong>max latency: 表示消息处理的最大耗时</strong></p><p><strong>50th,95th,99th,99.th</strong> 表示50%,95%,99%,99.9%时消息处理耗时</p><p><strong></strong></p><p>消费者压测工具的脚本使用也比较简单,下面的简单实例演示了消费主题 <code>topic-1</code> 中的100万条消息.命令使用方法:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-consumer-perf-test.sh --topic topic-1 --messages 1000000 --broker-list localhost:9092</span><br><span class="line">start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec</span><br><span class="line">2020-12-26 14:07:29:612, 2020-12-26 14:07:35:272, 977.0264, 172.6195, 1000475, 176762.3675</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p><strong>data.consumed.in.MB: 消费的消息总量,单位为MB</strong></p><p><strong>MB.sec</strong>: 按字节大小计算的消费吞吐量(单位:MB/s)</p><p><strong>data.consumed.in.nMsg</strong>: 消费的消息消息总数</p><p><strong>nMsg.sec</strong>: 按消息个数计算的吞独量(单位n/s)</p><blockquote><p>可以创建多个分区,比如10,100,200,500等(副本数量都为1)来测试生产和消费的性能表现,</p></blockquote><h4 id="4-2-5-2-分区数量越多不代表吞独量越高"><a href="#4-2-5-2-分区数量越多不代表吞独量越高" class="headerlink" title="4.2.5.2 分区数量越多不代表吞独量越高"></a>4.2.5.2 分区数量越多不代表吞独量越高</h4><p>消息中间件的性能一般是指吞吐量(还包括延迟),吞吐量会受到硬件资源,消息大小,消息压缩,消息发送方式(同步,异步),副本因子等参数影响.分区数量越多不一定吞吐量越高,超过一定的临界值后,kafka的吞吐量会不升反降.</p><h4 id="4-2-5-3-分区数量的上限"><a href="#4-2-5-3-分区数量的上限" class="headerlink" title="4.2.5.3 分区数量的上限"></a>4.2.5.3 分区数量的上限</h4><p>一味的增加分区数量并不能使吞吐量得到提升,并且分区的数量也不能一直增加,如果超过一定的临界值还会引起kafka进程的崩溃.</p><p><strong>每次创建一个分区,都会消耗一个Linux系统的文件描述符.</strong></p><p>通过kafka的pid编号,可以查看当前kafka进程占用的文件描述符数量:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ ls /proc/22858/fd/ | wc -l</span><br><span class="line">173</span><br></pre></td></tr></table></figure><p>此时创建一个分区数量为400个的topic-demo4的主题.由于分区会平均创建在集群内的3个broker节点,所以需要统计一下152这个本地节点的分区数量.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics --describe --topic topic-demo4 | grep -Eo &quot;Leader:\s[0-9]+&quot; | sort | uniq -c</span><br><span class="line">    134 Leader: 152</span><br><span class="line">    133 Leader: 153</span><br><span class="line">    133 Leader: 154</span><br></pre></td></tr></table></figure><p>可以看到152这个节点创建了134个分区.接下来看看系统文件描述符的数量.正好增加了134个文件描述符</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ ls /proc/22858/fd/ | wc -l</span><br><span class="line">307</span><br></pre></td></tr></table></figure><p>可以想见的是,一旦分区数量超过了操作系统规定的文件描述符上限,kafka进程就会崩溃</p><h4 id="4-2-5-4-分区数量的考量"><a href="#4-2-5-4-分区数量的考量" class="headerlink" title="4.2.5.4 分区数量的考量"></a>4.2.5.4 分区数量的考量</h4><p>如何选择合适的分区数量,一个恰当的答案就是视具体情况而定.</p><p>从吞吐量方面考虑,增加合适的分区数量可以在一定程度上提升整体吞吐量,但是超过临界值之后吞吐量不升反降.在投入生产环境之前,应该对吞吐量进行相关的测试,以找到合适的分区数量</p><p>分区数量太多会影响系统可用性,当broker发生故障时,broker节点上的所有分区的leader副本不可用,此时如果有大量的分区要进行leader角色切换,这个切换的过程会耗费相当的时间,并且这个时间段内分区会变的不可用.并且分区数量太多不仅为增加日志清理的耗时,而且在被删除时也会消费更多时间.</p><p>一个好的建议是,创建主题之前对分区数量性能进行充分压测,在创建主题之后,还需要对其进行追踪,监控,调优.如果分区数量较少,还能通过增加分区数量,或者增加broker进行分区重分配等改进.</p><p>最后,一个通用的准则是,建议分区数量设定为集群中broker的倍数,例如集群中有3个broker节点,可以设定分区数为3,6,9等.</p><h3 id="4-2-6-总结"><a href="#4-2-6-总结" class="headerlink" title="4.2.6 总结"></a>4.2.6 总结</h3><p><code>kafka-topics.sh</code> 查看,创建主题分区,副本</p><p><code>kafka-configs.sh</code> 修改主题配置文件</p><p><code>kafka_perferred-replica-elections.sh</code> 优先副本选举</p><p><code>kafka-reassign-partitions.sh</code> 分区重分配,副本复制限流,修改副本因子数量</p><p><code>kafka-producer-perf-test.sh</code> 生产者分区数和吞吐量性能压测</p><p><code>kafka-consumer-perf-test.sh</code> 消费者性能压测</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;4-2-1-优选副本的选举&quot;&gt;&lt;a href=&quot;#4-2-1-优选副本的选举&quot; class=&quot;headerlink&quot; title=&quot;4.2.1 优选副本的选举&quot;&gt;&lt;/a&gt;4.2.1 优选副本的选举&lt;/h3&gt;&lt;h4 id=&quot;4-2-1-1-什么是优先副本&quot;&gt;&lt;a href=&quot;#4-2-1-1-什么是优先副本&quot; class=&quot;headerlink&quot; title=&quot;4.2.1.1 什么是优先副本&quot;&gt;&lt;/a&gt;4.2.1.1 什么是优先副本&lt;/h4&gt;&lt;p&gt;分区使用多副本机制来提升可靠性,但是只有leader副本对外提供读写服务.而follower副本只负责在内部进行消息的同步.如果一个分区的leader副本不可用,那么就意味着整个分区变得不可用.此时就需要从剩余的follower副本中挑选一个新的leader副本继续对外提供服务.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;broker节点中的Leader副本个数决定了这个节点负载的高低&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在创建主题的时候,主题的分区和副本会尽可能的均匀分布在kafka集群的各个broker节点.对应的Leader副本的分配也比较均匀.例如下面的 &lt;code&gt;topic-demo&lt;/code&gt; 主题:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev152 ~]$&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="4-主题和分区" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-1.1基本概念介绍</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/1.1%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/1-概念介绍/1.1 基本概念介绍/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:48:01.269Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-1-基本概念介绍"><a href="#1-1-基本概念介绍" class="headerlink" title="1.1 基本概念介绍"></a>1.1 基本概念介绍</h1><h3 id="kafka特性"><a href="#kafka特性" class="headerlink" title="kafka特性"></a>kafka特性</h3><ul><li><strong>消息系统</strong>: Kafka和传统的消息中间件都具备流量削峰,缓冲,异步通信,扩展性等.另外,Kafka还提供了大多数消息中间件难以实现的消息顺序保障及回溯消费的功能</li><li><strong>存储系统</strong>: 消息持久化到存盘,可以实现永久存储</li><li><strong>流式处理平台</strong>: Kafka提供了流式处理类库</li></ul><a id="more"></a><h3 id="Kafka架构"><a href="#Kafka架构" class="headerlink" title="Kafka架构"></a>Kafka架构</h3><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608447536390-cba7d090-f67a-435c-8d7d-704fb446e573.png" alt="image.png"></p><p>一个Kafka体系主要包括:</p><ul><li>producer: 生产者</li><li>broker: kafka节点服务器</li><li>consumer: 消费者</li><li>zookeeper: 负责管理kafka集群元数据,集群选举等</li></ul><p>producer将消息发送到Broker,Broker负责将受到的消息存储到磁盘中,Consumer负责从Broker订阅并消费消息.</p><h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><ul><li>Kafka 通过 <em>topic</em> 对存储的流数据进行分类。</li><li>每条记录中包含一个key，一个value和一个timestamp（时间戳).</li><li>kafka保留所有的发布记录(无论是否已经被消费过).通过一个可配置的参数—保留期限来控制记录存在时间.</li></ul><blockquote><p>举个例子， 如果保留策略设置为2天，一条记录发布后两天内，可以随时被消费，两天过后这条记录会被抛弃并释放磁盘空间。</p></blockquote><h3 id="kafka核心API"><a href="#kafka核心API" class="headerlink" title="kafka核心API"></a>kafka核心API</h3><ul><li><a href="https://kafka.apachecn.org/documentation.html#producerapi" target="_blank" rel="noopener">Producer API</a> : 允许一个应用程序发布一串流式的数据到一个或者多个Kafka topic。</li><li><a href="https://kafka.apachecn.org/documentation.html#consumerapi" target="_blank" rel="noopener">Consumer API</a>: 允许一个应用程序订阅一个或多个 topic ，并且对发布给他们的流式数据进行处理。</li><li><a href="https://kafka.apachecn.org/documentation/streams" target="_blank" rel="noopener">Streams API</a>: 允许一个应用程序作为一个<em>流处理器</em>，消费一个或者多个topic产生的输入流，然后生产一个输出流到一个或多个topic中去，在输入输出流中进行有效的转换。</li><li><a href="https://kafka.apachecn.org/documentation.html#connect" target="_blank" rel="noopener">Connector API</a>: 允许构建并运行可重用的生产者或者消费者，将Kafka topics连接到已存在的应用程序或者数据系统。比如，连接到一个关系型数据库，捕捉表（table）的所有变更内容。</li></ul><h3 id="理解topics和Partition和offset"><a href="#理解topics和Partition和offset" class="headerlink" title="理解topics和Partition和offset"></a>理解topics和Partition和offset</h3><p><strong>Topic</strong>: 就是数据主题，生产者将消息发送到特点的主题.消费者负责订阅主题并进行消费.</p><p><strong>Partition</strong>: 一个Topic可以划分成多个partition(分区).但是一个分区只属于单个主题.很多时候也会将partition称为主题分区(Topic-Partition).同一个主题下的不同分区包含的消息也不同.分区在存储层面可以看做一个追加的日志(Log)文件.</p><p>一个主题的分区可以在不同的节点服务器上,所有的消息会均匀的分配到不同的分区中(也就是不同的节点服务器),这样可以提高磁盘IO和性能.在创建主题的时候可以设置分区数量,当然也可以在主题创建完成后去修改分区数量.通过增加分区的数量实现水平扩展.</p><p>好比是为公路运输，不同的起始点和目的地需要修不同高速公路（主题），高速公路上可以提供多条车道（分区），流量大的公路多修几条车道保证畅通，流量小的公路少修几条车道避免浪费。收费站好比消费者，车多的时候多开几个一起收费避免堵在路上，车少的时候开几个让汽车并道就好了</p><p>Kafka中的Topics总是多订阅者模式，一个topic可以拥有一个或者多个消费者来订阅它的数据。对于每一个topic， Kafka集群都会维持一个分区日志，如下所示：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608119181003-5ca1cc98-c0ec-4c00-b7b0-d0b916204ab0.png" alt="image.png"></p><p>每个partition分区都是有序切不可变的记录集.并且不断的追加到结构化的commit log文件.</p><p><strong>Offset</strong>: 消息被存储到分区的日志文件时会分片一个偏移量(offset).offset是消息在分区中的唯一表示.kafka通过它来保障消息在分区内的顺序.</p><p>不过Offset并不跨越分区,也就是说Kafka保证的是分区有序,而不是主题有序.</p><p>在每一个消费者中唯一保存的元数据是offset（偏移量）即消费在log中的位置.偏移量由消费者所控制:通常在读取记录后，消费者会以线性的方式增加偏移量，但是实际上，由于这个位置由消费者控制，所以消费者可以采用任何顺序来消费记录。例如，一个消费者可以重置到一个旧的偏移量，从而重新处理过去的数据；也可以跳过最近的记录，从”现在”开始消费。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608120423518-7cddb09d-e7fa-4f35-809f-908a36b5a4d1.png?x-oss-process=image%2Fresize%2Cw_1500" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-1-基本概念介绍&quot;&gt;&lt;a href=&quot;#1-1-基本概念介绍&quot; class=&quot;headerlink&quot; title=&quot;1.1 基本概念介绍&quot;&gt;&lt;/a&gt;1.1 基本概念介绍&lt;/h1&gt;&lt;h3 id=&quot;kafka特性&quot;&gt;&lt;a href=&quot;#kafka特性&quot; class=&quot;headerlink&quot; title=&quot;kafka特性&quot;&gt;&lt;/a&gt;kafka特性&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;消息系统&lt;/strong&gt;: Kafka和传统的消息中间件都具备流量削峰,缓冲,异步通信,扩展性等.另外,Kafka还提供了大多数消息中间件难以实现的消息顺序保障及回溯消费的功能&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;存储系统&lt;/strong&gt;: 消息持久化到存盘,可以实现永久存储&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;流式处理平台&lt;/strong&gt;: Kafka提供了流式处理类库&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="1-概念介绍" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-2.1Kafka副本</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/2-%E5%89%AF%E6%9C%AC%E4%BB%8B%E7%BB%8D/2.1%20%20Kafka%E5%89%AF%E6%9C%AC/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/2-副本介绍/2.1  Kafka副本/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T15:14:57.936Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2-1-Kafka副本"><a href="#2-1-Kafka副本" class="headerlink" title="2.1  Kafka副本"></a>2.1  Kafka副本</h1><h3 id="副本介绍"><a href="#副本介绍" class="headerlink" title="副本介绍"></a>副本介绍</h3><p>Kafka为分区引入了副本(Replica)机制.通过增加副本数量提升容灾能力.一个Topic主题可以有多个分区,一个分区又可以有多个副本.这多个副本中，只有一个是leader，而其他的都是follower副本。仅有leader副本可以对外提供服务。所以副本之间是一主多从的关系,而且每个副本中保存的相同的消息.(严格来说,同一时刻副本之间的消息并非能一定完全同步)</p><p>多个follower副本通常存放在和leader副本不同的broker中。通过这样的机制实现了高可用，当某台机器挂掉后，其他follower副本也能迅速”转正“，开始对外提供服务。</p><a id="more"></a> <p>在kafka中，实现副本的目的就是冗余备份，且仅仅是冗余备份，所有的读写请求都是由leader副本进行处理的。follower副本仅有一个功能，那就是从leader副本拉取消息，尽量让自己跟leader副本的内容一致。</p><blockquote><p>follower副本之所以不能对外提供服务,主要是为了保障数据一致性</p></blockquote><p>下图是一个多副本架构图.</p><p>Kafka集群中有4个broker，某个主题中有3个分区，且副本因子（即副本个数）也为3，如此每个分区便有1个leader副本和2个follower副本。生产者和消费者只与leader副本进行交互，而follower副本只负责消息的同步，很多时候follower副本中的消息相对leader副本而言会有一定的滞后。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608450120303-2d66cd2e-611a-4e3e-a507-53b4f81adfa0.png" alt="image.png"></p><h3 id="副本同步"><a href="#副本同步" class="headerlink" title="副本同步"></a>副本同步</h3><p><strong>AR</strong>: 分区内的所有副本统称.</p><p><strong>ISR</strong>: In-Sync Replicas.所有与Leader副本保持一定程度同步的副本(包括Leader副本).一起组成ISR</p><p><strong>OSR</strong>: Out-of-Sync Replicas: 与leader副本同步滞后过多的副本(不包括leader副本),一起注册呢个OSR</p><p><strong>AR = ISR + OSR.</strong></p><blockquote><p>正常情况下,所有的follower副本都应该与leader副本保持一定程度的同步,即AR = ISR,OSR集合为空</p></blockquote><p>Leader副本负责维护和跟踪ISR集合中所有follower副本的滞后状态,当follower副本落后太多或者失效时,leader副本会把它从ISR集合中剔除,如果OSR的follower副本追上了leader副本,那会从OSR转移到ISR.</p><blockquote><p>默认情况下,只有ISR集合中的follower副本才有资格被选举为新的Leader</p></blockquote><h3 id="HW和LEO"><a href="#HW和LEO" class="headerlink" title="HW和LEO"></a>HW和LEO</h3><p><strong>HW(High Watermark)</strong>: 俗称高水位.它标识了一个特点的消息偏移量(offset).消费者只能拉取这个offset之前的信息.</p><p><strong>LEO(Log End Offset)</strong>: 标识当前日志文件中下一条代写入消息的offset. </p><p><strong></strong></p><p>下面一张图能说明这两个概念</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608450932841-d4f33228-91b2-484f-b28e-24d0762ef670.png" alt="image.png"></p><p>上面的图代表一个日志文件.这个日志文件中有 9 条消息，第一条消息的 offset（LogStartOffset）为0，最后一条消息的offset为8，offset为9的消息用虚线框表示，代表下一条待写入的消息。日志文件的HW为6，表示消费者只能拉取到offset在0至5之间的消息，而offset为6的消息对消费者而言是不可见的。</p><p>offset为9的位置即为当前日志文件的LEO，LEO的大小相当于当前日志分区中最后一条消息的offset值加1。分区ISR集合中的每个副本都会维护自身的LEO，<strong>而ISR集合中最小的LEO即为分区的HW，对消费者而言只能消费HW之前的消息。</strong></p><h3 id="ISR和HW-LEO的关系"><a href="#ISR和HW-LEO的关系" class="headerlink" title="ISR和HW,LEO的关系"></a>ISR和HW,LEO的关系</h3><p>为了让读者更好地理解ISR集合，以及HW和LEO之间的关系，下面通过一个简单的示例来进行相关的说明。如图1-5所示，假设某个分区的ISR集合中有3个副本，即一个leader副本和2个follower副本，此时分区的LEO和HW都为3。消息3和消息4从生产者发出之后会被先存入leader副本</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451481234-2309dd97-7662-49d0-a43a-42b822f7a7d4.png" alt="image.png"></p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451501313-69357f66-f23b-4d07-940f-ef9f0afb9260.png" alt="image.png"></p><p>在同步过程中，不同的 follower 副本的同步效率也不尽相同。如图 所示，在某一时刻follower1完全跟上了leader副本而follower2只同步了消息3，如此leader副本的LEO为5，follower1的LEO为5，follower2的LEO为4，那么当前分区的HW取最小值4，此时消费者可以消费到offset为0至3之间的消息。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451550216-2bc62531-a164-40a1-b039-e9e911401d02.png" alt="image.png"></p><p>如果所有的副本都成功写入了消息3和消息4，整个分区的HW和LEO都变为5，因此消费者可以消费到offset为4的消息了。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451625357-266f1693-1d4b-4524-9f64-89256893555e.png" alt="image.png"></p><p>Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的 follower 副本都复制完，这条消息才会被确认为已成功提交，这种复制方式极大地影响了性能。而在异步复制方式下，follower副本异步地从leader副本中复制数据，数据只要被leader副本写入就被认为已经成功提交。在这种情况下，如果follower副本都还没有复制完而落后于leader副本，突然leader副本宕机，则会造成数据丢失。Kafka使用的这种ISR的方式则有效地权衡了数据可靠性和性能之间的关系。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;2-1-Kafka副本&quot;&gt;&lt;a href=&quot;#2-1-Kafka副本&quot; class=&quot;headerlink&quot; title=&quot;2.1  Kafka副本&quot;&gt;&lt;/a&gt;2.1  Kafka副本&lt;/h1&gt;&lt;h3 id=&quot;副本介绍&quot;&gt;&lt;a href=&quot;#副本介绍&quot; class=&quot;headerlink&quot; title=&quot;副本介绍&quot;&gt;&lt;/a&gt;副本介绍&lt;/h3&gt;&lt;p&gt;Kafka为分区引入了副本(Replica)机制.通过增加副本数量提升容灾能力.一个Topic主题可以有多个分区,一个分区又可以有多个副本.这多个副本中，只有一个是leader，而其他的都是follower副本。仅有leader副本可以对外提供服务。所以副本之间是一主多从的关系,而且每个副本中保存的相同的消息.(严格来说,同一时刻副本之间的消息并非能一定完全同步)&lt;/p&gt;
&lt;p&gt;多个follower副本通常存放在和leader副本不同的broker中。通过这样的机制实现了高可用，当某台机器挂掉后，其他follower副本也能迅速”转正“，开始对外提供服务。&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="2-副本介绍" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/2-%E5%89%AF%E6%9C%AC%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-1.2 生产与消费简单实例</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/1.2%20%E7%94%9F%E4%BA%A7%E4%B8%8E%E6%B6%88%E8%B4%B9%E7%AE%80%E5%8D%95%E5%AE%9E%E4%BE%8B/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/1-概念介绍/1.2 生产与消费简单实例/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:48:34.104Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-2-生产与消费简单实例"><a href="#1-2-生产与消费简单实例" class="headerlink" title="1.2 生产与消费简单实例"></a>1.2 生产与消费简单实例</h1><h3 id="创建topic"><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h3><p>kafka提供了许多实用的脚本工具,存放在$KAFKA_HOME的bin目录下.其中与主题相关的就是kafka-topic.sh脚本.例如.下面创建一个分区数为4,副本为3的主题topic-demon<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-demo --replication-factor 3 --partitions 4</span><br><span class="line"></span><br><span class="line">Created topic <span class="string">"topic-demo"</span>.</span><br></pre></td></tr></table></figure></p><p><code>--zoopkeer</code> 指定kafka连接的zookeeper服务地址<br><code>--topic</code> 指定一个topic主题<br><code>--replication-factor</code>  指定副本因子数量<br><code>--partition</code> 指定分区数量<br><code>--create</code> 表示创建</p><a id="more"></a> <p>下面命令展示了刚创建的主题信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 bin]$ ./kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demoPartitionCount:4ReplicationFactor:3Configs:</span><br><span class="line">Topic: topic-demoPartition: 0Leader: 152Replicas: 152,153,154Isr: 152,153,154</span><br><span class="line">Topic: topic-demoPartition: 1Leader: 153Replicas: 153,154,152Isr: 153,154,152</span><br><span class="line">Topic: topic-demoPartition: 2Leader: 154Replicas: 154,152,153Isr: 154,152,153</span><br><span class="line">Topic: topic-demoPartition: 3Leader: 152Replicas: 152,154,153Isr: 152,154,153</span><br></pre></td></tr></table></figure></p><p>上面的命令结果表示 <code>topic-demon</code> 这个主题一共有4个分区,存放在3台Kafka broker服务器节点.3个broker均是ISR集合,没有OSR集合</p><blockquote><p>在任意一台kafka集群内的节点服务器上执行上述命令,会得到完全相同的结果</p></blockquote><h3 id="创建consumer"><a href="#创建consumer" class="headerlink" title="创建consumer"></a>创建consumer</h3><p><code>kafka-console-consumer.sh</code> 在任意一台kafka集群内的节点服务器上可以通过控制台创建一个 <code>consumer</code> 消费者.示例如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev154 bin]$ ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-demo</span><br></pre></td></tr></table></figure></p><p><code>--bootstrap-server</code> 指定连接的kafka集群地址<br><code>--topic</code> 指定消费者订阅的主题</p><h3 id="创建producer"><a href="#创建producer" class="headerlink" title="创建producer"></a>创建producer</h3><p><code>kafka-console-producer.sh</code> 在任意一台kafka集群内的节点服务器上可以通过控制台创建一个 <code>producer</code> 消费者.示例如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev153 bin]$ ./kafka-console-producer.sh --broker-list localhost:9092 --topic topic-demo</span><br></pre></td></tr></table></figure></p><p><code>--broker-list</code> 指定连接的kafka集群地址<br><code>--topic</code> 指定发小时时的主题<br>在弹出的shell终端中,输入 <code>hello world!</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;hello,world!</span><br></pre></td></tr></table></figure></p><p>回到 <code>consumer</code> 的shell终端界面,发现消费到了刚生产的消息:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev154 bin]$ ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-demo</span><br><span class="line">hello,world!</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-2-生产与消费简单实例&quot;&gt;&lt;a href=&quot;#1-2-生产与消费简单实例&quot; class=&quot;headerlink&quot; title=&quot;1.2 生产与消费简单实例&quot;&gt;&lt;/a&gt;1.2 生产与消费简单实例&lt;/h1&gt;&lt;h3 id=&quot;创建topic&quot;&gt;&lt;a href=&quot;#创建topic&quot; class=&quot;headerlink&quot; title=&quot;创建topic&quot;&gt;&lt;/a&gt;创建topic&lt;/h3&gt;&lt;p&gt;kafka提供了许多实用的脚本工具,存放在$KAFKA_HOME的bin目录下.其中与主题相关的就是kafka-topic.sh脚本.例如.下面创建一个分区数为4,副本为3的主题topic-demon&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;./kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-demo --replication-factor 3 --partitions 4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Created topic &lt;span class=&quot;string&quot;&gt;&quot;topic-demo&quot;&lt;/span&gt;.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;--zoopkeer&lt;/code&gt; 指定kafka连接的zookeeper服务地址&lt;br&gt;&lt;code&gt;--topic&lt;/code&gt; 指定一个topic主题&lt;br&gt;&lt;code&gt;--replication-factor&lt;/code&gt;  指定副本因子数量&lt;br&gt;&lt;code&gt;--partition&lt;/code&gt; 指定分区数量&lt;br&gt;&lt;code&gt;--create&lt;/code&gt; 表示创建&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="1-概念介绍" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-3.1消费者与消费组</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/3-%E6%B6%88%E8%B4%B9%E8%80%85/3.1%20%E6%B6%88%E8%B4%B9%E8%80%85%E4%B8%8E%E6%B6%88%E8%B4%B9%E7%BB%84/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/3-消费者/3.1 消费者与消费组/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:49:55.160Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-1-消费者与消费组"><a href="#3-1-消费者与消费组" class="headerlink" title="3.1 消费者与消费组"></a>3.1 消费者与消费组</h2><h3 id="1-消费者和消费组介绍"><a href="#1-消费者和消费组介绍" class="headerlink" title="1.消费者和消费组介绍"></a>1.消费者和消费组介绍</h3><p>消费者( Consumer)负责订阅Kafka中的主题( Topic)，并且从订阅的主题上拉取消息.与其他一些消息中间件不同的是:在 Kafka的消费理念中还有一层消费组( Consumer Group)的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者 。</p><p>以下图为例,某个主题中共有 4 个分区( Partition) : PO、 Pl、 P2、 P3。 有两个消费组 A和 B 都订阅了这个主题，消费组 A 中有 4 个消费者 (CO、 Cl、 C2 和 C3)，消费组 B 中有 2个消费者 CC4 和 CS) 。按照 Kafka默认的规则，最后的分配结果是消费组 A 中的每一个消费 者分配到1个分区，消费组 B 中的每一个消费者分配到 2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。换言之 每一个分区只能被一个消费组中的一个消费者所消费.</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608547700645-c525d96a-ac02-471f-9276-ee885e471c86.png" alt="image.png"></p><a id="more"></a> <p>假设目前某消费组内只有一个消费者 co，订阅了一个主题，这个主题包含 7 个分区: PO、 Pl、 P2、 P3、 P4、PS、 P6o 也就是说，这个消费者co订阅了7个分区，具体分配情形参考图3-2。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608547782387-e3cd0cab-f862-465d-bfd7-96c7fa613b21.png" alt="image.png"></p><p>​                                        </p><p>此时消费组内又加入了一个新的消费者 Cl，按照既定的逻辑，需要将原来消费者 co 的部分分区分配给消费者 Cl 消费 ， 如图 3-3 所示 。 消费者 co 和 Cl 各自负责消费所分配到的分区 ，彼此之间并无逻辑上的干扰 </p><p>消费者与消费组这种模型可以让整体的消费能力具备横向伸缩性，我们 可以增加(或减少) 消费者的个数来提高 (或降低〕整体的消费能力 。 对于分区数固定的情况， 一昧地增加消费者并不会让消费能力 一直得到提升，<strong>如果消费者过多，出现了消费者的个数大于分区个数的情况，**</strong>就会有消费者分配不到任何分区**。</p><h3 id="2-两种消息投递模式"><a href="#2-两种消息投递模式" class="headerlink" title="2.两种消息投递模式"></a>2.两种消息投递模式</h3><p>对于消息中间件而言,一般有两种消息投递模式:<strong>点对点</strong>(P2P, Point-to-Point)模式和<strong>发**</strong>布/订阅**( Pub/Sub)模式.</p><p><strong>点对点模式</strong>是基于队列的，消息生产者发送消息到队列，消息消费者从队列中接收消息。</p><p><strong>发布订阅模式</strong>定义了如何向一个内容节点发布和订阅消息,这个内容节点称为主题(Topic),主题可以认为是消息传递的中介,消息发布者将消息发布到某个主题,而消息订阅者从主题中订阅消息.主题使得消息的订阅者和发布者互相保持独立,不需要进行接触即可保证消息的传递,发布/订阅模式在消息的一对多广播时采用.Kafka同时支持两种消息投递模式，而这正是得益于消费者与消费组模型的契合:</p><ul><li>如果所有的消费者都隶属于同一个消费组,那么所有的消息都会被均衡地投递给每一个消费者,即每条消息只会被一个消费者处理,这就相当于点对点模式的应用 。</li><li>如果所有的消费者都隶属于不同的消费组,那么所有的消息都会被广播给所有的消费者,即每条消息会被所有的消费者处理,这就相当于发布/订阅模式的应用.</li></ul><p>消费组是一个逻辑上的概念，它将旗下的消费者归为一类 ，每一个消费者只隶属于一个消费组。每一个消费组都会有一个固定的名称，消费者在进行消费前需要指定其所属消费组的名称，这个可以通过消费者客户端参数 group.id来配置，默认值为空宇符串。</p><p>消费者并非逻辑上的概念它是实际的应用实例它可以是一个线程，也可以是一个进程。同一个消费组内的消费者既可以部署在同一台机器上，也可以部署在不同的机器上。</p><p>​                                        </p><p>​                                        </p><p>​                                                                                </p><p>​                                        </p><p>​                                        </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-1-消费者与消费组&quot;&gt;&lt;a href=&quot;#3-1-消费者与消费组&quot; class=&quot;headerlink&quot; title=&quot;3.1 消费者与消费组&quot;&gt;&lt;/a&gt;3.1 消费者与消费组&lt;/h2&gt;&lt;h3 id=&quot;1-消费者和消费组介绍&quot;&gt;&lt;a href=&quot;#1-消费者和消费组介绍&quot; class=&quot;headerlink&quot; title=&quot;1.消费者和消费组介绍&quot;&gt;&lt;/a&gt;1.消费者和消费组介绍&lt;/h3&gt;&lt;p&gt;消费者( Consumer)负责订阅Kafka中的主题( Topic)，并且从订阅的主题上拉取消息.与其他一些消息中间件不同的是:在 Kafka的消费理念中还有一层消费组( Consumer Group)的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者 。&lt;/p&gt;
&lt;p&gt;以下图为例,某个主题中共有 4 个分区( Partition) : PO、 Pl、 P2、 P3。 有两个消费组 A和 B 都订阅了这个主题，消费组 A 中有 4 个消费者 (CO、 Cl、 C2 和 C3)，消费组 B 中有 2个消费者 CC4 和 CS) 。按照 Kafka默认的规则，最后的分配结果是消费组 A 中的每一个消费 者分配到1个分区，消费组 B 中的每一个消费者分配到 2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。换言之 每一个分区只能被一个消费组中的一个消费者所消费.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.nlark.com/yuque/0/2020/png/2992889/1608547700645-c525d96a-ac02-471f-9276-ee885e471c86.png&quot; alt=&quot;image.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="3-消费者" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/3-%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-4.1主题管理</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/4.1%E4%B8%BB%E9%A2%98%E7%AE%A1%E7%90%86/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/4-主题和分区/4.1主题管理/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:55:25.320Z</updated>
    
    <content type="html"><![CDATA[<h3 id="4-1-1-创建主题"><a href="#4-1-1-创建主题" class="headerlink" title="4.1.1 创建主题"></a>4.1.1 创建主题</h3><h4 id="4-1-1-1-自动创建主题以及分区副本"><a href="#4-1-1-1-自动创建主题以及分区副本" class="headerlink" title="4.1.1.1 自动创建主题以及分区副本"></a>4.1.1.1 自动创建主题以及分区副本</h4><p>在之前的笔记中提到了创建主题的一个简单示例.kafka提供 <code>kafka-topics.sh</code> 脚本来创建主题.下面这个示例创建了一个 <code>topic-test</code> 的主题,包含4个分区和2个副本.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-test --replication-factor 2 --partitions 4</span><br><span class="line"></span><br><span class="line">Created topic &quot;topic-test&quot;.</span><br></pre></td></tr></table></figure><p>分区创建完成后,会在kafka的 <code>log.dirs</code> 或者 <code>log.dir</code> 的目录下创建相应的主题分区.下面是在其中一台Broker节点的信息展示:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ ls /opt/logs/kafka/ | grep &quot;topic-test&quot;</span><br><span class="line">topic-test-0</span><br><span class="line">topic-test-2</span><br></pre></td></tr></table></figure><p>可以看到152节点中创建了2个文件夹 topic-test-0 和 topic-test-2,对应主题 topic-test的2个分区编号为0和2的分区，命名方式可以概括为 <code>&lt;topic&gt;-&lt;partition&gt;</code> .严谨地说,其实这类文件夹对应的不是分区,分区同主题一样是一个逻辑的概念而没有物理上的存在.并且这里我们也只是看到了2个分区,而我们创建的是4个分区,其余2个分区被分配到了153和154节点中，参考如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#153节点</span><br><span class="line">[hadoop@bi-dev153 ~]$ ls /opt/logs/kafka/ | grep &quot;topic-test&quot;</span><br><span class="line">topic-test-0</span><br><span class="line">topic-test-1</span><br><span class="line">topic-test-3</span><br><span class="line"></span><br><span class="line">#154节点</span><br><span class="line">[hadoop@bi-dev154 ~]$ ls /opt/logs/kafka/ | grep &quot;topic-test&quot;</span><br><span class="line">topic-test-1</span><br><span class="line">topic-test-2</span><br><span class="line">topic-test-3</span><br></pre></td></tr></table></figure><p>三个broker节点一共创建了8个文件夹,这个数字8实质上是分区数4与副本因子2的乘积.每个副本(或者更确切地说应该是日志,副本与日志一一对应)才真正对应 了一个命名形式.</p><a id="more"></a> <p><strong>主题</strong>,<strong>分区,副本和日志</strong>的关系如下图所示.<strong>主题</strong>和<strong>分区</strong>是提供给上层用户的抽象,而在副本层面(或者更确切的说是Log日志层面)才会实际物理存在.</p><p>同一个分区中的多个副本必须分布在不同broker中,并且一个分区副本同时存在多个broker中,这样才能提供有效的数据冗余.上面的示例中,每个副本都分布在至少2台不同的broker中.</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608631181435-40a87763-f28c-45c4-a25e-d85651c26d2e.png" alt="image.png"></p><h4 id="4-1-1-2-手动创建主题以及分区副本"><a href="#4-1-1-2-手动创建主题以及分区副本" class="headerlink" title="4.1.1.2 手动创建主题以及分区副本"></a>4.1.1.2 手动创建主题以及分区副本</h4><p>通过 <code>kafka-topics.sh</code> 脚本创建的主题会按照内部既定逻辑来分配分区和副本到Broker节点上.其实该脚本还提供一个 <code>replica-assignment</code> 参数来手动指定分区副本的分配方案.用法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">格式为: 分区1broker节点1:分区1broker节点2,分区2broker节点1:分区2broker节点2.副本集合用冒号隔开,分区之间用逗号隔开</span><br><span class="line">--replica-assignment broker_id_for_partition1_replica1:broker_id_for_partition1_replica2,broker_id_for_partition2_replica1:broker_id_for_partition2_replica2.......</span><br></pre></td></tr></table></figure><p>例如下面这个实例通过手动方式创建了一个和 <code>topic-test</code> 一样分区副本分配的 <code>topic-test-same</code> 主题.</p><p>下面是刚创建的自动分配的topic-test主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic &quot;topic-test&quot;</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br></pre></td></tr></table></figure><p>通过 <code>--replica-assignment</code> 手动指定分区副本分配情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-test-same --replica-assignment 153:152,154:153,152:154,153:154</span><br></pre></td></tr></table></figure><blockquote><p>–replica-assignment参数其实就是逗号隔开的所有分区的Replicas副本集合.副本集合内部用:冒号隔开</p></blockquote><p>查看 <code>topic-test-same</code> 分区信息.和 <code>topic-test</code> 主题分区副本分配一致</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic &quot;topic-test-same&quot;</span><br><span class="line">Topic:topic-test-same   PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test-same  Partition: 0    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">    Topic: topic-test-same  Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test-same  Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test-same  Partition: 3    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br></pre></td></tr></table></figure><p>手动分配分区副本需要遵循以下原则,否则会报错:</p><ul><li>同一个分区内的副本不能有重复.比如153:153</li><li>分区之间所指定的副本数量要相同.比如153:154,152,154:152</li><li>不能跳过一个分区.比如153:154,,154:152</li></ul><h4 id="4-1-1-3-自定义相关参数"><a href="#4-1-1-3-自定义相关参数" class="headerlink" title="4.1.1.3 自定义相关参数"></a>4.1.1.3 自定义相关参数</h4><p>在创建主题时,还可以通过 <code>config</code> 参数设置要创建主题的相关参数.可以覆盖原本的默认配置参数. <code>config</code> 可以指定多个参数.用法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--config 参数名=值 --config 参数名=值 ......</span><br></pre></td></tr></table></figure><p>下面示例使用 <code>config</code> 参数创建主题 <code>topic-config</code>.并且携带2个参数 :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-config --replication-factor 1 --partitions 1 --config cleanup.policy=compact --config max.message.bytes=10000</span><br><span class="line">Created topic &quot;topic-config&quot;.</span><br></pre></td></tr></table></figure><p>查看主题信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:1    ReplicationFactor:1 Configs:cleanup.policy=compact,max.message.bytes=10000</span><br><span class="line">    Topic: topic-config Partition: 0    Leader: 154 Replicas: 154   Isr: 154</span><br></pre></td></tr></table></figure><p>通过zk也能查看到config信息,config信息保存在 <code>/config/topics/TOPIC_NAME</code> 目录下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] get /config/topics/topic-config</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;config&quot;:&#123;&quot;max.message.bytes&quot;:&quot;10000&quot;,&quot;cleanup.policy&quot;:&quot;compact&quot;&#125;&#125;</span><br></pre></td></tr></table></figure><h4 id="4-1-1-4-总结"><a href="#4-1-1-4-总结" class="headerlink" title="4.1.1.4 总结"></a>4.1.1.4 总结</h4><p>创建主题时需要遵循几个原则</p><ul><li>主题名不能重复,否则会报错.(使用 <code>if-not-exists</code> 参数可以避免出现报错信息,但是不会成功创建一个同名主题)</li><li>主题名不推荐使用__双下划线开头的命名,双下划线开头主题一般看做Kafka的内部主题</li><li>主题名由大小写祖母,数字,点号,下划线,连接线等组成.不能只有特殊符号</li></ul><p><code>kafka-topics.sh</code> 创建主题信息支持以下参数:</p><ul><li><p><code>--create</code> 创建主题</p></li><li><ul><li><code>--replica-assignment</code> 手动创建主题的分区副本分配</li><li><code>--config</code> 手动指定参数</li></ul></li></ul><h3 id="4-1-2-查看主题的分区和副本信息"><a href="#4-1-2-查看主题的分区和副本信息" class="headerlink" title="4.1.2 查看主题的分区和副本信息"></a>4.1.2 查看主题的分区和副本信息</h3><h4 id="4-1-2-1-查看具体某个topic的信息"><a href="#4-1-2-1-查看具体某个topic的信息" class="headerlink" title="4.1.2.1.查看具体某个topic的信息"></a>4.1.2.1.查看具体某个topic的信息</h4><p><code>kafka-topics.sh</code> 脚本提供了 <code>--describe</code> 参数来查看一个topic的信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic &quot;topic-test&quot;</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br></pre></td></tr></table></figure><p>在上面的示例中,命令行提供了以下几个信息:</p><p>一共有3个broker节点:152,153,154</p><p><code>PartitionCount</code> 表示一共有3个分区</p><p><code>ReplicationFactor</code> 副本因子为2</p><p><code>Leader</code> 表示某个分区对应的leader副本在具体的Broker节点</p><p><code>Replicas</code> 表示分区内所有AR副本的集合</p><p><code>Isr</code> 表示ISR副本集合</p><h4 id="4-1-2-2-查看所有topic的信息"><a href="#4-1-2-2-查看所有topic的信息" class="headerlink" title="4.1.2.2 查看所有topic的信息"></a>4.1.2.2 查看所有topic的信息</h4><p>如果 <code>kafka-topics.sh</code> 脚本没有指定具体的 <code>--topic</code> 字段.则会展示所有的topic主题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe | head</span><br><span class="line">Topic:__consumer_offsets    PartitionCount:50   ReplicationFactor:1 Configs:segment.bytes=104857600,cleanup.policy=compact,compression.type=producer</span><br><span class="line">    Topic: __consumer_offsets   Partition: 0    Leader: 153 Replicas: 153   Isr: 153</span><br><span class="line">    Topic: __consumer_offsets   Partition: 1    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: __consumer_offsets   Partition: 2    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">    Topic: __consumer_offsets   Partition: 3    Leader: 153 Replicas: 153   Isr: 153</span><br><span class="line">    Topic: __consumer_offsets   Partition: 4    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: __consumer_offsets   Partition: 5    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">    Topic: __consumer_offsets   Partition: 6    Leader: 153 Replicas: 153   Isr: 153</span><br><span class="line">    Topic: __consumer_offsets   Partition: 7    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: __consumer_offsets   Partition: 8    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">  .....略.......</span><br></pre></td></tr></table></figure><h4 id="4-1-2-3-zookeeper查看topic信息"><a href="#4-1-2-3-zookeeper查看topic信息" class="headerlink" title="4.1.2.3 zookeeper查看topic信息"></a>4.1.2.3 zookeeper查看topic信息</h4><p>zookeeper提供了 <code>zkCli.sh</code> 客户端.使用客户端连接zookeeper:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/zookeeper-3.4.10/bin/zkCli.sh  -server localhost:2181</span><br><span class="line">[zk: localhost:2181(CONNECTED) 0]</span><br></pre></td></tr></table></figure><p>zookeeer的 <code>/brokers/topics</code> 目录下保存了主题的分区副本分片方案.通过查看这个目录即可查看主题的分区和副本信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] get /brokers/topics/topic-test</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:&#123;&quot;2&quot;:[152,154],&quot;1&quot;:[154,153],&quot;3&quot;:[153,154],&quot;0&quot;:[153,152]&#125;&#125;</span><br></pre></td></tr></table></figure><p>如上示例所示, <code>&quot;2&quot;:[152,154]</code> 表示分区2分配了2个副本,分别在152和153这2个broker节点上.</p><h4 id="4-1-2-4-查看kafka集群当前所有主题"><a href="#4-1-2-4-查看kafka集群当前所有主题" class="headerlink" title="4.1.2.4 查看kafka集群当前所有主题"></a>4.1.2.4 查看kafka集群当前所有主题</h4><p><code>--list</code> 参数可以列出当前的所有topic</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --list</span><br><span class="line">__consumer_offsets</span><br><span class="line">delivery_message</span><br><span class="line">example</span><br><span class="line">example1</span><br><span class="line">goods_center</span><br><span class="line">hsq-aliapp</span><br><span class="line">hsq-wxapp</span><br><span class="line">hsq_online_test</span><br><span class="line">monitor_report_app_log</span><br><span class="line">sample_consumer_dlq</span><br><span class="line">tidb_test</span><br><span class="line">topic-config</span><br><span class="line">topic-demo</span><br><span class="line">topic-demo1</span><br><span class="line">topic-test</span><br><span class="line">topic-test-same</span><br></pre></td></tr></table></figure><h4 id="4-1-2-5-查看主题其他详细信息"><a href="#4-1-2-5-查看主题其他详细信息" class="headerlink" title="4.1.2.5 查看主题其他详细信息"></a>4.1.2.5 查看主题其他详细信息</h4><p><code>kafka-topics.sh</code> 脚本的 <code>describe</code> 参数还支持很多额外的指令,用于查看更详细的信息.</p><p>1.<strong><code>--topics-with-overrides</code></strong> 参数表示查看覆盖配置的主题,列出包含了与集群不一样配置的主题.下面列出了 <code>topic-config</code> 这个主题,这个主题使用了 <code>--config</code> 参数创建</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topics-with-overrides</span><br><span class="line"></span><br><span class="line">Topic:topic-config  PartitionCount:1    ReplicationFactor:1 Configs:cleanup.policy=compact,max.message.bytes=10000</span><br></pre></td></tr></table></figure><p>2.<strong><code>--under-replicated-paritions</code></strong> 参数列出包含失效副本的分区.失效副本的分区可能正在进行同步操作,也有可能同步发生异常.此时分区的ISR集合小于AR集合.失效副本的分区是重点监控对象,因为这可能意味着集群中的某个broker已经失效或者同步效率降低等.</p><p>正常情况下此命令不会出现任何信息.例如查看主题 <code>topic-demo</code> 的失效副本信息,但是没有任何输出信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo --under-replicated-partitions</span><br></pre></td></tr></table></figure><p>此时将153这个节点下线.再次查看:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo --under-replicated-partitions</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 152,154</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 154 Replicas: 153,154,152   Isr: 154,152</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 154,152</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 152,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>可以看到Leader和ISR集合中都没有了153这个节点.将153节点上线.此时再次查询,恢复正常.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo --under-replicated-partitions</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>\3. <strong><code>unavailable-partitions</code></strong> 参数可以查看主题中没有leader副本的分区.这些分区已经处于离线状态,对于生产者或者消费者来说不可用.</p><p>同样正常情况下,该命令没有展示任何信息.</p><p>例如,下面的 <code>topic-test</code> 主题有4个分区,每个分区有2个副本.其中分区1和分区3的副本ISR是153和154这2个节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 153 Replicas: 153,152   Isr: 152,153</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: 153 Replicas: 153,154   Isr: 154,153</span><br></pre></td></tr></table></figure><p>现在停掉153和154这2个节点的kafka进程.使用 <code>unavailable-partitions</code> 参数查看分区信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test --unavailable-partitions</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: -1  Replicas: 154,153   Isr: 154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: -1  Replicas: 153,154   Isr: 154</span><br><span class="line">  </span><br><span class="line">  [hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 152 Replicas: 153,152   Isr: 152</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: -1  Replicas: 154,153   Isr: 154</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: -1  Replicas: 153,154   Isr: 154</span><br></pre></td></tr></table></figure><p>leader显示为-1,表示没有可用leader</p><p>节点恢复后,再次执行该命令,没有任何显示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test --unavailable-partitions</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><h4 id="4-1-2-6-总结"><a href="#4-1-2-6-总结" class="headerlink" title="4.1.2.6 总结"></a>4.1.2.6 总结</h4><p><code>kafka-topics.sh</code> 查看主题信息支持以下参数:</p><ul><li><p><code>--describe</code> </p></li><li><ul><li>默认展示所有topic的分区副本信息</li><li><code>--topic TOPIC_NAME</code> 展示具体某个topic主题的分区副本信息</li><li><code>--topics-with-overrides</code> 列出覆盖配置参数的主题</li><li><code>--under-replicated-partitions</code> 列出失效副本的主题分区信息</li><li><code>--unavailable-partitions</code> 列出没有副本的主题分区</li></ul></li><li><p><code>--list</code> 列出kafka集群下的所有topic主题名称</p></li></ul><h3 id="4-1-3-修改主题"><a href="#4-1-3-修改主题" class="headerlink" title="4.1.3 修改主题"></a>4.1.3 修改主题</h3><h4 id="4-1-3-1-修改主题分区数量"><a href="#4-1-3-1-修改主题分区数量" class="headerlink" title="4.1.3.1 修改主题分区数量"></a>4.1.3.1 修改主题分区数量</h4><p>当一个主题被修改后,依然允许我们对其做一定的修改,比如修改分区个数,修改配置等.这个功能就是 <code>kafka-topic.sh</code> 脚本中的 <code>alter</code> 指令提供的.</p><p>以 <code>topic-config</code> 主题为例,该主题下只有一个分区.将分区修改为3:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --partitions 3</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Adding partitions succeeded!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:3    ReplicationFactor:1 Configs:cleanup.policy=compact,max.message.bytes=10000</span><br><span class="line">    Topic: topic-config Partition: 0    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: topic-config Partition: 1    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">    Topic: topic-config Partition: 2    Leader: 153 Replicas: 153   Isr: 153</span><br></pre></td></tr></table></figure><p><code>--partition</code> 参数表示扩展后的分区个数.</p><blockquote><p>注意告警信息.如果主题中的消息包含key(key不为Null)时,根据key计算分区的行为就会受到影响.当分区数为1时,所以key的消息都会发送到这个分区.当分区扩展到3,会根据消息的key来计算区号.原本发往分区0的消息可能会发送到分区1或者2.此外,还会影响既定消息的顺序.</p></blockquote><p>对于基于key计算的主题,不建议修改分区数量.在一开始就设置好分区数量.另外需要注意的是,Kafka不支持减少分区.只能增加不能减少.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --partitions 1</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line"></span><br><span class="line">Error while executing topic command : The number of partitions for a topic can only be increased</span><br></pre></td></tr></table></figure><blockquote><p>不支持减少分区主要是考虑到保障kafka的消息可靠性和顺序性,事务性问题.</p></blockquote><p>如果修改一个不存在的主题分区,则会报错.添加 <code>--if-exists</code> 参数会忽略一些异常</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-none-exist  --partitions 3</span><br><span class="line">Error while executing topic command : Topic topic-none-exist does not exist on ZK path localhost:2181</span><br><span class="line"></span><br><span class="line">#使用--if-exists参数,没有报错,但是不会产生任何效果</span><br><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-none-exist  --if-exists --partitions 3</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><h4 id=""><a href="#" class="headerlink" title=" "></a> </h4><h4 id="4-1-3-2-修改主题配置"><a href="#4-1-3-2-修改主题配置" class="headerlink" title="4.1.3.2 修改主题配置"></a>4.1.3.2 修改主题配置</h4><p>还可以使用 <code>kafka-topics.sh</code> 脚本的 <code>alter</code> 指令修改主题的配置.在创建主题的时候通过 <code>config</code> 参数来设置要创建的主题相关参数.在创建完主题之后,还可以通过 <code>alter</code> 和 <code>config</code> 配合增加或者修改一些配置文件覆盖原有的值</p><p>下面例子演示修改主题 <code>topic-config</code> 的 <code>max.message.bytes</code> 配置.从10000修改到20000</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --config max.message.bytes=20000</span><br><span class="line">WARNING: Altering topic configuration from this script has been deprecated and may be removed in future releases.</span><br><span class="line">         Going forward, please use kafka-configs.sh for this functionality</span><br><span class="line">Updated config for topic &quot;topic-config&quot;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:3    ReplicationFactor:1 Configs:max.message.bytes=20000,cleanup.policy=compact</span><br></pre></td></tr></table></figure><p>通过 <code>alter</code> 也可以删除创建主题时候的自定义配置.使用 <code>--delete-config</code> 参数.下面这个例子中删除了 <code>max.message.bytes</code> 配置.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --delete-config max.message.bytes</span><br><span class="line">WARNING: Altering topic configuration from this script has been deprecated and may be removed in future releases.</span><br><span class="line">         Going forward, please use kafka-configs.sh for this functionality</span><br><span class="line">Updated config for topic &quot;topic-config&quot;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:3    ReplicationFactor:1 Configs:cleanup.policy=compact</span><br></pre></td></tr></table></figure><blockquote><p>注意.在对config配置进行增删改查时候,都会提示建议使用kafka-configs.sh这个脚本来实现.该脚本的使用方式下面马上讲到</p></blockquote><h3 id="4-1-4-配置管理"><a href="#4-1-4-配置管理" class="headerlink" title="4.1.4 配置管理"></a>4.1.4 配置管理</h3><p><code>kafka-configs.sh</code> 脚本专门用来对配置进行操作.可以在运行状态下动态更改配置.也可以查询主题的相关配置.而且该脚本不仅可以支持主题相关配置修改,还可以修改broker,用户和客户端这3个类型的配置</p><p><code>kafka-configs.sh</code> 脚本使用 <code>entity-type</code> 参数指定操作配置的类型, <code>entity-name</code> 参数指定操作配置的名称.</p><h4 id="4-1-4-1-查询配置"><a href="#4-1-4-1-查询配置" class="headerlink" title="4.1.4.1 查询配置"></a>4.1.4.1 查询配置</h4><p>下面这个例子查看主题 <code>topic-config</code> 的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics --entity-name topic-config</span><br><span class="line">Configs for topic &apos;topic-config&apos; are cleanup.policy=compact</span><br></pre></td></tr></table></figure><p><code>--entity-type</code> 指定查看的实体类型.支持以下几种类型:</p><ul><li>topics</li><li>clients</li><li>users</li><li>brokers</li></ul><p><code>--entity-name</code> 配置的实体名称:</p><ul><li>topic name (主题名称)</li><li>client id (客户端ID)</li><li>user principal name (用户名)</li><li>broker id (kafka节点ID)</li></ul><p>如果不指定 <code>--entity-name</code> 参数则会查询所有的 <code>entity-type</code> 对应的所有配置信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics</span><br><span class="line">Configs for topic &apos;topic-config&apos; are</span><br><span class="line">Configs for topic &apos;__consumer_offsets&apos; are segment.bytes=104857600,cleanup.policy=compact,compression.type=producer</span><br><span class="line">......</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>通过zookeeper也可以查询主题的配置信息.路径为 <code>/config/topics/TOPIC_NAME</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] get /config/topics/topic-config</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;config&quot;:&#123;&quot;cleanup.policy&quot;:&quot;compact&quot;,&quot;max.message.bytes&quot;:&quot;20000&quot;&#125;&#125;</span><br></pre></td></tr></table></figure><h4 id="4-1-4-2-修改配置"><a href="#4-1-4-2-修改配置" class="headerlink" title="4.1.4.2 修改配置"></a>4.1.4.2 修改配置</h4><p>使用 <code>alter</code> 对配置进行变更.需要配合 <code>add-config</code> 或者 <code>delete-config</code> 这2个参数一起使用.</p><p><code>add-config</code> 参数实现配置的增,改</p><p>下面的例子中,为主题 <code>topic-config</code> 添加 <code>max.message.bytes</code> 参数配置和 <code>cleanup.policy</code> 参数配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name topic-config --add-config cleanup.policy=compact,max.message.bytes=20000</span><br><span class="line">Completed Updating config for entity: topic &apos;topic-config&apos;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics --entity-name topic-config</span><br><span class="line">Configs for topic &apos;topic-config&apos; are cleanup.policy=compact,max.message.bytes=20000</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p><code>delete-config</code> 参数可以实现配置删除.</p><p>下面的例子中,删除上面的2个配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name topic-config --delete-config cleanup.policy,max.message.bytes</span><br><span class="line">Completed Updating config for entity: topic &apos;topic-config&apos;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics --entity-name topic-config</span><br><span class="line">Configs for topic &apos;topic-config&apos; are</span><br></pre></td></tr></table></figure><h3 id="4-1-5-删除主题"><a href="#4-1-5-删除主题" class="headerlink" title="4.1.5 删除主题"></a>4.1.5 删除主题</h3><p>如果确定不再使用一个主题,那么最好的方式是将其删除.这样可以释放一些资源,比如磁盘,文件句柄等. <code>kafka-topics.sh</code> 脚本中的 <code>delete</code> 命令可以用来删除主题.比如下面删除主题 <code>topic-demo1</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --delete --topic topic-demo1</span><br><span class="line">Topic topic-demo1 is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br></pre></td></tr></table></figure><blockquote><p>注意.必须将kafka服务器配置文件的delete.topic.enable选项设置为true才能删除.这个参数的默认值是false.删除主题的操作会被忽略.主题并没有被删除</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --list | grep topic-demo1</span><br><span class="line">topic-demo1</span><br></pre></td></tr></table></figure><p>编辑配置文件 <code>/opt/kafka/config/server.properties</code> 修改下面的参数为true</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Switch to enable topic deletion or not, default value is false</span><br><span class="line">delete.topic.enable=true</span><br></pre></td></tr></table></figure><p>如果删除一个kafka的内部主题,那么会报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --delete --topic __consumer_offsets</span><br><span class="line">Error while executing topic command : Topic __consumer_offsets is a kafka internal topic and is not allowed to be marked for deletion.</span><br></pre></td></tr></table></figure><p>删除一个不存在的主题也会报错,此时可以通过 <code>if-exists</code> 参数来忽略异常.</p><h3 id="4-1-5-总结"><a href="#4-1-5-总结" class="headerlink" title="4.1.5 总结"></a>4.1.5 总结</h3><p>下面这张图是 <code>kafka-topics.sh</code> 脚本的常用参数</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608805092662-042718a3-9a6a-489e-a987-db4e38217171.png" alt="image.png"></p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608805117734-2bd94456-e966-41b1-9465-dde20a8a9129.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;4-1-1-创建主题&quot;&gt;&lt;a href=&quot;#4-1-1-创建主题&quot; class=&quot;headerlink&quot; title=&quot;4.1.1 创建主题&quot;&gt;&lt;/a&gt;4.1.1 创建主题&lt;/h3&gt;&lt;h4 id=&quot;4-1-1-1-自动创建主题以及分区副本&quot;&gt;&lt;a href=&quot;#4-1-1-1-自动创建主题以及分区副本&quot; class=&quot;headerlink&quot; title=&quot;4.1.1.1 自动创建主题以及分区副本&quot;&gt;&lt;/a&gt;4.1.1.1 自动创建主题以及分区副本&lt;/h4&gt;&lt;p&gt;在之前的笔记中提到了创建主题的一个简单示例.kafka提供 &lt;code&gt;kafka-topics.sh&lt;/code&gt; 脚本来创建主题.下面这个示例创建了一个 &lt;code&gt;topic-test&lt;/code&gt; 的主题,包含4个分区和2个副本.&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;/opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-test --replication-factor 2 --partitions 4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Created topic &amp;quot;topic-test&amp;quot;.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;分区创建完成后,会在kafka的 &lt;code&gt;log.dirs&lt;/code&gt; 或者 &lt;code&gt;log.dir&lt;/code&gt; 的目录下创建相应的主题分区.下面是在其中一台Broker节点的信息展示:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev152 ~]$ ls /opt/logs/kafka/ | grep &amp;quot;topic-test&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;可以看到152节点中创建了2个文件夹 topic-test-0 和 topic-test-2,对应主题 topic-test的2个分区编号为0和2的分区，命名方式可以概括为 &lt;code&gt;&amp;lt;topic&amp;gt;-&amp;lt;partition&amp;gt;&lt;/code&gt; .严谨地说,其实这类文件夹对应的不是分区,分区同主题一样是一个逻辑的概念而没有物理上的存在.并且这里我们也只是看到了2个分区,而我们创建的是4个分区,其余2个分区被分配到了153和154节点中，参考如下:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;#153节点&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev153 ~]$ ls /opt/logs/kafka/ | grep &amp;quot;topic-test&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#154节点&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev154 ~]$ ls /opt/logs/kafka/ | grep &amp;quot;topic-test&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;三个broker节点一共创建了8个文件夹,这个数字8实质上是分区数4与副本因子2的乘积.每个副本(或者更确切地说应该是日志,副本与日志一一对应)才真正对应 了一个命名形式.&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="4-主题和分区" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch索引迁移</title>
    <link href="https://jesse.top/2020/09/30/elk/Elasticsearch%E7%B4%A2%E5%BC%95%E8%BF%81%E7%A7%BB/"/>
    <id>https://jesse.top/2020/09/30/elk/Elasticsearch索引迁移/</id>
    <published>2020-09-30T14:59:58.000Z</published>
    <updated>2021-01-19T14:39:47.543Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Elasticsearch索引迁移"><a href="#Elasticsearch索引迁移" class="headerlink" title="Elasticsearch索引迁移"></a>Elasticsearch索引迁移</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>旧Elasticsearch版本:2.4.4</p><p>新Elasticsearch版本:2.4.4</p><p>近期dev环境服务器迁移到一台新的物理机,所以需要迁移部分Elasticsearch索引数据.</p><p>Elasticsearch索引迁移有许多方法.测试过elasticsearch-exporter.但是没有成功.报错如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[work@docker elasticsearch-exporter]$ node exporter.js -a 10.0.0.250 -b 10.0.0.101 -p 9200 -q 9200 -i mid_mg_gc_datasource_items -j mid_mg_gc_datasource_items</span><br><span class="line">Elasticsearch Exporter - Version 1.4.0</span><br><span class="line">Reading source statistics from ElasticSearch</span><br><span class="line">The source driver has not reported any documents that can be exported. Not exporting.</span><br><span class="line">Number of calls:0</span><br><span class="line">Fetched Entries:0 documents</span><br><span class="line">Processed Entries:0 documents</span><br><span class="line">Source DB Size:0 documents</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="Elasticsearch-dump"><a href="#Elasticsearch-dump" class="headerlink" title="Elasticsearch-dump"></a>Elasticsearch-dump</h3><p>本次使用elasticsearch-dump进行索引迁移.在github上可以找到具体使用方法:<a href="https://github.com/elasticsearch-dump/elasticsearch-dump" target="_blank" rel="noopener">elasticsearch-dump</a></p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install elasticdump</span><br></pre></td></tr></table></figure><p>这里稍微踩了个坑,如果报错:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pm WARN deprecated nomnom@1.8.1: Package no longer supported. Contact support@npmjs.com for more info.</span><br><span class="line">npm WARN saveError ENOENT: no such file or directory, open &apos;/home/work/package.json&apos;</span><br><span class="line">npm WARN enoent ENOENT: no such file or directory, open &apos;/home/work/package.json&apos;</span><br><span class="line">npm WARN work No description</span><br><span class="line">npm WARN work No repository field.</span><br><span class="line">npm WARN work No README data</span><br><span class="line">npm WARN work No license field.</span><br></pre></td></tr></table></figure><p>则需要初始化一下npm</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">work@docker ~]$ npm init</span><br></pre></td></tr></table></figure><p>安装完成后,进入到<code>elasticsearch dump</code>的<code>bin</code>目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[work@docker ~]$ cd node_modules/elasticdump/bin/</span><br></pre></td></tr></table></figure><h3 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h3><p>查看elasticsearchdump的具体用法</p><p>[work@docker bin]$ ./elasticdump –help</p><p><code>elaticsearchdump</code>支持两个ES跨版本迁移索引,还支持索引备份到文件,以及从文件恢复到Elasticsearch</p><h5 id="迁移mid-gm-gc-brand这个索引数据"><a href="#迁移mid-gm-gc-brand这个索引数据" class="headerlink" title="迁移mid_gm_gc_brand这个索引数据"></a>迁移mid_gm_gc_brand这个索引数据</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[work@docker bin]$ ./elasticdump --input=http://10.0.0.250:9200/mid_mg_gc_brand --output=http://10.0.0.101:9200/mid_mg_gc_brand --type=analyzer</span><br><span class="line"></span><br><span class="line">[work@docker bin]$ ./elasticdump --input=http://10.0.0.250:9200/mid_mg_gc_brand --</span><br><span class="line"></span><br><span class="line">[work@docker bin]$ ./elasticdump --input=http://10.0.0.250:9200/mid_mg_gc_brand --output=http://10.0.0.101:9200/mid_mg_gc_brand --type=data</span><br></pre></td></tr></table></figure><blockquote><p>文档中的type类型有settings, analyzer, data, mapping, alias, template</p></blockquote><h5 id="查看新服务器上的索引-迁移成功"><a href="#查看新服务器上的索引-迁移成功" class="headerlink" title="查看新服务器上的索引.迁移成功"></a>查看新服务器上的索引.迁移成功</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl -Ssl &apos;http://10.0.0.101:9200/_cat/indices?v&apos; | grep &apos;mid_mg*&apos;</span><br><span class="line">yellow open   mid_mg_gc_datasource_items             5   1         96            1     87.3kb         87.3kb</span><br><span class="line">yellow open   mid_mg_gc_brand                        5   1        966            0    312.1kb        312.1kb</span><br><span class="line">yellow open   mid_mg_gc_synonyms                     5   1        116            0     82.1kb         82.1kb</span><br><span class="line"></span><br><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl -Ssl &apos;http://10.0.0.250:9200/_cat/indices?v&apos; | grep &apos;mid_mg*&apos;</span><br><span class="line">yellow open   mid_mg_gc_datasource_items             3   1         92            9     99.1kb         99.1kb</span><br><span class="line">yellow open   mid_mg_gc_brand                        3   1        966            0    345.4kb        345.4kb</span><br><span class="line">yellow open   mid_mg_gc_synonyms                     3   1        116           16    106.1kb        106.1kb</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Elasticsearch索引迁移&quot;&gt;&lt;a href=&quot;#Elasticsearch索引迁移&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearch索引迁移&quot;&gt;&lt;/a&gt;Elasticsearch索引迁移&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;旧Elasticsearch版本:2.4.4&lt;/p&gt;
&lt;p&gt;新Elasticsearch版本:2.4.4&lt;/p&gt;
&lt;p&gt;近期dev环境服务器迁移到一台新的物理机,所以需要迁移部分Elasticsearch索引数据.&lt;/p&gt;
&lt;p&gt;Elasticsearch索引迁移有许多方法.测试过elasticsearch-exporter.但是没有成功.报错如下:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[work@docker elasticsearch-exporter]$ node exporter.js -a 10.0.0.250 -b 10.0.0.101 -p 9200 -q 9200 -i mid_mg_gc_datasource_items -j mid_mg_gc_datasource_items&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Elasticsearch Exporter - Version 1.4.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Reading source statistics from ElasticSearch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;The source driver has not reported any documents that can be exported. Not exporting.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Number of calls:	0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Fetched Entries:	0 documents&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Processed Entries:	0 documents&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Source DB Size:		0 documents&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>Openvpn客户端无法连接OpenSSL</title>
    <link href="https://jesse.top/2020/09/22/Linux-Service/Openvpn%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5%20OpenSSL-%20error/"/>
    <id>https://jesse.top/2020/09/22/Linux-Service/Openvpn客户端无法连接 OpenSSL- error/</id>
    <published>2020-09-22T14:59:58.000Z</published>
    <updated>2021-01-19T14:42:39.686Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Openvpn客户端无法连接OpenSSL-error"><a href="#Openvpn客户端无法连接OpenSSL-error" class="headerlink" title="Openvpn客户端无法连接OpenSSL: error"></a>Openvpn客户端无法连接OpenSSL: error</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>今天阿里云的Openvpn服务器部署了docker服务后,需要升级内存配置.服务器升级重启后,发现客户端无法连接Openvpn.</p><hr><h3 id="故障表现"><a href="#故障表现" class="headerlink" title="故障表现"></a>故障表现</h3><p>在openvpn客户端日志中发现下面报错信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2020-09-22 16:00:49 WARNING: No server certificate verification method has been enabled.  See http://openvpn.net/howto.html#mitm for more info.</span><br></pre></td></tr></table></figure><p>openvpn服务端日志报错信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS: Initial packet from [AF_INET]27.115.51.166:26184, sid=c0cb2b12 4b3187b2</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 OpenSSL: error:14089086:SSL routines:ssl3_get_client_certificate:certificate verify failed</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS_ERROR: BIO read tls_read_plaintext error</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS object -&gt; incoming plaintext read error</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS handshake failed</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 SIGUSR1[soft,tls-error] received, client-instance restarting</span><br><span class="line">Tue Sep 22 15:48:06 2020 27.115.51.166:33480 TLS: Initial packet from [AF_INET]27.115.51.166:33480, sid=11b9760e 97f6d068</span><br><span class="line">Tue Sep 22 15:48:07 2020 27.115.51.166:33480 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx</span><br></pre></td></tr></table></figure><p><strong>日志关键字</strong></p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OpenSSL: error:14089086:SSL routines:ssl3_get_client_certificate:certificate verify failed</span><br></pre></td></tr></table></figure><hr><h3 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h3><p>服务器重启后需要注意的几个问题:</p><p>1.检查以下几个服务是否启动:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl status openvpn@server</span><br><span class="line">systemctl status iptables</span><br></pre></td></tr></table></figure><p>2.由于docker服务会初始化iptables,所以docker启动后需要手动添加一条iptables规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A POSTROUTING -s 10.111.255.0/24 -o eth0 -j MASQUERADE</span><br></pre></td></tr></table></figure><p>3.检查ip转发功能</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysctl.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br></pre></td></tr></table></figure><p>4.检查阿里云的安全组规则,是否开通了1094的udp协议</p><hr><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>最终解决方案还是要靠google解决,在openvpn的wiki上找到了解决方案,</p><p>具体网站链接:<a href="https://community.openvpn.net/openvpn/wiki/CertificateRevocationListExpired?__cf_chl_jschl_tk__=40e85ba16653b7db84d828db819eafbc2b5a9faf-1600761449-0-AUAdZiIXwLUqBJDJAcDe9htVtUTZlGJm8m_RYLUsxLu2he8Myk5WXwzQn-CZdZyBDJRHn9clM-6y0ITsnKk0Pru3AwB7EOc0LhjyrV9unNnBv0V9_skxNC2n3per9e1TQJjcmmtwnaNl23Sp5D8p9FZYyX5PO-vtkdp1i7dyh_x1KSFwZqibI8Zt4saVoABGkfMJ3nKeUJpIIlnEhRGhfJrQwQZqvvG6EAS1CJUHRR8uqKNyqmEz90RmqcLGc8ytoOTIBYhJMs8OsPk_dA2nKA47QObzI5-4SEUZkC5ZaxhNCfkRBGx9kwimWfBJFtxJkPzI5_vkXONWXhoB0wi5cfE" target="_blank" rel="noopener">openvpn wiki</a></p><p>解决问题很简单:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#需要进入到下面这个目录下</span><br><span class="line">cd /etc/openvpn/easy-rsa/3</span><br><span class="line"></span><br><span class="line">#更新一下crl文件.</span><br><span class="line">[root@dwd-Dnsmasq 3]# ./easyrsa gen-crl</span><br><span class="line"></span><br><span class="line">Using configuration from ./openssl-1.0.cnf</span><br><span class="line"></span><br><span class="line">An updated CRL has been created.</span><br><span class="line">CRL file: /etc/openvpn/easy-rsa/3/pki/crl.pem</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Openvpn客户端无法连接OpenSSL-error&quot;&gt;&lt;a href=&quot;#Openvpn客户端无法连接OpenSSL-error&quot; class=&quot;headerlink&quot; title=&quot;Openvpn客户端无法连接OpenSSL: error&quot;&gt;&lt;/a&gt;Openvpn客户端无法连接OpenSSL: error&lt;/h2&gt;&lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;今天阿里云的Openvpn服务器部署了docker服务后,需要升级内存配置.服务器升级重启后,发现客户端无法连接Openvpn.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;故障表现&quot;&gt;&lt;a href=&quot;#故障表现&quot; class=&quot;headerlink&quot; title=&quot;故障表现&quot;&gt;&lt;/a&gt;故障表现&lt;/h3&gt;&lt;p&gt;在openvpn客户端日志中发现下面报错信息:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;2020-09-22 16:00:49 WARNING: No server certificate verification method has been enabled.  See http://openvpn.net/howto.html#mitm for more info.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;openvpn服务端日志报错信息:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;ue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS: Initial packet from [AF_INET]27.115.51.166:26184, sid=c0cb2b12 4b3187b2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 OpenSSL: error:14089086:SSL routines:ssl3_get_client_certificate:certificate verify failed&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS_ERROR: BIO read tls_read_plaintext error&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS object -&amp;gt; incoming plaintext read error&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS handshake failed&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 SIGUSR1[soft,tls-error] received, client-instance restarting&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:48:06 2020 27.115.51.166:33480 TLS: Initial packet from [AF_INET]27.115.51.166:33480, sid=11b9760e 97f6d068&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:48:07 2020 27.115.51.166:33480 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;日志关键字&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-Service" scheme="https://jesse.top/categories/Linux-Service/"/>
    
    
      <category term="Linux" scheme="https://jesse.top/tags/Linux/"/>
    
      <category term="openvpn" scheme="https://jesse.top/tags/openvpn/"/>
    
  </entry>
  
</feed>

<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jesse&#39;s home</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jesse.top/"/>
  <updated>2021-01-19T14:39:47.543Z</updated>
  <id>https://jesse.top/</id>
  
  <author>
    <name>Jesse</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kong实现限流</title>
    <link href="https://jesse.top/2021/01/19/Linux-Web/Kong%20Rate%20Limiting%E9%99%90%E6%B5%81%E6%8F%92%E4%BB%B6/"/>
    <id>https://jesse.top/2021/01/19/Linux-Web/Kong Rate Limiting限流插件/</id>
    <published>2021-01-19T03:59:58.000Z</published>
    <updated>2021-01-19T14:39:47.543Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kong实现限流"><a href="#Kong实现限流" class="headerlink" title="Kong实现限流"></a>Kong实现限流</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>近期发现公司某个业务对外的openapi接口的/merchantapi路径异常调用非常频繁.公司的第三方商户需要通过这个路径来调用ERP接口,但是经常发生被恶意刷接口的情况,导致公司的业务服务器资源使用率飙升,面临很大的宕机风险和隐患.</p><p>目前外部客户端访问公司业务仍然是阿里云SLB—–Nginx—php-fpm的架构.由于Nginx的限流能力并不出色,特别是针对具体path路径的限流.所以,引入了Kong api网关</p><h3 id="Rate-Limiting限流插件介绍"><a href="#Rate-Limiting限流插件介绍" class="headerlink" title="Rate Limiting限流插件介绍"></a>Rate Limiting限流插件介绍</h3><p>Rate Limiting是Kong社区版就已经自带的官方流量控制插件.详细信息可以参考Kong官网介绍. <a href="https://docs.konghq.com/hub/kong-inc/rate-limiting/" target="_blank" rel="noopener">https://docs.konghq.com/hub/kong-inc/rate-limiting/</a></p><p>它可以针对<code>consumer</code> ,<code>credential</code> ,<code>ip</code> ,<code>service</code>,<code>path</code>,<code>header</code> 等多种维度来进行限流.流量控制的精准度也有多种方式可以参考,比如可以做到秒级,分钟级,小时级等限流控制.</p><h4 id="响应客户端头部信息"><a href="#响应客户端头部信息" class="headerlink" title="响应客户端头部信息"></a>响应客户端头部信息</h4><p>当启用这个插件后.Kong会响应客户端一些额外的头部信息,告诉客户端限流信息.例如下面是Kong响应给客户端的header信息,告诉客户端当前的限流策略是10r/s</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RateLimit-Limit: 10</span><br><span class="line">RateLimit-Remaining: 0</span><br><span class="line">RateLimit-Reset: 1</span><br><span class="line"></span><br><span class="line">X-Kong-Response-Latency: 1</span><br><span class="line">X-RateLimit-Limit-Second: 10</span><br><span class="line">X-RateLimit-Remaining-Second: 0</span><br></pre></td></tr></table></figure><p>如果客户端的访问请求超过限流的阈值,Kong会返回status<code>429</code>的状态码以及下面的错误信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; &quot;message&quot;: &quot;API rate limit exceeded&quot; &#125;</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="限流策略对Kong性能影响"><a href="#限流策略对Kong性能影响" class="headerlink" title="限流策略对Kong性能影响"></a>限流策略对Kong性能影响</h4><p>Rate limiting插件支持3种限流策略.</p><p><code>cluster</code> 集群策略.Kong的数据库会维护一个计数器,并且在所有的Kong集群内每个节点共享这个计数器.如果计数器触发限流上线,所有的Kong节点都拒绝客户端的转发.这就意味着每个节点接收到客户端的请求,都会对数据库进行读写操作.</p><p><code>redis</code> redis策略和<code>cluster</code> 相似,唯一不同的是,计数器是存储在redis数据中.并且在集群内所有节点共享.</p><p><code>local</code> 本地策略.计数器保存在Kong节点服务器本地内存缓冲区.并且计数器只对该节点有效.这意味着<code>local</code>策略有最好的性能表现.但是由于计数器存储在本地.所以限流的精度没有<code>redis</code>和<code>cluster</code> 准确.并且会影响Kong节点服务器弹性扩容(比如限流设置30r/s,Kong集群从2个节点扩容到4个节点.限流就从60r/s变成了120r/s.此时需要手动将限流设置从30r/s降低到15r/s)</p><blockquote><p>或者,可以在Kong前面配置一个hash转发策略的负载均衡,将同一个外部客户端的请求代理到同一个节点.这样local策略的精确度可以提升,并且kong节点的弹性扩容不会影响限流效果</p></blockquote><p>下面是3种限流策略的对比表</p><table><thead><tr><th>policy</th><th>describe</th><th>pros</th><th>cons</th></tr></thead><tbody><tr><td>cluster</td><td>集群策略</td><td>限流精准度高,不需要第三方组件支持</td><td>对Kong性能影响比较大</td></tr><tr><td>redis</td><td>redis策略</td><td>限流精准度高,对Kong性能影响较低</td><td>需要额外的redis服务</td></tr><tr><td>local</td><td>本地策略</td><td>对Kong性能影响最低</td><td>精准度比较差,Kong节点扩容和缩容需要手动调整限流速率</td></tr></tbody></table><p>下面是以上集群策略的使用场景:</p><ul><li>如果对流量精确度要求非常高.比如金融,交易等.那么适合redis或者cluster的限流策略</li><li>如果是为了保护后端服务,避免大流量带来的服务器过载.那么适合local限流策略,这种场景对限流的精度要求不高</li></ul><h3 id="针对客户端IP限流"><a href="#针对客户端IP限流" class="headerlink" title="针对客户端IP限流"></a>针对客户端IP限流</h3><p>我们场景中针对客户端IP进行限流.但是由于Kong是在SLB或者Nginx的负载均衡后面,所以默认情况下,Kong采用的IP是上一级负载均衡器的IP.此时就需要将客户端的真实IP传递到Kong,并且使用该IP作为<code>remote_ip</code> .实现方法如下:</p><ul><li>虚拟机运行Kong</li></ul><p>针对rpm包或者其他方式安装的Kong服务,可以修改默认的<code>/etc/kong/kong.conf</code> 配置文件.加入下面2行配置信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trusted_ips = 0.0.0.0/0,::/0</span><br><span class="line">real_ip_header = X-Forwarded-For</span><br></pre></td></tr></table></figure><p>重载kong配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kong reload</span><br></pre></td></tr></table></figure><ul><li>docker容器方式运行kong</li></ul><p>针对docker容器方式运行的Kong,修改配置文件不方便,此时可以通过变量注入的方式自定义配置<code>kong.conf</code> 配置文件.还可以通过这种方式注入nginx自定义配置,具体可以参考官方的文档介绍:<a href="https://docs.konghq.com/2.2.x/configuration/#environment-variables" target="_blank" rel="noopener">environment-variables</a></p><p>例如,上面的2行配置内容可以通过在配置参数前面加<code>KONG_</code>以及大写的参数名的方式注入环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">KONG_TRUSTED_IPS=0.0.0.0/0,::/0</span><br><span class="line">KONG_REAL_IP_HEADER=X-Forwarded-For</span><br></pre></td></tr></table></figure><p>修改Kong的<code>docker-compose</code>文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">kong:</span><br><span class="line">    image: dwd-kong:2.2.0  #自定义kong镜像.也可以使用官方的docker镜像</span><br><span class="line">    container_name: kong</span><br><span class="line">    hostname: kong</span><br><span class="line">    environment:</span><br><span class="line">      - KONG_DATABASE=postgres</span><br><span class="line">      - KONG_PG_HOST=kong-database</span><br><span class="line">      - KONG_PROXY_ACCESS_LOG=/var/log/kong/access.log</span><br><span class="line">      - KONG_ADMIN_ACCESS_LOG=/var/log/kong/admin_access.log</span><br><span class="line">      - KONG_PROXY_ERROR_LOG=/var/log/kong/error.log</span><br><span class="line">      - KONG_ADMIN_ERROR_LOG=/var/log/kong/admin_error.log</span><br><span class="line">      - KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl</span><br><span class="line">      - KONG_TRUSTED_IPS=0.0.0.0/0,::/0       #增加这两行</span><br><span class="line">      - KONG_REAL_IP_HEADER=X-Forwarded-For   #增加这两行</span><br><span class="line">    volumes:</span><br><span class="line">      - /data/logs/kong:/var/log/kong</span><br><span class="line">      - /etc/localtime:/etc/localtime</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8000:8000&quot;</span><br><span class="line">      - &quot;8443:8443&quot;</span><br><span class="line">      - &quot;8001:8001&quot;</span><br><span class="line">      - &quot;8444:8444&quot;</span><br><span class="line">    expose:</span><br><span class="line">      - &quot;8000&quot;</span><br><span class="line">      - &quot;8443&quot;</span><br><span class="line">      - &quot;8001&quot;</span><br><span class="line">      - &quot;8444&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - dev-net</span><br><span class="line">    restart: always</span><br><span class="line">    depends_on:</span><br><span class="line">        - kong-database</span><br><span class="line">        - kong-migration</span><br><span class="line">        - kong-migration-finish</span><br></pre></td></tr></table></figure><h3 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h3><p>Rate Limiting插件由Kong默认提供,所以无需自行安装.由于是针对<code>/merchantapi</code> 这个借口进行限流,所以只需配置该route,并且将插件应用到这个route下即可.由于我日常使用的python进行Kong的配置,所以这里只列出我的python配置文件中相关配置.不演示具体配置了.</p><blockquote><p>使用kong的dashboard也可以很方便的实现配置</p></blockquote><ul><li>配置service</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hsq_openapi_dev = &#123; &quot;name&quot;: &quot;hsq_openapi_dev&quot;,</span><br><span class="line">            &quot;host&quot;: &quot;kong.devapi.hsq.net&quot;,</span><br><span class="line">            &quot;port&quot;: 80,</span><br><span class="line">            &quot;protocol&quot;: &quot;http&quot;,</span><br><span class="line">            &quot;path&quot;: &quot;/&quot;</span><br><span class="line">          &#125;</span><br></pre></td></tr></table></figure><ul><li>配置route</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#默认转发路由</span><br><span class="line">hsq_openapi_dev =   &#123;  &quot;service_name&quot;:&quot;hsq_openapi_dev&quot;,</span><br><span class="line">                &quot;data&quot;:&#123;</span><br><span class="line">                &quot;name&quot;: &quot;hsq-openapi-dev&quot;,</span><br><span class="line">                &quot;hosts&quot;: &quot;m.devapi.hsq.net&quot;,</span><br><span class="line">                &quot;strip_path&quot;: &quot;false&quot;,</span><br><span class="line">                &quot;protocols&quot;: [&quot;http&quot;, &quot;https&quot;],</span><br><span class="line">                &quot;paths&quot;: &quot;/&quot;,</span><br><span class="line">                &quot;methods&quot;: [&quot;GET&quot;, &quot;POST&quot;,&quot;PUT&quot;,&quot;OPTIONS&quot;,&quot;DELETE&quot;]&#125;</span><br><span class="line">             &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#限流接口转发路由</span><br><span class="line">hsq_merchant_api_limit = &#123;  &quot;service_name&quot;:&quot;hsq_openapi_dev&quot;,</span><br><span class="line">                    &quot;data&quot;:&#123;</span><br><span class="line">                    &quot;name&quot;: &quot;hsq_merchant_api_limit&quot;,</span><br><span class="line">                    &quot;hosts&quot;: &quot;m.devapi.hsq.net&quot;,</span><br><span class="line">                    &quot;strip_path&quot;: &quot;false&quot;,</span><br><span class="line">                    &quot;protocols&quot;: [&quot;http&quot;, &quot;https&quot;],</span><br><span class="line">                    &quot;paths&quot;: &quot;/merchantapi&quot;,</span><br><span class="line">                    &quot;methods&quot;: [&quot;GET&quot;, &quot;POST&quot;,&quot;PUT&quot;,&quot;OPTIONS&quot;,&quot;DELETE&quot;]&#125;</span><br><span class="line">                 &#125;</span><br></pre></td></tr></table></figure><ul><li>配置rate-limiting插件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hsq_merchantapi_limit = &#123; &quot;route_name&quot;: &quot;hsq_merchant_api_limit&quot;,  #关联到上面的route.表示该插件作用在route级别</span><br><span class="line">         &quot;data&quot;: &#123;</span><br><span class="line">         &quot;name&quot;: &quot;rate-limiting&quot;, #插件名称</span><br><span class="line">         &quot;config.second&quot;: 10, # 限流.每秒10个请求</span><br><span class="line">         &quot;config.policy&quot;: &quot;local&quot;, #限流策略</span><br><span class="line">         &quot;config.limit_by&quot;: &quot;ip&quot;    #针对客户端IP限流</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><p>运行python脚本,配置Kong</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~/Desktop/kong-python   master ●✚  python3 kong.py</span><br><span class="line">请输入你要配置的Kong的环境名称,例如:dev,beta,prod:&gt;&gt;&gt;dev</span><br><span class="line">正在创建service:hsq_openapi_dev</span><br><span class="line">service:hsq_openapi_dev创建成功</span><br><span class="line">开始创建routes:hsq-openapi-dev</span><br><span class="line">routes路由hsq-openapi-dev创建成功</span><br><span class="line">开始创建routes:hsq_merchant_api_limit</span><br><span class="line">routes路由hsq_merchant_api_limit创建成功</span><br><span class="line">plugins:rate-limiting创建成功.绑定在route路由:hsq_merchant_api_limit中</span><br></pre></td></tr></table></figure><h3 id="压测效果"><a href="#压测效果" class="headerlink" title="压测效果"></a>压测效果</h3><p>为了验证插件效果,这里使用<code>ab</code> 这个简单的压测工具进行测试.</p><p>1.开启一个终端,执行下面的命令.压测命令运行了1.18秒,只有20个请求成功响应,其余80个请求失败.这恰好符合了rate-limiting插件每秒10个请求的限流策略</p><blockquote><p>由于是在dev环境,所有只有一个Kong节点.如果外部流量负载均衡分发到Kong集群的所有节点,那么总体的限流应该是:Kong节点数量x限流数量</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">ab -n 100 -c 10 https://m.devapi.hsq.net/merchantapi</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">Document Path:          /merchantapi</span><br><span class="line">Document Length:        122 bytes</span><br><span class="line"></span><br><span class="line">Concurrency Level:      10</span><br><span class="line">Time taken for tests:   1.180 seconds</span><br><span class="line">Complete requests:      100         #总共100个请求</span><br><span class="line">Failed requests:        80          #失败了80个</span><br><span class="line">   (Connect: 0, Receive: 0, Length: 80, Exceptions: 0)</span><br><span class="line">Non-2xx responses:      80</span><br><span class="line">Total transferred:      41888 bytes</span><br><span class="line">HTML transferred:       5720 bytes</span><br><span class="line">Requests per second:    84.71 [#/sec] (mean)</span><br><span class="line">Time per request:       118.044 [ms] (mean)</span><br><span class="line">Time per request:       11.804 [ms] (mean, across all concurrent requests)</span><br><span class="line">Transfer rate:          34.65 [Kbytes/sec] received</span><br><span class="line">......</span><br></pre></td></tr></table></figure><ol start="2"><li>将请求继续增大,同时使用curl和浏览器访问该域名.发现请求被拒绝</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl https://m.devapi.hsq.net/merchantapi</span><br><span class="line">&#123;</span><br><span class="line">  &quot;message&quot;:&quot;API rate limit exceeded&quot;</span><br><span class="line">&#125;%</span><br></pre></td></tr></table></figure><p><img src="https://img2.jesse.top/20210119164349.png" alt=""></p><ol start="3"><li>在压测的同时,使用另外一个客户端来同时访问该接口,可以正常访问</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.99.1 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 200 122 &quot;-&quot; &quot;curl/7.29.0&quot;   #其他客户端仍然可以正常访问</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br><span class="line">10.0.2.20 - - [19/Jan/2021:14:05:14 +0800] &quot;GET /merchantapi HTTP/1.0&quot; 429 41 &quot;-&quot; &quot;ApacheBench/2.3&quot;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kong实现限流&quot;&gt;&lt;a href=&quot;#Kong实现限流&quot; class=&quot;headerlink&quot; title=&quot;Kong实现限流&quot;&gt;&lt;/a&gt;Kong实现限流&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;近期发现公司某个业务对外的openapi接口的/merchantapi路径异常调用非常频繁.公司的第三方商户需要通过这个路径来调用ERP接口,但是经常发生被恶意刷接口的情况,导致公司的业务服务器资源使用率飙升,面临很大的宕机风险和隐患.&lt;/p&gt;
&lt;p&gt;目前外部客户端访问公司业务仍然是阿里云SLB—–Nginx—php-fpm的架构.由于Nginx的限流能力并不出色,特别是针对具体path路径的限流.所以,引入了Kong api网关&lt;/p&gt;
&lt;h3 id=&quot;Rate-Limiting限流插件介绍&quot;&gt;&lt;a href=&quot;#Rate-Limiting限流插件介绍&quot; class=&quot;headerlink&quot; title=&quot;Rate Limiting限流插件介绍&quot;&gt;&lt;/a&gt;Rate Limiting限流插件介绍&lt;/h3&gt;&lt;p&gt;Rate Limiting是Kong社区版就已经自带的官方流量控制插件.详细信息可以参考Kong官网介绍. &lt;a href=&quot;https://docs.konghq.com/hub/kong-inc/rate-limiting/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://docs.konghq.com/hub/kong-inc/rate-limiting/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;它可以针对&lt;code&gt;consumer&lt;/code&gt; ,&lt;code&gt;credential&lt;/code&gt; ,&lt;code&gt;ip&lt;/code&gt; ,&lt;code&gt;service&lt;/code&gt;,&lt;code&gt;path&lt;/code&gt;,&lt;code&gt;header&lt;/code&gt; 等多种维度来进行限流.流量控制的精准度也有多种方式可以参考,比如可以做到秒级,分钟级,小时级等限流控制.&lt;/p&gt;
&lt;h4 id=&quot;响应客户端头部信息&quot;&gt;&lt;a href=&quot;#响应客户端头部信息&quot; class=&quot;headerlink&quot; title=&quot;响应客户端头部信息&quot;&gt;&lt;/a&gt;响应客户端头部信息&lt;/h4&gt;&lt;p&gt;当启用这个插件后.Kong会响应客户端一些额外的头部信息,告诉客户端限流信息.例如下面是Kong响应给客户端的header信息,告诉客户端当前的限流策略是10r/s&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;RateLimit-Limit: 10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;RateLimit-Remaining: 0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;RateLimit-Reset: 1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X-Kong-Response-Latency: 1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X-RateLimit-Limit-Second: 10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;X-RateLimit-Remaining-Second: 0&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果客户端的访问请求超过限流的阈值,Kong会返回status&lt;code&gt;429&lt;/code&gt;的状态码以及下面的错误信息&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123; &amp;quot;message&amp;quot;: &amp;quot;API rate limit exceeded&amp;quot; &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>kafka-3.1消费者与消费组</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/3-%E6%B6%88%E8%B4%B9%E8%80%85/3.1%20%E6%B6%88%E8%B4%B9%E8%80%85%E4%B8%8E%E6%B6%88%E8%B4%B9%E7%BB%84/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/3-消费者/3.1 消费者与消费组/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:49:55.160Z</updated>
    
    <content type="html"><![CDATA[<h2 id="3-1-消费者与消费组"><a href="#3-1-消费者与消费组" class="headerlink" title="3.1 消费者与消费组"></a>3.1 消费者与消费组</h2><h3 id="1-消费者和消费组介绍"><a href="#1-消费者和消费组介绍" class="headerlink" title="1.消费者和消费组介绍"></a>1.消费者和消费组介绍</h3><p>消费者( Consumer)负责订阅Kafka中的主题( Topic)，并且从订阅的主题上拉取消息.与其他一些消息中间件不同的是:在 Kafka的消费理念中还有一层消费组( Consumer Group)的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者 。</p><p>以下图为例,某个主题中共有 4 个分区( Partition) : PO、 Pl、 P2、 P3。 有两个消费组 A和 B 都订阅了这个主题，消费组 A 中有 4 个消费者 (CO、 Cl、 C2 和 C3)，消费组 B 中有 2个消费者 CC4 和 CS) 。按照 Kafka默认的规则，最后的分配结果是消费组 A 中的每一个消费 者分配到1个分区，消费组 B 中的每一个消费者分配到 2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。换言之 每一个分区只能被一个消费组中的一个消费者所消费.</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608547700645-c525d96a-ac02-471f-9276-ee885e471c86.png" alt="image.png"></p><a id="more"></a> <p>假设目前某消费组内只有一个消费者 co，订阅了一个主题，这个主题包含 7 个分区: PO、 Pl、 P2、 P3、 P4、PS、 P6o 也就是说，这个消费者co订阅了7个分区，具体分配情形参考图3-2。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608547782387-e3cd0cab-f862-465d-bfd7-96c7fa613b21.png" alt="image.png"></p><p>​                                        </p><p>此时消费组内又加入了一个新的消费者 Cl，按照既定的逻辑，需要将原来消费者 co 的部分分区分配给消费者 Cl 消费 ， 如图 3-3 所示 。 消费者 co 和 Cl 各自负责消费所分配到的分区 ，彼此之间并无逻辑上的干扰 </p><p>消费者与消费组这种模型可以让整体的消费能力具备横向伸缩性，我们 可以增加(或减少) 消费者的个数来提高 (或降低〕整体的消费能力 。 对于分区数固定的情况， 一昧地增加消费者并不会让消费能力 一直得到提升，<strong>如果消费者过多，出现了消费者的个数大于分区个数的情况，**</strong>就会有消费者分配不到任何分区**。</p><h3 id="2-两种消息投递模式"><a href="#2-两种消息投递模式" class="headerlink" title="2.两种消息投递模式"></a>2.两种消息投递模式</h3><p>对于消息中间件而言,一般有两种消息投递模式:<strong>点对点</strong>(P2P, Point-to-Point)模式和<strong>发**</strong>布/订阅**( Pub/Sub)模式.</p><p><strong>点对点模式</strong>是基于队列的，消息生产者发送消息到队列，消息消费者从队列中接收消息。</p><p><strong>发布订阅模式</strong>定义了如何向一个内容节点发布和订阅消息,这个内容节点称为主题(Topic),主题可以认为是消息传递的中介,消息发布者将消息发布到某个主题,而消息订阅者从主题中订阅消息.主题使得消息的订阅者和发布者互相保持独立,不需要进行接触即可保证消息的传递,发布/订阅模式在消息的一对多广播时采用.Kafka同时支持两种消息投递模式，而这正是得益于消费者与消费组模型的契合:</p><ul><li>如果所有的消费者都隶属于同一个消费组,那么所有的消息都会被均衡地投递给每一个消费者,即每条消息只会被一个消费者处理,这就相当于点对点模式的应用 。</li><li>如果所有的消费者都隶属于不同的消费组,那么所有的消息都会被广播给所有的消费者,即每条消息会被所有的消费者处理,这就相当于发布/订阅模式的应用.</li></ul><p>消费组是一个逻辑上的概念，它将旗下的消费者归为一类 ，每一个消费者只隶属于一个消费组。每一个消费组都会有一个固定的名称，消费者在进行消费前需要指定其所属消费组的名称，这个可以通过消费者客户端参数 group.id来配置，默认值为空宇符串。</p><p>消费者并非逻辑上的概念它是实际的应用实例它可以是一个线程，也可以是一个进程。同一个消费组内的消费者既可以部署在同一台机器上，也可以部署在不同的机器上。</p><p>​                                        </p><p>​                                        </p><p>​                                                                                </p><p>​                                        </p><p>​                                        </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;3-1-消费者与消费组&quot;&gt;&lt;a href=&quot;#3-1-消费者与消费组&quot; class=&quot;headerlink&quot; title=&quot;3.1 消费者与消费组&quot;&gt;&lt;/a&gt;3.1 消费者与消费组&lt;/h2&gt;&lt;h3 id=&quot;1-消费者和消费组介绍&quot;&gt;&lt;a href=&quot;#1-消费者和消费组介绍&quot; class=&quot;headerlink&quot; title=&quot;1.消费者和消费组介绍&quot;&gt;&lt;/a&gt;1.消费者和消费组介绍&lt;/h3&gt;&lt;p&gt;消费者( Consumer)负责订阅Kafka中的主题( Topic)，并且从订阅的主题上拉取消息.与其他一些消息中间件不同的是:在 Kafka的消费理念中还有一层消费组( Consumer Group)的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者 。&lt;/p&gt;
&lt;p&gt;以下图为例,某个主题中共有 4 个分区( Partition) : PO、 Pl、 P2、 P3。 有两个消费组 A和 B 都订阅了这个主题，消费组 A 中有 4 个消费者 (CO、 Cl、 C2 和 C3)，消费组 B 中有 2个消费者 CC4 和 CS) 。按照 Kafka默认的规则，最后的分配结果是消费组 A 中的每一个消费 者分配到1个分区，消费组 B 中的每一个消费者分配到 2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。换言之 每一个分区只能被一个消费组中的一个消费者所消费.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://cdn.nlark.com/yuque/0/2020/png/2992889/1608547700645-c525d96a-ac02-471f-9276-ee885e471c86.png&quot; alt=&quot;image.png&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="3-消费者" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/3-%E6%B6%88%E8%B4%B9%E8%80%85/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-1.2 生产与消费简单实例</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/1.2%20%E7%94%9F%E4%BA%A7%E4%B8%8E%E6%B6%88%E8%B4%B9%E7%AE%80%E5%8D%95%E5%AE%9E%E4%BE%8B/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/1-概念介绍/1.2 生产与消费简单实例/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:48:34.104Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-2-生产与消费简单实例"><a href="#1-2-生产与消费简单实例" class="headerlink" title="1.2 生产与消费简单实例"></a>1.2 生产与消费简单实例</h1><h3 id="创建topic"><a href="#创建topic" class="headerlink" title="创建topic"></a>创建topic</h3><p>kafka提供了许多实用的脚本工具,存放在$KAFKA_HOME的bin目录下.其中与主题相关的就是kafka-topic.sh脚本.例如.下面创建一个分区数为4,副本为3的主题topic-demon<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-demo --replication-factor 3 --partitions 4</span><br><span class="line"></span><br><span class="line">Created topic <span class="string">"topic-demo"</span>.</span><br></pre></td></tr></table></figure></p><p><code>--zoopkeer</code> 指定kafka连接的zookeeper服务地址<br><code>--topic</code> 指定一个topic主题<br><code>--replication-factor</code>  指定副本因子数量<br><code>--partition</code> 指定分区数量<br><code>--create</code> 表示创建</p><a id="more"></a> <p>下面命令展示了刚创建的主题信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 bin]$ ./kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demoPartitionCount:4ReplicationFactor:3Configs:</span><br><span class="line">Topic: topic-demoPartition: 0Leader: 152Replicas: 152,153,154Isr: 152,153,154</span><br><span class="line">Topic: topic-demoPartition: 1Leader: 153Replicas: 153,154,152Isr: 153,154,152</span><br><span class="line">Topic: topic-demoPartition: 2Leader: 154Replicas: 154,152,153Isr: 154,152,153</span><br><span class="line">Topic: topic-demoPartition: 3Leader: 152Replicas: 152,154,153Isr: 152,154,153</span><br></pre></td></tr></table></figure></p><p>上面的命令结果表示 <code>topic-demon</code> 这个主题一共有4个分区,存放在3台Kafka broker服务器节点.3个broker均是ISR集合,没有OSR集合</p><blockquote><p>在任意一台kafka集群内的节点服务器上执行上述命令,会得到完全相同的结果</p></blockquote><h3 id="创建consumer"><a href="#创建consumer" class="headerlink" title="创建consumer"></a>创建consumer</h3><p><code>kafka-console-consumer.sh</code> 在任意一台kafka集群内的节点服务器上可以通过控制台创建一个 <code>consumer</code> 消费者.示例如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev154 bin]$ ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-demo</span><br></pre></td></tr></table></figure></p><p><code>--bootstrap-server</code> 指定连接的kafka集群地址<br><code>--topic</code> 指定消费者订阅的主题</p><h3 id="创建producer"><a href="#创建producer" class="headerlink" title="创建producer"></a>创建producer</h3><p><code>kafka-console-producer.sh</code> 在任意一台kafka集群内的节点服务器上可以通过控制台创建一个 <code>producer</code> 消费者.示例如下<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev153 bin]$ ./kafka-console-producer.sh --broker-list localhost:9092 --topic topic-demo</span><br></pre></td></tr></table></figure></p><p><code>--broker-list</code> 指定连接的kafka集群地址<br><code>--topic</code> 指定发小时时的主题<br>在弹出的shell终端中,输入 <code>hello world!</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;hello,world!</span><br></pre></td></tr></table></figure></p><p>回到 <code>consumer</code> 的shell终端界面,发现消费到了刚生产的消息:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev154 bin]$ ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic topic-demo</span><br><span class="line">hello,world!</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-2-生产与消费简单实例&quot;&gt;&lt;a href=&quot;#1-2-生产与消费简单实例&quot; class=&quot;headerlink&quot; title=&quot;1.2 生产与消费简单实例&quot;&gt;&lt;/a&gt;1.2 生产与消费简单实例&lt;/h1&gt;&lt;h3 id=&quot;创建topic&quot;&gt;&lt;a href=&quot;#创建topic&quot; class=&quot;headerlink&quot; title=&quot;创建topic&quot;&gt;&lt;/a&gt;创建topic&lt;/h3&gt;&lt;p&gt;kafka提供了许多实用的脚本工具,存放在$KAFKA_HOME的bin目录下.其中与主题相关的就是kafka-topic.sh脚本.例如.下面创建一个分区数为4,副本为3的主题topic-demon&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;./kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-demo --replication-factor 3 --partitions 4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Created topic &lt;span class=&quot;string&quot;&gt;&quot;topic-demo&quot;&lt;/span&gt;.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;--zoopkeer&lt;/code&gt; 指定kafka连接的zookeeper服务地址&lt;br&gt;&lt;code&gt;--topic&lt;/code&gt; 指定一个topic主题&lt;br&gt;&lt;code&gt;--replication-factor&lt;/code&gt;  指定副本因子数量&lt;br&gt;&lt;code&gt;--partition&lt;/code&gt; 指定分区数量&lt;br&gt;&lt;code&gt;--create&lt;/code&gt; 表示创建&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="1-概念介绍" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-1.1基本概念介绍</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/1.1%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/1-概念介绍/1.1 基本概念介绍/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:48:01.269Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-1-基本概念介绍"><a href="#1-1-基本概念介绍" class="headerlink" title="1.1 基本概念介绍"></a>1.1 基本概念介绍</h1><h3 id="kafka特性"><a href="#kafka特性" class="headerlink" title="kafka特性"></a>kafka特性</h3><ul><li><strong>消息系统</strong>: Kafka和传统的消息中间件都具备流量削峰,缓冲,异步通信,扩展性等.另外,Kafka还提供了大多数消息中间件难以实现的消息顺序保障及回溯消费的功能</li><li><strong>存储系统</strong>: 消息持久化到存盘,可以实现永久存储</li><li><strong>流式处理平台</strong>: Kafka提供了流式处理类库</li></ul><a id="more"></a><h3 id="Kafka架构"><a href="#Kafka架构" class="headerlink" title="Kafka架构"></a>Kafka架构</h3><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608447536390-cba7d090-f67a-435c-8d7d-704fb446e573.png" alt="image.png"></p><p>一个Kafka体系主要包括:</p><ul><li>producer: 生产者</li><li>broker: kafka节点服务器</li><li>consumer: 消费者</li><li>zookeeper: 负责管理kafka集群元数据,集群选举等</li></ul><p>producer将消息发送到Broker,Broker负责将受到的消息存储到磁盘中,Consumer负责从Broker订阅并消费消息.</p><h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><ul><li>Kafka 通过 <em>topic</em> 对存储的流数据进行分类。</li><li>每条记录中包含一个key，一个value和一个timestamp（时间戳).</li><li>kafka保留所有的发布记录(无论是否已经被消费过).通过一个可配置的参数—保留期限来控制记录存在时间.</li></ul><blockquote><p>举个例子， 如果保留策略设置为2天，一条记录发布后两天内，可以随时被消费，两天过后这条记录会被抛弃并释放磁盘空间。</p></blockquote><h3 id="kafka核心API"><a href="#kafka核心API" class="headerlink" title="kafka核心API"></a>kafka核心API</h3><ul><li><a href="https://kafka.apachecn.org/documentation.html#producerapi" target="_blank" rel="noopener">Producer API</a> : 允许一个应用程序发布一串流式的数据到一个或者多个Kafka topic。</li><li><a href="https://kafka.apachecn.org/documentation.html#consumerapi" target="_blank" rel="noopener">Consumer API</a>: 允许一个应用程序订阅一个或多个 topic ，并且对发布给他们的流式数据进行处理。</li><li><a href="https://kafka.apachecn.org/documentation/streams" target="_blank" rel="noopener">Streams API</a>: 允许一个应用程序作为一个<em>流处理器</em>，消费一个或者多个topic产生的输入流，然后生产一个输出流到一个或多个topic中去，在输入输出流中进行有效的转换。</li><li><a href="https://kafka.apachecn.org/documentation.html#connect" target="_blank" rel="noopener">Connector API</a>: 允许构建并运行可重用的生产者或者消费者，将Kafka topics连接到已存在的应用程序或者数据系统。比如，连接到一个关系型数据库，捕捉表（table）的所有变更内容。</li></ul><h3 id="理解topics和Partition和offset"><a href="#理解topics和Partition和offset" class="headerlink" title="理解topics和Partition和offset"></a>理解topics和Partition和offset</h3><p><strong>Topic</strong>: 就是数据主题，生产者将消息发送到特点的主题.消费者负责订阅主题并进行消费.</p><p><strong>Partition</strong>: 一个Topic可以划分成多个partition(分区).但是一个分区只属于单个主题.很多时候也会将partition称为主题分区(Topic-Partition).同一个主题下的不同分区包含的消息也不同.分区在存储层面可以看做一个追加的日志(Log)文件.</p><p>一个主题的分区可以在不同的节点服务器上,所有的消息会均匀的分配到不同的分区中(也就是不同的节点服务器),这样可以提高磁盘IO和性能.在创建主题的时候可以设置分区数量,当然也可以在主题创建完成后去修改分区数量.通过增加分区的数量实现水平扩展.</p><p>好比是为公路运输，不同的起始点和目的地需要修不同高速公路（主题），高速公路上可以提供多条车道（分区），流量大的公路多修几条车道保证畅通，流量小的公路少修几条车道避免浪费。收费站好比消费者，车多的时候多开几个一起收费避免堵在路上，车少的时候开几个让汽车并道就好了</p><p>Kafka中的Topics总是多订阅者模式，一个topic可以拥有一个或者多个消费者来订阅它的数据。对于每一个topic， Kafka集群都会维持一个分区日志，如下所示：</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608119181003-5ca1cc98-c0ec-4c00-b7b0-d0b916204ab0.png" alt="image.png"></p><p>每个partition分区都是有序切不可变的记录集.并且不断的追加到结构化的commit log文件.</p><p><strong>Offset</strong>: 消息被存储到分区的日志文件时会分片一个偏移量(offset).offset是消息在分区中的唯一表示.kafka通过它来保障消息在分区内的顺序.</p><p>不过Offset并不跨越分区,也就是说Kafka保证的是分区有序,而不是主题有序.</p><p>在每一个消费者中唯一保存的元数据是offset（偏移量）即消费在log中的位置.偏移量由消费者所控制:通常在读取记录后，消费者会以线性的方式增加偏移量，但是实际上，由于这个位置由消费者控制，所以消费者可以采用任何顺序来消费记录。例如，一个消费者可以重置到一个旧的偏移量，从而重新处理过去的数据；也可以跳过最近的记录，从”现在”开始消费。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608120423518-7cddb09d-e7fa-4f35-809f-908a36b5a4d1.png?x-oss-process=image%2Fresize%2Cw_1500" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-1-基本概念介绍&quot;&gt;&lt;a href=&quot;#1-1-基本概念介绍&quot; class=&quot;headerlink&quot; title=&quot;1.1 基本概念介绍&quot;&gt;&lt;/a&gt;1.1 基本概念介绍&lt;/h1&gt;&lt;h3 id=&quot;kafka特性&quot;&gt;&lt;a href=&quot;#kafka特性&quot; class=&quot;headerlink&quot; title=&quot;kafka特性&quot;&gt;&lt;/a&gt;kafka特性&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;消息系统&lt;/strong&gt;: Kafka和传统的消息中间件都具备流量削峰,缓冲,异步通信,扩展性等.另外,Kafka还提供了大多数消息中间件难以实现的消息顺序保障及回溯消费的功能&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;存储系统&lt;/strong&gt;: 消息持久化到存盘,可以实现永久存储&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;流式处理平台&lt;/strong&gt;: Kafka提供了流式处理类库&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="1-概念介绍" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/1-%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-2.1Kafka副本</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/2-%E5%89%AF%E6%9C%AC%E4%BB%8B%E7%BB%8D/2.1%20%20Kafka%E5%89%AF%E6%9C%AC/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/2-副本介绍/2.1  Kafka副本/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T15:14:57.936Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2-1-Kafka副本"><a href="#2-1-Kafka副本" class="headerlink" title="2.1  Kafka副本"></a>2.1  Kafka副本</h1><h3 id="副本介绍"><a href="#副本介绍" class="headerlink" title="副本介绍"></a>副本介绍</h3><p>Kafka为分区引入了副本(Replica)机制.通过增加副本数量提升容灾能力.一个Topic主题可以有多个分区,一个分区又可以有多个副本.这多个副本中，只有一个是leader，而其他的都是follower副本。仅有leader副本可以对外提供服务。所以副本之间是一主多从的关系,而且每个副本中保存的相同的消息.(严格来说,同一时刻副本之间的消息并非能一定完全同步)</p><p>多个follower副本通常存放在和leader副本不同的broker中。通过这样的机制实现了高可用，当某台机器挂掉后，其他follower副本也能迅速”转正“，开始对外提供服务。</p><a id="more"></a> <p>在kafka中，实现副本的目的就是冗余备份，且仅仅是冗余备份，所有的读写请求都是由leader副本进行处理的。follower副本仅有一个功能，那就是从leader副本拉取消息，尽量让自己跟leader副本的内容一致。</p><blockquote><p>follower副本之所以不能对外提供服务,主要是为了保障数据一致性</p></blockquote><p>下图是一个多副本架构图.</p><p>Kafka集群中有4个broker，某个主题中有3个分区，且副本因子（即副本个数）也为3，如此每个分区便有1个leader副本和2个follower副本。生产者和消费者只与leader副本进行交互，而follower副本只负责消息的同步，很多时候follower副本中的消息相对leader副本而言会有一定的滞后。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608450120303-2d66cd2e-611a-4e3e-a507-53b4f81adfa0.png" alt="image.png"></p><h3 id="副本同步"><a href="#副本同步" class="headerlink" title="副本同步"></a>副本同步</h3><p><strong>AR</strong>: 分区内的所有副本统称.</p><p><strong>ISR</strong>: In-Sync Replicas.所有与Leader副本保持一定程度同步的副本(包括Leader副本).一起组成ISR</p><p><strong>OSR</strong>: Out-of-Sync Replicas: 与leader副本同步滞后过多的副本(不包括leader副本),一起注册呢个OSR</p><p><strong>AR = ISR + OSR.</strong></p><blockquote><p>正常情况下,所有的follower副本都应该与leader副本保持一定程度的同步,即AR = ISR,OSR集合为空</p></blockquote><p>Leader副本负责维护和跟踪ISR集合中所有follower副本的滞后状态,当follower副本落后太多或者失效时,leader副本会把它从ISR集合中剔除,如果OSR的follower副本追上了leader副本,那会从OSR转移到ISR.</p><blockquote><p>默认情况下,只有ISR集合中的follower副本才有资格被选举为新的Leader</p></blockquote><h3 id="HW和LEO"><a href="#HW和LEO" class="headerlink" title="HW和LEO"></a>HW和LEO</h3><p><strong>HW(High Watermark)</strong>: 俗称高水位.它标识了一个特点的消息偏移量(offset).消费者只能拉取这个offset之前的信息.</p><p><strong>LEO(Log End Offset)</strong>: 标识当前日志文件中下一条代写入消息的offset. </p><p><strong></strong></p><p>下面一张图能说明这两个概念</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608450932841-d4f33228-91b2-484f-b28e-24d0762ef670.png" alt="image.png"></p><p>上面的图代表一个日志文件.这个日志文件中有 9 条消息，第一条消息的 offset（LogStartOffset）为0，最后一条消息的offset为8，offset为9的消息用虚线框表示，代表下一条待写入的消息。日志文件的HW为6，表示消费者只能拉取到offset在0至5之间的消息，而offset为6的消息对消费者而言是不可见的。</p><p>offset为9的位置即为当前日志文件的LEO，LEO的大小相当于当前日志分区中最后一条消息的offset值加1。分区ISR集合中的每个副本都会维护自身的LEO，<strong>而ISR集合中最小的LEO即为分区的HW，对消费者而言只能消费HW之前的消息。</strong></p><h3 id="ISR和HW-LEO的关系"><a href="#ISR和HW-LEO的关系" class="headerlink" title="ISR和HW,LEO的关系"></a>ISR和HW,LEO的关系</h3><p>为了让读者更好地理解ISR集合，以及HW和LEO之间的关系，下面通过一个简单的示例来进行相关的说明。如图1-5所示，假设某个分区的ISR集合中有3个副本，即一个leader副本和2个follower副本，此时分区的LEO和HW都为3。消息3和消息4从生产者发出之后会被先存入leader副本</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451481234-2309dd97-7662-49d0-a43a-42b822f7a7d4.png" alt="image.png"></p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451501313-69357f66-f23b-4d07-940f-ef9f0afb9260.png" alt="image.png"></p><p>在同步过程中，不同的 follower 副本的同步效率也不尽相同。如图 所示，在某一时刻follower1完全跟上了leader副本而follower2只同步了消息3，如此leader副本的LEO为5，follower1的LEO为5，follower2的LEO为4，那么当前分区的HW取最小值4，此时消费者可以消费到offset为0至3之间的消息。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451550216-2bc62531-a164-40a1-b039-e9e911401d02.png" alt="image.png"></p><p>如果所有的副本都成功写入了消息3和消息4，整个分区的HW和LEO都变为5，因此消费者可以消费到offset为4的消息了。</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608451625357-266f1693-1d4b-4524-9f64-89256893555e.png" alt="image.png"></p><p>Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的 follower 副本都复制完，这条消息才会被确认为已成功提交，这种复制方式极大地影响了性能。而在异步复制方式下，follower副本异步地从leader副本中复制数据，数据只要被leader副本写入就被认为已经成功提交。在这种情况下，如果follower副本都还没有复制完而落后于leader副本，突然leader副本宕机，则会造成数据丢失。Kafka使用的这种ISR的方式则有效地权衡了数据可靠性和性能之间的关系。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;2-1-Kafka副本&quot;&gt;&lt;a href=&quot;#2-1-Kafka副本&quot; class=&quot;headerlink&quot; title=&quot;2.1  Kafka副本&quot;&gt;&lt;/a&gt;2.1  Kafka副本&lt;/h1&gt;&lt;h3 id=&quot;副本介绍&quot;&gt;&lt;a href=&quot;#副本介绍&quot; class=&quot;headerlink&quot; title=&quot;副本介绍&quot;&gt;&lt;/a&gt;副本介绍&lt;/h3&gt;&lt;p&gt;Kafka为分区引入了副本(Replica)机制.通过增加副本数量提升容灾能力.一个Topic主题可以有多个分区,一个分区又可以有多个副本.这多个副本中，只有一个是leader，而其他的都是follower副本。仅有leader副本可以对外提供服务。所以副本之间是一主多从的关系,而且每个副本中保存的相同的消息.(严格来说,同一时刻副本之间的消息并非能一定完全同步)&lt;/p&gt;
&lt;p&gt;多个follower副本通常存放在和leader副本不同的broker中。通过这样的机制实现了高可用，当某台机器挂掉后，其他follower副本也能迅速”转正“，开始对外提供服务。&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="2-副本介绍" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/2-%E5%89%AF%E6%9C%AC%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-4.1主题管理</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/4.1%E4%B8%BB%E9%A2%98%E7%AE%A1%E7%90%86/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/4-主题和分区/4.1主题管理/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:55:25.320Z</updated>
    
    <content type="html"><![CDATA[<h3 id="4-1-1-创建主题"><a href="#4-1-1-创建主题" class="headerlink" title="4.1.1 创建主题"></a>4.1.1 创建主题</h3><h4 id="4-1-1-1-自动创建主题以及分区副本"><a href="#4-1-1-1-自动创建主题以及分区副本" class="headerlink" title="4.1.1.1 自动创建主题以及分区副本"></a>4.1.1.1 自动创建主题以及分区副本</h4><p>在之前的笔记中提到了创建主题的一个简单示例.kafka提供 <code>kafka-topics.sh</code> 脚本来创建主题.下面这个示例创建了一个 <code>topic-test</code> 的主题,包含4个分区和2个副本.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-test --replication-factor 2 --partitions 4</span><br><span class="line"></span><br><span class="line">Created topic &quot;topic-test&quot;.</span><br></pre></td></tr></table></figure><p>分区创建完成后,会在kafka的 <code>log.dirs</code> 或者 <code>log.dir</code> 的目录下创建相应的主题分区.下面是在其中一台Broker节点的信息展示:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ ls /opt/logs/kafka/ | grep &quot;topic-test&quot;</span><br><span class="line">topic-test-0</span><br><span class="line">topic-test-2</span><br></pre></td></tr></table></figure><p>可以看到152节点中创建了2个文件夹 topic-test-0 和 topic-test-2,对应主题 topic-test的2个分区编号为0和2的分区，命名方式可以概括为 <code>&lt;topic&gt;-&lt;partition&gt;</code> .严谨地说,其实这类文件夹对应的不是分区,分区同主题一样是一个逻辑的概念而没有物理上的存在.并且这里我们也只是看到了2个分区,而我们创建的是4个分区,其余2个分区被分配到了153和154节点中，参考如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#153节点</span><br><span class="line">[hadoop@bi-dev153 ~]$ ls /opt/logs/kafka/ | grep &quot;topic-test&quot;</span><br><span class="line">topic-test-0</span><br><span class="line">topic-test-1</span><br><span class="line">topic-test-3</span><br><span class="line"></span><br><span class="line">#154节点</span><br><span class="line">[hadoop@bi-dev154 ~]$ ls /opt/logs/kafka/ | grep &quot;topic-test&quot;</span><br><span class="line">topic-test-1</span><br><span class="line">topic-test-2</span><br><span class="line">topic-test-3</span><br></pre></td></tr></table></figure><p>三个broker节点一共创建了8个文件夹,这个数字8实质上是分区数4与副本因子2的乘积.每个副本(或者更确切地说应该是日志,副本与日志一一对应)才真正对应 了一个命名形式.</p><a id="more"></a> <p><strong>主题</strong>,<strong>分区,副本和日志</strong>的关系如下图所示.<strong>主题</strong>和<strong>分区</strong>是提供给上层用户的抽象,而在副本层面(或者更确切的说是Log日志层面)才会实际物理存在.</p><p>同一个分区中的多个副本必须分布在不同broker中,并且一个分区副本同时存在多个broker中,这样才能提供有效的数据冗余.上面的示例中,每个副本都分布在至少2台不同的broker中.</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608631181435-40a87763-f28c-45c4-a25e-d85651c26d2e.png" alt="image.png"></p><h4 id="4-1-1-2-手动创建主题以及分区副本"><a href="#4-1-1-2-手动创建主题以及分区副本" class="headerlink" title="4.1.1.2 手动创建主题以及分区副本"></a>4.1.1.2 手动创建主题以及分区副本</h4><p>通过 <code>kafka-topics.sh</code> 脚本创建的主题会按照内部既定逻辑来分配分区和副本到Broker节点上.其实该脚本还提供一个 <code>replica-assignment</code> 参数来手动指定分区副本的分配方案.用法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">格式为: 分区1broker节点1:分区1broker节点2,分区2broker节点1:分区2broker节点2.副本集合用冒号隔开,分区之间用逗号隔开</span><br><span class="line">--replica-assignment broker_id_for_partition1_replica1:broker_id_for_partition1_replica2,broker_id_for_partition2_replica1:broker_id_for_partition2_replica2.......</span><br></pre></td></tr></table></figure><p>例如下面这个实例通过手动方式创建了一个和 <code>topic-test</code> 一样分区副本分配的 <code>topic-test-same</code> 主题.</p><p>下面是刚创建的自动分配的topic-test主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic &quot;topic-test&quot;</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br></pre></td></tr></table></figure><p>通过 <code>--replica-assignment</code> 手动指定分区副本分配情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-test-same --replica-assignment 153:152,154:153,152:154,153:154</span><br></pre></td></tr></table></figure><blockquote><p>–replica-assignment参数其实就是逗号隔开的所有分区的Replicas副本集合.副本集合内部用:冒号隔开</p></blockquote><p>查看 <code>topic-test-same</code> 分区信息.和 <code>topic-test</code> 主题分区副本分配一致</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic &quot;topic-test-same&quot;</span><br><span class="line">Topic:topic-test-same   PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test-same  Partition: 0    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">    Topic: topic-test-same  Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test-same  Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test-same  Partition: 3    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br></pre></td></tr></table></figure><p>手动分配分区副本需要遵循以下原则,否则会报错:</p><ul><li>同一个分区内的副本不能有重复.比如153:153</li><li>分区之间所指定的副本数量要相同.比如153:154,152,154:152</li><li>不能跳过一个分区.比如153:154,,154:152</li></ul><h4 id="4-1-1-3-自定义相关参数"><a href="#4-1-1-3-自定义相关参数" class="headerlink" title="4.1.1.3 自定义相关参数"></a>4.1.1.3 自定义相关参数</h4><p>在创建主题时,还可以通过 <code>config</code> 参数设置要创建主题的相关参数.可以覆盖原本的默认配置参数. <code>config</code> 可以指定多个参数.用法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--config 参数名=值 --config 参数名=值 ......</span><br></pre></td></tr></table></figure><p>下面示例使用 <code>config</code> 参数创建主题 <code>topic-config</code>.并且携带2个参数 :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-config --replication-factor 1 --partitions 1 --config cleanup.policy=compact --config max.message.bytes=10000</span><br><span class="line">Created topic &quot;topic-config&quot;.</span><br></pre></td></tr></table></figure><p>查看主题信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:1    ReplicationFactor:1 Configs:cleanup.policy=compact,max.message.bytes=10000</span><br><span class="line">    Topic: topic-config Partition: 0    Leader: 154 Replicas: 154   Isr: 154</span><br></pre></td></tr></table></figure><p>通过zk也能查看到config信息,config信息保存在 <code>/config/topics/TOPIC_NAME</code> 目录下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] get /config/topics/topic-config</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;config&quot;:&#123;&quot;max.message.bytes&quot;:&quot;10000&quot;,&quot;cleanup.policy&quot;:&quot;compact&quot;&#125;&#125;</span><br></pre></td></tr></table></figure><h4 id="4-1-1-4-总结"><a href="#4-1-1-4-总结" class="headerlink" title="4.1.1.4 总结"></a>4.1.1.4 总结</h4><p>创建主题时需要遵循几个原则</p><ul><li>主题名不能重复,否则会报错.(使用 <code>if-not-exists</code> 参数可以避免出现报错信息,但是不会成功创建一个同名主题)</li><li>主题名不推荐使用__双下划线开头的命名,双下划线开头主题一般看做Kafka的内部主题</li><li>主题名由大小写祖母,数字,点号,下划线,连接线等组成.不能只有特殊符号</li></ul><p><code>kafka-topics.sh</code> 创建主题信息支持以下参数:</p><ul><li><p><code>--create</code> 创建主题</p></li><li><ul><li><code>--replica-assignment</code> 手动创建主题的分区副本分配</li><li><code>--config</code> 手动指定参数</li></ul></li></ul><h3 id="4-1-2-查看主题的分区和副本信息"><a href="#4-1-2-查看主题的分区和副本信息" class="headerlink" title="4.1.2 查看主题的分区和副本信息"></a>4.1.2 查看主题的分区和副本信息</h3><h4 id="4-1-2-1-查看具体某个topic的信息"><a href="#4-1-2-1-查看具体某个topic的信息" class="headerlink" title="4.1.2.1.查看具体某个topic的信息"></a>4.1.2.1.查看具体某个topic的信息</h4><p><code>kafka-topics.sh</code> 脚本提供了 <code>--describe</code> 参数来查看一个topic的信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic &quot;topic-test&quot;</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br></pre></td></tr></table></figure><p>在上面的示例中,命令行提供了以下几个信息:</p><p>一共有3个broker节点:152,153,154</p><p><code>PartitionCount</code> 表示一共有3个分区</p><p><code>ReplicationFactor</code> 副本因子为2</p><p><code>Leader</code> 表示某个分区对应的leader副本在具体的Broker节点</p><p><code>Replicas</code> 表示分区内所有AR副本的集合</p><p><code>Isr</code> 表示ISR副本集合</p><h4 id="4-1-2-2-查看所有topic的信息"><a href="#4-1-2-2-查看所有topic的信息" class="headerlink" title="4.1.2.2 查看所有topic的信息"></a>4.1.2.2 查看所有topic的信息</h4><p>如果 <code>kafka-topics.sh</code> 脚本没有指定具体的 <code>--topic</code> 字段.则会展示所有的topic主题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe | head</span><br><span class="line">Topic:__consumer_offsets    PartitionCount:50   ReplicationFactor:1 Configs:segment.bytes=104857600,cleanup.policy=compact,compression.type=producer</span><br><span class="line">    Topic: __consumer_offsets   Partition: 0    Leader: 153 Replicas: 153   Isr: 153</span><br><span class="line">    Topic: __consumer_offsets   Partition: 1    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: __consumer_offsets   Partition: 2    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">    Topic: __consumer_offsets   Partition: 3    Leader: 153 Replicas: 153   Isr: 153</span><br><span class="line">    Topic: __consumer_offsets   Partition: 4    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: __consumer_offsets   Partition: 5    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">    Topic: __consumer_offsets   Partition: 6    Leader: 153 Replicas: 153   Isr: 153</span><br><span class="line">    Topic: __consumer_offsets   Partition: 7    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: __consumer_offsets   Partition: 8    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">  .....略.......</span><br></pre></td></tr></table></figure><h4 id="4-1-2-3-zookeeper查看topic信息"><a href="#4-1-2-3-zookeeper查看topic信息" class="headerlink" title="4.1.2.3 zookeeper查看topic信息"></a>4.1.2.3 zookeeper查看topic信息</h4><p>zookeeper提供了 <code>zkCli.sh</code> 客户端.使用客户端连接zookeeper:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/zookeeper-3.4.10/bin/zkCli.sh  -server localhost:2181</span><br><span class="line">[zk: localhost:2181(CONNECTED) 0]</span><br></pre></td></tr></table></figure><p>zookeeer的 <code>/brokers/topics</code> 目录下保存了主题的分区副本分片方案.通过查看这个目录即可查看主题的分区和副本信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] get /brokers/topics/topic-test</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:&#123;&quot;2&quot;:[152,154],&quot;1&quot;:[154,153],&quot;3&quot;:[153,154],&quot;0&quot;:[153,152]&#125;&#125;</span><br></pre></td></tr></table></figure><p>如上示例所示, <code>&quot;2&quot;:[152,154]</code> 表示分区2分配了2个副本,分别在152和153这2个broker节点上.</p><h4 id="4-1-2-4-查看kafka集群当前所有主题"><a href="#4-1-2-4-查看kafka集群当前所有主题" class="headerlink" title="4.1.2.4 查看kafka集群当前所有主题"></a>4.1.2.4 查看kafka集群当前所有主题</h4><p><code>--list</code> 参数可以列出当前的所有topic</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --list</span><br><span class="line">__consumer_offsets</span><br><span class="line">delivery_message</span><br><span class="line">example</span><br><span class="line">example1</span><br><span class="line">goods_center</span><br><span class="line">hsq-aliapp</span><br><span class="line">hsq-wxapp</span><br><span class="line">hsq_online_test</span><br><span class="line">monitor_report_app_log</span><br><span class="line">sample_consumer_dlq</span><br><span class="line">tidb_test</span><br><span class="line">topic-config</span><br><span class="line">topic-demo</span><br><span class="line">topic-demo1</span><br><span class="line">topic-test</span><br><span class="line">topic-test-same</span><br></pre></td></tr></table></figure><h4 id="4-1-2-5-查看主题其他详细信息"><a href="#4-1-2-5-查看主题其他详细信息" class="headerlink" title="4.1.2.5 查看主题其他详细信息"></a>4.1.2.5 查看主题其他详细信息</h4><p><code>kafka-topics.sh</code> 脚本的 <code>describe</code> 参数还支持很多额外的指令,用于查看更详细的信息.</p><p>1.<strong><code>--topics-with-overrides</code></strong> 参数表示查看覆盖配置的主题,列出包含了与集群不一样配置的主题.下面列出了 <code>topic-config</code> 这个主题,这个主题使用了 <code>--config</code> 参数创建</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topics-with-overrides</span><br><span class="line"></span><br><span class="line">Topic:topic-config  PartitionCount:1    ReplicationFactor:1 Configs:cleanup.policy=compact,max.message.bytes=10000</span><br></pre></td></tr></table></figure><p>2.<strong><code>--under-replicated-paritions</code></strong> 参数列出包含失效副本的分区.失效副本的分区可能正在进行同步操作,也有可能同步发生异常.此时分区的ISR集合小于AR集合.失效副本的分区是重点监控对象,因为这可能意味着集群中的某个broker已经失效或者同步效率降低等.</p><p>正常情况下此命令不会出现任何信息.例如查看主题 <code>topic-demo</code> 的失效副本信息,但是没有任何输出信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo --under-replicated-partitions</span><br></pre></td></tr></table></figure><p>此时将153这个节点下线.再次查看:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo --under-replicated-partitions</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 152,154</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 154 Replicas: 153,154,152   Isr: 154,152</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 154,152</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 152,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>可以看到Leader和ISR集合中都没有了153这个节点.将153节点上线.此时再次查询,恢复正常.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo --under-replicated-partitions</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>\3. <strong><code>unavailable-partitions</code></strong> 参数可以查看主题中没有leader副本的分区.这些分区已经处于离线状态,对于生产者或者消费者来说不可用.</p><p>同样正常情况下,该命令没有展示任何信息.</p><p>例如,下面的 <code>topic-test</code> 主题有4个分区,每个分区有2个副本.其中分区1和分区3的副本ISR是153和154这2个节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 153 Replicas: 153,152   Isr: 152,153</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152,154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: 153 Replicas: 153,154   Isr: 154,153</span><br></pre></td></tr></table></figure><p>现在停掉153和154这2个节点的kafka进程.使用 <code>unavailable-partitions</code> 参数查看分区信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test --unavailable-partitions</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: -1  Replicas: 154,153   Isr: 154</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: -1  Replicas: 153,154   Isr: 154</span><br><span class="line">  </span><br><span class="line">  [hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test</span><br><span class="line">Topic:topic-test    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-test   Partition: 0    Leader: 152 Replicas: 153,152   Isr: 152</span><br><span class="line">    Topic: topic-test   Partition: 1    Leader: -1  Replicas: 154,153   Isr: 154</span><br><span class="line">    Topic: topic-test   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 152</span><br><span class="line">    Topic: topic-test   Partition: 3    Leader: -1  Replicas: 153,154   Isr: 154</span><br></pre></td></tr></table></figure><p>leader显示为-1,表示没有可用leader</p><p>节点恢复后,再次执行该命令,没有任何显示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-test --unavailable-partitions</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><h4 id="4-1-2-6-总结"><a href="#4-1-2-6-总结" class="headerlink" title="4.1.2.6 总结"></a>4.1.2.6 总结</h4><p><code>kafka-topics.sh</code> 查看主题信息支持以下参数:</p><ul><li><p><code>--describe</code> </p></li><li><ul><li>默认展示所有topic的分区副本信息</li><li><code>--topic TOPIC_NAME</code> 展示具体某个topic主题的分区副本信息</li><li><code>--topics-with-overrides</code> 列出覆盖配置参数的主题</li><li><code>--under-replicated-partitions</code> 列出失效副本的主题分区信息</li><li><code>--unavailable-partitions</code> 列出没有副本的主题分区</li></ul></li><li><p><code>--list</code> 列出kafka集群下的所有topic主题名称</p></li></ul><h3 id="4-1-3-修改主题"><a href="#4-1-3-修改主题" class="headerlink" title="4.1.3 修改主题"></a>4.1.3 修改主题</h3><h4 id="4-1-3-1-修改主题分区数量"><a href="#4-1-3-1-修改主题分区数量" class="headerlink" title="4.1.3.1 修改主题分区数量"></a>4.1.3.1 修改主题分区数量</h4><p>当一个主题被修改后,依然允许我们对其做一定的修改,比如修改分区个数,修改配置等.这个功能就是 <code>kafka-topic.sh</code> 脚本中的 <code>alter</code> 指令提供的.</p><p>以 <code>topic-config</code> 主题为例,该主题下只有一个分区.将分区修改为3:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --partitions 3</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line">Adding partitions succeeded!</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:3    ReplicationFactor:1 Configs:cleanup.policy=compact,max.message.bytes=10000</span><br><span class="line">    Topic: topic-config Partition: 0    Leader: 154 Replicas: 154   Isr: 154</span><br><span class="line">    Topic: topic-config Partition: 1    Leader: 152 Replicas: 152   Isr: 152</span><br><span class="line">    Topic: topic-config Partition: 2    Leader: 153 Replicas: 153   Isr: 153</span><br></pre></td></tr></table></figure><p><code>--partition</code> 参数表示扩展后的分区个数.</p><blockquote><p>注意告警信息.如果主题中的消息包含key(key不为Null)时,根据key计算分区的行为就会受到影响.当分区数为1时,所以key的消息都会发送到这个分区.当分区扩展到3,会根据消息的key来计算区号.原本发往分区0的消息可能会发送到分区1或者2.此外,还会影响既定消息的顺序.</p></blockquote><p>对于基于key计算的主题,不建议修改分区数量.在一开始就设置好分区数量.另外需要注意的是,Kafka不支持减少分区.只能增加不能减少.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --partitions 1</span><br><span class="line">WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affected</span><br><span class="line"></span><br><span class="line">Error while executing topic command : The number of partitions for a topic can only be increased</span><br></pre></td></tr></table></figure><blockquote><p>不支持减少分区主要是考虑到保障kafka的消息可靠性和顺序性,事务性问题.</p></blockquote><p>如果修改一个不存在的主题分区,则会报错.添加 <code>--if-exists</code> 参数会忽略一些异常</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-none-exist  --partitions 3</span><br><span class="line">Error while executing topic command : Topic topic-none-exist does not exist on ZK path localhost:2181</span><br><span class="line"></span><br><span class="line">#使用--if-exists参数,没有报错,但是不会产生任何效果</span><br><span class="line">[hadoop@bi-dev152 ~]$ /opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-none-exist  --if-exists --partitions 3</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><h4 id=""><a href="#" class="headerlink" title=" "></a> </h4><h4 id="4-1-3-2-修改主题配置"><a href="#4-1-3-2-修改主题配置" class="headerlink" title="4.1.3.2 修改主题配置"></a>4.1.3.2 修改主题配置</h4><p>还可以使用 <code>kafka-topics.sh</code> 脚本的 <code>alter</code> 指令修改主题的配置.在创建主题的时候通过 <code>config</code> 参数来设置要创建的主题相关参数.在创建完主题之后,还可以通过 <code>alter</code> 和 <code>config</code> 配合增加或者修改一些配置文件覆盖原有的值</p><p>下面例子演示修改主题 <code>topic-config</code> 的 <code>max.message.bytes</code> 配置.从10000修改到20000</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --config max.message.bytes=20000</span><br><span class="line">WARNING: Altering topic configuration from this script has been deprecated and may be removed in future releases.</span><br><span class="line">         Going forward, please use kafka-configs.sh for this functionality</span><br><span class="line">Updated config for topic &quot;topic-config&quot;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:3    ReplicationFactor:1 Configs:max.message.bytes=20000,cleanup.policy=compact</span><br></pre></td></tr></table></figure><p>通过 <code>alter</code> 也可以删除创建主题时候的自定义配置.使用 <code>--delete-config</code> 参数.下面这个例子中删除了 <code>max.message.bytes</code> 配置.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --alter --topic topic-config --delete-config max.message.bytes</span><br><span class="line">WARNING: Altering topic configuration from this script has been deprecated and may be removed in future releases.</span><br><span class="line">         Going forward, please use kafka-configs.sh for this functionality</span><br><span class="line">Updated config for topic &quot;topic-config&quot;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-config</span><br><span class="line">Topic:topic-config  PartitionCount:3    ReplicationFactor:1 Configs:cleanup.policy=compact</span><br></pre></td></tr></table></figure><blockquote><p>注意.在对config配置进行增删改查时候,都会提示建议使用kafka-configs.sh这个脚本来实现.该脚本的使用方式下面马上讲到</p></blockquote><h3 id="4-1-4-配置管理"><a href="#4-1-4-配置管理" class="headerlink" title="4.1.4 配置管理"></a>4.1.4 配置管理</h3><p><code>kafka-configs.sh</code> 脚本专门用来对配置进行操作.可以在运行状态下动态更改配置.也可以查询主题的相关配置.而且该脚本不仅可以支持主题相关配置修改,还可以修改broker,用户和客户端这3个类型的配置</p><p><code>kafka-configs.sh</code> 脚本使用 <code>entity-type</code> 参数指定操作配置的类型, <code>entity-name</code> 参数指定操作配置的名称.</p><h4 id="4-1-4-1-查询配置"><a href="#4-1-4-1-查询配置" class="headerlink" title="4.1.4.1 查询配置"></a>4.1.4.1 查询配置</h4><p>下面这个例子查看主题 <code>topic-config</code> 的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics --entity-name topic-config</span><br><span class="line">Configs for topic &apos;topic-config&apos; are cleanup.policy=compact</span><br></pre></td></tr></table></figure><p><code>--entity-type</code> 指定查看的实体类型.支持以下几种类型:</p><ul><li>topics</li><li>clients</li><li>users</li><li>brokers</li></ul><p><code>--entity-name</code> 配置的实体名称:</p><ul><li>topic name (主题名称)</li><li>client id (客户端ID)</li><li>user principal name (用户名)</li><li>broker id (kafka节点ID)</li></ul><p>如果不指定 <code>--entity-name</code> 参数则会查询所有的 <code>entity-type</code> 对应的所有配置信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics</span><br><span class="line">Configs for topic &apos;topic-config&apos; are</span><br><span class="line">Configs for topic &apos;__consumer_offsets&apos; are segment.bytes=104857600,cleanup.policy=compact,compression.type=producer</span><br><span class="line">......</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>通过zookeeper也可以查询主题的配置信息.路径为 <code>/config/topics/TOPIC_NAME</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] get /config/topics/topic-config</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;config&quot;:&#123;&quot;cleanup.policy&quot;:&quot;compact&quot;,&quot;max.message.bytes&quot;:&quot;20000&quot;&#125;&#125;</span><br></pre></td></tr></table></figure><h4 id="4-1-4-2-修改配置"><a href="#4-1-4-2-修改配置" class="headerlink" title="4.1.4.2 修改配置"></a>4.1.4.2 修改配置</h4><p>使用 <code>alter</code> 对配置进行变更.需要配合 <code>add-config</code> 或者 <code>delete-config</code> 这2个参数一起使用.</p><p><code>add-config</code> 参数实现配置的增,改</p><p>下面的例子中,为主题 <code>topic-config</code> 添加 <code>max.message.bytes</code> 参数配置和 <code>cleanup.policy</code> 参数配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name topic-config --add-config cleanup.policy=compact,max.message.bytes=20000</span><br><span class="line">Completed Updating config for entity: topic &apos;topic-config&apos;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics --entity-name topic-config</span><br><span class="line">Configs for topic &apos;topic-config&apos; are cleanup.policy=compact,max.message.bytes=20000</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p><code>delete-config</code> 参数可以实现配置删除.</p><p>下面的例子中,删除上面的2个配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name topic-config --delete-config cleanup.policy,max.message.bytes</span><br><span class="line">Completed Updating config for entity: topic &apos;topic-config&apos;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics --entity-name topic-config</span><br><span class="line">Configs for topic &apos;topic-config&apos; are</span><br></pre></td></tr></table></figure><h3 id="4-1-5-删除主题"><a href="#4-1-5-删除主题" class="headerlink" title="4.1.5 删除主题"></a>4.1.5 删除主题</h3><p>如果确定不再使用一个主题,那么最好的方式是将其删除.这样可以释放一些资源,比如磁盘,文件句柄等. <code>kafka-topics.sh</code> 脚本中的 <code>delete</code> 命令可以用来删除主题.比如下面删除主题 <code>topic-demo1</code> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --delete --topic topic-demo1</span><br><span class="line">Topic topic-demo1 is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br></pre></td></tr></table></figure><blockquote><p>注意.必须将kafka服务器配置文件的delete.topic.enable选项设置为true才能删除.这个参数的默认值是false.删除主题的操作会被忽略.主题并没有被删除</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --list | grep topic-demo1</span><br><span class="line">topic-demo1</span><br></pre></td></tr></table></figure><p>编辑配置文件 <code>/opt/kafka/config/server.properties</code> 修改下面的参数为true</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Switch to enable topic deletion or not, default value is false</span><br><span class="line">delete.topic.enable=true</span><br></pre></td></tr></table></figure><p>如果删除一个kafka的内部主题,那么会报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --delete --topic __consumer_offsets</span><br><span class="line">Error while executing topic command : Topic __consumer_offsets is a kafka internal topic and is not allowed to be marked for deletion.</span><br></pre></td></tr></table></figure><p>删除一个不存在的主题也会报错,此时可以通过 <code>if-exists</code> 参数来忽略异常.</p><h3 id="4-1-5-总结"><a href="#4-1-5-总结" class="headerlink" title="4.1.5 总结"></a>4.1.5 总结</h3><p>下面这张图是 <code>kafka-topics.sh</code> 脚本的常用参数</p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608805092662-042718a3-9a6a-489e-a987-db4e38217171.png" alt="image.png"></p><p><img src="https://cdn.nlark.com/yuque/0/2020/png/2992889/1608805117734-2bd94456-e966-41b1-9465-dde20a8a9129.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;4-1-1-创建主题&quot;&gt;&lt;a href=&quot;#4-1-1-创建主题&quot; class=&quot;headerlink&quot; title=&quot;4.1.1 创建主题&quot;&gt;&lt;/a&gt;4.1.1 创建主题&lt;/h3&gt;&lt;h4 id=&quot;4-1-1-1-自动创建主题以及分区副本&quot;&gt;&lt;a href=&quot;#4-1-1-1-自动创建主题以及分区副本&quot; class=&quot;headerlink&quot; title=&quot;4.1.1.1 自动创建主题以及分区副本&quot;&gt;&lt;/a&gt;4.1.1.1 自动创建主题以及分区副本&lt;/h4&gt;&lt;p&gt;在之前的笔记中提到了创建主题的一个简单示例.kafka提供 &lt;code&gt;kafka-topics.sh&lt;/code&gt; 脚本来创建主题.下面这个示例创建了一个 &lt;code&gt;topic-test&lt;/code&gt; 的主题,包含4个分区和2个副本.&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;/opt/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-test --replication-factor 2 --partitions 4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Created topic &amp;quot;topic-test&amp;quot;.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;分区创建完成后,会在kafka的 &lt;code&gt;log.dirs&lt;/code&gt; 或者 &lt;code&gt;log.dir&lt;/code&gt; 的目录下创建相应的主题分区.下面是在其中一台Broker节点的信息展示:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev152 ~]$ ls /opt/logs/kafka/ | grep &amp;quot;topic-test&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;可以看到152节点中创建了2个文件夹 topic-test-0 和 topic-test-2,对应主题 topic-test的2个分区编号为0和2的分区，命名方式可以概括为 &lt;code&gt;&amp;lt;topic&amp;gt;-&amp;lt;partition&amp;gt;&lt;/code&gt; .严谨地说,其实这类文件夹对应的不是分区,分区同主题一样是一个逻辑的概念而没有物理上的存在.并且这里我们也只是看到了2个分区,而我们创建的是4个分区,其余2个分区被分配到了153和154节点中，参考如下:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;#153节点&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev153 ~]$ ls /opt/logs/kafka/ | grep &amp;quot;topic-test&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#154节点&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev154 ~]$ ls /opt/logs/kafka/ | grep &amp;quot;topic-test&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;topic-test-3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;三个broker节点一共创建了8个文件夹,这个数字8实质上是分区数4与副本因子2的乘积.每个副本(或者更确切地说应该是日志,副本与日志一一对应)才真正对应 了一个命名形式.&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="4-主题和分区" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>kafka-4.2分区管理</title>
    <link href="https://jesse.top/2021/01/05/Linux-%E5%88%86%E5%B8%83%E5%BC%8F&amp;%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/4.2%E5%88%86%E5%8C%BA%E7%AE%A1%E7%90%86/"/>
    <id>https://jesse.top/2021/01/05/Linux-分布式&amp;消息队列/kafka/4-主题和分区/4.2分区管理/</id>
    <published>2021-01-05T09:59:58.000Z</published>
    <updated>2021-01-05T14:54:58.225Z</updated>
    
    <content type="html"><![CDATA[<h3 id="4-2-1-优选副本的选举"><a href="#4-2-1-优选副本的选举" class="headerlink" title="4.2.1 优选副本的选举"></a>4.2.1 优选副本的选举</h3><h4 id="4-2-1-1-什么是优先副本"><a href="#4-2-1-1-什么是优先副本" class="headerlink" title="4.2.1.1 什么是优先副本"></a>4.2.1.1 什么是优先副本</h4><p>分区使用多副本机制来提升可靠性,但是只有leader副本对外提供读写服务.而follower副本只负责在内部进行消息的同步.如果一个分区的leader副本不可用,那么就意味着整个分区变得不可用.此时就需要从剩余的follower副本中挑选一个新的leader副本继续对外提供服务.</p><blockquote><p>broker节点中的Leader副本个数决定了这个节点负载的高低</p></blockquote><p>在创建主题的时候,主题的分区和副本会尽可能的均匀分布在kafka集群的各个broker节点.对应的Leader副本的分配也比较均匀.例如下面的 <code>topic-demo</code> 主题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 152,153,154</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 152,153,154</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 152,153,154</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 152,153,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><a id="more"></a> <p>可以看到,leader副本均匀分布在所有的broker节点.另外,同一个分区,在同一台broker节点只能存在一个副本.所以leader副本所在的broker节点叫做分区的leader节点.而follower副本所在的broker节点叫做分区的follower节点.</p><p>可以想象的是,随着时间的推移,kafka集群中不可避免的出现节点宕机或者崩溃的情况.当分区的Leader节点发生故障时,其中一个follower节点就会成为新的Leader节点.这样导致集群中的节点之间负载不均衡,从而影响kafka整个集群的稳定性和健壮性.</p><p>即使原来的Leader节点恢复后,加入到集群时,也只能成为一个新的follower节点,而不会自动”抢班夺权”变成leader.</p><p>例如刚才的 <code>topic-demo</code> 分区重启152节点后,leader分布如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 153 Replicas: 152,153,154   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 154 Replicas: 152,154,153   Isr: 153,154,152</span><br></pre></td></tr></table></figure><p>尽管kafka非常均匀的将leader副本分布在其他另外2个几点.但是此时152节点的负载几乎为零.</p><p>为了有效的治理负载失衡的情况,kafka引入了<strong>优先副本(preferred replica)</strong>的概念.所谓的优先副本就是在AR集合列表中的第一个副本为优先副本,理想情况下优先副本就是该分区的leader副本.所以也可以称之为 <code>preferred leader</code> .</p><p>比如上面的例子中,分区0的AR集合(Replicas)是[152,153,154].那么分区0的优先副本就是152.<strong>Kafka会确保所有主题的优先副本均匀分布.这样就保证了所有分区的leader均衡分布.</strong></p><p><strong></strong></p><h4 id="4-2-1-2-优先副本选举"><a href="#4-2-1-2-优先副本选举" class="headerlink" title="4.2.1.2 优先副本选举"></a>4.2.1.2 优先副本选举</h4><p>所谓的优先副本选举是指通过一定的方式促使优先副本选举为Leader副本,促进集群的负载均衡.这一行为也称之为”分区平衡”.</p><p>kafka broker端(server.properties配置文件)有个 <code>auto.leader.rebalance.enble</code> 参数.默认为true.也就是分区自动平衡功能.Kafka会启动一个定时任务,轮询所有的broker节点,自动执行优先副本选举动作.</p><p>不过在生产环境中建议将该配置设置为 <code>false</code> .因为kafka自动平衡分区可能在某些关键高分期时刻引起负面性能问题.也有可能引起客户端的阻塞.为了防止出现此类情况,建议针对副本不均衡的问题进行相应监控和告警,然后在合适的时间通过手动来执行分区平衡.</p><p>Kafka中的 <code>kafka-preferred-replica-election.sh</code> 脚本提供了对分区leader副本进行重新平衡的功能.优先副本选举过程是一个安全的过程,kafka客户端会自动感知leader副本的变更.</p><p>命令用法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181</span><br></pre></td></tr></table></figure><p>但是这样一来会对kafka集群的所有主题和分区都执行一遍优先副本的选举操作.如果集群中包含大量的分区,那么可能选举会失败,并且会对性能造成一定的应用.比较建议的是使用 <code>path-to-json-file</code> 参数来小批量的对部分指定的主题分区进行优先副本的选举操作.该参数指定一个JSON文件,这个JSON文件保存需要执行优先副本选举的分区清单.</p><p>举个例子,对上面的 <code>topic-demo</code> 分区进行优先副本选举操作.先创建一个JSON文件,文件名可以任意:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">  &quot;partitions&quot;: [</span><br><span class="line">    &#123; </span><br><span class="line">        &quot;partition&quot;:0,</span><br><span class="line">        &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;partition&quot;:1,</span><br><span class="line">        &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;   &quot;partition&quot;:2,</span><br><span class="line">            &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;   &quot;partition&quot;:3,</span><br><span class="line">            &quot;topic&quot;:&quot;topic-demo&quot;</span><br><span class="line">    &#125;</span><br><span class="line">   ]</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>将上述内容保存为 <code>election.json</code> 文件.然后执行下列命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-preferred-replica-election.sh --zookeeper localhost:2181 --path-to-json-file ~/election.json</span><br><span class="line"> </span><br><span class="line">Created preferred replica election path with &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:0&#125;,&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:1&#125;,&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:2&#125;,&#123;&quot;topic&quot;:&quot;topic-demo&quot;,&quot;partition&quot;:3&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic-demo,0], [topic-demo,1], [topic-demo,2], [topic-demo,3])</span><br></pre></td></tr></table></figure><p>提示优先副本选举成功.下列结果显示leader副本已经均衡分配到所有Broker节点了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo</span><br><span class="line">Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 153,154,152</span><br><span class="line">    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 153,154,152</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>在实际生产环境中,建议使用这种方式来分批的执行优先副本选举操作.杜绝直接粗暴的进行所有分区的优先副本选举.另外,这类操作也应该需要避开业务高峰期,以免对性能造成负面影响,或者出现意外故障.</p><h3 id="4-2-2-分区重分配"><a href="#4-2-2-分区重分配" class="headerlink" title="4.2.2 分区重分配"></a>4.2.2 分区重分配</h3><p>当集群中一个Broker节点宕机,该节点的所有副本都处于丢失状态.kafka并不会自动将这些失效的分区副本自动迁移到集群其他broker节点.另外当集群中新增一台Broker节点时,只有新创建的主题分区才能被分配到这个节点上,而之前的主题分区并不会自动的加入到新节点(因为在创建时,并没有这个节点).这就导致新节点负载和原有节点负载之间严重不均衡.</p><p>为了解决这些问题,需要让分区副本再次进行合理的分配.也就是所谓的分区重分配.kafka提供了 <code>kafka-reassign-paritions.sh</code> 脚本执行分区重分配的工作.可以在集群节点失效或者扩容时使用.使用需要3个步骤:</p><ul><li>创建一个包含主题清单的JSON文件</li><li>根据主题清单和Broker节点清单生成一份重分配方案</li><li>执行具体重分配工作</li></ul><blockquote><p>要执行分区重分配,前提是broker节点清单数量要大于或者等于副本因子数量,否则会报错</p><p>Partitions reassignment failed due to replication factor: 3 larger than available brokers: 2</p></blockquote><p>下面创建一个4分区,2个副本因子的主题 <code>topic-reassign</code> 举例.假定要将152这个broker节点下线.下线之前需要将该节点上的分区副本迁移出去.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --create --topic topic-reassign --replication-factor 2 --partitions 4</span><br><span class="line">Created topic &quot;topic-reassign&quot;.</span><br><span class="line"></span><br><span class="line">[hadoop@bi-dev152 ~]$  kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,153   Isr: 152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 153 Replicas: 153,152   Isr: 153,152</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>第一步,创建一个JSON文件(文件名假定为reassign.json).文件内容是主题清单:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123; </span><br><span class="line">    &quot;topics&quot;:[</span><br><span class="line">      &#123; </span><br><span class="line">                &quot;topic&quot;:&quot;topic-reassign&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;version&quot;:1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第二步,根据这个JSON文件和指定要分配的broker节点列表生成一份候选重分配方案:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --generate --topics-to-move-json-file ~/reassign.json --broker-list 153,154</span><br><span class="line"> </span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[153,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,153]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file project.json</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[153,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,153]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>在上面的例子中有以下几个参数:</p><p><code>--zookeeper</code> 这个参数已经非常熟悉了</p><p><code>--generate</code> 指令类型参数,类似于kafka-topics.sh脚本中的 <code>--create</code> , <code>--list</code> . <code>--describe</code> 等</p><p><code>--topics-to-move-json-file</code> 指定主题清单文件路径</p><p><code>--broker-list</code> 指定要分配的broker节点列表</p><p>上面的例子中打印了2个JSON格式内容:</p><p><code>Current partition replica assignment</code> 表示目前的分区副本分配情况,在执行分区重分配前最好备份这个内容,以便后续回滚操作</p><p><code>Proposed partition reassignment configuration</code> 表示候选重分配方案.这里只是一个方案,并没有真正执行.</p><p>将第二个Json内容格式化输出后,我们发现这个方案正如我们计划的那样,将该主题的所有分区下的AR副本集合分配到153和154节点,所有副本已经从即将要下线的152节点迁移走.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;:1,</span><br><span class="line">    &quot;partitions&quot;:[</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:1,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                153</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:0,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                153,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:3,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                153</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:2,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                153,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第三步,将 <code>Proposed partition reassignment configuration</code> JSON文件内容保存在一个文件中(假定为project.json).然后执行具体的重分配的动作,命令如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file project.json</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[153,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,153]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><blockquote><p>这里仍然打印了之前的副本分配方案,并且提示保存到JSON文件,以便回滚</p></blockquote><p>这里使用了2个不同的命令参数:</p><ul><li><code>--execute</code> 指令类型参数,执行重分配动作</li><li><code>--reassignment-json-file</code> 指定重分配方案文件路径</li></ul><p>再次查看 <code>topic-reassign</code> 主题分区副本分配情况,所有的副本都从152迁移出去,此时该节点可以顺利下线</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 153 Replicas: 154,153   Isr: 153,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>当然,我们也可以直接编写第二个JSON文件来自定义重分配方案,这样就不需要执行上面的第一步和第二步操作了.</p><p>分区重分配的基本原理是为每个分区添加新副本(增加副本数量),新副本会从leader副本复制所有的数据.复制完成后,控制器将旧副本从副本清单里移除.(恢复成原来的副本数量).</p><blockquote><p>所以,分区重分配需要确保有足够的空间,并且避免在业务高峰期操作</p></blockquote><p>从刚才的主题分区结果可以看到,大部分的分区leader副本都集中在153这个broker节点.这样负载非常不均衡,我们可以继续借助 <code>kafka-preferred-replica-election.sh</code> 脚本执行一次优先副本选举.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-preferred-replica-election.sh --zookeeper localhost:2181 --path-to-json-file election.json</span><br><span class="line"></span><br><span class="line">Created preferred replica election path with &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic-reassign,0], [topic-reassign,1], [topic-reassign,2], [topic-reassign,3])</span><br><span class="line"></span><br><span class="line">#选举完成后,副本分配均衡</span><br><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,153   Isr: 154,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 153 Replicas: 153,154   Isr: 153,154</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 154 Replicas: 154,153   Isr: 153,154</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><blockquote><p>和优先副本选举一样,分区重分配对集群的性能有很大的影响.需要占用额外的磁盘,网络IO等资源.在生产环境执行操作时应该分批次执行.</p></blockquote><h3 id="4-2-3-复制限流"><a href="#4-2-3-复制限流" class="headerlink" title="4.2.3 复制限流"></a>4.2.3 复制限流</h3><p>我们了解分区重分配的本质在于数据复制,先增加新副本,进行数据同步,然后删除旧副本.如果副本数据量太大必然会占用很多额外的资源,从而影响集群整体性能.kafka有限流机制,可以对副本之间的复制流量进行限制.</p><p>副本复制限流有2种实现方式:</p><ul><li><code>kafka-config.sh</code> </li><li><code>kafka-reassign-partitions.sh</code> </li></ul><p>前者的实现方式有点繁琐,这里介绍后者的使用方式.</p><p><code>kafka-reassign-partitions.sh</code> 的实现方式非常简单,只需要一个throttle参数即可.例如上面的例子中副本都在153和154节点,现在继续使用分区重分配,让副本从153节点迁移到152节点.但是这次使用限流工具</p><p>首先,修改 <code>project.json</code> 文件,将153替换成152</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &apos;s/153/152/g&apos; project.json</span><br></pre></td></tr></table></figure><p>然后,执行分区重分配,这里使用 <code>--throttle</code> 参数,指定一个限流速度(单位是B/s)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file project.json --throttle 10</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,153]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[153,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[154,153]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[153,154]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Warning: You must run Verify periodically, until the reassignment completes, to ensure the throttle is removed. You can also alter the throttle by rerunning the Execute command passing a new value.</span><br><span class="line">The throttle limit was set to 10 B/s</span><br><span class="line">Successfully started reassignment of partitions.</span><br></pre></td></tr></table></figure><p>上面的示例输出中提示以下3点信息:</p><ol><li>需要周期性的使用 <code>--verify</code> 参数来周期性的查看副本复制进度,直到分区重分配完成,也就是说需要显示的使用这种方式确保分区重分配完成后解除限流的设置</li><li>限流的速度为10B/s</li><li>如果想要修改限流速度,重复此条执行命令,修改throttle的值即可</li></ol><p>接下来使用 <code>--verify</code> 参数查看复制进度.下面的示例显示复制已经完成,并且限流已被解除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --verify --reassignment-json-file project.json</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition [topic-reassign,1] completed successfully</span><br><span class="line">Reassignment of partition [topic-reassign,0] completed successfully</span><br><span class="line">Reassignment of partition [topic-reassign,3] completed successfully</span><br><span class="line">Reassignment of partition [topic-reassign,2] completed successfully</span><br><span class="line">Throttle was removed.</span><br></pre></td></tr></table></figure><p>此时153的副本已经被移除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics --describe --topic topic-reassign</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:2 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 152 Replicas: 152,154   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,154   Isr: 154,152</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 154 Replicas: 154,152   Isr: 154,152</span><br></pre></td></tr></table></figure><h3 id="4-2-4-修改副本因子"><a href="#4-2-4-修改副本因子" class="headerlink" title="4.2.4 修改副本因子"></a>4.2.4 修改副本因子</h3><p>上面的例子中分区重分配,将副本从一个broker节点中移除,此时kafka集群的broker节点数量只剩下2个.副本因子也只有2个.这里有个问题,此时153节点重启,或者新增broker节点后,如何将新增的broker节点加入进群,扩展副本数量呢?或者还有一种情况,当创建主题和分区后,想要修改副本因子呢?</p><p><code>kafka-reassign-parition.sh</code> 脚本同样实现了修改副本因子的功能..仔细观察一下分区重分配案例中的 <code>project.json</code> 文件内容:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ cat project.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;version&quot;:1,</span><br><span class="line">    &quot;partitions&quot;:[</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:1,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                152</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:0,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                152,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:3,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                152</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:2,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                152,</span><br><span class="line">                154</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>json文件中的副本集合(replicas)都是2个副本,我们可以很简单的添加一个副本.比如对于分区0而言,可以将153节点添加进去.(其他分区也是如此)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">            &quot;topic&quot;:&quot;topic-reassign&quot;,</span><br><span class="line">            &quot;partition&quot;:1,</span><br><span class="line">            &quot;replicas&quot;:[</span><br><span class="line">                154,</span><br><span class="line">                152,</span><br><span class="line">                153</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br></pre></td></tr></table></figure><p>执行 <code>kafka-reassign-partition.sh</code> 脚本,执行命令的方法和参数和分区重分片几乎一致:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-reassign-partitions.sh --zookeeper localhost:2181 --execute --reassignment-json-file add.json</span><br><span class="line">Current partition replica assignment</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[152,154]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[154,152]&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[152,154]&#125;]&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions.</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p>查看副本分配情况.副本数量已经增加到了3个</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,154,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 154 Replicas: 153,154,152   Isr: 154,152,153</span><br></pre></td></tr></table></figure><blockquote><p>虽然副本因子增加到3个,但是Leader还是没有分配到新的153这个broker节点.此时可以通过优先副本选举重新分配</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-preferred-replica-election.sh --zookeeper localhost:2181 --path-to-json-file election.json</span><br><span class="line">Created preferred replica election path with &#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:0&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:1&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:2&#125;,&#123;&quot;topic&quot;:&quot;topic-reassign&quot;,&quot;partition&quot;:3&#125;]&#125;</span><br><span class="line">Successfully started preferred replica election for partitions Set([topic-reassign,0], [topic-reassign,1], [topic-reassign,2], [topic-reassign,3])</span><br><span class="line"></span><br><span class="line">#再次查看topic-reassign主题,分区副本分配均衡</span><br><span class="line">Topic:topic-reassign    PartitionCount:4    ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: topic-reassign   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 1    Leader: 154 Replicas: 154,152,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 2    Leader: 152 Replicas: 152,154,153   Isr: 154,152,153</span><br><span class="line">    Topic: topic-reassign   Partition: 3    Leader: 153 Replicas: 153,154,152   Isr: 154,152,153</span><br></pre></td></tr></table></figure><p>重点: <strong>与修改分区数量不同,副本数还可以减少</strong>,修改方法和命令几乎一样,只需要编辑JSON配置文件即可.这里就不再演示</p><h3 id="4-2-5-如何选择合适的分区数量"><a href="#4-2-5-如何选择合适的分区数量" class="headerlink" title="4.2.5 如何选择合适的分区数量"></a>4.2.5 如何选择合适的分区数量</h3><p>如何选择合适的分区数量是需要经常面对的问题,但是这个问题似乎并没有权威的标准答案,需要根据实际的业务场景,硬件资源,应用软件,负载等情况做具体考量.这一章节主要介绍与本问题相关的一些决策因素,以供参考</p><h4 id="4-2-5-1-性能测试工具"><a href="#4-2-5-1-性能测试工具" class="headerlink" title="4.2.5.1 性能测试工具"></a>4.2.5.1 性能测试工具</h4><p>在生产环境中设定分区数量需要考虑性能因素.所以性能测试工具必不可少,kafka本身提供了用于生产者性能测试的 <code>kafka-producer-pref-test.sh</code> 脚本和用于消费者性能测试的 <code>kafka-consumer-perf-test.sh</code>脚本</p><p>首先创建一个用于测试的分区为1,副本为1的 <code>topic-1</code> 的主题:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Topic:topic-1   PartitionCount:1    ReplicationFactor:1 Configs:</span><br><span class="line">    Topic: topic-1  Partition: 0    Leader: 153 Replicas: 153   Isr: 153</span><br></pre></td></tr></table></figure><p>其次.我们往这个主题发送100万条消息,并且每条消息大小为1024B,生产者对应的acks参数为1:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-producer-perf-test.sh --topic topic-1 --num-records 1000000 --record-size 1024 --throughput -1 --producer-props bootstrap.servers=localhost:9092 acks=1</span><br><span class="line">76666 records sent, 15333.2 records/sec (14.97 MB/sec), 1517.5 ms avg latency, 2061.0 max latency.</span><br><span class="line">119400 records sent, 23880.0 records/sec (23.32 MB/sec), 1353.6 ms avg latency, 1631.0 max latency.</span><br><span class="line">124560 records sent, 24912.0 records/sec (24.33 MB/sec), 1231.2 ms avg latency, 1375.0 max latency.</span><br><span class="line">146520 records sent, 29304.0 records/sec (28.62 MB/sec), 1066.6 ms avg latency, 1146.0 max latency.</span><br><span class="line">156795 records sent, 31359.0 records/sec (30.62 MB/sec), 972.3 ms avg latency, 1051.0 max latency.</span><br><span class="line">133365 records sent, 26673.0 records/sec (26.05 MB/sec), 1141.1 ms avg latency, 1322.0 max latency.</span><br><span class="line">159945 records sent, 31989.0 records/sec (31.24 MB/sec), 964.3 ms avg latency, 1178.0 max latency.</span><br><span class="line">1000000 records sent, 26148.576210 records/sec (25.54 MB/sec), 1143.54 ms avg latency, 2061.00 ms max latency, 1114 ms 50th, 1654 ms 95th, 1869 ms 99th, 2036 ms 99.9th.</span><br></pre></td></tr></table></figure><p>示例中使用了多个参数:</p><p><code>num-records</code>:  指定发送消息的总条数</p><p><code>record-size</code>: 设置每条消息的字节数</p><p><code>throughtput</code> : 限流控制,-1表示不限流,大于0表示限流值</p><p><code>producer-props</code> : 指定生产者的配置,可以同时指定多组配置</p><p>除此之外还有其他参数,比如 <code>print-metrics</code> 在测试完成之后,打印很多指标信息.有兴趣可以执行 <code>--help</code> 查看更多参数信息.</p><p>回过头再看看上面示例中的压测结果信息,以第一条和最后一条为例:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">76666 records sent, 15333.2 records/sec (14.97 MB/sec), 1517.5 ms avg latency, 2061.0 max latency.</span><br><span class="line">1114 ms 50th, 1654 ms 95th, 1869 ms 99th, 2036 ms 99.9th</span><br></pre></td></tr></table></figure><p><strong>records sent: 表示发送的消息综述</strong></p><p><strong>records/sec: 吞吐量,表示每秒发送的消息数量</strong></p><p><strong>MB/sec: 吞吐量,表示每秒发送的消息大小</strong></p><p><strong>avg latency: 表示消息处理的平均耗时</strong></p><p><strong>max latency: 表示消息处理的最大耗时</strong></p><p><strong>50th,95th,99th,99.th</strong> 表示50%,95%,99%,99.9%时消息处理耗时</p><p><strong></strong></p><p>消费者压测工具的脚本使用也比较简单,下面的简单实例演示了消费主题 <code>topic-1</code> 中的100万条消息.命令使用方法:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-consumer-perf-test.sh --topic topic-1 --messages 1000000 --broker-list localhost:9092</span><br><span class="line">start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec</span><br><span class="line">2020-12-26 14:07:29:612, 2020-12-26 14:07:35:272, 977.0264, 172.6195, 1000475, 176762.3675</span><br><span class="line">[hadoop@bi-dev152 ~]$</span><br></pre></td></tr></table></figure><p><strong>data.consumed.in.MB: 消费的消息总量,单位为MB</strong></p><p><strong>MB.sec</strong>: 按字节大小计算的消费吞吐量(单位:MB/s)</p><p><strong>data.consumed.in.nMsg</strong>: 消费的消息消息总数</p><p><strong>nMsg.sec</strong>: 按消息个数计算的吞独量(单位n/s)</p><blockquote><p>可以创建多个分区,比如10,100,200,500等(副本数量都为1)来测试生产和消费的性能表现,</p></blockquote><h4 id="4-2-5-2-分区数量越多不代表吞独量越高"><a href="#4-2-5-2-分区数量越多不代表吞独量越高" class="headerlink" title="4.2.5.2 分区数量越多不代表吞独量越高"></a>4.2.5.2 分区数量越多不代表吞独量越高</h4><p>消息中间件的性能一般是指吞吐量(还包括延迟),吞吐量会受到硬件资源,消息大小,消息压缩,消息发送方式(同步,异步),副本因子等参数影响.分区数量越多不一定吞吐量越高,超过一定的临界值后,kafka的吞吐量会不升反降.</p><h4 id="4-2-5-3-分区数量的上限"><a href="#4-2-5-3-分区数量的上限" class="headerlink" title="4.2.5.3 分区数量的上限"></a>4.2.5.3 分区数量的上限</h4><p>一味的增加分区数量并不能使吞吐量得到提升,并且分区的数量也不能一直增加,如果超过一定的临界值还会引起kafka进程的崩溃.</p><p><strong>每次创建一个分区,都会消耗一个Linux系统的文件描述符.</strong></p><p>通过kafka的pid编号,可以查看当前kafka进程占用的文件描述符数量:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ ls /proc/22858/fd/ | wc -l</span><br><span class="line">173</span><br></pre></td></tr></table></figure><p>此时创建一个分区数量为400个的topic-demo4的主题.由于分区会平均创建在集群内的3个broker节点,所以需要统计一下152这个本地节点的分区数量.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ kafka-topics --describe --topic topic-demo4 | grep -Eo &quot;Leader:\s[0-9]+&quot; | sort | uniq -c</span><br><span class="line">    134 Leader: 152</span><br><span class="line">    133 Leader: 153</span><br><span class="line">    133 Leader: 154</span><br></pre></td></tr></table></figure><p>可以看到152这个节点创建了134个分区.接下来看看系统文件描述符的数量.正好增加了134个文件描述符</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@bi-dev152 ~]$ ls /proc/22858/fd/ | wc -l</span><br><span class="line">307</span><br></pre></td></tr></table></figure><p>可以想见的是,一旦分区数量超过了操作系统规定的文件描述符上限,kafka进程就会崩溃</p><h4 id="4-2-5-4-分区数量的考量"><a href="#4-2-5-4-分区数量的考量" class="headerlink" title="4.2.5.4 分区数量的考量"></a>4.2.5.4 分区数量的考量</h4><p>如何选择合适的分区数量,一个恰当的答案就是视具体情况而定.</p><p>从吞吐量方面考虑,增加合适的分区数量可以在一定程度上提升整体吞吐量,但是超过临界值之后吞吐量不升反降.在投入生产环境之前,应该对吞吐量进行相关的测试,以找到合适的分区数量</p><p>分区数量太多会影响系统可用性,当broker发生故障时,broker节点上的所有分区的leader副本不可用,此时如果有大量的分区要进行leader角色切换,这个切换的过程会耗费相当的时间,并且这个时间段内分区会变的不可用.并且分区数量太多不仅为增加日志清理的耗时,而且在被删除时也会消费更多时间.</p><p>一个好的建议是,创建主题之前对分区数量性能进行充分压测,在创建主题之后,还需要对其进行追踪,监控,调优.如果分区数量较少,还能通过增加分区数量,或者增加broker进行分区重分配等改进.</p><p>最后,一个通用的准则是,建议分区数量设定为集群中broker的倍数,例如集群中有3个broker节点,可以设定分区数为3,6,9等.</p><h3 id="4-2-6-总结"><a href="#4-2-6-总结" class="headerlink" title="4.2.6 总结"></a>4.2.6 总结</h3><p><code>kafka-topics.sh</code> 查看,创建主题分区,副本</p><p><code>kafka-configs.sh</code> 修改主题配置文件</p><p><code>kafka_perferred-replica-elections.sh</code> 优先副本选举</p><p><code>kafka-reassign-partitions.sh</code> 分区重分配,副本复制限流,修改副本因子数量</p><p><code>kafka-producer-perf-test.sh</code> 生产者分区数和吞吐量性能压测</p><p><code>kafka-consumer-perf-test.sh</code> 消费者性能压测</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;4-2-1-优选副本的选举&quot;&gt;&lt;a href=&quot;#4-2-1-优选副本的选举&quot; class=&quot;headerlink&quot; title=&quot;4.2.1 优选副本的选举&quot;&gt;&lt;/a&gt;4.2.1 优选副本的选举&lt;/h3&gt;&lt;h4 id=&quot;4-2-1-1-什么是优先副本&quot;&gt;&lt;a href=&quot;#4-2-1-1-什么是优先副本&quot; class=&quot;headerlink&quot; title=&quot;4.2.1.1 什么是优先副本&quot;&gt;&lt;/a&gt;4.2.1.1 什么是优先副本&lt;/h4&gt;&lt;p&gt;分区使用多副本机制来提升可靠性,但是只有leader副本对外提供读写服务.而follower副本只负责在内部进行消息的同步.如果一个分区的leader副本不可用,那么就意味着整个分区变得不可用.此时就需要从剩余的follower副本中挑选一个新的leader副本继续对外提供服务.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;broker节点中的Leader副本个数决定了这个节点负载的高低&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在创建主题的时候,主题的分区和副本会尽可能的均匀分布在kafka集群的各个broker节点.对应的Leader副本的分配也比较均匀.例如下面的 &lt;code&gt;topic-demo&lt;/code&gt; 主题:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev152 ~]$ kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic-demo&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Topic:topic-demo    PartitionCount:4    ReplicationFactor:3 Configs:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 0    Leader: 152 Replicas: 152,153,154   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 1    Leader: 153 Replicas: 153,154,152   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 2    Leader: 154 Replicas: 154,152,153   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    Topic: topic-demo   Partition: 3    Leader: 152 Replicas: 152,154,153   Isr: 152,153,154&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[hadoop@bi-dev152 ~]$&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Linux-分布式&amp;消息队列" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
      <category term="kafka" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/"/>
    
      <category term="4-主题和分区" scheme="https://jesse.top/categories/Linux-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/kafka/4-%E4%B8%BB%E9%A2%98%E5%92%8C%E5%88%86%E5%8C%BA/"/>
    
    
      <category term="kafka" scheme="https://jesse.top/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch索引迁移</title>
    <link href="https://jesse.top/2020/09/30/elk/Elasticsearch%E7%B4%A2%E5%BC%95%E8%BF%81%E7%A7%BB/"/>
    <id>https://jesse.top/2020/09/30/elk/Elasticsearch索引迁移/</id>
    <published>2020-09-30T14:59:58.000Z</published>
    <updated>2021-01-19T14:39:47.543Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Elasticsearch索引迁移"><a href="#Elasticsearch索引迁移" class="headerlink" title="Elasticsearch索引迁移"></a>Elasticsearch索引迁移</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>旧Elasticsearch版本:2.4.4</p><p>新Elasticsearch版本:2.4.4</p><p>近期dev环境服务器迁移到一台新的物理机,所以需要迁移部分Elasticsearch索引数据.</p><p>Elasticsearch索引迁移有许多方法.测试过elasticsearch-exporter.但是没有成功.报错如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[work@docker elasticsearch-exporter]$ node exporter.js -a 10.0.0.250 -b 10.0.0.101 -p 9200 -q 9200 -i mid_mg_gc_datasource_items -j mid_mg_gc_datasource_items</span><br><span class="line">Elasticsearch Exporter - Version 1.4.0</span><br><span class="line">Reading source statistics from ElasticSearch</span><br><span class="line">The source driver has not reported any documents that can be exported. Not exporting.</span><br><span class="line">Number of calls:0</span><br><span class="line">Fetched Entries:0 documents</span><br><span class="line">Processed Entries:0 documents</span><br><span class="line">Source DB Size:0 documents</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="Elasticsearch-dump"><a href="#Elasticsearch-dump" class="headerlink" title="Elasticsearch-dump"></a>Elasticsearch-dump</h3><p>本次使用elasticsearch-dump进行索引迁移.在github上可以找到具体使用方法:<a href="https://github.com/elasticsearch-dump/elasticsearch-dump" target="_blank" rel="noopener">elasticsearch-dump</a></p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install elasticdump</span><br></pre></td></tr></table></figure><p>这里稍微踩了个坑,如果报错:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pm WARN deprecated nomnom@1.8.1: Package no longer supported. Contact support@npmjs.com for more info.</span><br><span class="line">npm WARN saveError ENOENT: no such file or directory, open &apos;/home/work/package.json&apos;</span><br><span class="line">npm WARN enoent ENOENT: no such file or directory, open &apos;/home/work/package.json&apos;</span><br><span class="line">npm WARN work No description</span><br><span class="line">npm WARN work No repository field.</span><br><span class="line">npm WARN work No README data</span><br><span class="line">npm WARN work No license field.</span><br></pre></td></tr></table></figure><p>则需要初始化一下npm</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">work@docker ~]$ npm init</span><br></pre></td></tr></table></figure><p>安装完成后,进入到<code>elasticsearch dump</code>的<code>bin</code>目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[work@docker ~]$ cd node_modules/elasticdump/bin/</span><br></pre></td></tr></table></figure><h3 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h3><p>查看elasticsearchdump的具体用法</p><p>[work@docker bin]$ ./elasticdump –help</p><p><code>elaticsearchdump</code>支持两个ES跨版本迁移索引,还支持索引备份到文件,以及从文件恢复到Elasticsearch</p><h5 id="迁移mid-gm-gc-brand这个索引数据"><a href="#迁移mid-gm-gc-brand这个索引数据" class="headerlink" title="迁移mid_gm_gc_brand这个索引数据"></a>迁移mid_gm_gc_brand这个索引数据</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[work@docker bin]$ ./elasticdump --input=http://10.0.0.250:9200/mid_mg_gc_brand --output=http://10.0.0.101:9200/mid_mg_gc_brand --type=analyzer</span><br><span class="line"></span><br><span class="line">[work@docker bin]$ ./elasticdump --input=http://10.0.0.250:9200/mid_mg_gc_brand --</span><br><span class="line"></span><br><span class="line">[work@docker bin]$ ./elasticdump --input=http://10.0.0.250:9200/mid_mg_gc_brand --output=http://10.0.0.101:9200/mid_mg_gc_brand --type=data</span><br></pre></td></tr></table></figure><blockquote><p>文档中的type类型有settings, analyzer, data, mapping, alias, template</p></blockquote><h5 id="查看新服务器上的索引-迁移成功"><a href="#查看新服务器上的索引-迁移成功" class="headerlink" title="查看新服务器上的索引.迁移成功"></a>查看新服务器上的索引.迁移成功</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl -Ssl &apos;http://10.0.0.101:9200/_cat/indices?v&apos; | grep &apos;mid_mg*&apos;</span><br><span class="line">yellow open   mid_mg_gc_datasource_items             5   1         96            1     87.3kb         87.3kb</span><br><span class="line">yellow open   mid_mg_gc_brand                        5   1        966            0    312.1kb        312.1kb</span><br><span class="line">yellow open   mid_mg_gc_synonyms                     5   1        116            0     82.1kb         82.1kb</span><br><span class="line"></span><br><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl -Ssl &apos;http://10.0.0.250:9200/_cat/indices?v&apos; | grep &apos;mid_mg*&apos;</span><br><span class="line">yellow open   mid_mg_gc_datasource_items             3   1         92            9     99.1kb         99.1kb</span><br><span class="line">yellow open   mid_mg_gc_brand                        3   1        966            0    345.4kb        345.4kb</span><br><span class="line">yellow open   mid_mg_gc_synonyms                     3   1        116           16    106.1kb        106.1kb</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Elasticsearch索引迁移&quot;&gt;&lt;a href=&quot;#Elasticsearch索引迁移&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearch索引迁移&quot;&gt;&lt;/a&gt;Elasticsearch索引迁移&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;旧Elasticsearch版本:2.4.4&lt;/p&gt;
&lt;p&gt;新Elasticsearch版本:2.4.4&lt;/p&gt;
&lt;p&gt;近期dev环境服务器迁移到一台新的物理机,所以需要迁移部分Elasticsearch索引数据.&lt;/p&gt;
&lt;p&gt;Elasticsearch索引迁移有许多方法.测试过elasticsearch-exporter.但是没有成功.报错如下:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[work@docker elasticsearch-exporter]$ node exporter.js -a 10.0.0.250 -b 10.0.0.101 -p 9200 -q 9200 -i mid_mg_gc_datasource_items -j mid_mg_gc_datasource_items&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Elasticsearch Exporter - Version 1.4.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Reading source statistics from ElasticSearch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;The source driver has not reported any documents that can be exported. Not exporting.&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Number of calls:	0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Fetched Entries:	0 documents&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Processed Entries:	0 documents&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Source DB Size:		0 documents&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>Openvpn客户端无法连接OpenSSL</title>
    <link href="https://jesse.top/2020/09/22/Linux-Service/Openvpn%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%97%A0%E6%B3%95%E8%BF%9E%E6%8E%A5%20OpenSSL-%20error/"/>
    <id>https://jesse.top/2020/09/22/Linux-Service/Openvpn客户端无法连接 OpenSSL- error/</id>
    <published>2020-09-22T14:59:58.000Z</published>
    <updated>2021-01-19T14:42:39.686Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Openvpn客户端无法连接OpenSSL-error"><a href="#Openvpn客户端无法连接OpenSSL-error" class="headerlink" title="Openvpn客户端无法连接OpenSSL: error"></a>Openvpn客户端无法连接OpenSSL: error</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>今天阿里云的Openvpn服务器部署了docker服务后,需要升级内存配置.服务器升级重启后,发现客户端无法连接Openvpn.</p><hr><h3 id="故障表现"><a href="#故障表现" class="headerlink" title="故障表现"></a>故障表现</h3><p>在openvpn客户端日志中发现下面报错信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2020-09-22 16:00:49 WARNING: No server certificate verification method has been enabled.  See http://openvpn.net/howto.html#mitm for more info.</span><br></pre></td></tr></table></figure><p>openvpn服务端日志报错信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS: Initial packet from [AF_INET]27.115.51.166:26184, sid=c0cb2b12 4b3187b2</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 OpenSSL: error:14089086:SSL routines:ssl3_get_client_certificate:certificate verify failed</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS_ERROR: BIO read tls_read_plaintext error</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS object -&gt; incoming plaintext read error</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS handshake failed</span><br><span class="line">Tue Sep 22 15:47:49 2020 27.115.51.166:26184 SIGUSR1[soft,tls-error] received, client-instance restarting</span><br><span class="line">Tue Sep 22 15:48:06 2020 27.115.51.166:33480 TLS: Initial packet from [AF_INET]27.115.51.166:33480, sid=11b9760e 97f6d068</span><br><span class="line">Tue Sep 22 15:48:07 2020 27.115.51.166:33480 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx</span><br></pre></td></tr></table></figure><p><strong>日志关键字</strong></p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OpenSSL: error:14089086:SSL routines:ssl3_get_client_certificate:certificate verify failed</span><br></pre></td></tr></table></figure><hr><h3 id="排查"><a href="#排查" class="headerlink" title="排查"></a>排查</h3><p>服务器重启后需要注意的几个问题:</p><p>1.检查以下几个服务是否启动:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl status openvpn@server</span><br><span class="line">systemctl status iptables</span><br></pre></td></tr></table></figure><p>2.由于docker服务会初始化iptables,所以docker启动后需要手动添加一条iptables规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A POSTROUTING -s 10.111.255.0/24 -o eth0 -j MASQUERADE</span><br></pre></td></tr></table></figure><p>3.检查ip转发功能</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysctl.conf</span><br><span class="line">net.ipv4.ip_forward = 1</span><br></pre></td></tr></table></figure><p>4.检查阿里云的安全组规则,是否开通了1094的udp协议</p><hr><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>最终解决方案还是要靠google解决,在openvpn的wiki上找到了解决方案,</p><p>具体网站链接:<a href="https://community.openvpn.net/openvpn/wiki/CertificateRevocationListExpired?__cf_chl_jschl_tk__=40e85ba16653b7db84d828db819eafbc2b5a9faf-1600761449-0-AUAdZiIXwLUqBJDJAcDe9htVtUTZlGJm8m_RYLUsxLu2he8Myk5WXwzQn-CZdZyBDJRHn9clM-6y0ITsnKk0Pru3AwB7EOc0LhjyrV9unNnBv0V9_skxNC2n3per9e1TQJjcmmtwnaNl23Sp5D8p9FZYyX5PO-vtkdp1i7dyh_x1KSFwZqibI8Zt4saVoABGkfMJ3nKeUJpIIlnEhRGhfJrQwQZqvvG6EAS1CJUHRR8uqKNyqmEz90RmqcLGc8ytoOTIBYhJMs8OsPk_dA2nKA47QObzI5-4SEUZkC5ZaxhNCfkRBGx9kwimWfBJFtxJkPzI5_vkXONWXhoB0wi5cfE" target="_blank" rel="noopener">openvpn wiki</a></p><p>解决问题很简单:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#需要进入到下面这个目录下</span><br><span class="line">cd /etc/openvpn/easy-rsa/3</span><br><span class="line"></span><br><span class="line">#更新一下crl文件.</span><br><span class="line">[root@dwd-Dnsmasq 3]# ./easyrsa gen-crl</span><br><span class="line"></span><br><span class="line">Using configuration from ./openssl-1.0.cnf</span><br><span class="line"></span><br><span class="line">An updated CRL has been created.</span><br><span class="line">CRL file: /etc/openvpn/easy-rsa/3/pki/crl.pem</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Openvpn客户端无法连接OpenSSL-error&quot;&gt;&lt;a href=&quot;#Openvpn客户端无法连接OpenSSL-error&quot; class=&quot;headerlink&quot; title=&quot;Openvpn客户端无法连接OpenSSL: error&quot;&gt;&lt;/a&gt;Openvpn客户端无法连接OpenSSL: error&lt;/h2&gt;&lt;h3 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h3&gt;&lt;p&gt;今天阿里云的Openvpn服务器部署了docker服务后,需要升级内存配置.服务器升级重启后,发现客户端无法连接Openvpn.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;故障表现&quot;&gt;&lt;a href=&quot;#故障表现&quot; class=&quot;headerlink&quot; title=&quot;故障表现&quot;&gt;&lt;/a&gt;故障表现&lt;/h3&gt;&lt;p&gt;在openvpn客户端日志中发现下面报错信息:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;2020-09-22 16:00:49 WARNING: No server certificate verification method has been enabled.  See http://openvpn.net/howto.html#mitm for more info.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;openvpn服务端日志报错信息:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;ue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS: Initial packet from [AF_INET]27.115.51.166:26184, sid=c0cb2b12 4b3187b2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 OpenSSL: error:14089086:SSL routines:ssl3_get_client_certificate:certificate verify failed&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS_ERROR: BIO read tls_read_plaintext error&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS object -&amp;gt; incoming plaintext read error&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 TLS Error: TLS handshake failed&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:47:49 2020 27.115.51.166:26184 SIGUSR1[soft,tls-error] received, client-instance restarting&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:48:06 2020 27.115.51.166:33480 TLS: Initial packet from [AF_INET]27.115.51.166:33480, sid=11b9760e 97f6d068&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tue Sep 22 15:48:07 2020 27.115.51.166:33480 VERIFY ERROR: depth=0, error=CRL has expired: CN=xxxxxx&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;日志关键字&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-Service" scheme="https://jesse.top/categories/Linux-Service/"/>
    
    
      <category term="Linux" scheme="https://jesse.top/tags/Linux/"/>
    
      <category term="openvpn" scheme="https://jesse.top/tags/openvpn/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://jesse.top/2020/09/16/SRE%E8%BF%90%E7%BB%B4/SRE%E8%BF%90%E7%BB%B4%E7%AC%94%E8%AE%B0/"/>
    <id>https://jesse.top/2020/09/16/SRE运维/SRE运维笔记/</id>
    <published>2020-09-16T14:30:54.835Z</published>
    <updated>2020-09-16T14:30:54.836Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SRE运维笔记-chapter-One-概念介绍"><a href="#SRE运维笔记-chapter-One-概念介绍" class="headerlink" title="SRE运维笔记-chapter One-概念介绍"></a>SRE运维笔记-chapter One-概念介绍</h2><h3 id="SRE概念"><a href="#SRE概念" class="headerlink" title="SRE概念"></a>SRE概念</h3><p>SRE(sitereliabilityengineering).中文翻译为站点可靠性工程师.SRE概念中比较重要的特性在于:</p><p>1.engineer表示SRE是工程师,使用软件工程手段设计,研发和维护业务软件系统.</p><p>2.SRE的关注焦点在于<strong>可靠性</strong>,专注于软件系统架构设计,运维流程优化,让业务软件系统运行更可靠.</p><p>3.SRE主要工作是运维业务服务,</p><h4 id="SRE团队职责"><a href="#SRE团队职责" class="headerlink" title="SRE团队职责:"></a>SRE团队职责:</h4><ul><li>可用性改进</li><li>延迟优化</li><li>性能优化</li><li>效率优化</li><li>变更管理</li><li>监控</li><li>紧急事务处理</li><li>容量规划与管理</li></ul><h3 id="SRE方法论"><a href="#SRE方法论" class="headerlink" title="SRE方法论"></a>SRE方法论</h3><ul><li><p><strong>确保长期关注研发工作</strong></p><p>运维工作限制在50%以内,剩余的时间花在研发项目上.</p></li><li><p><strong>在保障服务SLO的前提下最大化迭代速度</strong></p><ul><li>错误预算: 1-可靠性目标.<ul><li>如果一个服务的可靠性目标是99.99%,那么错误预算就是0.01%.</li></ul></li></ul></li><li><p><strong>监控系统</strong></p><p>监控系统不应该依赖人来分析警报信息.而是应该由系统自动分析.仅当需要用户执行某种操作时,才需要通知用户</p><p>监控系统需要具备三类输出:</p><ul><li>紧急警报(alert)</li><li>工单:接受工单的用户应该执行某种操作,但是并非立即执行</li><li>日志</li></ul></li><li><p><strong>应急事件处理</strong></p><ul><li>可靠性:MTTF(平均失败时间),MTTR(平均恢复时间),MTBF(平均故障间隔时间).</li><li>任何需要人工操作的事情都只会延长恢复时间,一个可以自动恢复的系统即使有更多故障发生,也比事事都要人工干预的系统可用性更高.当不可避免需要人工介入时,最佳方法是事先预案,并且记录在<strong>运维手册(playbook)</strong>中,这能降低<strong>MTTR(平均恢复)</strong>时间.</li></ul></li><li><p><strong>变更管理</strong></p><p> 大概70%的生产事故是由某种变更触发,变更管理的最佳实践是使用自动化完成以下几个项目:</p><ul><li>采用渐进式发布机制</li><li>迅速而准确的检测到问题的发生</li><li>安全迅速的回滚</li></ul></li><li><p><strong>需求预测和容量规划</strong></p><ul><li>必须有一个准确的自然增城需求预测模型,需求预测的时间应该超过资源获取时间</li><li>规划中必须有准确的非自然增长的需求来源统计</li><li>必须有周期性压力测试,以便准确的将系统原始资源与业务容量对应起来</li></ul></li><li><p><strong>资源部署</strong></p></li><li><p><strong>效率与性能</strong></p><p>一个业务总体资源使用情况是由以下几个因素驱动的:</p><ul><li>用户需求(流量)</li><li>可用容量</li><li>软件资源使用效率</li></ul><p>SRE需要通过模型预测用户需求,合理部署和配置可用容量,改进软件以提高资源使用效率,这3个因素能够推动一个服务器的效率提升.</p><blockquote><p>软件系统在负载上升的时候,会导致延迟升高.SRE的目标是根据一个预设的延迟目标部署和维护足够的容量,SRE和研发团队应该共同监控和优化整个系统的性能.</p></blockquote></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;SRE运维笔记-chapter-One-概念介绍&quot;&gt;&lt;a href=&quot;#SRE运维笔记-chapter-One-概念介绍&quot; class=&quot;headerlink&quot; title=&quot;SRE运维笔记-chapter One-概念介绍&quot;&gt;&lt;/a&gt;SRE运维笔记-chapte
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>kong利用request-transformer插件重写URL</title>
    <link href="https://jesse.top/2020/09/10/Linux-Web/kong%E5%88%A9%E7%94%A8request-transformer%E6%8F%92%E4%BB%B6%E9%87%8D%E5%86%99URL/"/>
    <id>https://jesse.top/2020/09/10/Linux-Web/kong利用request-transformer插件重写URL/</id>
    <published>2020-09-10T04:59:58.000Z</published>
    <updated>2020-09-10T14:58:43.601Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kong利用request-transformer插件重写URL"><a href="#kong利用request-transformer插件重写URL" class="headerlink" title="kong利用request-transformer插件重写URL"></a>kong利用request-transformer插件重写URL</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>最近业务整合后有一个需求,将URL:<a href="http://www.abc.com/api/item/111" target="_blank" rel="noopener">www.abc.com/api/item/111</a> 想重写成<a href="http://www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转" target="_blank" rel="noopener">www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转</a>.</p><p>使用Nginx的rewrite redirect指令可以实现URL重写需求,但是redirect会跳转到新域名,不符合需求.</p><p>刚好该业务的的前端是用Kong网关处理,所以研究kong的插件实现这个需求</p><hr><h3 id="request-transformer介绍"><a href="#request-transformer介绍" class="headerlink" title="request-transformer介绍"></a>request-transformer介绍</h3><p><strong>request-transformer</strong>是Kong官方的插件,允许修改重写用户的请求,还可以使用正则表达式匹配URL,并将匹配到的字符串保存在变量中,然后使用模板将变量转换成用户的请求</p><p>简而言之:<strong>就是重写用户的请求</strong>,包括URL,args,headers,methods等等</p><p>官方地址: <a href="https://docs.konghq.com/hub/kong-inc/request-transformer/" target="_blank" rel="noopener">reuqest-transformer官方地址</a></p><p>github项目地址: <a href="https://github.com/Kong/kong-plugin-request-transformer" target="_blank" rel="noopener">request-transformer github</a></p><a id="more"></a><hr><h3 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h3><blockquote><p>kong使用的是2.1.3最新版本,试过使用Kong1.0版本插件无法生效</p></blockquote><p>这里举2个例子说明</p><ul><li><p>将URL:/v4/jkf/branch/qrcode&amp;code=100006 重写为 /v4/jkf/branch/qrcode?code=100006.也就是将<code>&amp;</code>转换为<code>?</code></p><ul><li>首先配置Service和Route.具体配置方法略过,这里主要关心一下Route中的Path设置:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/v4/jkf/branch/qrcode\&amp;code=(?&lt;code&gt;\d+)</span><br></pre></td></tr></table></figure><p>该PATH表示:</p><p>1.匹配/v4/jkf/branch/qrcode&amp;code=任意长度数字.</p><p>2.正则<code>\d</code>表示匹配数字,并且将匹配到的数字保存为<code>code</code>变量</p><p>3.<code>\&amp;</code>表示转义URL中的<code>$</code>符号</p><blockquote><p>关闭route中的script path可选项</p></blockquote><ul><li><p>其次在该route下添加<code>request-transformer</code>插件,表示该插件只应用到此条route下.并且配置插件参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://localhost:8001/routes/21292565-e7ae-40ff-a465-f449c16b7819/plugins \</span><br><span class="line">      --data &quot;name=request-transformer&quot; \</span><br><span class="line">      --data &quot;config.replace.uri=/jkf/branch/qrcode&quot; \</span><br><span class="line">      --data &quot;config.add.querystring=code:\$(uri_captures.code)&quot;</span><br></pre></td></tr></table></figure><p>对于上面的命令行解释如下</p><ol><li><code>21292565-e7ae-40ff-a465-f449c16b7819</code>就是刚才创建的路由ID</li><li><code>config.replace.uri</code>表示将route匹配到的PATH重写为<code>/jkf/branch/qrcode</code></li><li><code>onfig.add.querystring</code>表示给URL添加args参数</li><li><code>code:\$(uri_captures.code)</code>.参数名是<code>code</code>,<code>uri_captures.code</code>表示获取route的PATH中code变量的值,由于命令行shell环境关系,所以要在变量符号<code>$</code>前进行转义.</li></ol></li></ul></li></ul><pre><code>或者也可以使用konga的UI管理平台添加和编辑插件</code></pre><p><img src="https://img2.jesse.top/image-20200910160826719.png" alt=""></p><p>  <img src="https://img2.jesse.top/image-20200910160949836.png" alt="image-20200910160949836"></p><ul><li><p>最后,<code>reload</code>Kong进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker ~]# docker exec kong kong reload</span><br></pre></td></tr></table></figure><p><strong>验证</strong></p><p>本地访问网站:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">✘ huangyong@huangyong-Macbook-Pro  ~  curl -XGET https://m.devapi.xxx.com/v4/jkf/branch/qrcode\&amp;code\=100006</span><br></pre></td></tr></table></figure><p>Kong和后端nginx日志如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">172.18.0.1 - - [10/Sep/2020:16:52:11 +0800] &quot;GET /jkf/branch/qrcode?code=100006 HTTP/1.1&quot; 200 163 &quot;-&quot; &quot;curl/7.54.0&quot; &quot;10.0.4.9, 172.18.0.2&quot; 10.0.4.9, 172.18.0.2, 172.18.0.1 &quot;2b7dcdc621d1928f456d561f31e95b25&quot;0.065 0.065</span><br></pre></td></tr></table></figure><p>可以看到已经成功重写了URL</p></li></ul><hr><ul><li>第二个例子,将/api/item/111 重写为/open/item/itemdetail?id=111</li></ul><p>将URL后面的数字拼接到id的值,作为参数拼接成URL后,传递给后端</p><p>1.添加Service和Routes,Routes的PATH部分如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#该PATH只匹配/api/item/数字格式的URL.并且将\d+正则匹配到的数字保存为变量id</span><br><span class="line">/api/item/(?&lt;id&gt;\d+)$</span><br></pre></td></tr></table></figure><p>2.添加和配置插件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST http://localhost:8001/routes/38fce1b7-2a36-42cb-9619-f30539889137/plugins \</span><br><span class="line">      --data &quot;name=request-transformer&quot; \</span><br><span class="line">      --data &quot;config.replace.uri=/open/item/itemdetail&quot; \</span><br><span class="line">      --data &quot;config.add.querystring=id:\$(uri_captures.id)&quot;</span><br></pre></td></tr></table></figure><p>3.重载kong</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@docker ~]# docker exec kong kong reload</span><br></pre></td></tr></table></figure><p>4.验证</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">huangyong@huangyong-Macbook-Pro  ~  curl -XGET https://m.devapi.xxx.com/api/item/111</span><br></pre></td></tr></table></figure><p>后端日志如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">172.18.0.1 - - [10/Sep/2020:17:11:43 +0800] &quot;GET /open/item/itemdetail?id=111 HTTP/1.1&quot; 200 160 &quot;-&quot; &quot;curl/7.54.0&quot; &quot;10.0.4.9, 172.18.0.2&quot; 10.0.4.9, 172.18.0.2, 172.18.0.1 &quot;54ac589d36f90c1ef99ba6a43c4d488e&quot;0.105 0.105</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kong利用request-transformer插件重写URL&quot;&gt;&lt;a href=&quot;#kong利用request-transformer插件重写URL&quot; class=&quot;headerlink&quot; title=&quot;kong利用request-transformer插件重写URL&quot;&gt;&lt;/a&gt;kong利用request-transformer插件重写URL&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;最近业务整合后有一个需求,将URL:&lt;a href=&quot;http://www.abc.com/api/item/111&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;www.abc.com/api/item/111&lt;/a&gt; 想重写成&lt;a href=&quot;http://www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;www.xyz.com/open/item/itemdetail?id=111,并且域名不变,不能发生302跳转&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;使用Nginx的rewrite redirect指令可以实现URL重写需求,但是redirect会跳转到新域名,不符合需求.&lt;/p&gt;
&lt;p&gt;刚好该业务的的前端是用Kong网关处理,所以研究kong的插件实现这个需求&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;request-transformer介绍&quot;&gt;&lt;a href=&quot;#request-transformer介绍&quot; class=&quot;headerlink&quot; title=&quot;request-transformer介绍&quot;&gt;&lt;/a&gt;request-transformer介绍&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;request-transformer&lt;/strong&gt;是Kong官方的插件,允许修改重写用户的请求,还可以使用正则表达式匹配URL,并将匹配到的字符串保存在变量中,然后使用模板将变量转换成用户的请求&lt;/p&gt;
&lt;p&gt;简而言之:&lt;strong&gt;就是重写用户的请求&lt;/strong&gt;,包括URL,args,headers,methods等等&lt;/p&gt;
&lt;p&gt;官方地址: &lt;a href=&quot;https://docs.konghq.com/hub/kong-inc/request-transformer/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;reuqest-transformer官方地址&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;github项目地址: &lt;a href=&quot;https://github.com/Kong/kong-plugin-request-transformer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;request-transformer github&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>zabbix监控vmware主机以及GuestOS</title>
    <link href="https://jesse.top/2020/08/26/%E7%9B%91%E6%8E%A7/zabbix%E7%9B%91%E6%8E%A7vmware%E4%B8%BB%E6%9C%BA%E4%BB%A5%E5%8F%8AGuestOS/"/>
    <id>https://jesse.top/2020/08/26/监控/zabbix监控vmware主机以及GuestOS/</id>
    <published>2020-08-26T01:20:58.000Z</published>
    <updated>2020-08-26T23:55:20.314Z</updated>
    
    <content type="html"><![CDATA[<h3 id="zabbix监控vmware主机以及GuestOS"><a href="#zabbix监控vmware主机以及GuestOS" class="headerlink" title="zabbix监控vmware主机以及GuestOS"></a>zabbix监控vmware主机以及GuestOS</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>ESXI主机无法安装zabbix agent,所以不能使用传统的agent客户端监控vmware主机,但是Zabbix有自导的vmware hypervisors监控模板.Zabbix 通过 vmware collector 进程来监控虚拟机,使用SOAP协议从vmware web服务器获取必要的监控信息.</p><hr><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p>1.在zabbix服务器修改<code>zabbix_server.conf</code>配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">StartVMwareCollectors=6</span><br><span class="line">VMwareCacheSize=50M</span><br><span class="line">VMwareFrequency=10</span><br><span class="line">VMwarePerfFrequency=60</span><br><span class="line">VMwareTimeout=300</span><br></pre></td></tr></table></figure><a id="more"></a><p><strong>说明</strong>: </p><p><strong>StartVMwareCollectors</strong>：vmware 收集器实例的数量。<br>此值取决于要监控的 VMware 服务的数量。在大多数情况下，这应该是：<code>servicenum &lt; StartVMwareCollectors &lt; (servicenum * 2)</code>其中 servicenum 是 VMware 服务的数量。</p><p>例如：如果您有 1 个 VMware 服务要将 StartVMwareCollectors 设置为 2，那么如果您有 3 个 VMware 服务，请将其设置为 5。请注意，在大多数情况下，此值不应小于 2，不应大于 VMware 数量的 2 倍服务。</p><p>2.重启zabbix服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart zabbix_server</span><br></pre></td></tr></table></figure><hr><h4 id="Esxi物理主机配置"><a href="#Esxi物理主机配置" class="headerlink" title="Esxi物理主机配置"></a>Esxi物理主机配置</h4><p>1.登陆Esxi web界面: <a href="https://172.16.0.55" target="_blank" rel="noopener">https://172.16.0.55</a><br>2.在<code>manage</code>—<code>system</code>—-<code>advanced settings</code>.修改<code>Config.HostAgent.plugins.solo.enableMob</code>的值为True</p><p><img src="https://img2.jesse.top/image-20200818112727513.png" alt="image-20200818112727513"></p><p>3.访问:<a href="https://172.16.0.55/mob/?moid=ha-host&amp;doPath=hardware.systemInfo" target="_blank" rel="noopener">https://172.16.0.55/mob/?moid=ha-host&amp;doPath=hardware.systemInfo</a><br>记录UUID<br><img src="https://img2.jesse.top/image-20200818112949235.png" alt="image-20200818112949235"></p><p>4.在zabbix添加主机</p><p><img src="https://img2.jesse.top/image-20200818113114835.png" alt="image-20200818113114835"></p><ul><li><strong>主机名称</strong>:上面查到的UUID</li><li><strong>IP地址</strong>:Esxi的IP地址</li><li><strong>端口</strong>:80</li></ul><p><strong>模板</strong>:</p><p><img src="https://img2.jesse.top/image-20200818113315388.png" alt="image-20200818113315388"></p><p><strong>宏</strong></p><p><img src="https://img2.jesse.top/image-20200818113428859.png" alt="image-20200818113428859"></p><ul><li><strong>password</strong>: Esxi主机密码</li><li><p><strong>URL</strong>: <a href="https://Esxi_IP/sdk" target="_blank" rel="noopener">https://Esxi_IP/sdk</a> </p></li><li><p><strong>username</strong>: ESXI主机用户名</p></li><li><strong>UUID</strong>: 上文记录的UUID</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;zabbix监控vmware主机以及GuestOS&quot;&gt;&lt;a href=&quot;#zabbix监控vmware主机以及GuestOS&quot; class=&quot;headerlink&quot; title=&quot;zabbix监控vmware主机以及GuestOS&quot;&gt;&lt;/a&gt;zabbix监控vmware主机以及GuestOS&lt;/h3&gt;&lt;h4 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h4&gt;&lt;p&gt;ESXI主机无法安装zabbix agent,所以不能使用传统的agent客户端监控vmware主机,但是Zabbix有自导的vmware hypervisors监控模板.Zabbix 通过 vmware collector 进程来监控虚拟机,使用SOAP协议从vmware web服务器获取必要的监控信息.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h4&gt;&lt;p&gt;1.在zabbix服务器修改&lt;code&gt;zabbix_server.conf&lt;/code&gt;配置文件&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;StartVMwareCollectors=6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwareCacheSize=50M&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwareFrequency=10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwarePerfFrequency=60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;VMwareTimeout=300&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="监控" scheme="https://jesse.top/categories/%E7%9B%91%E6%8E%A7/"/>
    
    
      <category term="zabbix监控" scheme="https://jesse.top/tags/zabbix%E7%9B%91%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>记一次生产ELK性能优化</title>
    <link href="https://jesse.top/2020/08/25/elk/%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%94%9F%E4%BA%A7ELK%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"/>
    <id>https://jesse.top/2020/08/25/elk/记一次生产ELK性能优化/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-09-16T14:30:54.836Z</updated>
    
    <content type="html"><![CDATA[<h3 id="记一次生产ELK性能优化"><a href="#记一次生产ELK性能优化" class="headerlink" title="记一次生产ELK性能优化"></a>记一次生产ELK性能优化</h3><p>ES上线后遇到一些问题:</p><h3 id="一-内存压力过高"><a href="#一-内存压力过高" class="headerlink" title="一.内存压力过高"></a>一.内存压力过高</h3><p>目前ES节点的有两种规格内存.</p><p>80GB内存(节点同时运行了Logstash,分配了16G给logstash).</p><p>54GB内存(只运行ES,并且只作为data节点)</p><p>ES的JVM内存从32G下降到28G.建议JVM内存是总物理内存的一半左右,</p><p>节点内存使用情况如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&quot;mem&quot;: &#123; - </span><br><span class="line">          &quot;total&quot;: &quot;78.5gb&quot;,</span><br><span class="line">          &quot;total_in_bytes&quot;: 84296601600,</span><br><span class="line">          &quot;free&quot;: &quot;4.3gb&quot;,</span><br><span class="line">          &quot;free_in_bytes&quot;: 4688633856,</span><br><span class="line">          &quot;used&quot;: &quot;74.1gb&quot;,</span><br><span class="line">          &quot;used_in_bytes&quot;: 79607967744,</span><br><span class="line">          &quot;free_percent&quot;: 6,</span><br><span class="line">          &quot;used_percent&quot;: 94</span><br></pre></td></tr></table></figure><p>建议: ES节点内存建议配置高一点,虽然ES的JVM内存最大不超过32G.但是在其他方面仍然很吃内存</p><a id="more"></a><hr><h3 id="二-索引字段数"><a href="#二-索引字段数" class="headerlink" title="二.索引字段数"></a>二.索引字段数</h3><p>ES的单个索引默认最大字段数量是1000个.当超过这个字段数量时,会拒绝字段映射.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[2020-08-14T10:44:10,605][INFO ][o.e.a.b.TransportShardBulkAction] [idc-function-elk01] [logstash-mg-tc-netrcd-gateway-2020.08.14][0] mapping update rejected by primary</span><br><span class="line">java.lang.IllegalArgumentException: Limit of total fields [10000] in index [logstash-mg-tc-netrcd-gateway-2020.08.14] has been exceeded</span><br></pre></td></tr></table></figure><p>此时可以在索引模板中,修改默认字段数量.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PUT _template/logstash</span><br><span class="line">&#123;</span><br><span class="line">    &quot;index_patterns&quot;: &quot;logstash-*&quot;,</span><br><span class="line">    &quot;settings&quot;: &#123;</span><br><span class="line">        &quot;mapping.total_fields.limit&quot;:5000</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h3 id="三-ES线程和队列优化"><a href="#三-ES线程和队列优化" class="headerlink" title="三.ES线程和队列优化"></a>三.ES线程和队列优化</h3><p>ES日志有部分警告信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[2020-08-14T10:15:06,151][WARN ][o.e.c.r.a.AllocationService] [idc-function-elk01] failing shard [failed shard, shard [logstash-msf-internal-access-2020.08.14][0], node[3uwnN_KEQGiywi</span><br><span class="line">u9xR5Jng], [R], s[STARTED], a[id=ACE7jGGoS6yhS9T_Sn53UA], message [failed to perform indices:data/write/bulk[s] on replica [logstash-msf-internal-access-2020.08.14][0], node[3uwnN_KEQ</span><br><span class="line">Giywiu9xR5Jng], [R], s[STARTED], a[id=ACE7jGGoS6yhS9T_Sn53UA]], failure [RemoteTransportException[[idc-function-elk08][172.16.20.108:9300][indices:data/write/bulk[s][r]]]; nested: Cir</span><br><span class="line">cuitBreakingException[[parent] Data too large, data for [&lt;transport_request&gt;] would be [20720204958/19.2gb], which is larger than the limit of [20401094656/19gb], real usage: [2068074</span><br><span class="line">9096/19.2gb], new bytes reserved: [39455862/37.6mb], usages [request=197056/192.4kb, fielddata=1310/1.2kb, in_flight_requests=11463397212/10.6gb, accounting=54576960/52mb]]; ], markAs</span><br><span class="line">Stale [true]]</span><br><span class="line">org.elasticsearch.transport.RemoteTransportException: [idc-function-elk08][172.16.20.108:9300][indices:data/write/bulk[s][r]]</span><br><span class="line">Caused by: org.elasticsearch.common.breaker.CircuitBreakingException: [parent] Data too large, data for [&lt;transport_request&gt;] would be [20720204958/19.2gb], which is larger than the l</span><br><span class="line">imit of [20401094656/19gb], real usage: [20680749096/19.2gb], new bytes reserved: [39455862/37.6mb], usages [request=197056/192.4kb, fielddata=1310/1.2kb, in_flight_requests=114633972</span><br><span class="line">12/10.6gb, accounting=54576960/52mb]</span><br><span class="line">        at org.elasticsearch.indices.breaker.HierarchyCircuitBreakerService.checkParentLimit(HierarchyCircuitBreakerService.java:347) ~[elasticsearch-7.7.0.jar:7.7.0]</span><br><span class="line">        at org.elasticsearch.common.breaker.ChildMemoryCircuitBreaker.addEstimateBytesAndMaybeBreak(ChildMemoryCircuitBreaker.java:128) ~[elasticsearch-7.7.0.jar:7.7.0]</span><br></pre></td></tr></table></figure><p>编辑<code>/etc/elasticsearch/elasticsearch.yml</code>配置文件.新增如下配置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">thread_pool:</span><br><span class="line">    write:</span><br><span class="line">        size: 32</span><br><span class="line">        queue_size: 10000</span><br><span class="line">processors: 32</span><br></pre></td></tr></table></figure><p>同时,修改<code>/etc/logstash/logstash.yml</code>配置文件.修改如下配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pipeline.batch.size: 10000</span><br><span class="line">pipeline.batch.delay: 100</span><br></pre></td></tr></table></figure><blockquote><p>以上这些配置都需要不断的调试,修改,观察,找到最适合的一个范围和效果</p></blockquote><hr><h3 id="四-集群最大分片-shard-数"><a href="#四-集群最大分片-shard-数" class="headerlink" title="四.集群最大分片(shard)数"></a>四.集群最大分片(shard)数</h3><p>今天收集新的日志报错,logstash日志内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2020-08-24T10:32:41,009][WARN ][logstash.outputs.elasticsearch][main][7ca4981ae091d6b8f604e5695405b2c74a9cb7fc4a72b338be4e2ede66a04d7d] Could not index event to Elasticsearch. &#123;:status=&gt;400, :action=&gt;[&quot;index&quot;, &#123;:_id=&gt;nil, :_index=&gt;&quot;logstash-hsq-search-netrcd-2020.08.24&quot;, :routing=&gt;nil, :_type=&gt;&quot;_doc&quot;&#125;, #&lt;LogStash::Event:0x4a0b491&gt;], :response=&gt;&#123;&quot;index&quot;=&gt;&#123;&quot;_index&quot;=&gt;&quot;logstash-hsq-search-netrcd-2020.08.24&quot;, &quot;_type&quot;=&gt;&quot;_doc&quot;, &quot;_id&quot;=&gt;nil, &quot;status&quot;=&gt;400, &quot;error&quot;=&gt;&#123;&quot;type&quot;=&gt;&quot;illegal_argument_exception&quot;, &quot;reason&quot;=&gt;&quot;Validation Failed: 1: this action would add [14] total shards, but this cluster currently has [8998]/[9000] maximum shards open;&quot;&#125;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure><p>看日志关键字发现当前集群总共分片数是8998个,如果再加上14个分片,那么就超过了最大的9000个.</p><p><strong>14和9000个分片是怎么来的?</strong></p><p>当前ES集群中有7个热节点,每个索引分片数量是7个,副本数是1..也就是说一个索引就要14个分片(包括副本)</p><p>另外,集群中还有2个冷节点,总共是9个节点.从Elasticsearch v7.0.0 开始，集群中的每个节点默认限制 1000 个shard.所以集群所有节点总共是9000个shard(分片)</p><p><strong>解决方案</strong></p><p>1.每个索引(Index)分配多少个分片(shared)合适?</p><p>配置Elasticsearch集群后,对于分片的数量通常比较难确定.分配过小或者过大对性能都不好.实际上每个分片都会消耗硬件资源:</p><ul><li>由于分片本质上是Lucene索引，因此会消耗文件句柄，内存和CPU资源。</li><li>每个搜索请求都将触摸索引中每个分片的副本，当分片分布在多个节点上时，这不是问题。当分片争夺相同的硬件资源时，就会出现争用并且性能会下降。</li></ul><p>我个人理解一般设置分片数量有几个方向可以考虑</p><ul><li>由于Elasticsearch的最大JVM一般在30-32G.所以一个分片的数量不能超过30G大小.如果一个索引最大是200G.那么就需要分片7个分片.</li><li>最好是按照日志创建索引,可以按天,或者周,甚至月来创建索引,如果一个月的日志太大,那么就按天创建索引</li><li>分片数量的配置最好是基于当前的日志量级,集群节点数量评估.或者可以规划为可以预见的增长幅度评估,千万不能盲目的为一年或者2年以后的日志量分配资源</li><li>如果不能确定索引容量,或者对于分片设置完全没有概念和把握,那么建议按照ES集群节点数量设置分片,有多少个节点,就设置多少分片,后期再观察和考虑是否需要增加或删减</li><li><p>生产环境中,最好设置为1个副本.副本有助于加速查询和健壮高可用行,但是也会占用磁盘容量空间,没必要设置2个或以上的副本.如果是做了冷热分离,对于冷数据如果没有太大的安全性要求,也可以不设置副本</p><p>2.每个节点的<code>maximum shards open</code>设置为多大合适</p></li></ul><p><strong>根据以下几个指标来评估:</strong></p><p>1.当前集群的shard分片数</p><p>2.当前集群索引量</p><p>3.索引保存时间</p><p>以我们生产集群为例,当前集群共有7个热节点,每个索引7个分片.每节点是一个分片.每天是55个索引,加上副本是110个索引,也就是每个节点,每天有110个shard分片.</p><p>热节点索引保留14天,单节点总共是110*14=1540个Shard.预留20%的空间,也就是1848个分片.所以给每个节点分配1800-2000个分片比较合适</p><p>另外,集群中还有2个冷节点.超过14天的索引会自动迁移到热节点(没有副本),并且保留14天后删除.所以冷节点14天总共分片数是55*7*14=5390个Shard.分摊到2个节点,平均每个节点的Shard是2695..留出20%的空间,每节点的Shard为3234.</p><p><strong>经过评估下来,ES热节点每节点Shard数量2000,冷节点3500</strong></p><p>编辑ES配置文件,添加以下配置,修改每节点的Shard数量</p><ul><li><p>热节点: <code>cluster.max_shards_per_node: 2000</code></p></li><li><p>冷节点: <code>cluster.max_shards_per_node: 3500</code></p></li></ul><p>以下是Ansible发布模板</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;% if es_role  is defined and es_role == &apos;hot&apos; %&#125;</span><br><span class="line">cluster.max_shards_per_node: 2000</span><br><span class="line">&#123;% elif es_role  is defined and es_role == &apos;cold&apos; %&#125;</span><br><span class="line">cluster.max_shards_per_node: 3500</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure><p>重启ES后并没有生效.临时在Kibana的dev tool中设置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">PUT /_cluster/settings</span><br><span class="line">&#123;</span><br><span class="line">  &quot;transient&quot;: &#123;</span><br><span class="line">    &quot;cluster&quot;: &#123;</span><br><span class="line">      &quot;max_shards_per_node&quot;:3000</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>设置立即生效,在ELK集群面板中,总的shard分片数超过了9000的最大值,已经达到9,376 shards,</p><p>博客参考:<a href="https://studygolang.com/articles/25396" target="_blank" rel="noopener">https://studygolang.com/articles/25396</a></p><hr><h3 id="五-分片严重不均衡"><a href="#五-分片严重不均衡" class="headerlink" title="五.分片严重不均衡"></a>五.分片严重不均衡</h3><p>ES集群运行了将近一个月后,出现了分片验证不均衡的现象.当前7个Hot节点中,每个索引一共有7个分片,但是我们发现大部分索引的7个分片全部写入到ELK09这个节点,其他节点没有分配任何分片.如下图所示:</p><p><img src="https://img2.jesse.top/image-20200904103820376.png" alt="image-20200904103820376"></p><p>即使将elk09这个节点的磁盘警戒线和最高水位线下调到50%和80%,但是仍然不起作用,磁盘使用率依然上升到90%以上.</p><p>也尝试过,严格限定每个索引每节点最大分片数4个,但是ES会将所有4个分片全部写入到ELK09节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">PUT _template/logstash</span><br><span class="line">&#123;</span><br><span class="line">    &quot;index_patterns&quot;: &quot;logstash-*&quot;,</span><br><span class="line">    &quot;settings&quot;: &#123;</span><br><span class="line">        &quot;index.routing.allocation.total_shards_per_node&quot;: 4 #每节点单索引最大分片数4个</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>之所以设置为4个,是因为每个索引7个分片在14天后会自动迁移到ES冷节点,而一共2个冷节点,所以每个节点4分片.(冷数据没有副本)</p></blockquote><p>观察nodes节点,elk09这个节点的内存,CPU,负载等各指标相比其他节点都高出一大截</p><p><img src="https://img2.jesse.top/image-20200904103802250.png" alt="image-20200904103802250"></p><p><strong>故障原因:</strong></p><p>故障原因在于其他热节点(elk01-elk06)有2块1.92T的SSD磁盘,总容量3.4T,这台elk09节点只有一块磁盘1.92T的SSD磁盘..这台elk09节点的磁盘容量只有其他节点的一半.</p><p>ELK默认是平均分配分片在节点中.当ES中数据较少时,分片会平均分配到各节点中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1245 759.7gb     1tb   3.9tb 4.9tb 20 172.16.20.108 172.16.20.108 idc-function-elk08</span><br><span class="line">1426 771.3gb 798.9gb   2.6tb 3.4tb 22 172.16.20.101 172.16.20.101 idc-function-elk01</span><br><span class="line">1426   770gb 795.6gb   2.7tb 3.4tb 22 172.16.20.102 172.16.20.102 idc-function-elk02</span><br><span class="line">1426 770.6gb 796.1gb   2.7tb 3.4tb 22 172.16.20.104 172.16.20.104 idc-function-elk04</span><br><span class="line">1426 770.7gb   811gb   2.6tb 3.4tb 22 172.16.20.105 172.16.20.105 idc-function-elk05</span><br><span class="line">1426 771.1gb 843.9gb   2.6tb 3.4tb 23 172.16.20.103 172.16.20.103 idc-function-elk03</span><br><span class="line">1426 770.9gb 797.1gb   2.6tb 3.4tb 22 172.16.20.106 172.16.20.106 idc-function-elk06</span><br><span class="line">1426 771.4gb 798.6gb 982.2gb 1.7tb 44 172.16.20.109 172.16.20.109 idc-function-elk09</span><br><span class="line">1244 792.5gb   869gb   4.1tb 4.9tb 16 172.16.20.107 172.16.20.107 idc-function-elk07</span><br></pre></td></tr></table></figure><p>但是当索引数据越来越大,elk09磁盘由于警戒线和水位线设置,所以索引分片更多写入到elk01-06这些节点,这就造成elk09节点上的分片数量和其他节点相比差距越来越大.</p><p>下图是故障发生时,可以看到每个节点的分片数在2787左右.但是ELK09这个节点的分片在2155个.</p><p><img src="https://img2.jesse.top/image-20200904103746474.png" alt="image-20200904103746474"></p><p>这就导致ES为了平衡各节点分配数量,将新索引的分片集中全部写入到elk09这个节点,以求集群各节点分片数量平衡,从而忽略了节点的磁盘使用率和硬件资源状态.</p><p><strong>解决办法</strong></p><p>有以下3种解决办法可以参考</p><p>1.给elk09添加一块同容量SSD磁盘,这样所有节点的磁盘容量一样,这也是最根本的解决办法</p><p>2.适当减少ES的索引保留天数,减少数据量,使得elk09单节点总数据量不超过单块磁盘容量(1.92T)中的50%左右区间.</p><p>3.调整集群参数<code>cluster.routing.allocation.balance.shard</code>和<code>cluster.routing.allocation.balance.index</code>.这2个值越大,集群更倾向于在集群中平均负载分片,所以这2个需要调小,具体什么值合适,需要不断的观察和测试.</p><p>关于这2个参数的介绍可以参考官网或者博客:</p><p><a href="https://www.bookstack.cn/read/ELKstack-guide-cn/elasticsearch-principle-shard-allocate.md" target="_blank" rel="noopener">https://www.bookstack.cn/read/ELKstack-guide-cn/elasticsearch-principle-shard-allocate.md</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;记一次生产ELK性能优化&quot;&gt;&lt;a href=&quot;#记一次生产ELK性能优化&quot; class=&quot;headerlink&quot; title=&quot;记一次生产ELK性能优化&quot;&gt;&lt;/a&gt;记一次生产ELK性能优化&lt;/h3&gt;&lt;p&gt;ES上线后遇到一些问题:&lt;/p&gt;
&lt;h3 id=&quot;一-内存压力过高&quot;&gt;&lt;a href=&quot;#一-内存压力过高&quot; class=&quot;headerlink&quot; title=&quot;一.内存压力过高&quot;&gt;&lt;/a&gt;一.内存压力过高&lt;/h3&gt;&lt;p&gt;目前ES节点的有两种规格内存.&lt;/p&gt;
&lt;p&gt;80GB内存(节点同时运行了Logstash,分配了16G给logstash).&lt;/p&gt;
&lt;p&gt;54GB内存(只运行ES,并且只作为data节点)&lt;/p&gt;
&lt;p&gt;ES的JVM内存从32G下降到28G.建议JVM内存是总物理内存的一半左右,&lt;/p&gt;
&lt;p&gt;节点内存使用情况如下:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;quot;mem&amp;quot;: &amp;#123; - &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;total&amp;quot;: &amp;quot;78.5gb&amp;quot;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;total_in_bytes&amp;quot;: 84296601600,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;free&amp;quot;: &amp;quot;4.3gb&amp;quot;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;free_in_bytes&amp;quot;: 4688633856,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;used&amp;quot;: &amp;quot;74.1gb&amp;quot;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;used_in_bytes&amp;quot;: 79607967744,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;free_percent&amp;quot;: 6,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          &amp;quot;used_percent&amp;quot;: 94&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;建议: ES节点内存建议配置高一点,虽然ES的JVM内存最大不超过32G.但是在其他方面仍然很吃内存&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>kibana查询语法</title>
    <link href="https://jesse.top/2020/08/25/elk/kibana%E6%9F%A5%E8%AF%A2%E8%AF%AD%E6%B3%95/"/>
    <id>https://jesse.top/2020/08/25/elk/kibana查询语法/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kibana查询语法"><a href="#kibana查询语法" class="headerlink" title="kibana查询语法"></a>kibana查询语法</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>在kibana6.3版本开始,kibana引入了Kibana Query Language(KQL)查询语法.并且从7.0版本开始已经是Kibana的默认查询语言.KQL包含脚本化字段查询和简易化的查询.</p><p><img src="https://img2.jesse.top/image-20200724104504825.png" alt="image-20200724104504825"></p><p>kibana支持以下两种查询语法:</p><ul><li><p>Kibana query language (KQL)</p></li><li><p>Lucene query syntax (Lucene)</p></li></ul><hr><a id="more"></a><h3 id="Discover"><a href="#Discover" class="headerlink" title="Discover"></a>Discover</h3><p>kibana的日志展示和搜索查询在discover界面中.在查询之前需要先指定:</p><ul><li>索引名.指定要查询的索引,索引类似于关系型数据库(比如mysql)的库.</li><li>选择一个时间范围,或者时间点</li></ul><p><strong>discovery界面介绍</strong></p><p>下面是一个搜索展示的页面</p><p><img src="https://img2.jesse.top/image-20200724105414932.png" alt="image-20200724105414932"></p><p>在界面中会统计查询结果条数,以及日志时间,在下面会展示查询结果的具体日志内容,以及高亮显示关键字.</p><p>左侧栏中有字段值可以选择,例如在下面的搜索结果中,选择responsetime字段,可以详细看到字段的具体值,并且倒序排序..如果选定了字段后,在右侧就优先展示该字段的内容</p><p><img src="https://img2.jesse.top/image-20200724160502495.png" alt="image-20200724160502495"></p><p>也可以在查询结果中,对其他字段进行排序,统计,例如在responsetime大于2的文档中,对request进行排序,</p><p><img src="https://img2.jesse.top/image-20200724161002418.png" alt="image-20200724161002418"></p><h5 id="保存查询"><a href="#保存查询" class="headerlink" title="保存查询"></a>保存查询</h5><p>对经常需要查询的字段或者语法,可以保存查询,方便下次再次查询,</p><p><img src="https://img2.jesse.top/image-20200724161528813.png" alt="image-20200724161528813"></p><hr><h3 id="查询语法"><a href="#查询语法" class="headerlink" title="查询语法"></a>查询语法</h3><p>以下主要讲解KQL的查询语法.Lucene的查询语法大同小异,有区别之处会另行说明.</p><p>详细的语法可以参考elastic官网:<a href="https://www.elastic.co/guide/en/kibana/current/search.html" target="_blank" rel="noopener">kibana文档</a></p><h4 id="全文搜索"><a href="#全文搜索" class="headerlink" title="全文搜索"></a>全文搜索</h4><p>在搜索栏直接输入要查询的关键词,如果关键词包含空格,特殊字符,中文等,使用双引号起来作为一个短语进行搜索.</p><blockquote><p>当数据量较大时,全文搜索比较缓慢,因为ES需要去日志的所有字段中依次查找关键字</p></blockquote><p><strong>注意</strong></p><p>当需要搜索msf-api1这个关键词时,需要将这个短语用双引号括起来,否则会搜索任何满足msf关键词的数据</p><hr><h4 id="字段匹配"><a href="#字段匹配" class="headerlink" title="字段匹配"></a>字段匹配</h4><p>全文搜索比较缓慢,因为需要从所有字段中查找匹配的关键字,这个时候指定字段名,查询速度就会非常快,.</p><p><code>语法格式: 字段名:值</code> </p><p>查询状态为200的Nginx日志: <code>status:200</code></p><p>查询主机为msf-api1的所有日志: hostname: “msf-api1”</p><hr><h4 id="逻辑操作"><a href="#逻辑操作" class="headerlink" title="逻辑操作"></a>逻辑操作</h4><p>操作符: <code>AND</code> ,<code>OR</code></p><p>例如.我要寻找request请求URL字段中包含关键字632303f301bebf55724ed28eaba6ec6477cb5670,同时来自msf-api1的服务器,并且status状态为200的日志</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">request:632303f301bebf55724ed28eaba6ec6477cb5670 AND status:200 AND hostname:&quot;msf-api1&quot;</span><br></pre></td></tr></table></figure><blockquote><p>在KQL查询语法中,AND操作符不能省略,否则会报错,但是在Lucene中可以省略</p></blockquote><p>默认<code>AND</code>操作符的优先级要高于<code>OR</code>,例如<code>response:200 and extension:php or extension:css</code>会匹配所有response为200并且extension为php,或者extension为CSS但是response为所有的文档.</p><h5 id="单字段逻辑查询"><a href="#单字段逻辑查询" class="headerlink" title="单字段逻辑查询"></a>单字段逻辑查询</h5><p>逻辑操作符还有更方便的做法,可以针对一个单一的字段指定多个值.例如</p><p><code>status:(200 or 499)</code> 表示查询status为200或者499的文档</p><p><code>tags:(success and info and security)</code> 表示同时具有以上tags的文档</p><hr><h4 id="分组查询"><a href="#分组查询" class="headerlink" title="分组查询"></a>分组查询</h4><p>像数学运算一下,可以使用括号来指定优先级,</p><p>比如<code>response:200 and (extension:php or extension:css)</code>会匹配所有response为200并且extension是php或者css的查询结果</p><hr><h4 id="not操作符"><a href="#not操作符" class="headerlink" title="not操作符"></a>not操作符</h4><p>使用not前缀,表示一个否定的查询.</p><p>例如: <code>not response:200</code> 会匹配所有response不等于200的文档</p><p>not还可以结合分组查询使用</p><p><code>response:200 and not (extension:php or extension:css)</code>匹配所有response为200,并且extesnion不等于php或者css的文档</p><hr><h4 id="KQL范围查询"><a href="#KQL范围查询" class="headerlink" title="KQL范围查询"></a>KQL范围查询</h4><p>使用运算符号<code>&gt;</code>,<code>&gt;=</code>,<code>&lt;</code>,<code>&lt;=</code>等可以进行范围查询.</p><p>在使用范围查询的时候可以省略字段后的冒号,比如查询Nginx中的cost值大于1秒的文档.</p><p><code>responsetime&gt;1</code> (在我的nginx日志中定义responsetime字段表示cost值)</p><p>IP地址也可以使用运算符查询,例如查询大于10.111.30.34的主机IP:</p><p><code>host.ip&gt;10.111.30.34</code></p><hr><h4 id="Lucene的范围查询"><a href="#Lucene的范围查询" class="headerlink" title="Lucene的范围查询:"></a>Lucene的范围查询:</h4><p>格式: <code>[START_VALUE TO END_VALUE]</code></p><p>例如查询Nginx400-499的状态:</p><p><code>status: [400 TO 499]</code></p><p>例如查询10个以上的计数:</p><p><code>count:[10 TO *]</code></p><p>2012年之前的日期</p><p><code>date:{* TO 2012-01-01}</code> 或者 <code>date:[* TO 2012-01-01]</code></p><p>查询IP地址范围:<code>host_ip:[10.111.10.34 to 10.111.10.40]</code></p><p>中括号和大括号可以混合使用,例如下面例子中表示从1-4的数字,(不包括5)</p><p><code>count:[1 TO 5}</code></p><blockquote><p>注：<code>[ ]</code>表示端点数值包含在范围内，<code>{ }</code> 表示端点数值不包含在范围内</p></blockquote><p><strong>Lucene范围运算符</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">age:&gt;10</span><br><span class="line">age:&gt;=10</span><br><span class="line">age:&lt;10</span><br><span class="line">age:&lt;=10</span><br></pre></td></tr></table></figure><p>例如:要寻找cost值在1-2秒之间的文档,可以写成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#1.responsetime:(&gt;=1 AND &lt;=2)</span><br><span class="line">#2.responsetime:[1 TO 2]</span><br><span class="line">#3.responsetime:(+&gt;1 +&lt;2)</span><br></pre></td></tr></table></figure><hr><h4 id="Lucene-加减查询"><a href="#Lucene-加减查询" class="headerlink" title="Lucene 加减查询"></a>Lucene 加减查询</h4><p>Lucene的<code>+</code>和<code>-</code>号有特殊涵义:</p><p><code>+</code>: 表示匹配该项</p><p><code>-</code>:表示不匹配该项.和<code>NOT</code>作用类似</p><p>例如下面这个例子匹配responsetime大于2,但是status不为200的文档</p><p><code>+responsetime:&gt;2 -status:200</code></p><p>在下面的例子中表示fox关键字必须匹配,news关键字必须排除,quick和brown则是可选项,有就显示,没有也无所谓</p><p><code>quick brown +fox -news</code></p><hr><h4 id="通配符查询"><a href="#通配符查询" class="headerlink" title="通配符查询"></a>通配符查询</h4><p>KQL也支持通配符字段,使用<code>字段:*</code>表示查询所有存在该字段的文档.在下面的例子中表示查询所有msf-api开头的主机</p><p><code>hostname:&quot;msf-api*&quot;</code></p><p>通配符不仅可以应用在值上,在字段中也可以使用通配符,比如有<code>machine.os</code>和<code>machine.os.keyword</code>这2个字段.如果想查找这两个字段都包含一个<code>windows 10</code>的值,可以使用<code>machine.os*:windows 10</code></p><hr><h4 id="lucene正则"><a href="#lucene正则" class="headerlink" title="lucene正则"></a>lucene正则</h4><p>lucene支持部分正则语法.</p><p><code>?</code>表示匹配一个单一的字符</p><p><code>*</code>表示匹配0到无限个字符</p><p><code>~</code>波浪线表示模糊匹配..例如<code>quikc~ brwn~ fok~</code></p><blockquote><p>官方建议最好不要使用正则查询,因为会消耗大量内存并且查询速度较慢</p></blockquote><p>正则表达式可以使用斜杠包括起来.例如:</p><p><code>name:/joh?n(ath[oa]n)/</code></p><hr><h4 id="特殊字符转换"><a href="#特殊字符转换" class="headerlink" title="特殊字符转换"></a>特殊字符转换</h4><p>如果查询的字段或者内容中包含以下特殊字符,需要转义</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">+ - = &amp;&amp; || &gt; &lt; ! ( ) &#123; &#125; [ ] ^ &quot; ~ * ? : \ /</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kibana查询语法&quot;&gt;&lt;a href=&quot;#kibana查询语法&quot; class=&quot;headerlink&quot; title=&quot;kibana查询语法&quot;&gt;&lt;/a&gt;kibana查询语法&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;在kibana6.3版本开始,kibana引入了Kibana Query Language(KQL)查询语法.并且从7.0版本开始已经是Kibana的默认查询语言.KQL包含脚本化字段查询和简易化的查询.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2.jesse.top/image-20200724104504825.png&quot; alt=&quot;image-20200724104504825&quot;&gt;&lt;/p&gt;
&lt;p&gt;kibana支持以下两种查询语法:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Kibana query language (KQL)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lucene query syntax (Lucene)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>Kibana图表制作</title>
    <link href="https://jesse.top/2020/08/25/elk/Kibana%E5%9B%BE%E8%A1%A8%E5%88%B6%E4%BD%9C/"/>
    <id>https://jesse.top/2020/08/25/elk/Kibana图表制作/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kibana图表制作"><a href="#Kibana图表制作" class="headerlink" title="Kibana图表制作"></a>Kibana图表制作</h2><p>kibana的可视化可以制作各种统计分析图表,然后合并展示到dashbord中.下面介绍一些常用的nginx的访问图表.</p><blockquote><p>在kibana7.7版本中可以配置kibana web界面的中文语言.</p></blockquote><p>编辑kibana配置文件,<code>/etc/kibana/kibana.yml</code>,加入以下配置: </p><p><code>i18n.locale: &quot;zh-CN&quot;</code></p><h4 id="一-统计过去XXX时间的访问量"><a href="#一-统计过去XXX时间的访问量" class="headerlink" title="一.统计过去XXX时间的访问量"></a>一.统计过去XXX时间的访问量</h4><p>在可视化界面,添加仪表盘图表.指定nginx的openapi access访问日志索引.</p><p>无需进行任何配置,选择右上角的时间范围,会显示计数,也就是日志量的计数</p><p><img src="https://img2.jesse.top/image-20200723154706247.png" alt="image-20200723154706247"></p><a id="more"></a><h3 id="openapi-nginx流量图"><a href="#openapi-nginx流量图" class="headerlink" title="openapi-nginx流量图"></a>openapi-nginx流量图</h3><p><img src="https://img2.jesse.top/image-20200806155157188.png" alt="image-20200806155157188"></p><h4 id="添加城市访问地图"><a href="#添加城市访问地图" class="headerlink" title="添加城市访问地图"></a>添加城市访问地图</h4><p>在<code>map</code>界面新建一个地图.在<code>road map</code>的地图上添加图层.</p><p><img src="https://img2.jesse.top/image-20200723155049422.png" alt="image-20200723155049422"></p><p>选择数据源.选择文档类型:</p><p><img src="https://img2.jesse.top/image-20200723160002287.png" alt="image-20200723160002287"></p><p>选择索引后,选择<code>geoip.location</code>字段,此时客户端地图分布自动展现出现,而且会自动计数</p><p><img src="https://img2.jesse.top/image-20200723160127185.png" alt="image-20200723160127185"></p><h4 id="Nginx状态码统计图"><a href="#Nginx状态码统计图" class="headerlink" title="Nginx状态码统计图"></a>Nginx状态码统计图</h4><p>添加饼图,使用<code>status</code>指标来统计各状态码的次数.</p><p><img src="https://img2.jesse.top/image-20200723163110993.png" alt="image-20200723163110993"></p><blockquote><p>如果没有status字段,需要去刷新索引</p></blockquote><h4 id="Nginx访问客户端TOP5"><a href="#Nginx访问客户端TOP5" class="headerlink" title="Nginx访问客户端TOP5"></a>Nginx访问客户端TOP5</h4><p>添加垂直条形图.使用geoip关键词统计各IP的访问次数,并且按降序排序</p><p><img src="https://img2.jesse.top/image-20200723163406793.png" alt="image-20200723163406793"></p><h4 id="nginx请求URL的TOP5"><a href="#nginx请求URL的TOP5" class="headerlink" title="nginx请求URL的TOP5"></a>nginx请求URL的TOP5</h4><p>添加数据图表,使用request关键词统计各URL的访问次数,并按降序排序</p><p><img src="https://img2.jesse.top/image-20200723163554578.png" alt="image-20200723163554578"></p><h4 id="nginx的cost请求时间TOP10"><a href="#nginx的cost请求时间TOP10" class="headerlink" title="nginx的cost请求时间TOP10"></a>nginx的cost请求时间TOP10</h4><p>添加垂直条形图.使用responsetime关键词做聚合,统计请求最慢的cost时间</p><p><img src="https://img2.jesse.top/image-20200723163717829.png" alt="image-20200723163717829"></p><hr><h4 id="在Dashboard中将多个图表聚合成一个大盘"><a href="#在Dashboard中将多个图表聚合成一个大盘" class="headerlink" title="在Dashboard中将多个图表聚合成一个大盘"></a>在Dashboard中将多个图表聚合成一个大盘</h4><p><img src="https://img2.jesse.top/image-20200723163906182.png" alt="image-20200723163906182"></p><p>​                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kibana图表制作&quot;&gt;&lt;a href=&quot;#Kibana图表制作&quot; class=&quot;headerlink&quot; title=&quot;Kibana图表制作&quot;&gt;&lt;/a&gt;Kibana图表制作&lt;/h2&gt;&lt;p&gt;kibana的可视化可以制作各种统计分析图表,然后合并展示到dashbord中.下面介绍一些常用的nginx的访问图表.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在kibana7.7版本中可以配置kibana web界面的中文语言.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;编辑kibana配置文件,&lt;code&gt;/etc/kibana/kibana.yml&lt;/code&gt;,加入以下配置: &lt;/p&gt;
&lt;p&gt;&lt;code&gt;i18n.locale: &amp;quot;zh-CN&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&quot;一-统计过去XXX时间的访问量&quot;&gt;&lt;a href=&quot;#一-统计过去XXX时间的访问量&quot; class=&quot;headerlink&quot; title=&quot;一.统计过去XXX时间的访问量&quot;&gt;&lt;/a&gt;一.统计过去XXX时间的访问量&lt;/h4&gt;&lt;p&gt;在可视化界面,添加仪表盘图表.指定nginx的openapi access访问日志索引.&lt;/p&gt;
&lt;p&gt;无需进行任何配置,选择右上角的时间范围,会显示计数,也就是日志量的计数&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2.jesse.top/image-20200723154706247.png&quot; alt=&quot;image-20200723154706247&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>生产环境部署ELK7+ES冷热数据分离</title>
    <link href="https://jesse.top/2020/08/25/elk/%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2ELK7+ES%E5%86%B7%E7%83%AD%E6%95%B0%E6%8D%AE%E5%88%86%E7%A6%BB/"/>
    <id>https://jesse.top/2020/08/25/elk/生产环境部署ELK7+ES冷热数据分离/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.314Z</updated>
    
    <content type="html"><![CDATA[<h2 id="生产环境部署ELK7-ES冷热数据分离"><a href="#生产环境部署ELK7-ES冷热数据分离" class="headerlink" title="生产环境部署ELK7+ES冷热数据分离"></a>生产环境部署ELK7+ES冷热数据分离</h2><h3 id="需求整理"><a href="#需求整理" class="headerlink" title="需求整理"></a>需求整理</h3><p>当前公司还没有日志收集系统,.线上故障排错需要开发和运维人员通过跳板机登陆到业务服务器去查看日志,如果集群下服务器节点较多,可能需要登陆到每一台服务器才能找到准确的故障信息.这种方式会带来许多问题:</p><ul><li>效率低下</li><li>需要给研发人员分配生产服务器账号,有安全隐患</li><li>查看日志的方式极为麻烦,复杂</li><li>每台业务服务器需要大容量磁盘保存日志文件</li></ul><hr><h3 id="Elasticsearch冷热数据分离"><a href="#Elasticsearch冷热数据分离" class="headerlink" title="Elasticsearch冷热数据分离"></a>Elasticsearch冷热数据分离</h3><p>公司目前共有4个APP产品,每天的nginx日志+业务日志+php日志总共预计在1.5T左右.一周的日志数据量在10T左右.Elasticsearch规划了20T的SSD磁盘用来一周的日志数据(7天日志+1个副本).这些数据保存在Elasticsearch的热节点.</p><p>考虑到1周以后的日志数据查询频率非常低,所以可以将这部分的日志迁移和归档到Elasticsearch的冷节点.并且冷节点日志无需索引.所以预留了10T的SAS磁盘保留1周的冷数据.</p><p>总共是20TSSD磁盘的热节点和10TSAS机械磁盘的冷节点</p><a id="more"></a><hr><h3 id="Elasticsearch集群节点规划"><a href="#Elasticsearch集群节点规划" class="headerlink" title="Elasticsearch集群节点规划"></a>Elasticsearch集群节点规划</h3><ul><li><strong>热节点规划</strong></li></ul><p>ES集群的DATA节点负责保存收集到的日志数据,索引,整理和查询日志等工作.所以data节点对硬件资源要求较高.</p><p>计划使用3台物理服务器(VMware虚拟化集群),每台物理机的配置是32核134G,每台物理机有2个VM.一共计划6个elasticsearch的节点.</p><p>每个物理服务器上使用4块1.92T的SSD磁盘.平均每个VM虚拟机可以使用16核67G内存4TSSD磁盘.</p><blockquote><p>后期又增加了一台服务器,配置为16核48G内存2TSSD</p></blockquote><p>7个节点一共有20TSSD磁盘空间,刚好满足7天的日志数据的需求.</p><blockquote><p>为了避免磁盘IO竞争.每个节点使用独立的单块SSD磁盘,</p></blockquote><ul><li><strong>冷节点规划</strong></li></ul><p>集群冷节点可以适当放低硬件资源.计划使用2台VM作为ES冷节点.每台服务器为8核32G.为了更充分利用带宽和CPU资源.以及合理规划ES的节点角色.冷节点同时又作为集群的master节点和logstash的角色.</p><blockquote><p>这样热节点就有充分资源作为data角色</p></blockquote><hr><h3 id="ELK架构规划"><a href="#ELK架构规划" class="headerlink" title="ELK架构规划"></a>ELK架构规划</h3><p>为了ELK的高性能和数据传输安全性,完整的架构如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Filebeat------&gt;logstash-------&gt;Elasticsearch--------&gt;kibana-------&gt;nginx</span><br></pre></td></tr></table></figure><h5 id="以下是各组件作用介绍"><a href="#以下是各组件作用介绍" class="headerlink" title="以下是各组件作用介绍:"></a>以下是各组件作用介绍:</h5><ul><li>Filebeat: 轻量级的日志收集Agent,部署在每台业务服务器</li><li>logstash(5节点): 从filebeat费日志数据,并对数据进行加工处理,传输给ES.由于集群资源本身的稳定和健壮,logstash并不需要使用持久化数据特性.</li><li>Elasticsearch(7节点):保存和索引数据.7个热节点和2个冷节点</li><li>kibana:从Elasticsearch获取数据并在web界面展示</li><li>nginx: kibana的代理服务器</li></ul><p>官网要求ELK组件必须使用相同的版本,这次部署的是ELK的最新版.7.7版本</p><hr><h3 id="ELK服务器信息"><a href="#ELK服务器信息" class="headerlink" title="ELK服务器信息"></a>ELK服务器信息</h3><table><thead><tr><th>主机名</th><th>IP地址</th><th>操作系统</th><th>运行组件</th><th>功能</th><th>配置</th></tr></thead><tbody><tr><td>idc-function-elk01</td><td>172.16.20.101</td><td>CentOS7.7</td><td>logstash,es</td><td>logstash节点,data,master节点</td><td>16核80G3.84TSSD</td></tr><tr><td>idc-function-elk02</td><td>172.16.20.102</td><td>CentOS7.7</td><td>es</td><td>ES data节点</td><td>16核54G3.84TSSD</td></tr><tr><td>idc-function-elk03</td><td>172.16.20.103</td><td>CentOS7.7</td><td>logstash,es</td><td>logstash节点,data节点</td><td>16核80G3.84TSSD</td></tr><tr><td>idc-function-elk04</td><td>172.16.20.104</td><td>CentOS7.7</td><td>es</td><td>ES data节点</td><td>16核54G3.84TSSD</td></tr><tr><td>idc-function-elk05</td><td>172.16.20.105</td><td>CentOS7.7</td><td>logstash,es</td><td>logstash节点,data节点</td><td>16核80G3.84TSSD</td></tr><tr><td>idc-function-elk06</td><td>172.16.20.106</td><td>CentOS7.7</td><td>es</td><td>ES data节点</td><td>16核54G3.84TSSD</td></tr><tr><td>idc-function-elk07</td><td>172.16.20.107</td><td>CentOS7.7</td><td>es</td><td>ES data节点</td><td>16核54G1.92TSSD</td></tr><tr><td>idc-function-elk08</td><td>172.16.20.108</td><td>CentOS7.7</td><td>logstash,es</td><td>ES的master,冷节点,logstash节点</td><td>8核32G9.6TSAS</td></tr><tr><td>idc-function-elk09</td><td>172.16.20.109</td><td>CentOS7.7</td><td>logstash,es</td><td>ES的主master节点,冷节点,logstash节点</td><td>16核32G5TSAS</td></tr><tr><td>idc-function-docker</td><td>172.16.20.30</td><td>CentOS7.7</td><td>nginx</td><td>一台现有的服务器</td><td></td></tr><tr><td>idc-function-elk10</td><td>172.16.20.110</td><td>CentOS7.7</td><td>logstash,es</td><td>仅仅作为ES的master节点,,logstash节点</td><td>8核8G200GSAS</td></tr></tbody></table><hr><h3 id="部署安装"><a href="#部署安装" class="headerlink" title="部署安装"></a>部署安装</h3><h4 id="部署elasticsearch"><a href="#部署elasticsearch" class="headerlink" title="部署elasticsearch"></a>部署elasticsearch</h4><ul><li><strong>部署JAVA环境</strong>.</li></ul><p>官网要求需要JDK8或者11版本.这里部署的是11版本.在Oracle有rpm包.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@idc-function-elk01 ~]$ java -version</span><br><span class="line">java version &quot;11.0.7&quot; 2020-04-14 LTS</span><br><span class="line">Java(TM) SE Runtime Environment 18.9 (build 11.0.7+8-LTS)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.7+8-LTS, mixed mode)</span><br></pre></td></tr></table></figure><ul><li><strong>安装elasticsearch</strong></li></ul><p>官网的yum安装非常慢.先从elasticsearch中文社区下载rpm包.然后使用yum本地安装</p><p>中文社区下载中心:<a href="https://elasticsearch.cn/download/" target="_blank" rel="noopener">https://elasticsearch.cn/download/</a></p><ul><li><strong>配置elasticserch</strong></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#以下是ES的配置文件</span></span><br><span class="line"><span class="string">[root@idc-function-elk01</span> <span class="string">~]#</span> <span class="string">cat</span> <span class="string">/etc/elasticsearch/elasticsearch.yml</span> <span class="string">| sed '/^#/d'</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#ES集群名.每个节点需要定义同一个集群名</span></span><br><span class="line"><span class="string">cluster.name: dwd-elk  </span></span><br><span class="line"><span class="string">#节点名,一般和主机名一致</span></span><br><span class="line"><span class="string">node.name: idc-function-elk01</span></span><br><span class="line"><span class="string">#节点是否为master节点</span></span><br><span class="line"><span class="string">node.master: true</span></span><br><span class="line"><span class="string">#节点是否为data节点</span></span><br><span class="line"><span class="string">node.data: true</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#添加属性,标志位热节点.如果是冷节点,该值为cold</span></span><br><span class="line"><span class="string">node.attr.box_type: hot </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#数据存储路径,指定多块磁盘</span></span><br><span class="line"><span class="string">path.data: /data,/data1</span></span><br><span class="line"><span class="string">#日志存储路径</span></span><br><span class="line"><span class="string">path.logs: /data/logs/elasticsearch</span></span><br><span class="line"><span class="string">#是否需要锁定内存,不适应swap虚拟内存</span></span><br><span class="line"><span class="string">bootstrap.memory_lock: true</span></span><br><span class="line"><span class="string">#绑定IP.本机的内网IP</span></span><br><span class="line"><span class="string">network.host: 172.16.20.101</span></span><br><span class="line"><span class="string">#默认端口</span></span><br><span class="line"><span class="string">http.port: 9200</span></span><br><span class="line"><span class="string">#集群内其他ES节点地址</span></span><br><span class="line"><span class="string">discovery.seed_hosts: ["172.16.20.101","172.16.20.102","172.16.20.103","172.16.20.104","172.16.20.105","172.16.20.106","172.16.20.107","172.16.20.108","172.16.20.109"]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#初始化master节点,#master节点分布在不同的物理服务器</span></span><br><span class="line"><span class="string">cluster.initial_master_nodes: ["172.16.20.101","172.16.20.108","172.16.20.109"] </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#磁盘水位线</span></span><br><span class="line"><span class="string">cluster.routing.allocation.disk.watermark.low: 85%</span></span><br><span class="line"><span class="string">cluster.routing.allocation.disk.watermark.high: 90%</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#用于 fielddata 的最大内存，如果 fielddata 达到该阈值，就会把旧数据交换出去。该参数可以设置百分比或者绝对值。默认设置是不限制，所#以强烈建议设置该值</span></span><br><span class="line"><span class="string">indices.fielddata.cache.size: 10%</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#limit值需要大于indices.fielddata.cache.size的值。否则的话fielddata大小一到limit阈值就报错,就永远道不了size阈值,无法触发对##旧数据的交换任务</span></span><br><span class="line"><span class="string">indices.breaker.fielddata.limit: 30%</span></span><br><span class="line"><span class="string">[root@idc-function-elk01 ~]#</span></span><br></pre></td></tr></table></figure><ul><li><p><strong>优化系统和elasticsearch配置</strong></p><ul><li><strong>优化内核参数</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]<span class="comment"># cat /etc/sysctl.conf</span></span><br><span class="line"></span><br><span class="line">net.ipv4.ip_forward=1</span><br><span class="line">net.ipv6.conf.all.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 = 1</span><br><span class="line">net.ipv6.conf.lo.disable_ipv6 = 1</span><br><span class="line">vm.swappiness = 0</span><br><span class="line">net.ipv4.neigh.default.gc_stale_time=120</span><br><span class="line">net.ipv4.conf.all.rp_filter=0</span><br><span class="line">net.ipv4.conf.default.rp_filter=0</span><br><span class="line">net.ipv4.conf.default.arp_announce = 2</span><br><span class="line">net.ipv4.conf.lo.arp_announce=2</span><br><span class="line">net.ipv4.conf.all.arp_announce=2</span><br><span class="line">net.ipv4.tcp_max_tw_buckets = 5000</span><br><span class="line">net.ipv4.tcp_syncookies = 1</span><br><span class="line">net.ipv4.tcp_max_syn_backlog = 1024</span><br><span class="line">net.ipv4.tcp_synack_retries = 2</span><br><span class="line">kernel.core_uses_pid = 1 <span class="comment">#追加进程号到core文件名中</span></span><br><span class="line">fs.suid_dumpable = 2 <span class="comment">#确保设置属主的进程也可以生成core文件</span></span><br><span class="line">kernel.core_pattern = /tmp/core-%e-%s-%u-%g-%p-%t <span class="comment">#指定core文件生成的位置和文件名规则。</span></span><br><span class="line">vm.overcommit_memory = 1</span><br><span class="line">net.ipv4.conf.lo.arp_announce=2</span><br><span class="line">kernel.sysrq=1</span><br><span class="line">fs.file-max=6553500 <span class="comment">#最大文件句柄</span></span><br><span class="line">fs.nr_open=6553500  <span class="comment">#最大文件句柄</span></span><br><span class="line">net.core.somaxconn = 65535</span><br><span class="line">vm.max_map_count=262144  <span class="comment">#ES官网推荐</span></span><br><span class="line">net.ipv4.tcp_keepalive_time = 600</span><br><span class="line">net.ipv4.tcp_keepalive_probes = 3</span><br><span class="line">net.ipv4.tcp_keepalive_intvl = 15</span><br><span class="line">net.ipv4.tcp_tw_reuse = 1</span><br><span class="line">net.ipv4.tcp_tw_recycle = 0</span><br><span class="line">net.ipv4.tcp_syn_retries = 1</span><br><span class="line">net.ipv4.tcp_synack_retries = 1</span><br><span class="line">net.ipv4.tcp_timestamps=0</span><br><span class="line">net.ipv4.tcp_fin_timeout=30</span><br><span class="line">net.ipv4.ip_local_port_range = 10000 65000</span><br><span class="line">net.nf_conntrack_max = 655360</span><br><span class="line">net.netfilter.nf_conntrack_tcp_timeout_established = 1200</span><br></pre></td></tr></table></figure><ul><li><strong>设置文件句柄,内存</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]<span class="comment"># cat /etc/security/limits.conf</span></span><br><span class="line">elasticsearch soft memlock unlimited</span><br><span class="line">elasticsearch  hard memlock unlimited</span><br><span class="line">* soft nofile 655360</span><br><span class="line">* hard nofile 655360</span><br></pre></td></tr></table></figure><ul><li><strong>关闭Selinux和防火墙</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line"></span><br><span class="line">setenforce 0</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk01 ~]# cat /etc/selinux/config</span><br><span class="line"></span><br><span class="line">SELINUX=disabled</span><br></pre></td></tr></table></figure><ul><li><strong>创建目录,并且修改权限</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]# ll /data/elasticsearch/ -d</span><br><span class="line">drwxr-xr-x 3 elasticsearch elasticsearch 27 May 25 23:08 /data/elasticsearch/</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk01 ~]# ll /data/logs/elasticsearch/ -d</span><br><span class="line">drwxr-xr-x 2 elasticsearch elasticsearch 4096 May 26 00:00 /data/logs/elasticsearch/</span><br></pre></td></tr></table></figure><ul><li><strong>修改elasticsearch的systemctl启动文件,加入下面这一行</strong>:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]# cat /usr/lib/systemd/system/elasticsearch.service</span><br><span class="line"></span><br><span class="line">#新增这一行.否则启动的时候会提示在配置文件中开启了bootstrap.memory_lock: true.但是系统中没有检测到配置</span><br><span class="line">#unlimit memory</span><br><span class="line">LimitMEMLOCK=infinity</span><br></pre></td></tr></table></figure><ul><li><strong>修改elasticsearch的JVM内存</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]# cat /etc/elasticsearch/jvm.options</span><br><span class="line"></span><br><span class="line">-Xms32g</span><br><span class="line">-Xmx32g</span><br></pre></td></tr></table></figure><ul><li>启动elasticsearch</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable elasticsearch</span><br><span class="line">  systemctl start elasticsearch</span><br></pre></td></tr></table></figure></li></ul><hr><h3 id="Elasticsearch集群监控工具"><a href="#Elasticsearch集群监控工具" class="headerlink" title="Elasticsearch集群监控工具"></a>Elasticsearch集群监控工具</h3><p>这里推荐使用cerebro工具,是一个独立的小工具,无需安装到es插件.具体使用方法请参考:<a href="https://github.com/lmenezes/cerebro" target="_blank" rel="noopener">github项目地址</a></p><p>启动cerebro后.访问cerebro的9000端口.可以看到ES集群状态监控界面,界面还挺不错</p><p><img src="https://img2.jesse.top/image-20200724092916238.png" alt="image-20200724092916238"></p><blockquote><p>从上面的集群截图中可以看到,每个ES节点都有<code>hot</code>或者<code>cold</code>标签以区分节点属性.这就说明冷热节点属性配置正常</p></blockquote><p>但是每次登陆cerebro都需要选择连接的ES节点.无法实现登陆自动连接.这非常不方便,我在github上提交了相关<a href="https://github.com/lmenezes/cerebro/issues/450" target="_blank" rel="noopener">issue</a>.得到回复是使用以下URL:  <a href="http://localhost:9000/#/overview?host=http:%2F%2Flocalhost:9200" target="_blank" rel="noopener">http://localhost:9000/#/overview?host=http:%2F%2Flocalhost:9200</a></p><p>所以.可以配置Nginx反向代理:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line"> listen 80;</span><br><span class="line"> server_name elk.doweidu.com;</span><br><span class="line"></span><br><span class="line"> access_log /data/logs/nginx/elk_access.log main;</span><br><span class="line"> error_log /data/logs/nginx/elk_error.log;</span><br><span class="line"></span><br><span class="line">location / &#123;</span><br><span class="line">        proxy_set_header   Host  $host;</span><br><span class="line">        proxy_set_header   X-Real-IP $remote_addr;</span><br><span class="line">        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">   rewrite ^/(.*)  http://172.16.20.101:9000/#/overview?host=http:%2F%2Flocalhost:9200/$1 permanent;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>172.16.20.101:9000这个为cerebro所在服务器.localhost:9200是同台服务器的ES节点.如果ES节点为其他服务器,使用IP地址指定即可</p></blockquote><hr><h3 id="部署logstash"><a href="#部署logstash" class="headerlink" title="部署logstash"></a>部署logstash</h3><p>logstash也是运行在JVM的java环境中.安装方法和ES一样.</p><ul><li><strong>配置logstash文件</strong></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@idc-function-elk01</span> <span class="string">~]#</span> <span class="string">vim</span> <span class="string">/etc/logstash/logstash.yml</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#这个参数是指单次接收的块大小.可以根据实际场景和性能压力不断的调整和测试</span></span><br><span class="line"><span class="string">pipeline.batch.size:</span> <span class="number">1500</span></span><br><span class="line"><span class="string">pipeline.batch.delay:</span> <span class="number">10</span></span><br><span class="line"><span class="comment">#节点名称</span></span><br><span class="line"><span class="string">node.name:</span> <span class="string">idc-function-elk01</span></span><br><span class="line"><span class="comment">#数据路径</span></span><br><span class="line"><span class="string">path.data:</span> <span class="string">/data/logstash</span></span><br><span class="line"><span class="comment">#日志路径</span></span><br><span class="line"><span class="string">path.logs:</span> <span class="string">/data/logs/logstash</span></span><br><span class="line"><span class="comment">#绑定IP,本机内网IP地址</span></span><br><span class="line"><span class="string">http.host:</span> <span class="string">"172.16.20.101"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#下面这个对logstash性能比较重要的配置保持默认即可.默认就是CPU的内核数</span></span><br><span class="line"><span class="comment"># This defaults to the number of the host's CPU cores.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># pipeline.workers: 2</span></span><br></pre></td></tr></table></figure><ul><li><strong>修改JVM内存</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk01 ~]# vim /etc/logstash/jvm.options</span><br><span class="line">-Xms16g</span><br><span class="line">-Xmx16g</span><br></pre></td></tr></table></figure><ul><li>systemctl启动logstash</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service logstash start</span><br></pre></td></tr></table></figure><hr><h3 id="部署到其他服务器"><a href="#部署到其他服务器" class="headerlink" title="部署到其他服务器"></a>部署到其他服务器</h3><p>如果通过克隆或者模板的方式部署到其他服务器,那么ES集群应该会报错,提示无法将节点加入到集群中.这是因为集群中所有节点的data数据目录拥有同一个ID.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">can&apos;t add node &#123;idc-function-elk03&#125;&#123;dt57hBjCQ9uCAp8G6sxJYw&#125;&#123;7-s_05ZMSl6CtpmMZGZEUA&#125;&#123;172.16.20.103&#125;&#123;172.16.20.103:9300&#125;&#123;dilmrt&#125;&#123;ml.machine_memory=61044445184, ml.max_open_jobs=20, xpack.installed=true, transform.node=true&#125;, found existing node &#123;idc-function-elk01&#125;&#123;dt57hBjCQ9uCAp8G6sxJYw&#125;&#123;biZPCqjCSx-PX3eU-CquUw&#125;&#123;172.16.20.101&#125;&#123;172.16.20.101:9300&#125;&#123;dilmrt&#125;&#123;ml.machine_memory=61044445184, ml.max_open_jobs=20, xpack.installed=true, transform.node=true&#125; with the same id but is a different node instance</span><br></pre></td></tr></table></figure><p>删除data数据路径下的内容,然后重启ES.等待集群自动同步</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk03 ~]# rm -rf /data/elasticsearch/nodes</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk03 ~]# ll /data/elasticsearch/</span><br><span class="line">total 0</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk03 ~]# systemctl restart elasticsearch</span><br><span class="line"></span><br><span class="line">[root@idc-function-elk03 ~]# ll /data/elasticsearch/</span><br><span class="line">drwxr-xr-x 3 elasticsearch elasticsearch 15 May 26 17:30 nodes</span><br><span class="line">[root@idc-function-elk03 ~]#</span><br></pre></td></tr></table></figure><hr><h3 id="kibana安装"><a href="#kibana安装" class="headerlink" title="kibana安装"></a>kibana安装</h3><p>安装方式非常简单,直接下载rpm包安装即可.安装完成后修改<code>/etc/kibana/kibana.yaml</code>配置文件.新增以下配置.将kibana的web界面改为中文:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">i18n.locale: &quot;zh-CN&quot;</span><br></pre></td></tr></table></figure><blockquote><p>我这里是采用rpm包安装,如果是其他安装方式,请自行寻找配置文件路径</p></blockquote><p><strong>nginx反向代理配置</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">  server_name kibana.doweidu.com;</span><br><span class="line"></span><br><span class="line">  error_log /data/logs/nginx/kibana.error.log;</span><br><span class="line">  access_log /data/logs/nginx/kibana.access.log main;</span><br><span class="line"></span><br><span class="line">listen 443 ssl http2;</span><br><span class="line">  ssl_certificate /data/letsencrypt/kibana.doweidu.com/fullchain.cer;</span><br><span class="line">  ssl_certificate_key /data/letsencrypt/kibana.doweidu.com/kibana.doweidu.com.key;</span><br><span class="line">  include /data/letsencrypt/options-ssl-nginx.conf;</span><br><span class="line">  ssl_dhparam /data/letsencrypt/ssl-dhparams.pem;</span><br><span class="line"></span><br><span class="line">  location / &#123;</span><br><span class="line">    proxy_pass http://172.16.20.105:5601;</span><br><span class="line">    proxy_set_header Host $host;</span><br><span class="line">    proxy_redirect off;</span><br><span class="line">    proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">    proxy_connect_timeout    60;</span><br><span class="line">    proxy_read_timeout       60;</span><br><span class="line">    proxy_send_timeout       60;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">if ($host = kibana.doweidu.com) &#123;</span><br><span class="line">    return 301 https://$host$request_uri;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">listen 80;</span><br><span class="line">server_name kibana.doweidu.com;</span><br><span class="line">return 404;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h4 id="安装filebeat"><a href="#安装filebeat" class="headerlink" title="安装filebeat."></a>安装filebeat.</h4><p>对于有supervisor的服务器,采用二进制安装,否则使用rpm包安装</p><p>二进制包下载地址:<a href="http://repo.doweidu.com/ELK/filebeat-7.7.0-linux-x86_64.tar.gz" target="_blank" rel="noopener">http://repo.doweidu.com/ELK/filebeat-7.7.0-linux-x86_64.tar.gz</a></p><p>rpm包下载地址:<a href="http://repo.doweidu.com/ELK/filebeat-7.7.0-x86_64.rpm" target="_blank" rel="noopener">http://repo.doweidu.com/ELK/filebeat-7.7.0-x86_64.rpm</a></p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;生产环境部署ELK7-ES冷热数据分离&quot;&gt;&lt;a href=&quot;#生产环境部署ELK7-ES冷热数据分离&quot; class=&quot;headerlink&quot; title=&quot;生产环境部署ELK7+ES冷热数据分离&quot;&gt;&lt;/a&gt;生产环境部署ELK7+ES冷热数据分离&lt;/h2&gt;&lt;h3 id=&quot;需求整理&quot;&gt;&lt;a href=&quot;#需求整理&quot; class=&quot;headerlink&quot; title=&quot;需求整理&quot;&gt;&lt;/a&gt;需求整理&lt;/h3&gt;&lt;p&gt;当前公司还没有日志收集系统,.线上故障排错需要开发和运维人员通过跳板机登陆到业务服务器去查看日志,如果集群下服务器节点较多,可能需要登陆到每一台服务器才能找到准确的故障信息.这种方式会带来许多问题:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;效率低下&lt;/li&gt;
&lt;li&gt;需要给研发人员分配生产服务器账号,有安全隐患&lt;/li&gt;
&lt;li&gt;查看日志的方式极为麻烦,复杂&lt;/li&gt;
&lt;li&gt;每台业务服务器需要大容量磁盘保存日志文件&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&quot;Elasticsearch冷热数据分离&quot;&gt;&lt;a href=&quot;#Elasticsearch冷热数据分离&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearch冷热数据分离&quot;&gt;&lt;/a&gt;Elasticsearch冷热数据分离&lt;/h3&gt;&lt;p&gt;公司目前共有4个APP产品,每天的nginx日志+业务日志+php日志总共预计在1.5T左右.一周的日志数据量在10T左右.Elasticsearch规划了20T的SSD磁盘用来一周的日志数据(7天日志+1个副本).这些数据保存在Elasticsearch的热节点.&lt;/p&gt;
&lt;p&gt;考虑到1周以后的日志数据查询频率非常低,所以可以将这部分的日志迁移和归档到Elasticsearch的冷节点.并且冷节点日志无需索引.所以预留了10T的SAS磁盘保留1周的冷数据.&lt;/p&gt;
&lt;p&gt;总共是20TSSD磁盘的热节点和10TSAS机械磁盘的冷节点&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>使用ElastAlert+ELK实现日志监控钉钉告警</title>
    <link href="https://jesse.top/2020/08/25/elk/%E4%BD%BF%E7%94%A8ElastAlert+ELK%E5%AE%9E%E7%8E%B0%E6%97%A5%E5%BF%97%E7%9B%91%E6%8E%A7%E5%91%8A%E8%AD%A6/"/>
    <id>https://jesse.top/2020/08/25/elk/使用ElastAlert+ELK实现日志监控告警/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-29T01:51:30.236Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用ElastAlert-ELK实现日志监控钉钉告警"><a href="#使用ElastAlert-ELK实现日志监控钉钉告警" class="headerlink" title="使用ElastAlert+ELK实现日志监控钉钉告警"></a>使用ElastAlert+ELK实现日志监控钉钉告警</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>目前公司使用ELK做日志收集和展示分析.所以想对一些关键日志进行监控告警.比如Nginx的5xx日志,比如php-fpm的Fatal严重错误日志等.通过监控ES的日志数据,然后使用Python调用钉钉接口来实现日志的告警</p><hr><h3 id="ElastAlert介绍"><a href="#ElastAlert介绍" class="headerlink" title="ElastAlert介绍"></a>ElastAlert介绍</h3><p>ElastAlert是一个开源的工具,用于从Elastisearch中检索数据,并根据匹配模式发出告警.github项目地址如下:<a href="https://github.com/Yelp/elastalert" target="_blank" rel="noopener">https://github.com/Yelp/elastalert</a></p><p>官方文档如下:<a href="https://elastalert.readthedocs.io/en/latest/elastalert.html" target="_blank" rel="noopener">https://elastalert.readthedocs.io/en/latest/elastalert.html</a></p><p>它支持多种监控模式和告警方式,具体可以查阅Github项目介绍.但是自带的ElastAlert并不支持钉钉告警,在github上有第三方的钉钉python项目.地址如下:<a href="https://github.com/xuyaoqiang/elastalert-dingtalk-plugin" target="_blank" rel="noopener">https://github.com/xuyaoqiang/elastalert-dingtalk-plugin</a></p><p>第三方的钉钉告警插件并没有艾特相关人员的功能,所以我再此基础上进行了二次开发,增加了这个功能</p><hr><a id="more"></a><h3 id="ElastAlert安装"><a href="#ElastAlert安装" class="headerlink" title="ElastAlert安装"></a>ElastAlert安装</h3><blockquote><p>新版的ElastAlert不支持python2了.所以需要安装Python3环境</p></blockquote><ul><li>安装依赖</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum -y install python3 python3-devel python3-libs python3-setuptools git gcc</span><br></pre></td></tr></table></figure><p>如果是Ubuntu系统:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt -y upgrade</span><br><span class="line">sudo apt install -y python3.6-dev</span><br><span class="line">sudo apt install -y libffi-dev libssl-dev</span><br><span class="line">sudo apt install -y python3-pip</span><br><span class="line">sudo apt install -y python3-venv</span><br></pre></td></tr></table></figure><ul><li>安装elastalert模块</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install elastalert  -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com</span><br></pre></td></tr></table></figure><ul><li>克隆ElastAlert项目</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/Yelp/elastalert.git</span><br><span class="line">cp -r elastalert /data/</span><br></pre></td></tr></table></figure><ul><li>安装模块</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /data/elastalert/</span><br><span class="line">pip3 install &quot;setuptools&gt;=11.3&quot;</span><br><span class="line"></span><br><span class="line">pip install -r requirements.txt  -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com</span><br></pre></td></tr></table></figure><ul><li>创建ElastAlert的索引</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo elastalert-create-index --index elastalert</span><br></pre></td></tr></table></figure><ul><li>修改ElastAlert的配置文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cp config.yaml.example config.yaml</span><br><span class="line">vim config.yaml</span><br><span class="line"></span><br><span class="line">rules_folder: rule  #rule匹配模式的目录,可以自定义一个/data/elastalert路径下的相对目录</span><br><span class="line">run_every:          #ElastAlert多久向Elasticsearch发送一次请求</span><br><span class="line">  minutes: 1</span><br><span class="line">buffer_time:        #如果某些日志源不是实时的，则ElastAlert将缓冲最近一段时间的结果.这个值默认是15,但是无法触发告警,设置为1正常</span><br><span class="line">  minutes: 1</span><br><span class="line">es_host: localhost      #ES集群节点,随便指定任意一台均可</span><br><span class="line">es_port: 9200           #ES端口号</span><br><span class="line">es_username: elastic    # 如果ES使用了X-pack安全验证,则需要配置此项,否则注释</span><br><span class="line">es_password: password   # 同上</span><br><span class="line">writeback_index: elastalert_status  #ElastAlert索引名</span><br><span class="line">alert_time_limit:       #如果告警发送失败,则会在下面时间范围内尝试重新发送</span><br><span class="line">  days: 2</span><br></pre></td></tr></table></figure><ul><li>配置钉钉报警</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/xuyaoqiang/elastalert-dingtalk-plugin.git</span><br><span class="line">cd elastalert-dingtalk-plugin</span><br><span class="line">pip3 install -r requirements.txt -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com</span><br><span class="line">cp -r elastalert_modules /data/elastalert/</span><br></pre></td></tr></table></figure><hr><h3 id="Rule规则"><a href="#Rule规则" class="headerlink" title="Rule规则"></a>Rule规则</h3><p>官方支持很多Rule模式,在<code>example_rules</code>目录下也有很多参考Rule可以参考.一般常用的是类型(type)是<code>frequence</code></p><p>rule的yaml配置要放在<code>config.yml</code>配置文件中定义的目录下,我这里是rule目录.</p><p>下面这个rule是监控Nginx的5XX状态码,并且调用钉钉告警</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">#rule名字,必须唯一</span><br><span class="line">name:  the count of servnginx log that reponse status code is 5xx and it appears greater than 5 in the period 1 minute</span><br><span class="line"></span><br><span class="line">#类型,官方提供多种类型</span><br><span class="line">type: frequency</span><br><span class="line"></span><br><span class="line">#ES索引,支持通配符</span><br><span class="line">index: logstash-*-nginx-access-*</span><br><span class="line"></span><br><span class="line">#在timeframe时间内,匹配到多少个结果便告警</span><br><span class="line">num_events: 1</span><br><span class="line"></span><br><span class="line">#监控周期.默认是minutes: 1</span><br><span class="line">timeframe:</span><br><span class="line">  seconds: 5  </span><br><span class="line">  </span><br><span class="line">#匹配模式.</span><br><span class="line">filter:</span><br><span class="line">- range:</span><br><span class="line">    status:</span><br><span class="line">      from: 500</span><br><span class="line">      to: 599</span><br><span class="line">      </span><br><span class="line">#告警方式,下面是调用第三方的钉钉告警</span><br><span class="line">alert:</span><br><span class="line">- &quot;elastalert_modules.dingtalk_alert.DingTalkAlerter&quot;</span><br><span class="line"></span><br><span class="line">#钉钉的webhook</span><br><span class="line">dingtalk_webhook: &quot;https://oapi.dingtalk.com/robot/send?access_token=&quot;  #参考地址,需要自行配置</span><br><span class="line">dingtalk_msgtype: text</span><br><span class="line"></span><br><span class="line">#原生的告警信息不友好,自定义告警内容的格式</span><br><span class="line">alert_text: &quot;</span><br><span class="line">域    名: &#123;&#125;\n</span><br><span class="line">调用方式: &#123;&#125;\n</span><br><span class="line">请求链接: &#123;&#125;\n</span><br><span class="line">状 态 码: &#123;&#125;\n</span><br><span class="line">后端服务器: &#123;&#125;\n</span><br><span class="line">数      量: &#123;&#125;</span><br><span class="line">&quot;</span><br><span class="line">alert_text_type: alert_text_only</span><br><span class="line"></span><br><span class="line">#告警内容</span><br><span class="line">alert_text_args:</span><br><span class="line">- domain</span><br><span class="line">- request_method</span><br><span class="line">- request</span><br><span class="line">- status</span><br><span class="line">- upstreamaddr</span><br><span class="line">- num_hits</span><br></pre></td></tr></table></figure><p>测试Rule文件是否正确.在elastalert目录下执行下个命令可以测试某个rule是否正常工作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/bin/elastalert-test-rule --config config.yaml rule/nginx.yaml</span><br></pre></td></tr></table></figure><p>这一步可能会有一些报错情况.一般都是扩展模块版本或者依赖关系的问题.比如下面这个问题,就需要执行<code>pip3 install jira==2.0.0</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/local/bin/elastalert-test-rule&quot;, line 11, in &lt;module&gt;</span><br><span class="line">    load_entry_point(&apos;elastalert==0.1.20&apos;, &apos;console_scripts&apos;, &apos;elastalert-test-rule&apos;)()</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 476, in load_entry_point</span><br><span class="line">    return get_distribution(dist).load_entry_point(group, name)</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 2700, in load_entry_point</span><br><span class="line">    return ep.load()</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 2318, in load</span><br><span class="line">    return self.resolve()</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 2324, in resolve</span><br><span class="line">    module = __import__(self.module_name, fromlist=[&apos;__name__&apos;], level=0)</span><br><span class="line">  File &quot;/usr/local/lib/python3.6/site-packages/elastalert/test_rule.py&quot;, line 20, in &lt;module&gt;</span><br><span class="line">    import elastalert.config</span><br><span class="line">  File &quot;/usr/local/lib/python3.6/site-packages/elastalert/config.py&quot;, line 99</span><br><span class="line">    raise EAException(&quot;Could not import module %s: %s&quot; % (module_name, e)), None, sys.exc_info()[2]</span><br><span class="line">                                                                          ^</span><br><span class="line">SyntaxError: invalid syntax</span><br><span class="line"></span><br><span class="line">或者下面这个错误</span><br><span class="line"></span><br><span class="line">[work@idc-function-elk10 elastalert]$ /usr/local/bin/elastalert-test-rule --config config.yaml rule/nginx.yaml</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 570, in _build_master</span><br><span class="line">    ws.require(__requires__)</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 888, in require</span><br><span class="line">    needed = self.resolve(parse_requirements(requirements))</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 779, in resolve</span><br><span class="line">    raise VersionConflict(dist, req).with_context(dependent_req)</span><br><span class="line">pkg_resources.VersionConflict: (elastalert 0.1.20 (/usr/local/lib/python3.6/site-packages), Requirement.parse(&apos;elastalert==0.2.4&apos;))</span><br><span class="line"></span><br><span class="line">During handling of the above exception, another exception occurred:</span><br><span class="line"></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/local/bin/elastalert-test-rule&quot;, line 6, in &lt;module&gt;</span><br><span class="line">    from pkg_resources import load_entry_point</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 3095, in &lt;module&gt;</span><br><span class="line">    @_call_aside</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 3079, in _call_aside</span><br><span class="line">    f(*args, **kwargs)</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 3108, in _initialize_master_working_set</span><br><span class="line">    working_set = WorkingSet._build_master()</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 572, in _build_master</span><br><span class="line">    return cls._build_from_requirements(__requires__)</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 585, in _build_from_requirements</span><br><span class="line">    dists = ws.resolve(reqs, Environment())</span><br><span class="line">  File &quot;/usr/lib/python3.6/site-packages/pkg_resources/__init__.py&quot;, line 774, in resolve</span><br><span class="line">    raise DistributionNotFound(req, requirers)</span><br><span class="line">pkg_resources.DistributionNotFound: The &apos;jira&gt;=2.0.0&apos; distribution was not found and is required by elastalert</span><br></pre></td></tr></table></figure><hr><h3 id="执行ElastAlert"><a href="#执行ElastAlert" class="headerlink" title="执行ElastAlert"></a>执行ElastAlert</h3><p>一切没问题后,就可以执行ElastAlert.如果是针对单个Rule执行就使用下列命令.(在ElastAlert目录下)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk10 elastalert]# python3  -m elastalert.elastalert --verbose --rule /data/elastalert/rule/nginx.yaml</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">1 rules loaded</span><br><span class="line">INFO:elastalert:Starting up</span><br><span class="line">INFO:elastalert:Disabled rules are: []</span><br><span class="line">INFO:elastalert:Sleeping for 59.999906 seconds</span><br><span class="line">INFO:elastalert:Queried rule the count of servnginx log that reponse status code is 5xx and it appears greater than 5 in the period 1 minute from 2020-08-26 09:11 CST to 2020-08-26 09:26 CST: 10000 / 10000 hits (scrolling..)</span><br><span class="line">INFO:elastalert:Queried rule the count of servnginx log that reponse status code is 5xx and it appears greater than 5 in the period 1 minute from 2020-08-26 09:11 CST to 2020-08-26 09:26 CST: 20000 / 10000 hits (scrolling..)</span><br></pre></td></tr></table></figure><p>等待几秒钟后,钉钉会收到告警(我这里用的是200状态码测试).报警内容是Rule配置文件中自定义的格式和内容</p><p><img src="https://img2.jesse.top/image-20200826094746820.png" alt="image-20200826094746820"></p><hr><h3 id="Rule2-监控php-fpm的Fatal错误信息"><a href="#Rule2-监控php-fpm的Fatal错误信息" class="headerlink" title="Rule2. 监控php-fpm的Fatal错误信息"></a>Rule2. 监控php-fpm的Fatal错误信息</h3><p>fpm的错误日志也收集到了ELK中.我们期望只要pfm日志中出现”Fatal”关键字错误信息就立即告警.最初计划是用ElastAlert的黑名单(blacklist)类型的Rule.但是由于fpm的错误日志没有解析,而是直接保存原始日志,所以不符合要求.</p><p>参考github上我提的ISSUE:<a href="https://github.com/Yelp/elastalert/issues/2937" target="_blank" rel="noopener">balacklist query hits but no matches no alerts</a></p><p>也可以用Any类型的type.Rule文件如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk10 elastalert]# sed &apos;/^#/d&apos; rule/php-fpm.yaml | sed &apos;/^$/d&apos;</span><br><span class="line">name: monitor the fatal,error log in php-fpm log</span><br><span class="line">type: any</span><br><span class="line">index: logstash-*-fpm-error-*</span><br><span class="line">num_events: 1</span><br><span class="line">timeframe:</span><br><span class="line">  seconds: 5</span><br><span class="line">filter:</span><br><span class="line"> - query:</span><br><span class="line">     query_string:</span><br><span class="line">       query: &quot;message: \&quot;PHP Fatal\&quot;&quot;  #匹配Fatal关键字</span><br><span class="line">alert:</span><br><span class="line">- &quot;elastalert_modules.dingtalk_alert.DingTalkAlerter&quot;</span><br><span class="line">dingtalk_webhook: &quot;https://oapi.dingtalk.com/robot/send?access_token=&quot; </span><br><span class="line">dingtalk_msgtype: text</span><br><span class="line">alert_text: &quot;</span><br><span class="line"> 主机: &#123;&#125;\n</span><br><span class="line"> IP地址: &#123;&#125;\n</span><br><span class="line"> 业务线: &#123;&#125;\n</span><br><span class="line"> 日志类型: &#123;&#125;\n</span><br><span class="line"> 完整日志: &#123;&#125;</span><br><span class="line">&quot;</span><br><span class="line">alert_text_type: alert_text_only</span><br><span class="line">alert_text_args:</span><br><span class="line">  - host.name</span><br><span class="line">  - host.ip</span><br><span class="line">  - fields.project</span><br><span class="line">  - fields.type</span><br><span class="line">  - message</span><br></pre></td></tr></table></figure><hr><h3 id="启动ElastAlert"><a href="#启动ElastAlert" class="headerlink" title="启动ElastAlert"></a>启动ElastAlert</h3><p>开启一个Screen然后,使用nohup挂起执行.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python3  -m elastalert.elastalert --verbose &amp;</span><br></pre></td></tr></table></figure><hr><h3 id="钉钉告警二次开发"><a href="#钉钉告警二次开发" class="headerlink" title="钉钉告警二次开发"></a>钉钉告警二次开发</h3><p>当前日志告警只是简单的发送到告警群,由于没有艾特相关人员,所以大家还是无法第一时间看到告警信息,所以需要增加这个功能,大致思路是根据业务线来艾特相关负责人.</p><p>但是中台的业务线有些复杂,因为不同的项目负责人不同.所以需要特殊对待.</p><p><strong>准备工作</strong>:</p><p>日志告警中必须含有以下几个字段:</p><ul><li>业务线</li><li>日志类型</li><li>如果是Nginx日志,则需要有Nginx的域名</li></ul><p>修改原生的钉钉告警的alert动态方法,内容如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">   </span><br><span class="line">   def alert(self, matches):</span><br><span class="line">        headers = &#123;</span><br><span class="line">            &quot;Content-Type&quot;: &quot;application/json&quot;,</span><br><span class="line">            &quot;Accept&quot;: &quot;application/json;charset=utf-8&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        #body拿到的是告警内容字符串</span><br><span class="line">        body = self.create_alert_body(matches) </span><br><span class="line">        #利用正则找到告警日志中的type关键字,也就是日志类别.当前主要有Nginx日志和fpm日志</span><br><span class="line">        res_type = re.findall(&quot;type: ([a-z]+)&quot;, body)</span><br><span class="line">        #找到业务线,当前有hsq,iqg,msf.bbh,mg等业务线</span><br><span class="line">        res_Project = re.findall(r&quot;业务线: ([a-z]+)&quot;, body)</span><br><span class="line">        #如果告警日志没有相关字段,则抛出异常</span><br><span class="line">        if not res_type or not res_Project:</span><br><span class="line">            raise EAException(&quot;告警字段中日志类型或者业务线没有配置&quot;)</span><br><span class="line">            </span><br><span class="line">        #将正则匹配到的列表类型转换为字符串</span><br><span class="line">        Type = &quot;&quot;.join(res_type)</span><br><span class="line">        Project = &quot;&quot;.join(res_Project)</span><br><span class="line">        </span><br><span class="line">        #根据相关业务,艾特相关人员</span><br><span class="line">        if Project == &quot;hsq&quot;:</span><br><span class="line">            at_list = [&apos;1560xxxxxx&apos;]</span><br><span class="line">        elif Project == &quot;iqg&quot;:</span><br><span class="line">            at_list = [&quot;137xxxxxx&quot;]</span><br><span class="line">        elif Project == &quot;bbh&quot;:</span><br><span class="line">            at_list = [&quot;176xxxxxx&quot;]</span><br><span class="line">        elif Project == &quot;msf&quot;:</span><br><span class="line">            at_list = [&quot;180xxxxxx&quot;]</span><br><span class="line">        #如果是中台业务线,并且是Nginx的告警,则需要艾特具体人员</span><br><span class="line">        elif Project == &quot;mg&quot;:</span><br><span class="line">            if &quot;nginx&quot; in Type:</span><br><span class="line">              #匹配到域名</span><br><span class="line">                mg_Project = re.findall(&quot;domain: (.*)\.doweidu\.com&quot;, body)[0]</span><br><span class="line">                #如果是交易中台的域名</span><br><span class="line">                if mg_Project == &quot;trade&quot;: </span><br><span class="line">                    at_list = [&quot;177xxxxxx&quot;]</span><br><span class="line">                #如果是消息中台域名</span><br><span class="line">                elif mg_Project == &quot;message.center&quot;:</span><br><span class="line">                    at_list = [&quot;170xxxxxx&quot;]</span><br><span class="line">                #如果是商品中台域名</span><br><span class="line">                elif mg_Project == &quot;goods.center&quot;:</span><br><span class="line">                    at_list = [&quot;186xxxxxx&quot;]</span><br><span class="line">                #否则艾特中台负责人</span><br><span class="line">                else:</span><br><span class="line">                    at_list = [&quot;186xxxxx&quot;]</span><br><span class="line">            else:</span><br><span class="line">                at_list = [&quot;186xxxxx&quot;]</span><br><span class="line"></span><br><span class="line">#为了防止遗漏,如果没有at_list变量,则艾特我本人.使用locals().keys()可以判断某个变量是否被定义</span><br><span class="line">        if (not &quot;at_list&quot; in locals().keys()): at_list = [&quot;17749739691&quot;]</span><br><span class="line">        payload = &#123;</span><br><span class="line">            &quot;msgtype&quot;: self.dingtalk_msgtype,</span><br><span class="line">            &quot;text&quot;: &#123;</span><br><span class="line">                &quot;content&quot;: body</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;at&quot;: &#123;</span><br><span class="line">                &quot;atMobiles&quot;: at_list,   #艾特相关人员</span><br><span class="line">                &quot;isAtAll&quot;:False</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        try:</span><br><span class="line">            response = requests.post(self.dingtalk_webhook_url, </span><br><span class="line">                        data=json.dumps(payload, cls=DateTimeEncoder),</span><br><span class="line">                        headers=headers)</span><br><span class="line">            response.raise_for_status()</span><br><span class="line">        except RequestException as e:</span><br><span class="line">            raise EAException(&quot;Error request to Dingtalk: &#123;0&#125;&quot;.format(str(e)))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;使用ElastAlert-ELK实现日志监控钉钉告警&quot;&gt;&lt;a href=&quot;#使用ElastAlert-ELK实现日志监控钉钉告警&quot; class=&quot;headerlink&quot; title=&quot;使用ElastAlert+ELK实现日志监控钉钉告警&quot;&gt;&lt;/a&gt;使用ElastAlert+ELK实现日志监控钉钉告警&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;目前公司使用ELK做日志收集和展示分析.所以想对一些关键日志进行监控告警.比如Nginx的5xx日志,比如php-fpm的Fatal严重错误日志等.通过监控ES的日志数据,然后使用Python调用钉钉接口来实现日志的告警&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;ElastAlert介绍&quot;&gt;&lt;a href=&quot;#ElastAlert介绍&quot; class=&quot;headerlink&quot; title=&quot;ElastAlert介绍&quot;&gt;&lt;/a&gt;ElastAlert介绍&lt;/h3&gt;&lt;p&gt;ElastAlert是一个开源的工具,用于从Elastisearch中检索数据,并根据匹配模式发出告警.github项目地址如下:&lt;a href=&quot;https://github.com/Yelp/elastalert&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Yelp/elastalert&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;官方文档如下:&lt;a href=&quot;https://elastalert.readthedocs.io/en/latest/elastalert.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://elastalert.readthedocs.io/en/latest/elastalert.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;它支持多种监控模式和告警方式,具体可以查阅Github项目介绍.但是自带的ElastAlert并不支持钉钉告警,在github上有第三方的钉钉python项目.地址如下:&lt;a href=&quot;https://github.com/xuyaoqiang/elastalert-dingtalk-plugin&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/xuyaoqiang/elastalert-dingtalk-plugin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;第三方的钉钉告警插件并没有艾特相关人员的功能,所以我再此基础上进行了二次开发,增加了这个功能&lt;/p&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>Filebeat+logstash收集Nginx访问日志</title>
    <link href="https://jesse.top/2020/08/25/elk/Filebeat+logstash%E6%94%B6%E9%9B%86Nginx%E8%AE%BF%E9%97%AE%E6%97%A5%E5%BF%97/"/>
    <id>https://jesse.top/2020/08/25/elk/Filebeat+logstash收集Nginx访问日志/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.313Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Filebeat-logstash收集Nginx访问日志"><a href="#Filebeat-logstash收集Nginx访问日志" class="headerlink" title="Filebeat+logstash收集Nginx访问日志"></a>Filebeat+logstash收集Nginx访问日志</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境:"></a>环境:</h3><p><strong>Filebeat</strong>: 7.0</p><p><strong>Logstash</strong>:7.0</p><p><strong>elasticsearch</strong>:7.0</p><hr><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>logstash默认自带了apache标准日志格式的grok正则表达式:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">COMMONAPACHELOG %&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;NOTSPACE:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;(?:%&#123;WORD:verb&#125; %&#123;NOTSPACE:request&#125;(?: HTTP/%&#123;NUMBER:httpversion&#125;)?|%&#123;DATA:rawrequest&#125;)&quot; %&#123;NUMBER:response&#125; (?:%&#123;NUMBER:bytes&#125;|-)</span><br><span class="line">COMBINEDAPACHELOG %&#123;COMMONAPACHELOG&#125; %&#123;QS:referrer&#125; %&#123;QS:agent&#125;</span><br></pre></td></tr></table></figure><p>对于 nginx 标准日志格式，可以发现只是最后多了一个 <code>$http_x_forwarded_for</code> 变量。所以 nginx 标准日志的 grok 正则定义是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MAINNGINXLOG %&#123;COMBINEDAPACHELOG&#125; %&#123;QS:x_forwarded_for&#125;</span><br></pre></td></tr></table></figure><p>如果Nginx日志格式不是标准的日志格式,则需要自行编写grok正则,匹配日志内容.</p><p>但是grok正则表达式不容易上手,非常难写.复杂不说,而且logstash使用正则表达式来处理日志格式,其性能也会受到很大的影响.</p><p>所以这里推荐另外一种收集方式.</p><p>本文档参考博客:<a href="https://www.popyone.com/post/13.html" target="_blank" rel="noopener">https://www.popyone.com/post/13.html</a></p><p>本文档参考书籍:<strong>ELK权威指南中文版第二版</strong></p><a id="more"></a><hr><h3 id="json格式的Nginx日志"><a href="#json格式的Nginx日志" class="headerlink" title="json格式的Nginx日志"></a>json格式的Nginx日志</h3><p>对 logstash 来说，nginx 日志还有另一种更简便的处理方式。就是自定义日志格式时，通过手工拼写，直接输出成 JSON 格式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">log_format json escape=json  <span class="string">'&#123;"@timestamp":"$time_iso8601",'</span></span><br><span class="line">                    <span class="string">'"@source":"$server_addr",'</span></span><br><span class="line">                    <span class="string">'"hostname":"$hostname",'</span></span><br><span class="line">                    <span class="string">'"ip":"$http_x_forwarded_for",'</span></span><br><span class="line">                    <span class="string">'"client":"$remote_addr",'</span></span><br><span class="line">                    <span class="string">'"request_method":"$request_method",'</span></span><br><span class="line">                    <span class="string">'"scheme":"$scheme",'</span></span><br><span class="line">                    <span class="string">'"domain":"$server_name",'</span></span><br><span class="line">                    <span class="string">'"client_host":"$host",'</span></span><br><span class="line">                    <span class="string">'"referer":"$http_referer",'</span></span><br><span class="line">                    <span class="string">'"request":"$request_uri",'</span></span><br><span class="line">                    <span class="string">'"args":"$args",'</span></span><br><span class="line">                    <span class="string">'"size":$body_bytes_sent,'</span></span><br><span class="line">                    <span class="string">'"status": $status,'</span></span><br><span class="line">                    <span class="string">'"responsetime":$request_time,'</span></span><br><span class="line">                    <span class="string">'"upstreamtime":"$upstream_response_time",'</span></span><br><span class="line">                    <span class="string">'"upstreamaddr":"$upstream_addr",'</span></span><br><span class="line">                    <span class="string">'"http_user_agent":"$http_user_agent"'</span></span><br><span class="line">                    <span class="string">'&#125;'</span>;</span><br></pre></td></tr></table></figure><blockquote><p>escape=json 参数，在配置日志格式时加上此参数可以不转义变量内容</p></blockquote><p>nginx虚拟主机的access_log应用了Json的格式后,日志格式自动转换成了json格式了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;@timestamp&quot;:&quot;2019-01-31T11:26:48+08:00&quot;,&quot;@source&quot;:&quot;192.168.12.200&quot;,&quot;hostname&quot;:&quot;proxy.server&quot;,&quot;ip&quot;:&quot;&quot;,&quot;client&quot;:&quot;183.3.239.170&quot;,&quot;request_method&quot;:&quot;GET&quot;,&quot;scheme&quot;:&quot;http&quot;,&quot;domain&quot;:&quot;www.51hangyu.com&quot;,&quot;referer&quot;:&quot;http://www.51hangyu.com/chat&quot;,&quot;request&quot;:&quot;/static/static/css/main.97f6b853.css&quot;,&quot;args&quot;:&quot;&quot;,&quot;size&quot;:460413,&quot;status&quot;: 200,&quot;responsetime&quot;:0.108,&quot;upstreamtime&quot;:&quot;0.002&quot;,&quot;upstreamaddr&quot;:&quot;192.168.12.15:80&quot;,&quot;http_user_agent&quot;:&quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)&quot;&#125;</span><br></pre></td></tr></table></figure><p>这样一来在客户端就自动转换成了json格式,无需logstash通过正则去过滤和匹配日志内容,大大提高logstash的收集性能</p><hr><h3 id="Filebeat配置"><a href="#Filebeat配置" class="headerlink" title="Filebeat配置"></a>Filebeat配置</h3><p>filebeat.yml配置如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">- type: log</span><br><span class="line"></span><br><span class="line">  # Change to true to enable this input configuration.</span><br><span class="line">  enabled: true</span><br><span class="line"></span><br><span class="line">  # Paths that should be crawled and fetched. Glob based paths.</span><br><span class="line">  paths:</span><br><span class="line">    - /data/logs/nginx/hsq_openapi_beta.access.log</span><br><span class="line"></span><br><span class="line">#默认这个值是FALSE的，也就是我们的json日志解析后会被放在json键上。设为TRUE，所有的keys就会被放到根节点</span><br><span class="line">  json.keys_under_root: true</span><br><span class="line"></span><br><span class="line">#是否要覆盖原有的key，这是关键配置，将keys_under_root设为TRUE后，再将overwrite_keys也设为TRUE，就能把filebeat默认的key值给覆盖了</span><br><span class="line">  json.overwrite_keys: true</span><br><span class="line">  exclude_lines: [&apos;HEAD&apos;]    #排除HEAD访问日志,因为HEAD是阿里云SLB的健康检查访问,而且没有外网IP,所有不需要上传</span><br><span class="line">  json.message_key: request_method #HEAD是request_mothod的字段值,所以需要指定字段名</span><br><span class="line">#fields字段用于打标签和索引,在logstash里判断日志来源</span><br><span class="line">  fields:</span><br><span class="line">     type: nginx-openapi</span><br><span class="line">     project: msf</span><br><span class="line">     level: access</span><br><span class="line">     </span><br><span class="line">---output配置,将日志输出到logstash-----</span><br><span class="line">output.logstash:</span><br><span class="line">  # The Logstash hosts</span><br><span class="line">  hosts: [&quot;172.16.20.107:5044&quot;]</span><br></pre></td></tr></table></figure><hr><h3 id="logstash配置"><a href="#logstash配置" class="headerlink" title="logstash配置"></a>logstash配置</h3><p>在logstash的con.d下新建一个Nginx_access的配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk07 elasticsearch]# mkdir /etc/logstash/conf.d/nginx_access.conf</span><br></pre></td></tr></table></figure><p>配置文件内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">[root@idc-function-elk07 elasticsearch]# cat /etc/logstash/conf.d/nginx_access.conf</span><br><span class="line">input &#123;</span><br><span class="line">    beats &#123;</span><br><span class="line">      port =&gt; 5044</span><br><span class="line">      codec =&gt; json</span><br><span class="line">      client_inactivity_timeout =&gt; 600</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">filter &#123;</span><br><span class="line">   if  &quot;openapi-nginx-access&quot; in [tags] &#123;</span><br><span class="line">       date &#123;</span><br><span class="line">           match =&gt; [ &quot;timestamp&quot; ,&quot;dd/MMM/yyyy:HH:mm:ss Z&quot; ]</span><br><span class="line">           target =&gt; &quot;@timestamp&quot;</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">      mutate &#123;</span><br><span class="line">        remove_field =&gt; &quot;timestamp&quot;</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">      geoip &#123;</span><br><span class="line">        source =&gt; &quot;ip&quot;</span><br><span class="line">        target =&gt; &quot;geoip&quot;</span><br><span class="line">        database =&gt; &quot;/etc/logstash/GeoLite2/GeoLite2-City.mmdb&quot;</span><br><span class="line">        add_field =&gt; [&quot;[geoip][coordinates]&quot;,&quot;%&#123;[geoip][longitude]&#125;&quot;]</span><br><span class="line">        add_field =&gt; [&quot;[geoip][coordinates]&quot;,&quot;%&#123;[geoip][latitude]&#125;&quot;]</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">      mutate &#123;</span><br><span class="line">        remove_field =&gt; &quot;geoip.continent_code&quot;</span><br><span class="line">        remove_field =&gt; &quot;geoip.country_code2&quot;</span><br><span class="line">        remove_field =&gt; &quot;geoip.country_code3&quot;</span><br><span class="line">        convert =&gt; [ &quot;[geoip][coordinates]&quot;,&quot;float&quot; ]</span><br><span class="line">        convert =&gt; [ &quot;status&quot;,&quot;integer&quot; ]</span><br><span class="line">        convert =&gt; [ &quot;size&quot;,&quot;integer&quot; ]</span><br><span class="line">        convert =&gt; [ &quot;upstreatime&quot;,&quot;float&quot; ]</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line">    elasticsearch &#123;</span><br><span class="line">            hosts =&gt; [&quot;172.16.20.101:9200&quot;,&quot;172.16.20.110:9200&quot;,&quot;172.16.20.107:9200&quot;,&quot;172.16.20.108:9200&quot;,&quot;172.16.20.102:9200&quot;,&quot;172.16.20.103:9200&quot;,&quot;172.16.20.104:9200&quot;,&quot;172.16.20.105:9200&quot;,&quot;172.16.20.106:9200&quot;,&quot;172.16.20.109:9200&quot;]</span><br><span class="line">            index =&gt; &quot;logstash-%&#123;[fields][project]&#125;-%&#123;[fields][type]&#125;-%&#123;[fields][level]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">            user =&gt; &quot;elastic&quot;</span><br><span class="line">            password =&gt; &quot;password&quot;</span><br><span class="line">            #flush_size =&gt; 20000</span><br><span class="line">            #idle_flush_time =&gt; 10</span><br><span class="line">            sniffing =&gt; true</span><br><span class="line">            template_overwrite =&gt; true</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>建议logstash先将日志输出到控制台测试,确定无误后再输出到es.输出到控制台只需要将output字段修改为:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; output &#123;stdout &#123;&#125;&#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><hr><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>打开es集群的http界面,可以看到产生了索引分片</p><p><img src="https://img2.jesse.top/image-20200706112236216.png" alt="image-20200706112236216"></p><p>打开kibana配置了logstash-hsq-nginx-access-*的索引以后,可以看到展示的日志:</p><p><img src="https://img2.jesse.top/image-20200706112436147.png" alt="image-20200706112436147"></p><hr><h3 id="多个http-x-forwarded-fors上游IP的Nginx访问日志"><a href="#多个http-x-forwarded-fors上游IP的Nginx访问日志" class="headerlink" title="多个http_x_forwarded_fors上游IP的Nginx访问日志"></a>多个http_x_forwarded_fors上游IP的Nginx访问日志</h3><p>iqg业务的Nginx经过了2层代理(阿里云SLB—–&gt;Kong—-&gt;本地Nginx).所以上游IP有2个,使用json格式转换后,Nginx的访问日志内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;@timestamp&quot;:&quot;2020-08-14T03:15:38+08:00&quot;,&quot;@source&quot;:&quot;10.111.10.35&quot;,&quot;hostname&quot;:&quot;iqg-new1&quot;,&quot;ip&quot;:&quot;183.192.43.174, 100.117.85.32&quot;,&quot;client&quot;:&quot;10.111.30.194&quot;,&quot;request_method&quot;:&quot;GET&quot;,&quot;scheme&quot;:&quot;http&quot;,&quot;domain&quot;:&quot;api.v4.iqianggou.com&quot;,&quot;client_host&quot;:&quot;v4.api.iqg-new1&quot;,&quot;referer&quot;:&quot;&quot;,&quot;request&quot;:&quot;/open/user/clientupload?......&#125;</span><br></pre></td></tr></table></figure><p>可以看到IP字段包含了用户真实IP和阿里云SLB的IP.这样的访问日志上传到Logstash后,geoip模块无法处理IP字段.所以需要在logstash中解析和处理IP字段,</p><p>但是还有一个更好的办法,不需要logstash去单独处理,减少logstash的负担.可以在nginx本地将处理IP字段.</p><p>在nginx.conf配置文件的http字段中添加以下配置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">......</span><br><span class="line">map $http_x_forwarded_for  $clientRealIp &#123;</span><br><span class="line">                    &quot;&quot;      $remote_addr;</span><br><span class="line">                            ~^(?P&lt;firstAddr&gt;[0-9\.|:|a-f\.|:|A-F\.|:]+),?.*$  $firstAddr;</span><br><span class="line">         &#125;</span><br><span class="line">......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上map函数解析<code>$http_x_forwarded_for</code>字段,当字段为空时,使用<code>$remote_addr</code>映射为<code>$clientRealIp</code>.否则使用<code>$firstAddr</code>映射.</p><p>同时修改json日志格式,使用<code>$clientRealIp</code>替代<code>$http_x_forwarded_for</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">log_format json escape=json  &apos;&#123;&quot;@timestamp&quot;:&quot;$time_iso8601&quot;,&apos;</span><br><span class="line">                    &apos;&quot;@source&quot;:&quot;$server_addr&quot;,&apos;</span><br><span class="line">                    &apos;&quot;hostname&quot;:&quot;$hostname&quot;,&apos;</span><br><span class="line">                    &apos;&quot;ip&quot;:&quot;$clientRealIp&quot;,&apos;</span><br><span class="line">                    &apos;&quot;client&quot;:&quot;$remote_addr&quot;,&apos;</span><br><span class="line">                    &apos;&quot;request_method&quot;:&quot;$request_method&quot;,&apos;</span><br><span class="line">                    &apos;&quot;scheme&quot;:&quot;$scheme&quot;,&apos;</span><br><span class="line">                    &apos;&quot;domain&quot;:&quot;$server_name&quot;,&apos;</span><br><span class="line">                    &apos;&quot;client_host&quot;:&quot;$host&quot;,&apos;</span><br><span class="line">                    &apos;&quot;referer&quot;:&quot;$http_referer&quot;,&apos;</span><br><span class="line">                    &apos;&quot;request&quot;:&quot;$request_uri&quot;,&apos;</span><br><span class="line">                    &apos;&quot;args&quot;:&quot;$args&quot;,&apos;</span><br><span class="line">                    &apos;&quot;size&quot;:$body_bytes_sent,&apos;</span><br><span class="line">                    &apos;&quot;status&quot;: $status,&apos;</span><br><span class="line">                    &apos;&quot;responsetime&quot;:$request_time,&apos;</span><br><span class="line">                    &apos;&quot;upstreamtime&quot;:&quot;$upstream_response_time&quot;,&apos;</span><br><span class="line">                    &apos;&quot;upstreamaddr&quot;:&quot;$upstream_addr&quot;,&apos;</span><br><span class="line">                    &apos;&quot;http_user_agent&quot;:&quot;$http_user_agent&quot;&apos;</span><br><span class="line">                    &apos;&#125;&apos;;</span><br></pre></td></tr></table></figure><p>重启Nginx</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo nginx -t</span><br><span class="line">sudo nginx -s reload</span><br></pre></td></tr></table></figure><p>再次查看日志,发现此时ip字段只保留了公网IP地址,去掉了阿里云的SLB内网地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;@timestamp&quot;:&quot;2020-08-14T09:46:59+08:00&quot;,&quot;@source&quot;:&quot;10.111.10.35&quot;,&quot;hostname&quot;:&quot;iqg-new1&quot;,&quot;ip&quot;:&quot;113.247.47.77&quot;,&quot;client&quot;:&quot;10.111.30.194&quot;,&quot;request_method&quot;:&quot;GET&quot;,&quot;scheme&quot;:&quot;http&quot;,&quot;domain&quot;:&quot;api.v4.iqianggou.com&quot;,&quot;client_host&quot;:&quot;v4.api.iqg-new1&quot;,&quot;referer&quot;:&quot;https://servicewechat.com/wxa36b4671d95ba753/86/page-frame.html&quot;,&quot;request&quot;:&quot;/api/item?......&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Filebeat-logstash收集Nginx访问日志&quot;&gt;&lt;a href=&quot;#Filebeat-logstash收集Nginx访问日志&quot; class=&quot;headerlink&quot; title=&quot;Filebeat+logstash收集Nginx访问日志&quot;&gt;&lt;/a&gt;Filebeat+logstash收集Nginx访问日志&lt;/h2&gt;&lt;h3 id=&quot;环境&quot;&gt;&lt;a href=&quot;#环境&quot; class=&quot;headerlink&quot; title=&quot;环境:&quot;&gt;&lt;/a&gt;环境:&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Filebeat&lt;/strong&gt;: 7.0&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Logstash&lt;/strong&gt;:7.0&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;elasticsearch&lt;/strong&gt;:7.0&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;logstash默认自带了apache标准日志格式的grok正则表达式:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;COMMONAPACHELOG %&amp;#123;IPORHOST:clientip&amp;#125; %&amp;#123;USER:ident&amp;#125; %&amp;#123;NOTSPACE:auth&amp;#125; \[%&amp;#123;HTTPDATE:timestamp&amp;#125;\] &amp;quot;(?:%&amp;#123;WORD:verb&amp;#125; %&amp;#123;NOTSPACE:request&amp;#125;(?: HTTP/%&amp;#123;NUMBER:httpversion&amp;#125;)?|%&amp;#123;DATA:rawrequest&amp;#125;)&amp;quot; %&amp;#123;NUMBER:response&amp;#125; (?:%&amp;#123;NUMBER:bytes&amp;#125;|-)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;COMBINEDAPACHELOG %&amp;#123;COMMONAPACHELOG&amp;#125; %&amp;#123;QS:referrer&amp;#125; %&amp;#123;QS:agent&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;对于 nginx 标准日志格式，可以发现只是最后多了一个 &lt;code&gt;$http_x_forwarded_for&lt;/code&gt; 变量。所以 nginx 标准日志的 grok 正则定义是：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;MAINNGINXLOG %&amp;#123;COMBINEDAPACHELOG&amp;#125; %&amp;#123;QS:x_forwarded_for&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果Nginx日志格式不是标准的日志格式,则需要自行编写grok正则,匹配日志内容.&lt;/p&gt;
&lt;p&gt;但是grok正则表达式不容易上手,非常难写.复杂不说,而且logstash使用正则表达式来处理日志格式,其性能也会受到很大的影响.&lt;/p&gt;
&lt;p&gt;所以这里推荐另外一种收集方式.&lt;/p&gt;
&lt;p&gt;本文档参考博客:&lt;a href=&quot;https://www.popyone.com/post/13.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.popyone.com/post/13.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本文档参考书籍:&lt;strong&gt;ELK权威指南中文版第二版&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>ELK收集mysql5.7慢日志</title>
    <link href="https://jesse.top/2020/08/25/elk/ELK%E6%94%B6%E9%9B%86mysql5.7%E6%85%A2%E6%97%A5%E5%BF%97/"/>
    <id>https://jesse.top/2020/08/25/elk/ELK收集mysql5.7慢日志/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.312Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ELK收集mysql5-7慢日志"><a href="#ELK收集mysql5-7慢日志" class="headerlink" title="ELK收集mysql5.7慢日志"></a>ELK收集mysql5.7慢日志</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>公司ELK平台计划收集生产业务的所有mysql慢日志.由于所有环境中均使用mysql5.7版本,所以其他mysql版本的慢日志格式不在讨论范围之内.</p><p>慢日志的grok正则匹配我折腾了很久,网上的大多文档中给出的logstash的grok正则其实并不能正确的解析到mysql慢日志的字段.</p><p>这个博客的grok正则经过实践可行.而且filebeat,logstash的filter配置也是参考这个博客配置的:<a href="https://www.cnblogs.com/minseo/p/10441913.html" target="_blank" rel="noopener">博客地址</a></p><hr><h3 id="MySQL慢日志"><a href="#MySQL慢日志" class="headerlink" title="MySQL慢日志"></a>MySQL慢日志</h3><p>慢日志格式如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[work@msf-mysql-master log]$ sudo head slow_2019072103.log</span><br><span class="line">/usr/local/mysql5.7/bin/mysqld, Version: 5.7.24-log (MySQL Community Server (GPL)). started with:</span><br><span class="line">Tcp port: 3306  Unix socket: /mysql_log/msf/tmp/mysql.sock</span><br><span class="line">Time                 Id Command    Argument</span><br><span class="line"># Time: 2019-07-21T08:54:04.145255+08:00</span><br><span class="line"># User@Host: u_msf[u_msf] @  [10.111.10.40]  Id: 131421254</span><br><span class="line"># Query_time: 1.595300  Lock_time: 0.000031 Rows_sent: 20  Rows_examined: 809259</span><br><span class="line">use msf_prod;</span><br><span class="line">SET timestamp=1563670444;</span><br><span class="line">SELECT `id`,`type`,`honey`,`remark`,`created_at` FROM `t_log_user_honey` WHERE `user_id` = &apos;1000014423&apos; ORDER BY `created_at` DESC LIMIT 20 OFFSET 0;</span><br><span class="line"># Time: 2019-07-21T10:51:06.184010+08:00</span><br></pre></td></tr></table></figure><a id="more"></a><p>每个日志文件的格式为<code>slow_日期.log</code> .7天切割一次新的日志文件</p><p>每个日志的开头三行是不需要的内容,所以需要filebeat排除</p><p>每一条慢日志有以下几行组成:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Time: 2019-07-21T08:54:04.145255+08:00</span><br><span class="line"># User@Host: u_msf[u_msf] @  [10.111.10.40]  Id: 131421254</span><br><span class="line"># Query_time: 1.595300  Lock_time: 0.000031 Rows_sent: 20  Rows_examined: 809259</span><br><span class="line">use msf_prod;</span><br><span class="line">SET timestamp=1563670444;</span><br><span class="line">SELECT `id`,`type`,`honey`,`remark`,`created_at` FROM `t_log_user_honey` WHERE `user_id` = &apos;1000014423&apos; ORDER BY `created_at` DESC LIMIT 20 OFFSET 0;</span><br></pre></td></tr></table></figure><p>第一行Time时间不需要,所以也需要filebeat排除.</p><p>从第二行开始匹配,有些慢日志可能没有<code>use database;</code>的语句.所以需要分别针对对待</p><hr><h3 id="filebeat配置"><a href="#filebeat配置" class="headerlink" title="filebeat配置"></a>filebeat配置</h3><p>filebeat需要开启多行日志功能.并且排除特定的字段.除此之外,和其他的日志收集配置一样.下面是生产环境中filebeat的配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">- type: log</span><br><span class="line"></span><br><span class="line">  # Change to true to enable this input configuration.</span><br><span class="line">  enabled: true</span><br><span class="line"></span><br><span class="line">  # Paths that should be crawled and fetched. Glob based paths.</span><br><span class="line">  paths:</span><br><span class="line">    - /mysql_log/msf/log/slow_*.log</span><br><span class="line">  exclude_lines: [&apos;^\# Time&apos;,&apos;^\/usr&apos;,&apos;^Tcp&apos;,&apos;^Time&apos;]</span><br><span class="line">  multiline.pattern: &apos;^\# Time|^\# User&apos;</span><br><span class="line">  multiline.negate: true</span><br><span class="line">  multiline.match: after</span><br><span class="line">  fields:</span><br><span class="line">    project: msf</span><br><span class="line">    type: mysql</span><br><span class="line">    level: slow</span><br></pre></td></tr></table></figure><hr><h3 id="logstash配置"><a href="#logstash配置" class="headerlink" title="logstash配置"></a>logstash配置</h3><p>logstash需要使用正则匹配2种格式的慢日志.当一种grok匹配到了后,logstash就不会再接着往下匹配了,所以每条日志只会匹配一种grok规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">      if [fields][type] == &quot;mysql&quot; &#123;</span><br><span class="line">          grok &#123;</span><br><span class="line">            #有use database语句</span><br><span class="line">            match =&gt; [ &quot;message&quot; , &quot;^#\s+User@Host:\s+%&#123;USER:user&#125;\[[^\]]+\]\s+@\s+(?:(?&lt;clienthost&gt;\S*) )?\[(?:%&#123;IPV4:clientip&#125;)?\]\s+Id:\s+%&#123;NUMBER:row_id:int&#125;\n#\s+Query_time:\s+%&#123;NUMBER:query_time:float&#125;\s+Lock_time:\s+%&#123;NUMBER:lock_time:float&#125;\s+Rows_sent:\s+%&#123;NUMBER:rows_sent:int&#125;\s+Rows_examined:\s+%&#123;NUMBER:rows_examined:int&#125;\n\s*(?:use %&#123;DATA:database&#125;;\s*\n)?SET\s+timestamp=%&#123;NUMBER:timestamp&#125;;\n\s*(?&lt;sql&gt;(?&lt;action&gt;\w+)\b.*;)\s*(?:\n#\s+Time)?.*$&quot; ]</span><br><span class="line"></span><br><span class="line">            remove_field =&gt; [&quot;message&quot;] #删除原始日志,我试过写在mutate中,发现不起作用</span><br><span class="line">&#125;</span><br><span class="line">            #无use database语句</span><br><span class="line">          grok &#123;</span><br><span class="line">            match =&gt; [ &quot;message&quot; , &quot;^#\s+User@Host:\s+%&#123;USER:user&#125;\[[^\]]+\]\s+@\s+(?:(?&lt;clienthost&gt;\S*) )?\[(?:%&#123;IPV4:clientip&#125;)?\]\s+Id:\s+%&#123;NUMBER:row_id:int&#125;\n#\s+Query_time:\s+%&#123;NUMBER:query_time:float&#125;\s+Lock_time:\s+%&#123;NUMBER:lock_time:float&#125;\s+Rows_sent:\s+%&#123;NUMBER:rows_sent:int&#125;\s+Rows_examined:\s+%&#123;NUMBER:rows_examined:int&#125;\nSET\s+timestamp=%&#123;NUMBER:timestamp&#125;;\n\s*(?&lt;sql&gt;(?&lt;action&gt;\w+)\b.*;)\s*(?:\n#\s+Time)?.*$&quot; ]</span><br><span class="line">           remove_field =&gt; [&quot;message&quot;]  #删除原始日志,我试过写在mutate中,发现不起作用</span><br><span class="line">    &#125;</span><br><span class="line">        date &#123;</span><br><span class="line">            match =&gt; [&quot;timestamp_mysql&quot;, &quot;UNIX&quot;]</span><br><span class="line">            target =&gt; &quot;@timestamp&quot;</span><br><span class="line">        &#125;</span><br><span class="line">       mutate &#123;</span><br><span class="line">            remove_field =&gt; &quot;@version&quot;  #删除filebeat传输过来的无用字段,可以视情况删除</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><blockquote><p>也可以将2个match语句写入到一个grok中,但是我试过好像不行,有些慢日志并不能正常解析</p></blockquote><p>实际上无论是grok debbuger在线工具还是Kibana的dev tool均提供了grok的在线调试工具,可以检测和调试grok的正则匹配.例如使用上述中的grok正则检测一下是否能正常匹配到前文提到的mysql原始日志数据.可以使用kibana的dev tool工具中的Grok Debugger工具来校验:</p><p><img src="https://img2.jesse.top/image-20200729142559717.png" alt="image-20200729142559717"></p><p>上面的结构化数据输出中可以看到grok正则能正常解析原始日志中的数据,并且以json格式将日志内容映射给各字段.</p><hr><h3 id="Kibana展示"><a href="#Kibana展示" class="headerlink" title="Kibana展示"></a>Kibana展示</h3><p>我尝试在kibana中使用图形化展示query_time(也就是SQL执行时间)最长的TOP5的SQL语句,制作成可视化图表,方便动态展示.但是发现可视化的聚合图形并不能满足这个需求.</p><p>但是Kibana的Discover界面通过选定字段,也能对query_time进行排序.例如下面的截图中先选定<code>query_time</code>和<code>sql</code>这2个字段,然后再对<code>query_time</code>进行排序(在右边的query_time字段下有个倒三角形表示倒序排序).</p><p><img src="https://img2.jesse.top/image-20200729143159670.png" alt="image-20200729143159670"></p><p>然后将这个discover的筛选结果保存到Dashboard中,方便以后查看:</p><p><img src="https://img2.jesse.top/image-20200729143309655.png" alt="image-20200729143309655"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ELK收集mysql5-7慢日志&quot;&gt;&lt;a href=&quot;#ELK收集mysql5-7慢日志&quot; class=&quot;headerlink&quot; title=&quot;ELK收集mysql5.7慢日志&quot;&gt;&lt;/a&gt;ELK收集mysql5.7慢日志&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;公司ELK平台计划收集生产业务的所有mysql慢日志.由于所有环境中均使用mysql5.7版本,所以其他mysql版本的慢日志格式不在讨论范围之内.&lt;/p&gt;
&lt;p&gt;慢日志的grok正则匹配我折腾了很久,网上的大多文档中给出的logstash的grok正则其实并不能正确的解析到mysql慢日志的字段.&lt;/p&gt;
&lt;p&gt;这个博客的grok正则经过实践可行.而且filebeat,logstash的filter配置也是参考这个博客配置的:&lt;a href=&quot;https://www.cnblogs.com/minseo/p/10441913.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;博客地址&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;MySQL慢日志&quot;&gt;&lt;a href=&quot;#MySQL慢日志&quot; class=&quot;headerlink&quot; title=&quot;MySQL慢日志&quot;&gt;&lt;/a&gt;MySQL慢日志&lt;/h3&gt;&lt;p&gt;慢日志格式如下:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[work@msf-mysql-master log]$ sudo head slow_2019072103.log&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/usr/local/mysql5.7/bin/mysqld, Version: 5.7.24-log (MySQL Community Server (GPL)). started with:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Tcp port: 3306  Unix socket: /mysql_log/msf/tmp/mysql.sock&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Time                 Id Command    Argument&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# Time: 2019-07-21T08:54:04.145255+08:00&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# User@Host: u_msf[u_msf] @  [10.111.10.40]  Id: 131421254&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# Query_time: 1.595300  Lock_time: 0.000031 Rows_sent: 20  Rows_examined: 809259&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;use msf_prod;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;SET timestamp=1563670444;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;SELECT `id`,`type`,`honey`,`remark`,`created_at` FROM `t_log_user_honey` WHERE `user_id` = &amp;apos;1000014423&amp;apos; ORDER BY `created_at` DESC LIMIT 20 OFFSET 0;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# Time: 2019-07-21T10:51:06.184010+08:00&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="elk" scheme="https://jesse.top/categories/elk/"/>
    
    
      <category term="elk" scheme="https://jesse.top/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>搭建Anaconda和jupyter notebook</title>
    <link href="https://jesse.top/2020/08/25/Linux-Service/%E6%90%AD%E5%BB%BAAnaconda%E5%92%8Cjupyter%20notebook/"/>
    <id>https://jesse.top/2020/08/25/Linux-Service/搭建Anaconda和jupyter notebook/</id>
    <published>2020-08-25T14:59:58.000Z</published>
    <updated>2020-08-26T23:55:20.309Z</updated>
    
    <content type="html"><![CDATA[<h2 id="搭建Anaconda和jupyter-notebook"><a href="#搭建Anaconda和jupyter-notebook" class="headerlink" title="搭建Anaconda和jupyter notebook"></a>搭建Anaconda和jupyter notebook</h2><h3 id="一-什么是Anaconda"><a href="#一-什么是Anaconda" class="headerlink" title="一.什么是Anaconda"></a>一.什么是Anaconda</h3><p>Anaconda可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。</p><h2 id="2-特点"><a href="#2-特点" class="headerlink" title="2. 特点"></a>2. 特点</h2><p>Anaconda具有如下特点：</p><ul><li>开源</li><li>安装过程简单</li><li>高性能使用Python和R语言</li><li>免费的社区支持</li></ul><p>其特点的实现主要基于Anaconda拥有的：</p><ul><li>conda包</li><li>环境管理器</li><li>1,000+开源库</li></ul><a id="more"></a><hr><h3 id="3-Anaconda安装"><a href="#3-Anaconda安装" class="headerlink" title="3.Anaconda安装"></a>3.Anaconda安装</h3><p><a href="https://www.anaconda.com/" target="_blank" rel="noopener">Anaconda官方</a>提供了三种不同的版本,除了Individual Edition个人版免费以外,其他2种版本都是收费的.所以这里选择Anaconda个人版.</p><h4 id="3-1-Docker安装-有坑-弃用了"><a href="#3-1-Docker安装-有坑-弃用了" class="headerlink" title="3.1 Docker安装(有坑,弃用了)"></a>3.1 Docker安装(有坑,弃用了)</h4><p>我刚开始选择的是用Docker运行,使用的是Anaconda的镜像:<a href="https://hub.docker.com/r/continuumio/anaconda3" target="_blank" rel="noopener">continuumio/anaconda3</a>.用Dockerfile在此基础之上安装了多个python扩展模块自定义了一个镜像.Dockerfile内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">FROM continuumio/anaconda3:latest</span><br><span class="line">LABEL author=jessehuang</span><br><span class="line">LABEL description=&quot;自定义制作annaconda镜像&quot;</span><br><span class="line"></span><br><span class="line">#更新debian源.使用清华大学的debian 10 brust的源</span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y apt-transport-https ca-certificates</span><br><span class="line"></span><br><span class="line">ADD sources.list /etc/apt/sources.list</span><br><span class="line"></span><br><span class="line">#安装python模块的依赖.否则thriftpy的安装会出现问题</span><br><span class="line"></span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \</span><br><span class="line">    python-dev \</span><br><span class="line">    vim \</span><br><span class="line">    gcc \</span><br><span class="line">    g++ \</span><br><span class="line">    libsasl2-dev </span><br><span class="line"></span><br><span class="line">#安装python扩展模块</span><br><span class="line"></span><br><span class="line">ADD requirement.txt /tmp/requirement.txt </span><br><span class="line">RUN pip install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r /tmp/requirement.txt</span><br><span class="line"></span><br><span class="line">RUN /opt/conda/bin/conda install jupyter -y --quiet &amp;&amp; mkdir /opt/notebooks </span><br><span class="line">EXPOSE 8888</span><br><span class="line"></span><br><span class="line">#启动命令</span><br><span class="line">CMD [&quot;/opt/conda/bin/jupyter&quot;,&quot;notebook&quot;, &quot;--notebook-dir=/opt/notebooks&quot;,&quot;--ip=&apos;*&apos;&quot;,&quot;--port=8888&quot;,&quot;--no-browser&quot;,&quot;--allow-root&quot;]</span><br></pre></td></tr></table></figure><p>requirement.txt内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pyecharts</span><br><span class="line">pymysql</span><br><span class="line">psycopg2-binary</span><br><span class="line">six</span><br><span class="line">bit_array</span><br><span class="line">thriftpy</span><br><span class="line">thrift_sasl==0.2.1</span><br><span class="line">impyla</span><br><span class="line">jupyter_contrib_nbextensions</span><br></pre></td></tr></table></figure><p>docker部署的anaconda在运行jupyter notebook的时候提示<code>kernel restarting</code>.容器日志内容如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[I 07:25:59.512 NotebookApp] Restoring connection for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.541 NotebookApp] Starting buffering for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.565 NotebookApp] Restoring connection for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.595 NotebookApp] Starting buffering for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[I 07:25:59.620 NotebookApp] Restoring connection for da9ccce4-dac5-42d1-aef3-2f37c0689e0c:61003e8cadbc408b9ee98174729d5086</span><br><span class="line">[W 07:26:04.401 NotebookApp] Replacing stale connection: 63b3942c-a225-4692-8989-893af2c992cc:743009a3e3684e64a9b9fcceedc30775</span><br><span class="line">[W 07:26:05.062 NotebookApp] Replacing stale connection: a9543d90-08b5-4395-8a3c-f7bbec9f44a1:0bc5b6496d59406b82ab54a181db7215</span><br></pre></td></tr></table></figure><p>这个故障在搜遍了Google和baidu后也无解,尝试了网上各种解决办法也不行.所以果断弃掉,转而使用Linux虚拟机部署</p><h3 id="3-2-Linux部署安装-成功"><a href="#3-2-Linux部署安装-成功" class="headerlink" title="3.2 Linux部署安装(成功)"></a>3.2 Linux部署安装(成功)</h3><p>Anaconda的<a href="https://docs.anaconda.com/anaconda/install/linux/" target="_blank" rel="noopener">安装官方文档</a>.建议已root用户安装,或者你的普通用户有执行<code>/home/$(USER)/anaconda3/bin/python3</code>的sudo权限</p><p>1.首先下载python的anaconda安装脚本,建议选择python3的版本</p><p><a href="https://www.anaconda.com/products/individual#linux" target="_blank" rel="noopener">https://www.anaconda.com/products/individual#linux</a></p><p>2.安装依赖包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install libXcomposite libXcursor libXi libXtst libXrandr alsa-lib mesa-libEGL libXdamage mesa-libGL libXScrnSaver</span><br></pre></td></tr></table></figure><p>3.执行下载下来的脚本文件.根据提示,一直选择默认就可以了,官方不建议更改安装路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash ~/Anaconda3-2020.02-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><blockquote><p>We recommend you accept the default install location. Do not choose the path as /usr for the Anaconda/Miniconda installation.</p></blockquote><p>4.如果提示<code>Thank you for installing Anaconda&lt;2 or 3&gt;!</code>则说明安装完成,应用环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure><p>5.验证安装结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">condal list #显示已经安装的包和版本好</span><br></pre></td></tr></table></figure><p>至此,Anaconda和anaconda自带的jupyter notebook都已经安装完了</p><hr><h3 id="4-jupyter-notebook-扩展模块安装"><a href="#4-jupyter-notebook-扩展模块安装" class="headerlink" title="4.jupyter notebook 扩展模块安装"></a>4.jupyter notebook 扩展模块安装</h3><p>公司的BI团队需要使用jupyter notebook访问MySQL、pgsql、hive,画图等工作,所以需要安装python的部分扩展模块.首先查看服务器上pip版本.需要使用python3的pip来安装模块</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#显示pip版本是python3</span><br><span class="line">(base) [root@anaconda ~]# pip --version</span><br><span class="line">pip 20.0.2 from /root/anaconda3/lib/python3.7/site-packages/pip (python 3.7)</span><br><span class="line">(base) [root@anaconda ~]#</span><br></pre></td></tr></table></figure><ol><li>安装依赖文件,否则安装<code>thriftpy</code>模块会报错</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install gcc-c++ gcc python3-devel python-dev cyrus-sasl cyrus-sasl-devel cyrus-sasl-lib</span><br></pre></td></tr></table></figure><blockquote><p>如果是ubuntu系统,则需要安装如下安装包: apt-get install -y python-dev gcc g++ libsasl2-dev</p></blockquote><ol start="2"><li>编写requirement文件,将所需要安装的python扩展模块加入到文件中</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pyecharts</span><br><span class="line">pymysql</span><br><span class="line">psycopg2-binary</span><br><span class="line">six</span><br><span class="line">bit_array</span><br><span class="line">thriftpy</span><br><span class="line">thrift_sasl==0.2.1</span><br><span class="line">impyla</span><br><span class="line">jupyter_contrib_nbextensions</span><br></pre></td></tr></table></figure><p>3.安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r requirement.txt</span><br></pre></td></tr></table></figure><p>安装完依赖包<code>jupyter_contrib_nbextensions</code>以后,安装一个服务:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter-contrib-nbextension install --user</span><br></pre></td></tr></table></figure><p>4.初始化jupyter配置文件,会在默认路径下初始化一个配置文件<code>/root/.jupyter/jupyter_notebook_config.py</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure><p>5.编辑文件,修改如下配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.ip=&apos;当前服务器IP&apos;</span><br><span class="line">c.NotebookApp.notebook_dir = u&apos;/data/notebooks&apos; #jupyter笔记的工作目录</span><br><span class="line">c.NotebookApp.open_browser = False #服务端无需启动浏览器</span><br><span class="line">c.NotebookApp.port = 80 #监听端口,默认是8888</span><br><span class="line">c.NotebookApp.allow_root = True  #允许root用户运行jupyter notebook</span><br><span class="line">c.NotebookApp.allow_origin = &apos;*&apos; #允许所有来源访问,解决跨域问题</span><br><span class="line">c.NotebookApp.quit_button = False #关闭jupyter notebook浏览器界面的quit按钮功能.因为quit按钮会关闭jupyter服务</span><br></pre></td></tr></table></figure><p>6.启动服务.记下pid号,后面要重启</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup jupyter-notebook --config=/root/.jupyter/jupyter_notebook_config.py &amp;</span><br></pre></td></tr></table></figure><p>7.打开浏览器,输入IP地址,此时会进入jupyter的界面.要求输入Token,或者使用Token设置一个密码</p><p><img src="https://img2.jesse.top/image-20200708105619499.png" alt="image-20200708105619499"></p><p>8.根据提示,使用命令<code>jupyter notebook list</code>查看Token</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">base) root@anaconda:/# jupyter notebook list</span><br><span class="line">Currently running servers:</span><br><span class="line">http://localhost:8888/?token=5469d940ce3a70950299e5c907e1d47b09acc61457348cd9 :: /data/notebooks</span><br></pre></td></tr></table></figure><p>9.拿到Token后,在浏览器中下方位置设置一个新密码.</p><p><img src="https://img2.jesse.top/image-20200708105756748.png" alt="image-20200708105756748"></p><p>10.设置完密码后,成功登陆了jupyter的工作界面.此时kill掉jupyter进程,重启</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#关闭进程</span><br><span class="line">kill $PID</span><br><span class="line"></span><br><span class="line">#重新启动</span><br><span class="line">nohup jupyter-notebook --config=/root/.jupyter/jupyter_notebook_config.py &amp;</span><br></pre></td></tr></table></figure><p>11.此时再次打开浏览器,就提示输入密码</p><p><img src="https://img2.jesse.top/image-20200708110014017.png" alt="image-20200708110014017"></p><p>12.输入密码后,成功登陆</p><p><img src="https://img2.jesse.top/image-20200708110048546.png" alt="image-20200708110048546"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;搭建Anaconda和jupyter-notebook&quot;&gt;&lt;a href=&quot;#搭建Anaconda和jupyter-notebook&quot; class=&quot;headerlink&quot; title=&quot;搭建Anaconda和jupyter notebook&quot;&gt;&lt;/a&gt;搭建Anaconda和jupyter notebook&lt;/h2&gt;&lt;h3 id=&quot;一-什么是Anaconda&quot;&gt;&lt;a href=&quot;#一-什么是Anaconda&quot; class=&quot;headerlink&quot; title=&quot;一.什么是Anaconda&quot;&gt;&lt;/a&gt;一.什么是Anaconda&lt;/h3&gt;&lt;p&gt;Anaconda可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。&lt;/p&gt;
&lt;h2 id=&quot;2-特点&quot;&gt;&lt;a href=&quot;#2-特点&quot; class=&quot;headerlink&quot; title=&quot;2. 特点&quot;&gt;&lt;/a&gt;2. 特点&lt;/h2&gt;&lt;p&gt;Anaconda具有如下特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;开源&lt;/li&gt;
&lt;li&gt;安装过程简单&lt;/li&gt;
&lt;li&gt;高性能使用Python和R语言&lt;/li&gt;
&lt;li&gt;免费的社区支持&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其特点的实现主要基于Anaconda拥有的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;conda包&lt;/li&gt;
&lt;li&gt;环境管理器&lt;/li&gt;
&lt;li&gt;1,000+开源库&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Linux-Service" scheme="https://jesse.top/categories/Linux-Service/"/>
    
    
      <category term="Anaconda" scheme="https://jesse.top/tags/Anaconda/"/>
    
  </entry>
  
</feed>

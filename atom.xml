<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jesse&#39;s home</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jesse.top/"/>
  <updated>2020-06-29T14:28:34.926Z</updated>
  <id>https://jesse.top/</id>
  
  <author>
    <name>Jesse</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>docker学习笔记---docker容器篇</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E5%AE%B9%E5%99%A8%E7%AF%87/"/>
    <id>https://jesse.top/2020/06/29/docker/docker学习笔记—容器篇/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T14:28:34.926Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker学习笔记—-docker容器篇"><a href="#docker学习笔记—-docker容器篇" class="headerlink" title="docker学习笔记— docker容器篇"></a>docker学习笔记— docker容器篇</h2><p><strong>一.创建并启动容器</strong></p><ul><li><strong>创建容器</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker create -it 镜像名:tag</span><br></pre></td></tr></table></figure><ul><li><strong>查看所有容器</strong>(包括运行中,已退出,错误容器)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps -a</span><br></pre></td></tr></table></figure><ul><li>查看所有容器的ID</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps -qa</span><br></pre></td></tr></table></figure><ul><li><strong>启动容器</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker start container_id</span><br></pre></td></tr></table></figure><ul><li><strong>使用指定的镜像直接创建并启动容器</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -it 镜像名:标签 COMMAND</span><br><span class="line"></span><br><span class="line">jesse@jesse-virtual-machine:~$ docker run -it ubuntu:16.04 /bin/bash</span><br><span class="line">root@66a29f973548:/#   ----------此时已经进入到容器的bash环境</span><br></pre></td></tr></table></figure><a id="more"></a><p>当利用 docker run 来创建容器时，Docker 在后台运行的标准操作包括：</p><p>1.检查本地是否存在指定的镜像，不存在就从公有仓库下载</p><p>2.利用镜像创建并启动一个容器</p><p>3.分配一个文件系统，并在只读的镜像层外面挂载一层可读写层</p><p>4.从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去</p><p>5.从地址池配置一个 ip 地址给容器</p><p>6.执行用户指定的应用程序</p><p>7.执行完毕后容器被终止</p><ul><li><strong>以守护状态运行:</strong></li></ul><p>以守护态运行（加参数-d):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d registry.intra.weibo.com/yushuang3/centos:v1 /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;</span><br></pre></td></tr></table></figure><ul><li><strong>容器终止:</strong></li></ul><p><strong>获取容器输出的信息</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker logs container_id</span><br></pre></td></tr></table></figure><p><strong>停止容器</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker stop container_Id</span><br></pre></td></tr></table></figure><blockquote><p>终止一个容器  加入-t=10 表示等待10秒(不加-t选项则默认就是10秒)再次发送SIGKILL信号终止容器</p></blockquote><p><strong>重启容器</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker restart container_id</span><br></pre></td></tr></table></figure><p><strong>容器状态:</strong></p><ul><li><strong>status表示容器的状态..</strong></li></ul><ul><li><strong>exited 表示容器已经退出</strong></li><li><strong>up 表示容器正在运行</strong></li></ul><p>docker run启动容器时还可以指定其他的配置参数:</p><ul><li>-h HOSTNAME 或者 –hostname=HOSTNAME.设置容器的主机名</li><li>–dns=IP_ADDRESS:设置容器的DNS.写在容器的/etc/resolv.conf文件中.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost test]$docker run -itd --name busybox1 -h dwd-busybox --dns=8.8.8.8 busybox</span><br><span class="line">c25dac9c641705e00f02aefe302987f39f853a1feb8c0d3f32dc1675747edd84</span><br><span class="line"></span><br><span class="line">[root@localhost test]$docker exec -it busybox1 hostname</span><br><span class="line">dwd-busybox</span><br><span class="line"></span><br><span class="line">[root@localhost test]$docker exec -it busybox1 cat /etc/resolv.conf</span><br><span class="line">nameserver 8.8.8.8</span><br></pre></td></tr></table></figure><p>但是需要注意的是.这些修改不会被 docker commit保存,也就是不会保存在镜像中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#将busybox1容器保存为busybox1:test镜像</span><br><span class="line">[root@localhost test]$docker commit -m &apos;test&apos; -a &apos;jesse&apos; busybox1 busybox1:test</span><br><span class="line">sha256:3c8faee532f177cf8bb8736db89694c2c3ff5be1a30a15d604e450130909d123</span><br><span class="line"></span><br><span class="line">#用这个镜像,启动一个新的busybox1-test的容器</span><br><span class="line">[root@localhost test]$docker run --name busybox1-test -itd busybox1:test</span><br><span class="line">9326b615e9e3af64336683f7f82e048929de560d4ad0a5caf2485bbc4a62e18c</span><br><span class="line"></span><br><span class="line">#可以看到hostname和dns信息没有被保留</span><br><span class="line">[root@localhost test]$docker exec -it busybox1-test hostname</span><br><span class="line">9326b615e9e3</span><br><span class="line"></span><br><span class="line">[root@localhost test]$docker exec -it busybox1-test cat /etc/resolv.conf</span><br><span class="line">nameserver 114.114.114.114</span><br><span class="line">nameserver 114.114.115.115</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker学习笔记—-docker容器篇&quot;&gt;&lt;a href=&quot;#docker学习笔记—-docker容器篇&quot; class=&quot;headerlink&quot; title=&quot;docker学习笔记— docker容器篇&quot;&gt;&lt;/a&gt;docker学习笔记— docker容器篇&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;一.创建并启动容器&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;创建容器&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker create -it 镜像名:tag&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;查看所有容器&lt;/strong&gt;(包括运行中,已退出,错误容器)&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker ps -a&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;查看所有容器的ID&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker ps -qa&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;启动容器&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker start container_id&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;使用指定的镜像直接创建并启动容器&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker run -it 镜像名:标签 COMMAND&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;jesse@jesse-virtual-machine:~$ docker run -it ubuntu:16.04 /bin/bash&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;root@66a29f973548:/#   ----------此时已经进入到容器的bash环境&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---Dockerfile</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0%E2%80%942.dockerfile/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习—2.dockerfile/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T14:31:51.094Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h3><p>Dockerfile可以用来编译一个docker镜像.Dockerfile是一个包含一系列指令的文本文档,使用docker build命令,用户可以依据dockerfile和上下文编译一个镜像.</p><p>使用dockerfile需要注意一些事项</p><p><strong>1.上下文</strong></p><p>docker build编译镜像时,会将当前目录下的Dockerfile和所有文件打包添加发送到docker daemon服务端.所以一般情况下创建一个空目录编辑dockerfile文件.然后将需要copy和add的文件放进和dockerfile同一目录下.</p><p>dockerfile中的copy以及add命令,添加文件到docker镜像中时.不要使用绝对路径.例如/home/work/a.txt..docker deamon只能识别到当前上下文环境,无法识别到其他目录.但是可以使用当前上下文的相对路径.</p><p><strong>2.分层</strong></p><p>dockerfile编译镜像时,每条指令都是一个镜像层.除了From指令外,每一行指令都是基于上一行生成的临时镜像运行一个容器.执行一条指令就类似于docker commit命令生成一个新的镜像.所以两条指令之间互不关联.</p><a id="more"></a><p>例如,下列的dockerfile并不能在/data/目录下创建files文件.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu</span><br><span class="line">RUN mkdir /data</span><br><span class="line">RUN cd /data</span><br><span class="line">RUN touch files</span><br></pre></td></tr></table></figure><p>下列的dockerfile甚至不会创建/data/file文件,也不会修改/data/目录权限<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu</span><br><span class="line">RUN useradd foo</span><br><span class="line">VOLUME /data</span><br><span class="line">RUN touch /data/file</span><br><span class="line">RUN chown -R foo:foo /data</span><br></pre></td></tr></table></figure></p><p>想要实现这个需求,可以这样写:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu</span><br><span class="line">RUN useradd foo</span><br><span class="line">RUN mkdir /data &amp;&amp; touch /data/file</span><br><span class="line">RUN chown -R foo:foo /data</span><br><span class="line">VOLUME /data</span><br></pre></td></tr></table></figure><hr><p><strong>3.精简</strong></p><p>由于dockerfile在构建镜像时,dockerfile文本中每一行语句会产生每一层镜像.</p><p>例如下面这个dockerfile:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">From ubuntu</span><br><span class="line">RUN apt-get update</span><br><span class="line">RUN apt-get -y install vim git wget net-tools</span><br><span class="line">RUN useradd foo</span><br><span class="line">RUN mkdir /data</span><br><span class="line">RUN touch /data/file</span><br><span class="line">RUN chown -R foo:foo /data</span><br></pre></td></tr></table></figure><p>在编译时,每一个RUN语句都会构建一层镜像.(实际上所有指令都是这样,不仅仅是RUN)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost test]$docker build -t test:v1 .</span><br><span class="line">Sending build context to Docker daemon  2.048kB</span><br><span class="line">Step 1/7 : From ubuntu</span><br><span class="line"> ---&gt; 94e814e2efa8</span><br><span class="line">Step 2/7 : RUN apt-get update</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 5520126e7fcc</span><br><span class="line">Step 3/7 : RUN apt-get -y install vim git wget net-tools</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; cb24e170539c</span><br><span class="line">Step 4/7 : RUN useradd foo</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; ca31aeba0309</span><br><span class="line">Step 5/7 : RUN mkdir /data</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; d5c6e0f32f6b</span><br><span class="line">Step 6/7 : RUN touch /data/file</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 9c4b06e9b25d</span><br><span class="line">Step 7/7 : RUN chown -R foo:foo /data</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 75ecea0b0795</span><br><span class="line">Successfully built 75ecea0b0795</span><br><span class="line">Successfully tagged test:v1</span><br></pre></td></tr></table></figure><p>这种写法会导致镜像层非常多,镜像文件也会相对较大.所以一般推荐更精简的语法,每一条功能相同的语句,尽量写在一行.上面的dockerfile可以优化成:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">From ubuntu</span><br><span class="line">RUN apt-get update \</span><br><span class="line">    &amp;&amp; apt-get -y install \</span><br><span class="line">       vim \</span><br><span class="line">       git \</span><br><span class="line">       wget \</span><br><span class="line">       net-tools</span><br><span class="line"></span><br><span class="line">RUN useradd foo</span><br><span class="line">RUN mkdir /data &amp;&amp;  touch /data/file &amp;&amp;  chown -R foo:foo /data</span><br></pre></td></tr></table></figure><p>这次编译只需构建4层镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost test]$docker build -t test:v1 .</span><br><span class="line">Sending build context to Docker daemon  2.048kB</span><br><span class="line">Step 1/4 : From ubuntu</span><br><span class="line"> ---&gt; 94e814e2efa8</span><br><span class="line">Step 2/4 : RUN apt-get update     &amp;&amp; apt-get -y install        vim        git        wget        net-tools</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 7aa2bc9041e0</span><br><span class="line">Step 3/4 : RUN useradd foo</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; 5a13764414e6</span><br><span class="line">Step 4/4 : RUN mkdir /data &amp;&amp;  touch /data/file &amp;&amp;  chown -R foo:foo /data</span><br><span class="line"> ---&gt; Using cache</span><br><span class="line"> ---&gt; bd61817d7526</span><br><span class="line">Successfully built bd61817d7526</span><br><span class="line">Successfully tagged test:v1</span><br></pre></td></tr></table></figure><p>4.使用no-install-recommends</p><p>如果是使用APT包管理器,则应该在执行apt-get install 命令时加上no-install-recommends参数.这样ATP就仅安装核心依赖.而不安装其他推荐和建议的包,这会显著减少不必要包的下载数量</p><hr><h3 id="Dockerfile指令介绍"><a href="#Dockerfile指令介绍" class="headerlink" title="Dockerfile指令介绍"></a>Dockerfile指令介绍</h3><p>介绍完Dockerfile的概念和特点后,接下来了解一下Dockerfile语法中的具体指令的介绍和用法</p><p>下面是一个例子:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># Use an official Python runtime as a parent image</span><br><span class="line">FROM python:2.7-slim</span><br><span class="line"></span><br><span class="line"># Set the working directory to /app</span><br><span class="line">WORKDIR /app</span><br><span class="line"></span><br><span class="line"># Copy the current directory contents into the container at /app</span><br><span class="line">COPY . /app</span><br><span class="line"></span><br><span class="line"># Install any needed packages specified in requirements.txt</span><br><span class="line">RUN pip install --trusted-host pypi.python.org -r requirements.txt</span><br><span class="line"></span><br><span class="line"># Make port 80 available to the world outside this container</span><br><span class="line">EXPOSE 80</span><br><span class="line"></span><br><span class="line"># Define environment variable</span><br><span class="line">ENV NAME World</span><br><span class="line"></span><br><span class="line"># Run app.py when the container launches</span><br><span class="line">CMD [&quot;python&quot;, &quot;app.py&quot;]</span><br></pre></td></tr></table></figure><hr><ul><li>FROM </li></ul><p>格式: FROM image 或者 FROM image:tag</p><p>表示从一个基础镜像构建.Dockerfile必须以FROM语句作为第一条非注释语句.</p><ul><li><p>WORKDIR: 表示工作目录,后续的相对路径也是基于这个目录</p></li><li><p>COPY</p></li></ul><p>格式: copy src dest</p><p>复制宿主机上的文件到镜像中.src是当前上下文中的文件或者目录.dest是容器中的目标文件或者目录.src指定的源可以有多个.此外 src还支持通配符.例如: COPY hom* /mydir/ 表示添加所有当前目录下的hom开头的文件到目录/mydir/下</p><p><dest>可以是文件或者目录.但是必须是镜像中的绝对路径,或者是WORKDIR的相对路径.若<dest>以反斜杠/结尾,则指向的是目录,否则指向文件.当 src 有多个源时, dest必须是目录.如果 dest 目录不存在,则会自动被创建</dest></dest></p><ul><li>ADD</li></ul><p>格式: ADD src dest</p><p> ADD和COPY命令有相同功能,都支持复制本地文件到镜像里.但ADD能从互联网的URL下载文件到镜像..src还可以是一个本地的压缩归档文件.ADD会自动将tar,gz等压缩包上传到镜像后进行解压.</p><p> 但是如果src是一个URL的归档格式文件,则不会自动解压.</p><ul><li>RUN</li></ul><p>RUN命令有两种格式:</p><p>RUN <command> (shell格式)<br>RUN [“executable”,”param1”,”param2”] (exec格式)</p><p>RUN指令的两种格式表示命令在容器中的两种运行方式.当使用shell格式时,命令通过 /bin/sh -c 运行.当使用exec格式时.命令直接运行,不调用shell程序.exec格式中的参数会被当成JSON数组被Docker解析.所以必须使用双引号,不能使用单引号. </p><p>另外由于exec格式不会在shell中运行.所以无法识别ENV环境变量.例如当执行CMD [“echo”,”$HOME”]时,$HOME不会被变量替换.如果希望运行shell程序.可以写成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CMD [&quot;sh&quot;,&quot;-c&quot;,&quot;echo&quot;,&quot;$HOME&quot;]</span><br></pre></td></tr></table></figure><ul><li>EXPOSE: 镜像需要暴露出来的端口. </li></ul><blockquote><p>要注意的是,这里只是说明镜像需要暴露哪些端口,在镜像构建完毕,启动容器时,仍然需要-p参数来映射端口,否则端口不会自动映射</p></blockquote><ul><li><p>ENV</p><p>格式: ENV <key> <value> 或者 ENV <key>=<value></value></key></value></key></p><p>ENV指令用来声明环境变量,并且可以被(ADD,COPY,WORKDIR等)指令调用.调用ENV环境变量的格式和shell一样:\$variable_name或者 \${variable_name}</p></li><li><p>CMD </p></li></ul><p>CMD命令有3种格式:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CMD &lt;command&gt; (shell格式)  </span><br><span class="line">CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] (exec格式)  </span><br><span class="line">CMD [&quot;param1&quot;,&quot;param2&quot;] (为ENTRYPOINT命令提供参数)</span><br></pre></td></tr></table></figure><p>CMD提供容器启动后执行的命令.或者是为ENTRYPOINT传递一些参数.一个dockerfile文件只允许存在一条CMD指令.如果存在多条CMD指令,以最后一条为准.但是如果用户在 docker run 时指定了命令,则会覆盖CMD中的指令</p><ul><li>ENTRYPOINT</li></ul><p>ENTRYPOINT有两种格式.和上文CMD一样分为shell格式和exec格式.</p><p>ENTRYPOINT和CMD类似,指定容器启动时执行的命令.和CMD一样一个Dockerfile文件中可以有多个ENTRYPOINT命令.但只有最后一条生效.但是又有一些区别.当使用shell格式时,ENTRYPOINT会忽略任何CMD指令和 docker run启动容器时手动输入的指令.并且会运行在 /bin/sh -c环境中,成为它的子进程.进程在容器中PID不是1,也不能接收UNIX信号.(也就是在执行 docker stop <container>时,进程接收不到SIGTERM指令)</container></p><p>当使用exec格式时, docker run 手动指定的命令,将作为参数覆盖CMD指定的参数传递到ENTRYPOINT.(也就是说 docker run启动容器时指定的不再是具体命令,而是命令的参数).</p><hr><p>创建上面dockerfile中所需要的app.py和requirements.txt文件,并且将他们和Dockerfile文件放在同一目录下:</p><p>requirements.txt:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost docker_python]$cat requirements.txt</span><br><span class="line">Flask</span><br><span class="line">Redis</span><br></pre></td></tr></table></figure><p>app.py:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">root@localhost docker_python]$cat app.py</span><br><span class="line">from flask import Flask</span><br><span class="line">from redis import Redis, RedisError</span><br><span class="line">import os</span><br><span class="line">import socket</span><br><span class="line"></span><br><span class="line"># Connect to Redis</span><br><span class="line">redis = Redis(host=&quot;redis&quot;, db=0, socket_connect_timeout=2, socket_timeout=2)</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line">@app.route(&quot;/&quot;)</span><br><span class="line">def hello():</span><br><span class="line">    try:</span><br><span class="line">        visits = redis.incr(&quot;counter&quot;)</span><br><span class="line">    except RedisError:</span><br><span class="line">        visits = &quot;&lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;&quot;</span><br><span class="line"></span><br><span class="line">    html = &quot;&lt;h3&gt;Hello &#123;name&#125;!&lt;/h3&gt;&quot; \</span><br><span class="line">           &quot;&lt;b&gt;Hostname:&lt;/b&gt; &#123;hostname&#125;&lt;br/&gt;&quot; \</span><br><span class="line">           &quot;&lt;b&gt;Visits:&lt;/b&gt; &#123;visits&#125;&quot;</span><br><span class="line">    return html.format(name=os.getenv(&quot;NAME&quot;, &quot;world&quot;), hostname=socket.gethostname(), visits=visits)</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    app.run(host=&apos;0.0.0.0&apos;, port=80)</span><br><span class="line">[root@localhost docker_python]$</span><br></pre></td></tr></table></figure><hr><h4 id="开始构建镜像"><a href="#开始构建镜像" class="headerlink" title="开始构建镜像"></a>开始构建镜像</h4><p>首先确保Dockerfile里所需的文件,以及Dockerfile都在同一目录下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost docker_python]$ls</span><br><span class="line">app.py  Dockerfile  requirements.txt</span><br></pre></td></tr></table></figure><p>运行以下命令来构建一个镜像.使用–tag参数(或者-t),可以为镜像打个标签:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build --tag=friendlyhello .</span><br></pre></td></tr></table></figure><p>构建过程略..在构建过程中,注意以下现象:</p><p>1.构建镜像层:</p><p>部分指令会创建一个新的镜像层,而有些指令则不会.关于如何区分命令是否会新建镜像层,一个基本的原则是:</p><p>如果指令的作用是像镜像中添加新的文件或者程序,那么就会新建镜像层.(例如:RUN,COPY,ADD,FROM等)</p><p>如果只是告诉docker如何构建或者运行应用程序,增加或者修改容器的元数据,那么不会构建新的镜像层.(例如:WORKDIR,EXPOSE,ENV,ENTERPOINT等)</p><p>2.构建步骤:</p><p>基本等过程大致为:</p><p>运行临时容器—-&gt;在该容器中运行Dockerfile指令—-&gt;将运行结果保存为一个新等镜像层——&gt; 删除临时容器</p><p>构建完成后,通过以下命令可以看到构建的镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost docker_python]$docker images</span><br><span class="line">REPOSITORY                 TAG                 IMAGE ID            CREATED              SIZE</span><br><span class="line">friendlyhello              latest              f091d1bb803c        About a minute ago   131MB</span><br></pre></td></tr></table></figure><blockquote><p>或者也可以是输入以下命令 docker image ls</p></blockquote><blockquote><p>可能会疑惑,为什么tag标签是latest..镜像的完整标签格式应该是:friendlyhello:lastest.<br>如果需要在构建镜像时指定版本.可以使用: –tag=friendlyhello:v0.0.1</p></blockquote><hr><h4 id="使用构建的镜像启动一个容器"><a href="#使用构建的镜像启动一个容器" class="headerlink" title="使用构建的镜像启动一个容器"></a>使用构建的镜像启动一个容器</h4><p>输入以下命令,利用刚才的镜像启动一个容器:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 4000:80 friendlyhello</span><br></pre></td></tr></table></figure><p>执行结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost docker_python]$docker run -p 4000:80 friendlyhello</span><br><span class="line"> * Serving Flask app &quot;app&quot; (lazy loading)</span><br><span class="line"> * Environment: production</span><br><span class="line">   WARNING: Do not use the development server in a production environment.</span><br><span class="line">   Use a production WSGI server instead.</span><br><span class="line"> * Debug mode: off</span><br><span class="line"> * Running on http://0.0.0.0:80/ (Press CTRL+C to quit)</span><br></pre></td></tr></table></figure><ul><li>docker run 表示启动一个容器</li><li>-p 宿主机端口:容器端口  表示将宿主机的端口映射给容器.如果是-P 80 表示随机映射一个宿主机的端口给容器</li></ul><p>此时可以在其他电脑上访问这个容器的80端口,下面是在我的PC上访问宿主机的4000端口,也就是刚才启动的容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> ✘ huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; f9b1b804404f&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; &lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;%</span><br></pre></td></tr></table></figure><p>容器默认是在前台执行,加上-d参数可以时容器运行在后台:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 4000:80 friendlyhello</span><br></pre></td></tr></table></figure><p>docker ps命令可以显示正在运行中的容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost docker_python]$docker ps</span><br><span class="line">CONTAINER ID        IMAGE                      COMMAND             CREATED             STATUS              PORTS                    NAMES</span><br><span class="line">fcf7d29ac627        friendlyhello              &quot;python app.py&quot;     5 seconds ago       Up 1 second         0.0.0.0:4000-&gt;80/tcp     stoic_colden</span><br></pre></td></tr></table></figure><blockquote><p>docker container ls命令也有同样的效果</p></blockquote><hr><p>这一节(包括第3小节)涉及到的基础命令如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">docker build -t friendlyhello .  # Create image using this directory&apos;s Dockerfile</span><br><span class="line">docker run -p 4000:80 friendlyhello  # Run &quot;friendlyname&quot; mapping port 4000 to 80</span><br><span class="line">docker run -d -p 4000:80 friendlyhello         # Same thing, but in detached mode</span><br><span class="line">docker container ls                                # List all running containers</span><br><span class="line">docker container ls -a             # List all containers, even those not running</span><br><span class="line">docker container stop &lt;hash&gt;           # Gracefully stop the specified container</span><br><span class="line">docker container kill &lt;hash&gt;         # Force shutdown of the specified container</span><br><span class="line">docker container rm &lt;hash&gt;        # Remove specified container from this machine</span><br><span class="line">docker container rm $(docker container ls -a -q)         # Remove all containers</span><br><span class="line">docker image ls -a                             # List all images on this machine</span><br><span class="line">docker image rm &lt;image id&gt;            # Remove specified image from this machine</span><br><span class="line">docker image rm $(docker image ls -a -q)   # Remove all images from this machine</span><br><span class="line">docker login             # Log in this CLI session using your Docker credentials</span><br><span class="line">docker tag &lt;image&gt; username/repository:tag  # Tag &lt;image&gt; for upload to registry</span><br><span class="line">docker push username/repository:tag            # Upload tagged image to registry</span><br><span class="line">docker run username/repository:tag                   # Run image from a registry</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Dockerfile&quot;&gt;&lt;a href=&quot;#Dockerfile&quot; class=&quot;headerlink&quot; title=&quot;Dockerfile&quot;&gt;&lt;/a&gt;Dockerfile&lt;/h3&gt;&lt;p&gt;Dockerfile可以用来编译一个docker镜像.Dockerfile是一个包含一系列指令的文本文档,使用docker build命令,用户可以依据dockerfile和上下文编译一个镜像.&lt;/p&gt;
&lt;p&gt;使用dockerfile需要注意一些事项&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.上下文&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;docker build编译镜像时,会将当前目录下的Dockerfile和所有文件打包添加发送到docker daemon服务端.所以一般情况下创建一个空目录编辑dockerfile文件.然后将需要copy和add的文件放进和dockerfile同一目录下.&lt;/p&gt;
&lt;p&gt;dockerfile中的copy以及add命令,添加文件到docker镜像中时.不要使用绝对路径.例如/home/work/a.txt..docker deamon只能识别到当前上下文环境,无法识别到其他目录.但是可以使用当前上下文的相对路径.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.分层&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;dockerfile编译镜像时,每条指令都是一个镜像层.除了From指令外,每一行指令都是基于上一行生成的临时镜像运行一个容器.执行一条指令就类似于docker commit命令生成一个新的镜像.所以两条指令之间互不关联.&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>使用goreplay收集线上真实http流量</title>
    <link href="https://jesse.top/2020/06/29/Linux-Web/%E4%BD%BF%E7%94%A8goreplay%E6%94%B6%E9%9B%86%E7%BA%BF%E4%B8%8A%E7%9C%9F%E5%AE%9Ehttp%E6%B5%81%E9%87%8F/"/>
    <id>https://jesse.top/2020/06/29/Linux-Web/使用goreplay收集线上真实http流量/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:45:33.890Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用goreplay收集线上真实http流量"><a href="#使用goreplay收集线上真实http流量" class="headerlink" title="使用goreplay收集线上真实http流量"></a>使用goreplay收集线上真实http流量</h2><h3 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h3><p>在很多场景中,我们需要将线上服务器的真实Http请求复制转发到某台服务器中(或者测试环境中),并且前提是不影响线上生产业务进行.</p><p>例如:</p><ol><li>通常可能会通过ab等压测工具来对单一http接口进行压测。但如果是需要http服务整体压测，使用ab来压测工作量大且不方便，通过线上流量复制引流，通过将真实请求流量放大N倍来进行压测，能对服务有一个较为全面的检验.</li><li>将线上流量引入到测试环境中,测试某个中间件或者数据库的压力</li><li>上线前在预发布环境，使用线上真实的请求，检查是否准备发布的版本，是否具备发布标准</li><li>用线上的流量转发到预发，检查相同流量下一些指标的反馈情况，检查核心数据是否有改善、优化.</li></ol><a id="more"></a><hr><h3 id="goreplay介绍"><a href="#goreplay介绍" class="headerlink" title="goreplay介绍"></a>goreplay介绍</h3><p>goreplay项目请参考github:<a href="https://github.com/buger/goreplay" target="_blank" rel="noopener">goreplay介绍</a></p><p>goreplay是一款开源网络监控工具,可以在不影响业务的情况下,记录服务器真实流量,将该流量用来做镜像,压力测试,监控和分析等用途.</p><p>简单来说就是goreplay抓取线上真实的流量，并将捕捉到的流量转发到测试服务器上(或者保存到本地文件中)</p><p>goreplay大致工作流程如下:</p><p><img src="https://img2.jesse.top/20200629110035.png" alt=""></p><hr><h3 id="goreplay常见使用方式"><a href="#goreplay常见使用方式" class="headerlink" title="goreplay常见使用方式"></a>goreplay常见使用方式</h3><p>goreplay使用文档参考:<a href="https://github.com/buger/goreplay/wiki" target="_blank" rel="noopener">goreplay文档</a></p><p>常用的一些命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-input-raw 抓取指定端口的流量 gor --input-raw :8080</span><br><span class="line">-output-stdout 打印到控制台</span><br><span class="line">-output-file 将请求写到文件中 gor --input-raw :80 --output-file ./requests.gor</span><br><span class="line">-input-file 从文件中读取请求，与上一条命令呼应 gor --input-file ./requests.gor</span><br><span class="line">-exit-after 5s 持续时间</span><br><span class="line">-http-allow-url url白名单，其他请求将会被丢弃</span><br><span class="line">-http-allow-method 根据请求方式过滤</span><br><span class="line">-http-disallow-url 遇上一个url相反，黑名单，其他的请求会被捕获到</span><br></pre></td></tr></table></figure><blockquote><p>更多命令可以使用 ./gor –help查看</p></blockquote><hr><h3 id="goreplay安装"><a href="#goreplay安装" class="headerlink" title="goreplay安装"></a>goreplay安装</h3><p>在github上下载Linux的二进制文件: <a href="https://github.com/buger/goreplay/releases" target="_blank" rel="noopener">goreplay安装</a></p><blockquote><p>注意.虽然在github上提供了rpm安装包,但是实际安装发现无法安装:</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# rpm -ivh gor-1.0.0-1.x86_64.rpm</span><br><span class="line">Preparing...                          ################################# [100%]</span><br><span class="line">package goreplay-1.0.0-1.x86_64 is intended for a different operating system</span><br></pre></td></tr></table></figure><p>下载github上的二进制文件,解压后是一个gor的二进制可执行文件.复制到PATH变量路径下即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# wget https://github.com/buger/goreplay/releases/download/v1.0.0/gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# ls</span><br><span class="line"> gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# tar -xf gor_1.0.0_x64.tar.gz</span><br><span class="line">[root@dwd-tongji-3 ~]# ls</span><br><span class="line">gor </span><br><span class="line">[root@dwd-tongji-3 ~]# ll gor</span><br><span class="line">-rwxr-xr-x 1 501 games 17779040 Mar 30  2019 gor</span><br><span class="line">[root@dwd-tongji-3 ~]# cp gor /usr/local/bin/</span><br></pre></td></tr></table></figure><hr><h3 id="goreplay简单实践"><a href="#goreplay简单实践" class="headerlink" title="goreplay简单实践"></a>goreplay简单实践</h3><h4 id="1-将本地http的流量保存到本地文件中"><a href="#1-将本地http的流量保存到本地文件中" class="headerlink" title="1.将本地http的流量保存到本地文件中."></a>1.将本地http的流量保存到本地文件中.</h4><p>为了简便起见,以下命令都在root用户下执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">## 1.开启一个screen窗口</span><br><span class="line">[root@dwd-tongji-3 ~]# screen -S GOR</span><br><span class="line">## 2.将80流量保存到本地的文件</span><br><span class="line">[root@dwd-tongji-3 ~]# gor --input-raw :80 --output-file /data/requests.gor</span><br><span class="line">Version: 1.0.0</span><br></pre></td></tr></table></figure><p>默认情况下goreplay会以块文件存储,将流量保存为多个块文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@dwd-tongji-3 ~]# ls /data/requests_* | more</span><br><span class="line">/data/requests_0.gor</span><br><span class="line">/data/requests_100.gor</span><br><span class="line">/data/requests_101.gor</span><br><span class="line">/data/requests_102.gor</span><br><span class="line">/data/requests_103.gor</span><br><span class="line">/data/requests_104.gor</span><br><span class="line">/data/requests_105.gor</span><br><span class="line">/data/requests_106.gor</span><br><span class="line">/data/requests_107.gor</span><br><span class="line">/data/requests_108.gor</span><br><span class="line">/data/requests_109.gor</span><br><span class="line">/data/requests_10.gor</span><br></pre></td></tr></table></figure><p>使用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>[root@dwd-tongji-3 ~]#./gor –input-raw :80 –output-file /data/gor.gor –output-file-append</p><p>[root@dwd-tongji-3 ~]# ll /data -h<br>total 1.4M<br>-rw-r—– 1 root root 1.4M Jun 29 15:13 gor.gor<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 2.将http的请求打印到终端</span><br></pre></td></tr></table></figure></p><p>[root@dwd-tongji-3 ~]#gor –input-raw :8000 –output-stdout<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 3.将http的请求转发到测试环境</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-http=”<a href="http://beta:80&quot;" target="_blank" rel="noopener">http://beta:80&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在测试服务器上的nginx查看日志,.发现流量已经进来了</span><br></pre></td></tr></table></figure></p><p>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik.php HTTP/1.1” 200 5 “<a href="https://2021001151691008.hybrid.alipay-eco.com/2021001151691008/0.2.2006111453.18/index.html#pages/index/index?appid=2021001151691008&amp;taskId=415&quot;" target="_blank" rel="noopener">https://2021001151691008.hybrid.alipay-eco.com/2021001151691008/0.2.2006111453.18/index.html#pages/index/index?appid=2021001151691008&amp;taskId=415&quot;</a> “Mozilla/5.0 (iPhone; CPU iPhone OS 13_3_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/17D50 Ariver/1.0.12 AliApp(AP/10.1.95.7030) Nebula WK RVKType(0) AlipayDefined(nt:4G,ws:375|667|2.0) AlipayClient/10.1.95.7030 Language/zh-Hans Region/CN NebulaX/1.0.0” “112.96.179.238”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik.php HTTP/1.1” 200 5 “<a href="https://servicewechat.com/wxa090d3923fde0d4b/132/page-frame.html&quot;" target="_blank" rel="noopener">https://servicewechat.com/wxa090d3923fde0d4b/132/page-frame.html&quot;</a> “Mozilla/5.0 (iPhone; CPU iPhone OS 13_5_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 MicroMessenger/7.0.13(0x17000d29) NetType/4G Language/zh_CN” “14.106.171.11”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br>10.111.51.243 - - [29/Jun/2020:14:56:55 +0800] “POST /piwik_new.php?actionname=zt-template HTTP/1.1” 400 249 “-“ “-“ “118.31.36.251”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">####  也可以将流量输出到多个终端</span><br><span class="line"></span><br><span class="line">* 输出到多个http服务器</span><br></pre></td></tr></table></figure></p><p>gor –input-tcp :28020 –output-http “<a href="http://staging.com&quot;" target="_blank" rel="noopener">http://staging.com&quot;</a>  –output-http “<a href="http://dev.com&quot;" target="_blank" rel="noopener">http://dev.com&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 输出到文件或者Http服务器</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-file requests.log –output-http “<a href="http://staging.com&quot;" target="_blank" rel="noopener">http://staging.com&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">####  4.将流量从文件重放到http服务器</span><br><span class="line"></span><br><span class="line">1.首先将请求流量保存到本地文件</span><br></pre></td></tr></table></figure></p><p>sudo ./gor –input-raw :8000 –output-file=requests.gor<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2.再开一个窗口,运行gor,将请求流量从文件中重放</span><br></pre></td></tr></table></figure></p><p>./gor –input-file requests.gor –output-http=”<a href="http://localhost:8001&quot;" target="_blank" rel="noopener">http://localhost:8001&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 压力测试</span><br><span class="line"></span><br><span class="line">goreplay支持将捕获到的生产实际请求流量减少或者放大重播以用于测试环境的压力测试.压力测试一般针对Input流量减少或者放大.例如下面的例子</span><br></pre></td></tr></table></figure></p><h1 id="Replay-from-file-on-2x-speed"><a href="#Replay-from-file-on-2x-speed" class="headerlink" title="Replay from file on 2x speed"></a>Replay from file on 2x speed</h1><p>#将请求流量以2倍的速度放大重播<br>gor –input-file “requests.gor|200%” –output-http “staging.com”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">当然也也支持10%,20%等缩小请求流量</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 限速</span><br><span class="line"></span><br><span class="line">如果受限于测试环境的服务器资源压力,只想重播一部分流量到测试环境中,而不需要所有的实际生产流量,那么就可以用限速功能.有两种策略可以实现限流</span><br><span class="line"></span><br><span class="line">1.随机丢弃请求流量</span><br><span class="line"></span><br><span class="line">2.基于Header或者URL丢弃一定的流量(百分比)</span><br><span class="line"></span><br><span class="line">#####  随机丢弃请求流量</span><br><span class="line"></span><br><span class="line">input和output两端都支持限速,有两种限速算法:**百分比**或者**绝对值**</span><br><span class="line"></span><br><span class="line">* 百分比: input端支持缩小或者放大请求流量,基于指定的策略随机丢弃请求流量</span><br><span class="line">* 绝对值: 如果单位时间(秒)内达到临界值,则丢弃剩余请求流量,下一秒临界值还原</span><br><span class="line"></span><br><span class="line">**用法**:</span><br><span class="line"></span><br><span class="line">在output终端使用&quot;|&quot;运算符指定限速阈值,例如:</span><br><span class="line"></span><br><span class="line">* 使用绝对值限速</span><br></pre></td></tr></table></figure></p><h1 id="staging-server-will-not-get-more-than-ten-requests-per-second"><a href="#staging-server-will-not-get-more-than-ten-requests-per-second" class="headerlink" title="staging.server will not get more than ten requests per second"></a>staging.server will not get more than ten requests per second</h1><p>#staging服务每秒只接收10个请求<br>gor –input-tcp :28020 –output-http “<a href="http://staging.com|10&quot;" target="_blank" rel="noopener">http://staging.com|10&quot;</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 使用百分比限速</span><br></pre></td></tr></table></figure></p><h1 id="replay-server-will-not-get-more-than-10-of-requests"><a href="#replay-server-will-not-get-more-than-10-of-requests" class="headerlink" title="replay server will not get more than 10% of requests"></a>replay server will not get more than 10% of requests</h1><h1 id="useful-for-high-load-environments"><a href="#useful-for-high-load-environments" class="headerlink" title="useful for high-load environments"></a>useful for high-load environments</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##### 基于Header或者URL参数限速</span><br><span class="line"></span><br><span class="line">如果header或者URL参数中有唯一值,例如(API key),则可以转发指定百分比的流量到后端,例如:</span><br></pre></td></tr></table></figure></p><h1 id="Limit-based-on-header-value"><a href="#Limit-based-on-header-value" class="headerlink" title="Limit based on header value"></a>Limit based on header value</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%” –http-header-limiter “X-API-KEY: 10%”</p><h1 id="Limit-based-on-URL-param-value"><a href="#Limit-based-on-URL-param-value" class="headerlink" title="Limit based on URL param value"></a>Limit based on URL param value</h1><p>gor –input-raw :80 –output-tcp “replay.local:28020|10%” –http-param-limiter “api_key: 10%”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">###  过滤</span><br><span class="line"></span><br><span class="line">如果只想捕获指定的URL路径请求,或者http头部,或者Http方法,则可以使用过滤功能</span><br><span class="line"></span><br><span class="line">下面是几个例子</span><br><span class="line"></span><br><span class="line">* 只捕获某个URL</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-being-sent-to-the-api-endpoint"><a href="#only-forward-requests-being-sent-to-the-api-endpoint" class="headerlink" title="only forward requests being sent to the /api endpoint"></a>only forward requests being sent to the /api endpoint</h1><p>gor –input-raw :8080 –output-http staging.com –http-allow-url /api<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 拒绝某个URL</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-NOT-being-sent-to-the-api…-endpoint"><a href="#only-forward-requests-NOT-being-sent-to-the-api…-endpoint" class="headerlink" title="only forward requests NOT being sent to the /api… endpoint"></a>only forward requests NOT being sent to the /api… endpoint</h1><p>gor –input-raw :8080 –output-http staging.com –http-disallow-url /api<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 基于正则表达式过滤头部</span><br></pre></td></tr></table></figure></p><h1 id="only-forward-requests-with-an-api-version-of-1-0x"><a href="#only-forward-requests-with-an-api-version-of-1-0x" class="headerlink" title="only forward requests with an api version of 1.0x"></a>only forward requests with an api version of 1.0x</h1><p>gor –input-raw :8080 –output-http staging.com –http-allow-header api-version:^1.0\d</p><h1 id="only-forward-requests-NOT-containing-User-Agent-header-value-“Replayed-by-Gor”"><a href="#only-forward-requests-NOT-containing-User-Agent-header-value-“Replayed-by-Gor”" class="headerlink" title="only forward requests NOT containing User-Agent header value “Replayed by Gor”"></a>only forward requests NOT containing User-Agent header value “Replayed by Gor”</h1><p>gor –input-raw :8080 –output-http staging.com –http-disallow-header “User-Agent: Replayed by Gor”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 过滤HTTP请求方法</span><br></pre></td></tr></table></figure></p><p>gor –input-raw :80 –output-http “<a href="http://staging.server&quot;" target="_blank" rel="noopener">http://staging.server&quot;</a> \<br>    –http-allow-method GET \<br>    –http-allow-method OPTIONS<br><code>`</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;使用goreplay收集线上真实http流量&quot;&gt;&lt;a href=&quot;#使用goreplay收集线上真实http流量&quot; class=&quot;headerlink&quot; title=&quot;使用goreplay收集线上真实http流量&quot;&gt;&lt;/a&gt;使用goreplay收集线上真实http流量&lt;/h2&gt;&lt;h3 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;背景介绍&lt;/h3&gt;&lt;p&gt;在很多场景中,我们需要将线上服务器的真实Http请求复制转发到某台服务器中(或者测试环境中),并且前提是不影响线上生产业务进行.&lt;/p&gt;
&lt;p&gt;例如:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通常可能会通过ab等压测工具来对单一http接口进行压测。但如果是需要http服务整体压测，使用ab来压测工作量大且不方便，通过线上流量复制引流，通过将真实请求流量放大N倍来进行压测，能对服务有一个较为全面的检验.&lt;/li&gt;
&lt;li&gt;将线上流量引入到测试环境中,测试某个中间件或者数据库的压力&lt;/li&gt;
&lt;li&gt;上线前在预发布环境，使用线上真实的请求，检查是否准备发布的版本，是否具备发布标准&lt;/li&gt;
&lt;li&gt;用线上的流量转发到预发，检查相同流量下一些指标的反馈情况，检查核心数据是否有改善、优化.&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
    
      <category term="goreplay" scheme="https://jesse.top/tags/goreplay/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---Docker存储驱动篇</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E7%AC%94%E8%AE%B0%E2%80%94%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8%E7%AF%87/"/>
    <id>https://jesse.top/2020/06/29/docker/docker笔记—存储驱动篇/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:42:46.456Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker笔记——存储驱动篇"><a href="#docker笔记——存储驱动篇" class="headerlink" title="docker笔记——存储驱动篇"></a>docker笔记——存储驱动篇</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>通过前一篇的镜像笔记,我们知道docker的镜像是只读的,而且通过同一个镜像启动的docker容器,他们共享同一份底层镜像文件.</p><p>这里主要说一说.这些分层的多个只读Image镜像是如何在磁盘中存储的.</p><hr><h3 id="docker存储驱动"><a href="#docker存储驱动" class="headerlink" title="docker存储驱动"></a>docker存储驱动</h3><p>docker提供了多种存储驱动来实现不同的方式存储镜像，下面是常用的几种存储驱动：</p><ul><li>AUFS</li><li>OverlayFS</li><li>Devicemapper</li><li>Btrfs</li><li>ZFS</li></ul><p>下面说一说AUFS、OberlayFS及Devicemapper，更多的存储驱动说明可参考：<a href="http://dockone.io/article/1513" target="_blank" rel="noopener">http://dockone.io/article/1513</a></p><a id="more"></a><h4 id="AUFS"><a href="#AUFS" class="headerlink" title="AUFS"></a>AUFS</h4><p>AUFS（AnotherUnionFS）是一种Union FS，是文件级的存储驱动。AUFS是一个能透明覆盖一个或多个现有文件系统的层状文件系统，把多层合并成文件系统的单层表示。简单来说就是支持将不同目录挂载到同一个虚拟文件系统下的文件系统。这种文件系统可以一层一层地叠加修改文件。无论底下有多少层都是只读的，只有最上层的文件系统是可写的。当需要修改一个文件时，AUFS创建该文件的一个副本，使用CoW(写时复制)将文件从只读层复制到可写层进行修改，结果也保存在可写层。</p><p>通常来说最上层是可读写层,下层是只读层.当需要读取一个文件A时,会从最顶层的读写层开始向下寻找.本层没有则根据层关系到下一层开始找.直到找到第一个文件A</p><p>当需要写入一个文件A时,如果这个文件不存在,则在读写层新建一个.否则会像上面的步骤一样从顶层开始寻找,找到A文件后,复制到读写层进行修改</p><p>当需要删除一个文件A时,如果这个文件仅仅存在读写层,则直接删除.否则就需要先在读写层删除,然后再在读写层创建一个whiteout文件来标志这个文件不存在,而不是真正删除底层文件.</p><p>当新建一个文件A.如果这个文件在读写层存在对应的whiteout文件,则先将whiteout文件删除再新建.否则直接读写层新建</p><p>在Docker中，底下的只读层就是image，可写层就是Container。结构如下图所示：</p><p><img src="https://img1.jesse.top/docker-aufs.jpg" alt=""></p><p>如果你正在使用aufs作为存储驱动,那么在Docker的工作目录(/var/lib/docker)和image下发现aufs目录:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">root@docker:~# tree /var/lib/docker -d -L 1</span><br><span class="line">/var/lib/docker</span><br><span class="line">├── aufs</span><br><span class="line">├── containers</span><br><span class="line">├── image</span><br><span class="line">├── network</span><br><span class="line">├── plugins</span><br><span class="line">├── swarm</span><br><span class="line">├── tmp</span><br><span class="line">├── trust</span><br><span class="line">└── volumes</span><br><span class="line"></span><br><span class="line">root@docker:~# tree /var/lib/docker/image -d -L 1</span><br><span class="line">/var/lib/docker/image</span><br><span class="line">└── aufs</span><br><span class="line"></span><br><span class="line">root@docker:~# tree /var/lib/docker/aufs/ -d -L 1</span><br><span class="line">/var/lib/docker/aufs/</span><br><span class="line">├── diff</span><br><span class="line">├── layers</span><br><span class="line">└── mnt</span><br></pre></td></tr></table></figure><p>在docker工作目录的aufs目录下有3个目录.其中mnt为aufs的挂载目录,diff为实际数据,包括只读层和读写层.layers为每层依赖有关的层描述文件</p><hr><h3 id="Device-mapper"><a href="#Device-mapper" class="headerlink" title="Device mapper"></a>Device mapper</h3><p>Device mapper是Linux内核2.6.9后支持的，提供的一种从逻辑设备到物理设备的映射框架机制，在该机制下，用户可以很方便的根据自己的需要制定实现存储资源的管理策略。前面讲的AUFS和OverlayFS都是文件级存储，而Device mapper是块级存储，所有的操作都是直接对块进行操作，而不是文件。</p><p>Device mapper驱动会先在块设备上创建一个资源池，然后在资源池上创建一个带有文件系统的基本设备，所有镜像都是这个基本设备的快照，而容器则是镜像的快照。所以在容器里看到文件系统是资源池上基本设备的文件系统的快照，并不有为容器分配空间。当要写入一个新文件时，在容器的镜像内为其分配新的块并写入数据，这个叫用时分配。</p><p>当要修改已有文件时，再使用CoW为容器快照分配块空间，将要修改的数据复制到在容器快照中新的块里再进行修改。Device mapper 驱动默认会创建一个100G的文件包含镜像和容器。每一个容器被限制在10G大小的卷内，可以自己配置调整。结构如下图所示：</p><p><img src="https://img1.jesse.top/docker-devicemapper.jpg" alt=""></p><p>在Centos 7发行版上最新版的docker中,默认的存储驱动就是device mapper.但是提示已经被弃用了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker info</span><br><span class="line">Containers: 3</span><br><span class="line"> Running: 3</span><br><span class="line"> Paused: 0</span><br><span class="line"> Stopped: 0</span><br><span class="line">Images: 138</span><br><span class="line">Server Version: 18.09.2</span><br><span class="line">Storage Driver: devicemapper #这一行</span><br><span class="line">......略......</span><br><span class="line">WARNING: the devicemapper storage-driver is deprecated, and will be removed in a future release. #最后这一行提示devicemapper已经被弃用</span><br></pre></td></tr></table></figure><p>和aufs一样,在docker的工作目录下也能看到device mapper目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$ll /var/lib/docker/devicemapper/</span><br><span class="line">总用量 32</span><br><span class="line">drwx------ 2 root root    32 2月  23 16:25 devicemapper</span><br><span class="line">drwx------ 2 root root 24576 5月  16 10:30 metadata</span><br><span class="line">drwxr-xr-x 5 root root  4096 5月  16 10:30 mnt</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker笔记——存储驱动篇&quot;&gt;&lt;a href=&quot;#docker笔记——存储驱动篇&quot; class=&quot;headerlink&quot; title=&quot;docker笔记——存储驱动篇&quot;&gt;&lt;/a&gt;docker笔记——存储驱动篇&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;通过前一篇的镜像笔记,我们知道docker的镜像是只读的,而且通过同一个镜像启动的docker容器,他们共享同一份底层镜像文件.&lt;/p&gt;
&lt;p&gt;这里主要说一说.这些分层的多个只读Image镜像是如何在磁盘中存储的.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;docker存储驱动&quot;&gt;&lt;a href=&quot;#docker存储驱动&quot; class=&quot;headerlink&quot; title=&quot;docker存储驱动&quot;&gt;&lt;/a&gt;docker存储驱动&lt;/h3&gt;&lt;p&gt;docker提供了多种存储驱动来实现不同的方式存储镜像，下面是常用的几种存储驱动：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AUFS&lt;/li&gt;
&lt;li&gt;OverlayFS&lt;/li&gt;
&lt;li&gt;Devicemapper&lt;/li&gt;
&lt;li&gt;Btrfs&lt;/li&gt;
&lt;li&gt;ZFS&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面说一说AUFS、OberlayFS及Devicemapper，更多的存储驱动说明可参考：&lt;a href=&quot;http://dockone.io/article/1513&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://dockone.io/article/1513&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---Docker镜像篇</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E9%95%9C%E5%83%8F%E7%AF%87/"/>
    <id>https://jesse.top/2020/06/29/docker/docker学习笔记——镜像篇/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:28:47.322Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker笔记——镜像篇"><a href="#docker笔记——镜像篇" class="headerlink" title="docker笔记——镜像篇"></a>docker笔记——镜像篇</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>docker镜像是一个只读的Docker容器模板.含有启动docker容器所需的文件系统结构以及内容.因此是启动一个容器的基础.docker镜像的文件内容以及一些运行docker容器的配置文件组成了docker容器的静态文件运行环境—rootfs.</p><p>可以这么理解,docker镜像是docker容器的静态视角.docker容器是docker镜像的运行状态</p><hr><p><strong>1.rootfs</strong></p><p>rootfs是docker容器的根目录.如:/dev,/proc,/bin,/etc …….传统的Linux容器操作系统内核启动时,首先挂载一个只读(read-only)的rootfs.当系统检测到完整性后,再将其切换到读写(read-write)模式.而在docker架构中.也沿用了Linux内核的启动方法.在docker为容器挂载rootfs时,将rootfs设置为只读模式,挂载完毕后,在已有的只读rootfs上再挂载一个读写层.</p><p>读写层位于docker容器文件系统的最顶层.下面可能挂载了多个只读层.</p><a id="more"></a><p><strong>2.docker镜像的特点</strong></p><ul><li><strong>分层</strong></li></ul><p>每个镜像都由一系列的”镜像层”组成.当需要修改容器镜像内的某个文件时,只对最上方的读写层进行修改,不覆盖下面的只读层文件系统.例如删除一个只读文件系统中的文件时,只会在读写层标记这个文件”已经被删除”,但是这个文件在只读层中仍然存在.只不过不被用户感知.</p><ul><li><strong>写时复制(copy-on-write)</strong></li></ul><p>每个容器在启动的时候并不需要单独复制一份镜像文件,而是将所有镜像层以只读的方式挂载到一个挂载点,在多个容器之间共享.在未更改镜像文件内容时,所有容器共享一份数据,只有在docker容器运行过程中修改过文件时,才会把变化的文件内容写到读写层.并隐藏只读层中的老版本文件.</p><p>写时复制机制减少了镜像对磁盘空间的占用和容器的启动时间</p><ul><li><strong>联合挂载</strong></li></ul><p>联合挂载技术可以在一个挂载点同时挂载多个文件系统.实现这种联合挂载技术的文件系统被称为联合文件系统(union filesystem).从内核的角度来看,docker容器的文件系统分为只读rootfs层和读写层.但是在用户的视角看来,整个文件系统都是rootfs底层.</p><p>下面这个图可以理解,镜像是由一堆的只读层堆叠起来的统一视角:</p><p><img src="![Docker镜像](https://img1.jesse.top/docker-image1.gif" alt=""></p><p>下面这个图理解了docker镜像和docker容器的区别</p><p><img src="https://img1.jesse.top/docker-container1.png" alt=""></p><hr><h3 id="docker镜像的相关概念"><a href="#docker镜像的相关概念" class="headerlink" title="docker镜像的相关概念"></a>docker镜像的相关概念</h3><p>1.<strong>registry</strong></p><p>registry用来保存docker镜像.可以将registry简单的想象成类似于git仓库之类的实体.当<figure class="highlight docker"><figcaption><span>run```命令启动一个容器时,如果宿主机上并不存在该镜像,那么docker将从registry中下载镜像并保存到宿主机</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">可以使用docker官方的公共registry服务(docker hub),可以可以使用阿里云私有的registry,甚至还可以自己搭建私有的registry</span><br><span class="line"></span><br><span class="line">**<span class="number">2</span>.repository**</span><br><span class="line"></span><br><span class="line">repository是由具有某个功能的docker镜像的所有迭代版本构成的镜像组.repository通常表示镜像所具有的功能,例如ansible/ubunbu14.<span class="number">4</span>-ansible.而顶层仓库则只包含repository名.例如,Ubuntu</span><br><span class="line"></span><br><span class="line">repository是一个镜像集合,包含了多个不同版本的镜像.使用标签进行版本区分,例如ubuntu:<span class="number">14.04</span>,ubuntu12.<span class="number">04</span>.他们均属于ubuntu这个repository</span><br><span class="line"></span><br><span class="line">**总而言之,registry是repository的集合,repository是镜像的集合**</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"><span class="comment">### docker镜像相关的命令</span></span><br><span class="line"></span><br><span class="line">* **拉取镜像**</span><br><span class="line"></span><br><span class="line">```docker pull [OPTIONS] NAME[:TAG|@DIGEST]</span><br></pre></td></tr></table></figure></p><p>如果只指定了镜像名,则默认从docker hub官方拉取该镜像的最新latest版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker pull nginx</span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library/nginx</span><br><span class="line">743f2d6c1f65: Pull complete</span><br><span class="line">6bfc4ec4420a: Pull complete</span><br><span class="line">688a776db95f: Pull complete</span><br><span class="line">Digest: sha256:23b4dcdf0d34d4a129755fc6f52e1c6e23bb34ea011b315d87e193033bcd1b68</span><br><span class="line">Status: Downloaded newer image for nginx:latest</span><br></pre></td></tr></table></figure><p>如果指定了tag,则拉取指定的版本镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker pull nginx:1.15</span><br><span class="line">1.15: Pulling from library/nginx</span><br><span class="line">Digest: sha256:23b4dcdf0d34d4a129755fc6f52e1c6e23bb34ea011b315d87e193033bcd1b68</span><br><span class="line">Status: Downloaded newer image for nginx:1.15</span><br></pre></td></tr></table></figure><p>拉取我阿里云的私人registry下的镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#registry.cn-hangzhou.aliyuncs.com/jesse_images为仓库地址</span><br><span class="line">#php7.1.9为镜像版本</span><br><span class="line">[root@localhost ~]$docker pull registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:php7.1.9</span><br><span class="line"></span><br><span class="line">php7.1.9: Pulling from jesse_images/jesse_images</span><br><span class="line">Digest: sha256:ed9b7326b539f47a81697e51ed8ec698bec49fb62959990c1277d068fc55ff94</span><br><span class="line">Status: Downloaded newer image for registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:php7.1.9</span><br></pre></td></tr></table></figure><hr><ul><li><strong>删除镜像</strong></li></ul><p>命令格式:</p><figure class="highlight docker"><figcaption><span>rmi [OPTIONS] IMAGE [IMAGE…]```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">可以是docker rmi 镜像ID 或者 docker rmi 镜像名:tag</span><br></pre></td></tr></table></figure><p>docker images #查看当前宿主机上的镜像<br>[root@localhost ~]$docker images<br>REPOSITORY                                                    TAG                    IMAGE ID            CREATED             SIZE<br>busybox                                                       latest                 64f5d945efcc        6 days ago          1.2MB<br>nginx                                                         1.15                   53f3fd8007f7        7 days ago          109MB<br>nginx                                                         latest                 53f3fd8007f7        7 days ago          109MB<br>php-swoole                                                    7.1                    aa71c42a22ca        9 days ago          588MB</p><p><none>                                                        <none>                 01f5d7914e61        9 days ago          585MB</none></none></p><p>#删除镜像ID为01f5d7914e61的镜像</p><p>[root@localhost ~]$docker rmi 01f5d7914e61<br>Deleted: sha256:01f5d7914e615b0e2f7cc36a494c876dfc0c678963898374d9ef512d7a762aac<br>Deleted: sha256:b4dd4a057d2561647ff7bf6b299a143c99f66831c129618f49bca5e6ac82f99e<br>Deleted: sha256:c37c880338efd3d340bfa71b35b7653b6cec8eb4f5dfcfab8c7ad0045fef3ce6<br>Deleted: sha256:fb7d015f8921c1244134730b6c21f0bda6c7156ccd421d9e0069d5a1074b48dd<br>Deleted: sha256:ab74760ab0af7680fa9338100c92306392ffeb384b8976045a11dab9a4ebbc57<br>Deleted: sha256:8544a2552375c861955db9034e9c3c5a3e83530b84de9b9bb6d4a7d0d5e5b8ac<br>Deleted: sha256:4eebc2d39a0733b28992a064fc71852297927a3994b01a9d1123d71b042ab729</p><p>#删除nginx.tag为1.15的镜像<br>[root@localhost ~]$docker rmi nginx:1.15<br>Untagged: nginx:1.15<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">* **查看镜像**</span><br><span class="line"></span><br><span class="line">```docker history 镜像名```可以看到镜像的构建分层</span><br></pre></td></tr></table></figure></p><p>[root@localhost ~]$docker history nginx<br>IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT<br>53f3fd8007f7        7 days ago          /bin/sh -c #(nop)  CMD [“nginx” “-g” “daemon…   0B</p><p><missing>           7 days ago          /bin/sh -c #(nop)  STOPSIGNAL SIGTERM           0B</missing></p><p><missing>           7 days ago          /bin/sh -c #(nop)  EXPOSE 80                    0B</missing></p><p><missing>           7 days ago          /bin/sh -c ln -sf /dev/stdout /var/log/nginx…   0B</missing></p><p><missing>           7 days ago          /bin/sh -c set -x  &amp;&amp; apt-get update  &amp;&amp; apt…   54.1MB</missing></p><p><missing>           7 days ago          /bin/sh -c #(nop)  ENV NJS_VERSION=1.15.12.0…   0B</missing></p><p><missing>           7 days ago          /bin/sh -c #(nop)  ENV NGINX_VERSION=1.15.12…   0B</missing></p><p><missing>           7 days ago          /bin/sh -c #(nop)  LABEL maintainer=NGINX Do…   0B</missing></p><p><missing>           8 days ago          /bin/sh -c #(nop)  CMD [“bash”]                 0B</missing></p><p><missing>           8 days ago          /bin/sh -c #(nop) ADD file:fcb9328ea4c115670…   55.3MB<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```docker inspect 镜像名``` 可以看到镜像的具体信息</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">* **创建镜像**</span><br><span class="line"></span><br><span class="line">```docker commit```命令可以基于现有的容器创建出一个镜像</span><br></pre></td></tr></table></figure></missing></p><p>#用法格式:<br>docker commit -m ‘镜像说明信息’   -a  作者  容器ID 镜像名:版本</p><p>[root@localhost ~]$docker commit -h</p><p>Usage:    docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]</p><p>Create a new image from a container’s changes</p><p>Options:<br>  -a, –author string    Author (e.g., “John Hannibal Smith <a href="mailto:&#x68;&#x61;&#x6e;&#x6e;&#105;&#98;&#x61;&#x6c;&#64;&#97;&#45;&#116;&#x65;&#x61;&#x6d;&#x2e;&#x63;&#111;&#x6d;" target="_blank" rel="noopener">&#x68;&#x61;&#x6e;&#x6e;&#105;&#98;&#x61;&#x6c;&#64;&#97;&#45;&#116;&#x65;&#x61;&#x6d;&#x2e;&#x63;&#111;&#x6d;</a>“)<br>  -c, –change list      Apply Dockerfile instruction to the created image<br>  -m, –message string   Commit message<br>  -p, –pause            Pause container during commit (default true)</p><p>  #例如.将正在运行中的Nginx容器提交为一个新的nginx:test镜像<br>[root@localhost ~]$docker commit -m ‘test’ -a ‘jesse’ nginx nginx:test<br>sha256:028f5e2b21a66a1bf5f70727f20cac04e8918f57d5584cc2aeb09f18791d9680<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">* 导入导出镜像</span><br><span class="line"></span><br><span class="line">命令: </span><br><span class="line"></span><br><span class="line">```docker save -o 保存文件名 镜像名:tag```  ————将某个镜像保存为一个文件</span><br><span class="line"></span><br><span class="line">```docker load &lt; 文件名``` or ```docker load —input 文件名``` ——将某个文件导入到本地镜像</span><br><span class="line"></span><br><span class="line">例如</span><br></pre></td></tr></table></figure></p><p>#将nginx:test这个镜像保存为Nginx_test.tar文件<br>[root@localhost ~]$docker save -o nginx_test.tar nginx:test<br>[root@localhost ~]$ll nginx_test.tar<br>-rw——- 1 root root 113036800 5月  16 10:29 nginx_test.tar</p><p>#删除ningx:test这个镜像.然后再从该文件恢复<br>[root@localhost ~]$docker load &lt; nginx_test.tar<br>67392954caf5: Loading layer [==================================================&gt;]  8.192kB/8.192kB<br>Loaded image: nginx:test</p><p>#镜像已经被导入<br>[root@localhost ~]$docker images | grep nginx<br>nginx                                                         test                   028f5e2b21a6        4 minutes ago       109MB<br><code>`</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker笔记——镜像篇&quot;&gt;&lt;a href=&quot;#docker笔记——镜像篇&quot; class=&quot;headerlink&quot; title=&quot;docker笔记——镜像篇&quot;&gt;&lt;/a&gt;docker笔记——镜像篇&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;docker镜像是一个只读的Docker容器模板.含有启动docker容器所需的文件系统结构以及内容.因此是启动一个容器的基础.docker镜像的文件内容以及一些运行docker容器的配置文件组成了docker容器的静态文件运行环境—rootfs.&lt;/p&gt;
&lt;p&gt;可以这么理解,docker镜像是docker容器的静态视角.docker容器是docker镜像的运行状态&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;1.rootfs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;rootfs是docker容器的根目录.如:/dev,/proc,/bin,/etc …….传统的Linux容器操作系统内核启动时,首先挂载一个只读(read-only)的rootfs.当系统检测到完整性后,再将其切换到读写(read-write)模式.而在docker架构中.也沿用了Linux内核的启动方法.在docker为容器挂载rootfs时,将rootfs设置为只读模式,挂载完毕后,在已有的只读rootfs上再挂载一个读写层.&lt;/p&gt;
&lt;p&gt;读写层位于docker容器文件系统的最顶层.下面可能挂载了多个只读层.&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker-compose</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94docker-compose/"/>
    <id>https://jesse.top/2020/06/29/docker/docker学习笔记—docker-compose/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:15:08.679Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker学习笔记——docker-compose"><a href="#docker学习笔记——docker-compose" class="headerlink" title="docker学习笔记——docker-compose"></a>docker学习笔记——docker-compose</h2><p>docker compose 定义并且运行多个docker容器.使用YAML风格文件定义一个compose文件.利用compose文件创建和启动所有服务.</p><p>使用docker compose基本只需要3个步骤</p><ul><li>在Dockerfile文件定义app环境</li><li>在docker-compose.yml文件中定义组成app的各个服务</li><li>run docker-compose up 和compose 启动和运行app</li></ul><p>下面文档均可以在docker-compose官方找到详细资料:<a href="https://docs.docker.com/compose/" target="_blank" rel="noopener">docker-compose</a></p><h3 id="docker-compose安装"><a href="#docker-compose安装" class="headerlink" title="docker-compose安装"></a>docker-compose安装</h3><a id="more"></a><p>docker-compose的安装非常简单.下面是Linux上的安装方法.其他平台请自行参考官网</p><p>1.下载最近的1.24版本的二进制文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose</span><br></pre></td></tr></table></figure><p>2.给予执行权限.加入环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod +x /usr/local/bin/docker-compose</span><br><span class="line">sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose</span><br></pre></td></tr></table></figure><p>3.安装完成.查看是否安装成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose --version</span><br><span class="line">docker-compose version 1.24.0, build 1110ad01</span><br></pre></td></tr></table></figure><hr><h2 id="compose例子"><a href="#compose例子" class="headerlink" title="compose例子"></a>compose例子</h2><p>在官网上,或者去github上下载一个例子.这里我参考<docker 深入浅出="">这本书的例子</docker></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir /data/counter-app</span><br><span class="line">cd /data/counter-app</span><br><span class="line">git clone https://github.com/nigelpoulton/counter-app.git</span><br></pre></td></tr></table></figure><h3 id="docker-compose文件"><a href="#docker-compose文件" class="headerlink" title="docker-compose文件"></a>docker-compose文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost counter-app]$cat docker-compose.yml</span><br><span class="line"></span><br><span class="line">version: &quot; 3.5&quot;</span><br><span class="line">services:</span><br><span class="line">   web-fe:</span><br><span class="line">      build:</span><br><span class="line">         command: python app.py</span><br><span class="line">         ports:</span><br><span class="line">            - target: 5000</span><br><span class="line">              published: 5000</span><br><span class="line">         </span><br><span class="line">         networks:</span><br><span class="line">            - counter-net</span><br><span class="line">         </span><br><span class="line">         volumes:</span><br><span class="line">            - type: volume</span><br><span class="line">              source: counter-vol</span><br><span class="line">              target: /code</span><br><span class="line">    </span><br><span class="line">   redis:</span><br><span class="line">      image: &quot;redis:alpine&quot;</span><br><span class="line">      networks:</span><br><span class="line">         counter-net</span><br><span class="line">   </span><br><span class="line">networks:</span><br><span class="line">        counter-net:</span><br><span class="line">   </span><br><span class="line">volumes:</span><br><span class="line">       counter-vol:</span><br></pre></td></tr></table></figure><p><strong>compose文件结构</strong></p><p>包含4个一级key: version.services.network.volumes</p><ul><li>version: 必须指定,定义了compose文件格式版本.这里是3.5最新版</li><li>services: 用于定义不同的应用服务.这个例子中定义了2个服务.一个是web-fe的web前端.一个是redis的内存数据库.docker compose会将每个服务部署在各自的容器中</li><li>networks用于创建新的网络.默认情况下会创建bridge网络</li><li>volume用于创建新的卷</li></ul><p>上面的docker compose文件定义了2个服务.在web-fe的服务定义中.包含如下指令:</p><ul><li>build:  指定docker基于当前目录下的Dockerfile文件构建一个新镜像</li><li>command: 指定在容器中执行app.py脚本作为主程序 (这个指令可以忽略,因为dockerfile镜像中已经配置了CMD指令)</li><li>ports: 将容器(target)的5000端口映射到宿主机(published)5000端口</li><li>networks: docker将此容器连接到指定的网络上</li><li>volumes: 指定docker将宿主机counter-vol卷(source)挂载到容器内的/code(target)上.counter-vol卷是已经存在的,或者是在文件下方的volumes一级key中定义的</li></ul><p>redis服务比较简单,就不再赘述..</p><hr><h2 id="部署docker-compose"><a href="#部署docker-compose" class="headerlink" title="部署docker-compose"></a>部署docker-compose</h2><p>简要介绍counter-app目录内的几个文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost counter-app]$ll</span><br><span class="line">总用量 20</span><br><span class="line">-rw-r--r-- 1 root root 599 6月  18 17:34 app.py    #应用程序代码</span><br><span class="line">-rw-r--r-- 1 root root 475 6月  17 18:46 docker-compose.ymal  #compose文件,定义了如何部署容器</span><br><span class="line">-rw-r--r-- 1 root root 109 6月  18 17:34 Dockerfile  #构建web-fe服务镜像的dockerfile</span><br><span class="line">-rw-r--r-- 1 root root 128 6月  18 17:34 README.md   </span><br><span class="line">-rw-r--r-- 1 root root  11 6月  18 17:34 requirements.txt #列出app.py代码文件中python的依赖包</span><br></pre></td></tr></table></figure><h4 id="启动docker-compose"><a href="#启动docker-compose" class="headerlink" title="启动docker-compose"></a>启动docker-compose</h4><p>在当前目录下执行下列路径</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose up -d #后台启动</span><br></pre></td></tr></table></figure><p>默认情况下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>docker-compose -f compose_file up -d<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">如果找不到文件则会报错:</span><br></pre></td></tr></table></figure></p><p>ERROR:<br>        Can’t find a suitable configuration file in this directory or any<br>        parent. Are you in the right directory?</p><pre><code>Supported filenames: docker-compose.yml, docker-compose.yaml</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">部署过程中创建或者拉取了3个镜像: counterapp_web-fe,python,redis</span><br><span class="line"></span><br><span class="line">部署完成后,启动了如下2个容器:</span><br></pre></td></tr></table></figure><p>[root@localhost counter-app]$docker ps<br>CONTAINER ID        IMAGE                             COMMAND                  CREATED             STATUS              PORTS                    NAMES<br>474301996ccc        redis:alpine                      “docker-entrypoint.s…”   21 hours ago        Up 21 hours         6379/tcp                 counter-app_redis_1<br>c7a1e28b5e28        counter-app_web-fe                “python app.py”          21 hours ago        Up 21 hours         0.0.0.0:5000-&gt;5000/tcp   counter-app_web-fe_1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">每个容器都以项目名为前缀(所在目录名称).此外,还用一个数字为后缀用于表示容器序列(因为docker-compose允许扩容和缩减服务器数量)</span><br><span class="line"></span><br><span class="line">同时,docker-compose还创建了counter-app_counter-net网络:</span><br></pre></td></tr></table></figure></p><p>[root@localhost counter-app]$docker network ls<br>NETWORK ID          NAME                      DRIVER              SCOPE<br>6d40a81d76e7        bridge                    bridge              local<br>ef71284e9acc        counter-app_counter-net   bridge              local<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">应用部署成功后,可以查看容器的运行效果.每次访问,计数器就+1</span><br></pre></td></tr></table></figure></p><p>[root@localhost counter-app]$curl <a href="http://localhost:5000" target="_blank" rel="noopener">http://localhost:5000</a><br>What’s up Docker Deep Divers! You’ve visited me 1 times.<br>[root@localhost counter-app]$curl <a href="http://localhost:5000" target="_blank" rel="noopener">http://localhost:5000</a><br>What’s up Docker Deep Divers! You’ve visited me 2 times.<br>[root@localhost counter-app]$curl <a href="http://localhost:5000" target="_blank" rel="noopener">http://localhost:5000</a><br>What’s up Docker Deep Divers! You’ve visited me 3 times.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## docker Compose管理</span><br><span class="line"></span><br><span class="line">上面讲到如何部署一个compose应用..接下来讲解一下compose的管理命令.需要注意的是所有的docker-compose命令都需要在相关目录下执行.不然仍然会提示找不到docker-compose.yml(yaml)文件</span><br><span class="line"></span><br><span class="line">如果是停止应用.只需将up换成down即可.</span><br></pre></td></tr></table></figure></p><p>[root@localhost counter-app]$docker-compose down<br>Stopping counter-app_redis_1  … done<br>Stopping counter-app_web-fe_1 … done<br>Removing counter-app_redis_1  … done<br>Removing counter-app_web-fe_1 … done<br>Removing network counter-app_counter-net<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">停止compose经历了如下的过程:</span><br><span class="line"></span><br><span class="line">* 停止所有容器</span><br><span class="line">* 移除容器</span><br><span class="line">* 移除docker网络</span><br><span class="line"></span><br><span class="line">此时,无论是执行```docker ps ```还是```docker ps -a```都看不到容器</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 查看compose各个服务容器的运行的进程</span><br></pre></td></tr></table></figure></p><p>[root@localhost counter-app]$docker-compose top<br>counter-app_redis_1</p><h2 id="UID-PID-PPID-C-STIME-TTY-TIME-CMD"><a href="#UID-PID-PPID-C-STIME-TTY-TIME-CMD" class="headerlink" title="UID    PID    PPID    C   STIME   TTY     TIME         CMD"></a>UID    PID    PPID    C   STIME   TTY     TIME         CMD</h2><p>100   32558   32542   0   13:21   ?     00:00:00   redis-server</p><p>counter-app_web-fe_1</p><h2 id="UID-PID-PPID-C-STIME-TTY-TIME-CMD-1"><a href="#UID-PID-PPID-C-STIME-TTY-TIME-CMD-1" class="headerlink" title="UID     PID    PPID    C   STIME   TTY     TIME                    CMD"></a>UID     PID    PPID    C   STIME   TTY     TIME                    CMD</h2><p>root   32582   32564   6   13:21   ?     00:00:00   python app.py<br>root   32703   32582   4   13:21   ?     00:00:00   /usr/local/bin/python /code/app.py<br>[root@localhost counter-app]$<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; PID是docker宿主机的进程ID</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 停止应用容器.但是并不删除资源</span><br><span class="line"></span><br><span class="line">执行完```docker-compose stop```命令后,容器还存在</span><br></pre></td></tr></table></figure></p><p>[root@localhost counter-app]$docker-compose stop<br>Stopping counter-app_web-fe_1 … done<br>Stopping counter-app_redis_1  … done</p><p>[root@localhost counter-app]$docker-compose ps</p><pre><code>Name                      Command               State    Ports</code></pre><hr><p>counter-app_redis_1    docker-entrypoint.sh redis …   Exit 0<br>counter-app_web-fe_1   python app.py                    Exit 0<br>[root@localhost counter-app]$<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 删除,重启已停止的compose应用容器</span><br></pre></td></tr></table></figure></p><p>docker-compose rm #删除.删除应用相关的容器,但是不会删除卷和镜像和网络.<br>docker-compose restart #重启<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### </span><br><span class="line"></span><br><span class="line">#### 拉取服务镜像</span><br><span class="line"></span><br><span class="line">```docker-compose pull server_name ```这个命令会先拉取服务镜像到本地.例如在本文的docker compose例子中有2个服务:web-fe和redis.如果执行下列命令,会仅仅拉取redis镜像到本地</span><br></pre></td></tr></table></figure></p><p>docker-compose pull redis<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 其他命令</span><br><span class="line"></span><br><span class="line">日常docker管理容器的命令都可以使用```docker-compose```替代.例如:</span><br></pre></td></tr></table></figure></p><p>docker-compose logs service_name #查看服务容器日志<br>docker-compose exec service_name #开启终端登陆容器<br>docker-compose kill -s SIGINT    #杀死docker-compose服务容器<br>docker-compose ps                #列出容器<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### docker-compose配置文件指令解析</span><br><span class="line"></span><br><span class="line">以下配置文件以版本3.x为例.官网参考:&lt;https://docs.docker.com/compose/compose-file/&gt;</span><br><span class="line"></span><br><span class="line">下面是个包含完整指令的样例:</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:</p><p>  redis:<br>    image: redis:alpine<br>    ports:</p><pre><code>  - &quot;6379&quot;networks:  - frontenddeploy:  replicas: 2  update_config:    parallelism: 2    delay: 10s  restart_policy:    condition: on-failure</code></pre><p>  db:<br>    image: postgres:9.4<br>    volumes:</p><pre><code>  - db-data:/var/lib/postgresql/datanetworks:  - backenddeploy:  placement:    constraints: [node.role == manager]</code></pre><p>  vote:<br>    image: dockersamples/examplevotingapp_vote:before<br>    ports:</p><pre><code>  - &quot;5000:80&quot;networks:  - frontenddepends_on:  - redisdeploy:  replicas: 2  update_config:    parallelism: 2  restart_policy:    condition: on-failure</code></pre><p>  result:<br>    image: dockersamples/examplevotingapp_result:before<br>    ports:</p><pre><code>  - &quot;5001:80&quot;networks:  - backenddepends_on:  - dbdeploy:  replicas: 1  update_config:    parallelism: 2    delay: 10s  restart_policy:    condition: on-failure</code></pre><p>  worker:<br>    image: dockersamples/examplevotingapp_worker<br>    networks:</p><pre><code>  - frontend  - backenddeploy:  mode: replicated  replicas: 1  labels: [APP=VOTING]  restart_policy:    condition: on-failure    delay: 10s    max_attempts: 3    window: 120s  placement:    constraints: [node.role == manager]</code></pre><p>  visualizer:<br>    image: dockersamples/visualizer:stable<br>    ports:</p><pre><code>  - &quot;8080:8080&quot;stop_grace_period: 1m30svolumes:  - &quot;/var/run/docker.sock:/var/run/docker.sock&quot;deploy:  placement:    constraints: [node.role == manager]</code></pre><p>networks:<br>  frontend:<br>  backend:</p><p>volumes:<br>  db-data:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## Service块级别的配置文件指令</span><br><span class="line"></span><br><span class="line">#### build </span><br><span class="line"></span><br><span class="line">build可以指定一个目录或者在build下还可以指定context上下文环境和docker-file文件名</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  webapp:<br>    build: ./dir</p><p>或者<br>webapp:<br>    build:<br>      context: ./dir<br>      dockerfile: Dockerfile-alternate #如果指定dockerfile,则必须要指定一个build路径,也就是context<br>      args:<br>        buildno: 1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">如果同时指定了Image关键字,那么会构建一个指定的镜像名:tag</span><br></pre></td></tr></table></figure></p><p>build: ./dir<br>image: webapp:tag  #构建webapp:tag的镜像名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; build选项在swarm中部署stack时是无效的,因为docker stack命令只接受已经build好的镜像</span><br><span class="line"></span><br><span class="line">#### CONTEXT</span><br><span class="line"></span><br><span class="line">定义上下文目录.如果是一个相对目录,那么是相对Compose file文件的目录.</span><br><span class="line"></span><br><span class="line">#### ARGS</span><br><span class="line"></span><br><span class="line">在build过程中可以允许使用ARGS变量传递给dockerfile.具体用法参考官网</span><br><span class="line"></span><br><span class="line">#### COMMAND</span><br><span class="line"></span><br><span class="line">重写dockerfile或者镜像中的默认命令.和dockerfile一样可以是shell方式也可以是exec方式执行</span><br></pre></td></tr></table></figure></p><p>command: bundle exec thin -p 3000<br>command: [“bundle”, “exec”, “thin”, “-p”, “3000”]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### configs</span><br><span class="line"></span><br><span class="line">授予每个service的配置文件访问.具体用法参考官网</span><br><span class="line"></span><br><span class="line">#### container_name</span><br><span class="line"></span><br><span class="line">指定一个容器名,而不是使用默认名字</span><br></pre></td></tr></table></figure></p><p>container_name: my-web-container<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 需要注意的是由于容器名必须唯一,所以当扩展多个容器副本时,指定一个具体的容器名会报错,所以这个指令在swarm模式下部署stack时会被忽略</span><br><span class="line"></span><br><span class="line">#### depends_on</span><br><span class="line"></span><br><span class="line">用于在多个services之间指定依赖性.service dependencies会导致以下行为</span><br><span class="line"></span><br><span class="line">* ```docker-compose up``` 启动时会参考depndency顺序.在下面这个例子中.db和redis服务会先于web服务启动</span><br><span class="line">* ```docker-compose up SERVICE``` 会自动启动该SERVICE的依赖服务.在下面例子中```docker-compose up web```命令会自动创建和启动db和redis</span><br><span class="line">* ```docker-compose stop```会参考依赖顺序而停止服务.在下面例子中,web服务会先于db和redis服务停止</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  web:<br>    build: .<br>    depends_on:</p><pre><code>- db- redis</code></pre><p>  redis:<br>    image: redis<br>  db:<br>    image: postgres<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 使用depens_on需要注意以下几点:</span><br><span class="line">&gt;</span><br><span class="line">&gt; 1.denpds_on只会在web依赖的服务启动后就启动web服务,而不是等待db和redis服务启动并且处于ready状态才启动web.这有可能会带来一些问题,比如mysql启动较慢,数据库还没准备好等.如果你需要确定后端的db,redis数据库启动成功,并且可以连接时才启动web服务,可以参考https://docs.docker.com/compose/startup-order/</span><br><span class="line">&gt;</span><br><span class="line">&gt; 2.version3版本不再支持depends_on下的condition指令</span><br><span class="line">&gt;</span><br><span class="line">&gt; 3.version3版本的depends_on选项在swarm模式下部署stack时会被忽略</span><br><span class="line"></span><br><span class="line">depends_on选项的控制启动顺序参考:</span><br><span class="line"></span><br><span class="line">编写一个shell脚本循环判断后端的数据库是否ready.如果ready则执行CMD命令.然后在command指令中指定脚本的后端db数据库服务名,以及CMD命令参数</span><br></pre></td></tr></table></figure></p><p>#!/bin/sh</p><h1 id="wait-for-postgres-sh"><a href="#wait-for-postgres-sh" class="headerlink" title="wait-for-postgres.sh"></a>wait-for-postgres.sh</h1><p>#循环测试db服务($1参数)的状态.一旦可以连接了,执行cmd命令(python app.py)<br>set -e</p><p>host=”$1”<br>shift<br>cmd=”$@”</p><p>until PGPASSWORD=$POSTGRES_PASSWORD psql -h “$host” -U “postgres” -c ‘\q’; do</p><blockquote><p>&amp;2 echo “Postgres is unavailable - sleeping”<br>  sleep 1<br>done</p></blockquote><blockquote><p>&amp;2 echo “Postgres is up - executing command”<br>exec $cmd<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p></blockquote><p>command: [“./wait-for-postgres.sh”, “db”, “python”, “app.py”]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### deploy指令</span><br><span class="line"></span><br><span class="line">该指令用于配置服务相关的配置和部署方式.这个指令只在version3版本支持,而且只在swarm模式下才生效.单机```docker-compose up```方式执行会被忽略</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  redis:<br>    image: redis:alpine<br>    deploy:<br>      replicas: 6<br>      update_config:<br>        parallelism: 2<br>        delay: 10s<br>      restart_policy:<br>        condition: on-failure<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">下面是有关deploy的几个子指令介绍</span><br><span class="line"></span><br><span class="line">* ENDPOINT_MODE</span><br><span class="line"></span><br><span class="line">&gt; version 3.3 only</span><br><span class="line"></span><br><span class="line">```endpoint_mode: VIP```  Docker为service分配一个虚拟IP.作为用户的前端入口.docker路由用户请求到所有可用的worker节点..这也是默认模式</span><br><span class="line"></span><br><span class="line">```endpoint_mode: dnsrr``` DNS轮询服务,Docker发起一个service name的DNS查询,并且返回一个包含多个IP地址的列表.客户端通过轮询方式链接其中一个IP地址.</span><br><span class="line"></span><br><span class="line">* LABLES</span><br><span class="line"></span><br><span class="line">为service指定一个标签.只对service生效,无法为service的具体某个容器生效</span><br><span class="line"></span><br><span class="line">* MODE</span><br><span class="line"></span><br><span class="line">mode定义了在swarm节点上的副本部署模式,有global和replicated两种模式,默认是replicated</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  worker:<br>    image: dockersamples/examplevotingapp_worker<br>    deploy:<br>      mode: global<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">关于两种模式的区别参考:&lt;https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/#replicated-and-global-services&gt;</span><br><span class="line"></span><br><span class="line">* REPLACEMENT</span><br><span class="line"></span><br><span class="line">定义了constrants(约束条件)和preferences.的参数.</span><br><span class="line"></span><br><span class="line">下面是constrants指令的用法:</span><br><span class="line"></span><br><span class="line">constrants指令可以限制某个task在哪些swarm节点上运行.多个constrants指令是逻辑AND的关系来匹配满足条件的nodes.constrants可以匹配swarm节点或者Docker引擎标签:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">| node attribute  | matches                  | example                                       |</span><br><span class="line">| :-------------- | :----------------------- | :-------------------------------------------- |</span><br><span class="line">| `node.id`       | Node ID                  | `node.id==2ivku8v2gvtg4`                      |</span><br><span class="line">| `node.hostname` | Node hostname            | `node.hostname!=node-2`                       |</span><br><span class="line">| `node.role`     | Node role                | `node.role==manager`                          |</span><br><span class="line">| `node.labels`   | user defined node labels | `node.labels.security==high`                  |</span><br><span class="line">| `engine.labels` | Docker Engine&apos;s labels   | `engine.labels.operatingsystem==ubuntu 14.04` |</span><br><span class="line"></span><br><span class="line">例如下面这个例子中限制redis service的task运行在lable标签等于queue的swarm节点</span><br></pre></td></tr></table></figure></p><p>$ docker service create \<br>  –name redis_2 \<br>  –constraint ‘node.labels.type == queue’ \<br>  redis:3.0.6<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">回到刚才REPLACEMENT的例子,下面的例子中表示db service只运行在swarm manager节点,而且docker node节点的操作系统是Ubuntu 14.04</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  db:<br>    image: postgres<br>    deploy:<br>      placement:<br>        constraints:</p><pre><code>  - node.role == manager  - engine.labels.operatingsystem == ubuntu 14.04preferences:  - spread: node.labels.zone</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* REPLICAS</span><br><span class="line"></span><br><span class="line">如果service 是replicated模式(默认模式),定义容器的启动数量.在下面的例子中启动6个worker容器</span><br></pre></td></tr></table></figure><p>version: “3.7”<br>services:<br>  worker:<br>    image: dockersamples/examplevotingapp_worker<br>    networks:</p><pre><code>  - frontend  - backenddeploy:  mode: replicated  replicas: 6</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* RESOURCES</span><br><span class="line"></span><br><span class="line">配置容器限定的使用资源</span><br><span class="line"></span><br><span class="line">在下面这个例子中.redis service被限制只允许使用不超过50M内存,以及0.5的CPU处理器时间(单个CPU内核的50%).并且有20M内存和0.25的CPU处理器时间预留(也就是永远为redis service保留)</span><br></pre></td></tr></table></figure><p>version: “3.7”<br>services:<br>  redis:<br>    image: redis:alpine<br>    deploy:<br>      resources:<br>        limits:<br>          cpus: ‘0.50’<br>          memory: 50M<br>        reservations:<br>          cpus: ‘0.25’<br>          memory: 20M<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">**Out Of Memory Exceptions[OOME]**</span><br><span class="line"></span><br><span class="line">如果service 或者容器使用了超过限制的资源.容器或者docker引擎就会出现OOME错误.docker进程可能会被内核OOM killer给kill掉.关于如何规避这种问题,请参考[understand the risks of running out of memory](&lt;https://docs.docker.com/config/containers/resource_constraints/&gt;)</span><br><span class="line"></span><br><span class="line">* RESTART_POLICY</span><br><span class="line"></span><br><span class="line">配置当容器停止时如何重新启动容器的策略.有以下几种子指令</span><br><span class="line"></span><br><span class="line">1.```condition``` 重启容器的约束条件.有: ```none```,```on-failure```,和```any```(default:any)</span><br><span class="line"></span><br><span class="line">2.```delay```: 尝试重启容器的时间间隔.默认是0</span><br><span class="line"></span><br><span class="line">3.```max_attempts```:如果容器重启失败,重启最大尝试次数,默认是一直尝试</span><br><span class="line"></span><br><span class="line">4.```window```:重启后等待多久认定重启成功.默认是immediately</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  redis:<br>    image: redis:alpine<br>    deploy:<br>      restart_policy:<br>        condition: on-failure<br>        delay: 5s<br>        max_attempts: 3<br>        window: 120s<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### ROLLBACK_CONFIG</span><br><span class="line"></span><br><span class="line">更新失败的回滚指令.</span><br><span class="line"></span><br><span class="line">#### UPDATE_CONFIG</span><br><span class="line"></span><br><span class="line">配置service如何进行滚动更新.</span><br><span class="line"></span><br><span class="line">#### DNS</span><br><span class="line"></span><br><span class="line">自定义DNS地址,可以是单个值或者一个列表</span><br></pre></td></tr></table></figure></p><p>dns: 8.8.8.8</p><p>dns:</p><ul><li>8.8.8.8</li><li>9.9.9.9<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### entrypoint</span><br><span class="line"></span><br><span class="line">重写dockerfile或者镜像中的entrypoint指令</span><br></pre></td></tr></table></figure></li></ul><p>entrypoint: /code/entrypoint.sh</p><p>#也可以是一个列表格式:</p><p>entrypoint:</p><pre><code>- php- -d- zend_extension=/usr/local/lib/php/extensions/no-debug-non-zts-20100525/xdebug.so- -d- memory_limit=-1- vendor/bin/phpunit</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### env_file</span><br><span class="line"></span><br><span class="line">添加环境变量文件,可以是单个值,也可以是个列表.该文件最好是在当前docker-compose文件目录或者子目录下.</span><br><span class="line"></span><br><span class="line">如果是指定多个变量文件,而且有重复的变量且赋值不同,那么以最后一个变量文件的变量为准</span><br></pre></td></tr></table></figure><p>services:<br>  some-service:<br>    env_file:</p><pre><code>- a.env- b.env</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### environment</span><br><span class="line"></span><br><span class="line">添加环境变量.可以使用列表格式,或者字典格式</span><br></pre></td></tr></table></figure><p>environment:<br>  RACK_ENV: development<br>  SHOW: ‘true’<br>  SESSION_SECRET:</p><p>environment:</p><ul><li>RACK_ENV=development</li><li>SHOW=true</li><li>SESSION_SECRET<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### extra_hosts</span><br><span class="line"></span><br><span class="line">添加hostname和IP地址的绑定映射到hosts文件</span><br></pre></td></tr></table></figure></li></ul><p>extra_hosts:</p><ul><li>“somehost:162.242.195.82”</li><li><p>“otherhost:50.31.209.229”</p><p>#会在容器内的/etc/hosts文件生成如何内容<br>162.242.195.82  somehost<br>50.31.209.229   otherhost</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### healthcheck</span><br><span class="line"></span><br><span class="line">检查service的各个容器是否处于&quot;healthy&quot;状态.例如下面的例子</span><br></pre></td></tr></table></figure></li></ul><p>healthcheck:<br>  test: [“CMD”, “curl”, “-f”, “<a href="http://localhost&quot;]" target="_blank" rel="noopener">http://localhost&quot;]</a><br>  interval: 1m30s<br>  timeout: 10s<br>  retries: 3<br>  start_period: 40s<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```test```指令必须为一个字符串或者一个列表.如果是个上例子中的列表格式.则第一个参数必须为```NONE```,```CMD```,或者```CMD-SHELL```.如果是字符串相当于指定了```CMD-SHELL```参数</span><br><span class="line"></span><br><span class="line">下面2个写法和上文的例子效果一样</span><br></pre></td></tr></table></figure></p><p>test: [“CMD-SHELL”, “curl -f <a href="http://localhost" target="_blank" rel="noopener">http://localhost</a> || exit 1”]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>test: curl -f <a href="https://localhost" target="_blank" rel="noopener">https://localhost</a> || exit 1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">要关闭健康检查,可以使用```disable:true```.等同于```test:[&quot;NONE&quot;]</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">healthcheck:</span><br><span class="line">  disable: true</span><br></pre></td></tr></table></figure><h4 id="image"><a href="#image" class="headerlink" title="image"></a>image</h4><p>指定容器的启动镜像.可以是指定的repository/tag 或者一个镜像ID.如果本地不存在该镜像,会尝试去pull镜像到本地.如果指定了<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">下面这几种写法均正确</span><br></pre></td></tr></table></figure></p><p>image: redis<br>image: ubuntu:14.04<br>image: tutum/influxdb<br>image: example-registry.com:4000/postgresql<br>image: a4bc65fd<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### logging</span><br><span class="line"></span><br><span class="line">service的log配置.下面的例子中指定了一个syslog服务器的地址</span><br></pre></td></tr></table></figure></p><p>logging:<br>  driver: syslog<br>  options:<br>    syslog-address: “tcp://192.168.0.42:123”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```driver```为容器指定logging的驱动,一共以下3种驱动方式,默认是json-file</span><br></pre></td></tr></table></figure></p><p>driver: “json-file”<br>driver: “syslog”<br>driver: “none”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">也可以限定json-file驱动的日志转出.例如下列指定了最大的日志文件大小和日志保留份数</span><br></pre></td></tr></table></figure></p><p>options:<br>  max-size: “200k”<br>  max-file: “10”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### network_mode</span><br><span class="line"></span><br><span class="line">指定网络模式,有以下几种网络模式</span><br></pre></td></tr></table></figure></p><p>network_mode: “bridge”<br>network_mode: “host”<br>network_mode: “none”<br>network_mode: “service:[service name]”<br>network_mode: “container:[container name/id]”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### networks</span><br><span class="line"></span><br><span class="line">services加入的网络.这些网络名在顶级```network```指令中有指定</span><br></pre></td></tr></table></figure></p><p>services:<br>  some-service:<br>    networks:</p><pre><code>- some-network- other-network</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### IPV4_ADDRESS,IPV6_ADDRESS</span><br><span class="line"></span><br><span class="line">为容器指定一个静态的IP地址.但是对应的Network顶级指令中必须指定一个ipam块,定义该网络的IP子网范围.例如</span><br><span class="line"></span><br><span class="line">app_net网络的ipam快中指定了172.16.238.0/24的子网.然后为app service指定一个静态IP</span><br></pre></td></tr></table></figure><p>version: “3.7”</p><p>services:<br>  app:<br>    image: nginx:alpine<br>    networks:<br>      app_net:<br>        ipv4_address: 172.16.238.10<br>        ipv6_address: 2001:3984:3989::10</p><p>networks:<br>  app_net:<br>    ipam:<br>      driver: default<br>      config:</p><pre><code>- subnet: &quot;172.16.238.0/24&quot;- subnet: &quot;2001:3984:3989::/64&quot;</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### ports</span><br><span class="line"></span><br><span class="line">暴露端口.下面是几种短格式写法.推荐将端口用双引号括起来</span><br></pre></td></tr></table></figure><p>ports:</p><ul><li>“3000”</li><li>“3000-3005”</li><li>“8000:8000”</li><li>“9090-9091:8080-8081”</li><li>“49100:22”</li><li>“127.0.0.1:8001:8001”</li><li>“127.0.0.1:5000-5010:5000-5010”</li><li>“6060:6060/udp”<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 此外还有完整格式的写法.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### restart</span><br><span class="line"></span><br><span class="line">默认的restart策略是```no```有以下四种重启策略</span><br></pre></td></tr></table></figure></li></ul><p>restart: “no”<br>restart: always<br>restart: on-failure<br>restart: unless-stopped<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### sysctls</span><br><span class="line"></span><br><span class="line">配置容器的内核参数.可以是数组或者字典类型</span><br></pre></td></tr></table></figure></p><p>sysctls:<br>  net.core.somaxconn: 1024<br>  net.ipv4.tcp_syncookies: 0</p><p>sysctls:</p><ul><li>net.core.somaxconn=1024</li><li>net.ipv4.tcp_syncookies=0<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### ulimits</span><br><span class="line"></span><br><span class="line">配置ulimits</span><br></pre></td></tr></table></figure></li></ul><p>ulimits:<br>  nproc: 65535<br>  nofile:<br>    soft: 20000<br>    hard: 40000<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### volumes</span><br><span class="line"></span><br><span class="line">挂载一个路径或者一个卷.</span><br><span class="line"></span><br><span class="line">如果是在service层级挂载宿主机上的路径到容器,那么不需要在顶级指令中定义```volumes```key.但是如果是挂载一个卷到多个service,可以在顶级指令中定义个卷名.然后使用这个卷名去挂载</span><br><span class="line"></span><br><span class="line">下面这个例子在顶级```volumes```指令中定义了2个卷名:mydata和dbdata. mydata被web service挂载.dbdata被db service挂载.下面2个挂载格式都可以.</span><br></pre></td></tr></table></figure></p><p>version: “3.7”<br>services:<br>  web:<br>    image: nginx:alpine<br>    volumes:</p><pre><code>- type: volume  source: mydata  target: /data  volume:    nocopy: true- type: bind  source: ./static  target: /opt/app/static</code></pre><p>  db:<br>    image: postgres:latest<br>    volumes:</p><pre><code>- &quot;/var/run/postgres/postgres.sock:/var/run/postgres/postgres.sock&quot;- &quot;dbdata:/var/lib/postgresql/data&quot;</code></pre><p>volumes:<br>  mydata:<br>  dbdata:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">简短格式写法</span><br></pre></td></tr></table></figure></p><p>volumes:</p><h1 id="Just-specify-a-path-and-let-the-Engine-create-a-volume"><a href="#Just-specify-a-path-and-let-the-Engine-create-a-volume" class="headerlink" title="Just specify a path and let the Engine create a volume"></a>Just specify a path and let the Engine create a volume</h1><ul><li><p>/var/lib/mysql</p><h1 id="Specify-an-absolute-path-mapping"><a href="#Specify-an-absolute-path-mapping" class="headerlink" title="Specify an absolute path mapping"></a>Specify an absolute path mapping</h1></li><li><p>/opt/data:/var/lib/mysql</p><h1 id="Path-on-the-host-relative-to-the-Compose-file"><a href="#Path-on-the-host-relative-to-the-Compose-file" class="headerlink" title="Path on the host, relative to the Compose file"></a>Path on the host, relative to the Compose file</h1></li><li><p>./cache:/tmp/cache</p><h1 id="User-relative-path"><a href="#User-relative-path" class="headerlink" title="User-relative path"></a>User-relative path</h1></li><li><p>~/configs:/etc/configs/:ro</p><h1 id="Named-volume"><a href="#Named-volume" class="headerlink" title="Named volume"></a>Named volume</h1></li><li>datavolume:/var/lib/mysql<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">完整格式写法</span><br></pre></td></tr></table></figure></li></ul><p>version: “3.7”<br>services:<br>  web:<br>    image: nginx:alpine<br>    ports:</p><pre><code>  - &quot;80:80&quot;volumes:  - type: volume    source: mydata    target: /data    volume:      nocopy: true  - type: bind    source: ./static    target: /opt/app/static</code></pre><p>networks:<br>  webnet:</p><p>volumes:<br>  mydata:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## Volume块级别配置文件指令</span><br><span class="line"></span><br><span class="line">大部分volume的用法在上面都已经解释过了,Volume的顶级指令配置不多.</span><br><span class="line"></span><br><span class="line">#### driver</span><br><span class="line"></span><br><span class="line">指定volume卷的驱动,docker引擎默认指定的驱动是```local```.</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## Network块级别配置文件指令</span><br><span class="line"></span><br><span class="line">详细的docker network特性以及所有network驱动请见:[Network guide](&lt;https://docs.docker.com/compose/networking/&gt;)</span><br><span class="line"></span><br><span class="line">默认情况下Compose启动单一网络,一个services的每个容器加入到默认的网络,并且该网络下的所有容器之间都能互相访问</span><br><span class="line"></span><br><span class="line">假如下面的compose文件在```myapp```目录下.</span><br></pre></td></tr></table></figure></p><p>version: “3”<br>services:<br>  web:<br>    build: .<br>    ports:</p><pre><code>- &quot;8000:8000&quot;</code></pre><p>  db:<br>    image: postgres<br>    ports:</p><pre><code>- &quot;8001:5432&quot;</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">当执行```docker-compose up```命令启动时,会执行以下几个步骤</span><br><span class="line"></span><br><span class="line">1.创建一个```myapp_default```的网络</span><br><span class="line"></span><br><span class="line">2.使用web的配置文件启动一个容器,加入到```myapp_default```网络中</span><br><span class="line"></span><br><span class="line">3.使用db的配置文件启动一个容器.加入到```myapp_default```的网络中</span><br><span class="line"></span><br><span class="line">所有容器成功启动后,每个容器都能访问对方的```hostname```和对方的IP地址.</span><br><span class="line"></span><br><span class="line">另外,需要理解```HOST_PORT```和```COMTAINER_PORT```的区别.在上面这个例子中,db的```host_port```是8001,容器的端口是5432,services之间的容器都是通过```CONTAINER_PORT```也就是容器的IP进行通信的.例如访问数据库地址应该是:```postgres://db:5432</span><br></pre></td></tr></table></figure><h4 id="driver"><a href="#driver" class="headerlink" title="driver"></a>driver</h4><p>指定网络驱动.在单主机下Docker引擎默认使用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>driver: overlay<br><code>`</code></p><hr><h2 id="configs和secrets块级别配置文件指令"><a href="#configs和secrets块级别配置文件指令" class="headerlink" title="configs和secrets块级别配置文件指令"></a>configs和secrets块级别配置文件指令</h2><p>还不是很懂这个怎么用的,以后有空再研究</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker学习笔记——docker-compose&quot;&gt;&lt;a href=&quot;#docker学习笔记——docker-compose&quot; class=&quot;headerlink&quot; title=&quot;docker学习笔记——docker-compose&quot;&gt;&lt;/a&gt;docker学习笔记——docker-compose&lt;/h2&gt;&lt;p&gt;docker compose 定义并且运行多个docker容器.使用YAML风格文件定义一个compose文件.利用compose文件创建和启动所有服务.&lt;/p&gt;
&lt;p&gt;使用docker compose基本只需要3个步骤&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在Dockerfile文件定义app环境&lt;/li&gt;
&lt;li&gt;在docker-compose.yml文件中定义组成app的各个服务&lt;/li&gt;
&lt;li&gt;run docker-compose up 和compose 启动和运行app&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面文档均可以在docker-compose官方找到详细资料:&lt;a href=&quot;https://docs.docker.com/compose/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;docker-compose&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;docker-compose安装&quot;&gt;&lt;a href=&quot;#docker-compose安装&quot; class=&quot;headerlink&quot; title=&quot;docker-compose安装&quot;&gt;&lt;/a&gt;docker-compose安装&lt;/h3&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---理解swarm集群</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0-5.%E5%AD%A6%E4%B9%A0swarm%E9%9B%86%E7%BE%A4/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习-5.学习swarm集群/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:11:55.850Z</updated>
    
    <content type="html"><![CDATA[<h3 id="理解swarm集群"><a href="#理解swarm集群" class="headerlink" title="理解swarm集群"></a>理解swarm集群</h3><p>一个swarm是一组运行docker服务器的的集群,docker服务器可以是物理机也可以是虚拟机.</p><p>swarm manager可以使用多种策略来运行容器.比如”emptiest node”—部署容器到压力最小的服务器上,或者”global”—确保每台服务器都只允许一个容器实例.你可以在Compose文件中指示swarm manager去选择何种策略</p><p>swarm managers是swarm进群中唯一可以执行命令,或者授权其他服务器以”workers”身份加入swarm集群的服务器.</p><hr><h4 id="初始化swarm-加入节点"><a href="#初始化swarm-加入节点" class="headerlink" title="初始化swarm,加入节点"></a>初始化swarm,加入节点</h4><p>试验环境:</p><p>1.10.0.0.50 —swarm manager<br>2.10.0.0.12 —worker 节点</p><ul><li>初始化swarm,并且指定通告的IP</li></ul><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker swarm init --advertise-addr 10.0.0.50</span><br><span class="line">Swarm initialized: current node (zlvl9l94blu3rfcaaptdvo9u1) is now a manager.</span><br><span class="line"></span><br><span class="line">To add a worker to this swarm, run the following command:</span><br><span class="line"></span><br><span class="line">    docker swarm join --token SWMTKN-1-2i5fyjf2niw81tudcvpw33yuni277vz45lt6tyi5bvcnhvuwea-bj091dpe6e69ph9kt3lmsthgp 10.0.0.50:2377</span><br><span class="line"></span><br><span class="line">To add a manager to this swarm, run &apos;docker swarm join-token manager&apos; and follow the instructions.</span><br></pre></td></tr></table></figure><p>根据上面提示,在第二台服务器上以worker身份加入swarm集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@php ~]$docker swarm join --token SWMTKN-1-2i5fyjf2niw81tudcvpw33yuni277vz45lt6tyi5bvcnhvuwea-bj091dpe6e69ph9kt3lmsthgp 10.0.0.50:2377</span><br><span class="line">This node joined a swarm as a worker.</span><br><span class="line">[root@php ~]$</span><br></pre></td></tr></table></figure><p>执行docker node ls命令可以管理和查看swarm集群的所有节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker node ls</span><br><span class="line">ID                            HOSTNAME                STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class="line">zlvl9l94blu3rfcaaptdvo9u1 *   localhost.localdomain   Ready               Active              Leader              18.09.2</span><br><span class="line">ud5ztqzvvfwg3d3hwmts5y9ct     php                     Ready               Active                                  18.09.3</span><br><span class="line">[root@localhost ~]$</span><br></pre></td></tr></table></figure><p>执行docker swarm leave命令将某个节点退出swarm集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@php ~]$docker swarm leave</span><br><span class="line">Node left the swarm.</span><br><span class="line">[root@php ~]$</span><br><span class="line"></span><br><span class="line">此时这个节点在swarm集群中状态为down</span><br><span class="line"></span><br><span class="line">[root@localhost ~]$docker node ls</span><br><span class="line">ID                            HOSTNAME                STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class="line">zlvl9l94blu3rfcaaptdvo9u1 *   localhost.localdomain   Ready               Active              Leader              18.09.2</span><br><span class="line">ud5ztqzvvfwg3d3hwmts5y9ct     php                     Down                Active                                  18.09.3</span><br><span class="line">[root@localhost ~]$</span><br></pre></td></tr></table></figure><hr><h4 id="在swarm集群部署app"><a href="#在swarm集群部署app" class="headerlink" title="在swarm集群部署app"></a>在swarm集群部署app</h4><p>现在可以把上一小节的docker compose部署在swarm集群上了.执行命令和上一小节一样.但是需要注意的是只能在swarm manager节点服务器上执行命令.</p><p>在第一台服务器上执行如下命令:(确保docker compose文件和镜像文件在这台服务器上)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@localhost ~]$cd /data/compose</span><br><span class="line">[root@localhost compose]$ls</span><br><span class="line">docker-compose.yml</span><br><span class="line">[root@localhost compose]$docker stack deploy -c docker-compose.yml getstartedlab</span><br><span class="line">Creating network getstartedlab_webnet</span><br><span class="line">Creating service getstartedlab_web</span><br><span class="line">[root@localhost compose]$</span><br></pre></td></tr></table></figure><p>APP已经成功部署到swarm集群上,现在可以使用上一小节中的同样的命令来管理app集群,只不过这次services和容器已经部署到两台服务器上:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack ps getstartedlab</span><br><span class="line">ID                  NAME                      IMAGE                  NODE                    DESIRED STATE       CURRENT STATE              ERROR                              PORTS</span><br><span class="line">4borwslue8k5        getstartedlab_web.1       friendlyhello:latest   localhost.localdomain   Running             Preparing 6 seconds ago</span><br><span class="line">ixmkldh16otx        getstartedlab_web.2       friendlyhello:latest   php                     Ready               Preparing 22 seconds ago</span><br><span class="line">i2yhimkst4iq         \_ getstartedlab_web.2   friendlyhello:latest   php                     Shutdown            Rejected 23 seconds ago    &quot;No such image: friendlyhello:…&quot;</span><br><span class="line">gytqpcwnzvrm        getstartedlab_web.3       friendlyhello:latest   localhost.localdomain   Running             Preparing 6 seconds ago</span><br><span class="line">j94tblo4qjwa        getstartedlab_web.4       friendlyhello:latest   php                     Ready               Preparing 22 seconds ago</span><br><span class="line">b7r9xkf4glh6         \_ getstartedlab_web.4   friendlyhello:latest   php                     Shutdown            Rejected 22 seconds ago    &quot;No such image: friendlyhello:…&quot;</span><br><span class="line">i8sv4c293ata        getstartedlab_web.5       friendlyhello:latest   localhost.localdomain   Running             Preparing 6 seconds ago</span><br></pre></td></tr></table></figure><blockquote><p>需要在另外一台服务器上pull同样的镜像,否则容器无法启动</p></blockquote><ul><li><p>在第二台服务器上下载我阿里云私有仓库的镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">docker login --username=jessehuang408 registry.cn-hangzhou.aliyuncs.com</span><br><span class="line">Password:</span><br><span class="line"></span><br><span class="line">[root@php ~]$docker pull registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:frendlyhello-v1.0</span><br><span class="line">frendlyhello-v1.0: Pulling from jesse_images/jesse_images</span><br><span class="line">f7e2b70d04ae: Pull complete</span><br><span class="line">1e9214730e83: Pull complete</span><br><span class="line">5bd4ec081f7b: Pull complete</span><br><span class="line">be26b369a1e7: Pull complete</span><br><span class="line">236be9d80905: Pull complete</span><br><span class="line">1bf8a3675b0b: Pull complete</span><br><span class="line">5752f9477f0c: Pull complete</span><br><span class="line">Digest: sha256:8e8b57ef6e22c8c04c1c80cfab9f336928cffabacaa4ae4e74ec57e54bcffdb2</span><br><span class="line">Status: Downloaded newer image for registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:frendlyhello-v1.0</span><br><span class="line"></span><br><span class="line">[root@php ~]$docker images</span><br><span class="line">REPOSITORY                                                    TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images   frendlyhello-v1.0   f091d1bb803c        2 days ago          131MB</span><br><span class="line">[root@php ~]$*</span><br></pre></td></tr></table></figure></li><li><p>将镜像修改成和第一台服务器一样:frendlyhello:latest</p></li></ul><p>命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker tag 镜像ID REPOSITORY:TAG</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@php ~]$docker tag f091d1bb803c frendlyhello:latest</span><br><span class="line">[root@php ~]$docker images</span><br><span class="line">REPOSITORY                                                    TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">frendlyhello                                                  latest              f091d1bb803c        2 days ago          131MB</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images   frendlyhello-v1.0   f091d1bb803c        2 days ago          131MB</span><br><span class="line">[root@php ~]$</span><br></pre></td></tr></table></figure><ul><li>将第一台服务器的docker-compose文件拷贝到同样的目录下</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@php ~]$mkdir /data/compose</span><br><span class="line">[root@php ~]$scp root@10.0.0.50:/data/compose/docker-compose.yml /data/compose/</span><br></pre></td></tr></table></figure><ul><li>回到第一台服务器上删除刚才创建的getstartedlab</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack rm getstartedlab</span><br><span class="line">Removing service getstartedlab_web</span><br><span class="line">Removing network getstartedlab_webnet</span><br><span class="line"></span><br><span class="line">[root@localhost compose]$docker stack ps getstartedlab</span><br><span class="line">nothing found in stack: getstartedlab</span><br></pre></td></tr></table></figure><ul><li>重新部署docker compose</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack deploy -c docker-compose.yml getstartedlab</span><br><span class="line">Creating network getstartedlab_webnet</span><br><span class="line">Creating service getstartedlab_web</span><br><span class="line">[root@localhost compose]$</span><br></pre></td></tr></table></figure><ul><li>成功部署</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack ps getstartedlab</span><br><span class="line">ID                  NAME                  IMAGE                  NODE                    DESIRED STATE       CURRENT STATE              ERROR               PORTS</span><br><span class="line">uqsj8mim0sac        getstartedlab_web.1   friendlyhello:latest   localhost.localdomain   Running             Preparing 3 seconds ago</span><br><span class="line">shjiwlnj12sp        getstartedlab_web.2   friendlyhello:latest   php                     Running             Preparing 24 seconds ago</span><br><span class="line">8sqllvgid8jp        getstartedlab_web.3   friendlyhello:latest   php                     Running             Preparing 24 seconds ago</span><br><span class="line">v7fsecgcg504        getstartedlab_web.4   friendlyhello:latest   php                     Running             Preparing 24 seconds ago</span><br><span class="line">np8utmyvk5px        getstartedlab_web.5   friendlyhello:latest   localhost.localdomain   Running             Preparing 3 seconds ago</span><br><span class="line">[root@localhost compose]$</span><br></pre></td></tr></table></figure><p>在第二台的worker节点上执行命令会提示失败:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@php compose]$docker stack ps getstartedlab</span><br><span class="line">Error response from daemon: This node is not a swarm manager. Worker nodes can&apos;t be used to view or modify cluster state. Please run this command on a manager node or promote the current node to a manager.</span><br><span class="line">[root@php compose]$</span><br></pre></td></tr></table></figure><p>现在,在两台服务器上都能访问刚才部署的app</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.12:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 926f433b3896&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; &lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;% </span><br><span class="line"></span><br><span class="line">huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 1535f17586ea&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; &lt;i&gt;cannot connect to Redis, counter disabled&lt;/i&gt;%                                                            huangyong@huangyong-Macbook-Pro  ~ </span><br></pre></td></tr></table></figure><h4 id="扩展app"><a href="#扩展app" class="headerlink" title="扩展app"></a>扩展app</h4><p>扩展app还是直接编辑docker-compose.yml文件.然后重新docker stack deploy部署即可.</p><p>如果是需要将其他虚拟机或者物理服务器加入进swarm集群,就像第二台服务器一样使用docker swarm join命令加入即可,</p><h4 id="停止swarm"><a href="#停止swarm" class="headerlink" title="停止swarm"></a>停止swarm</h4><p>命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker stack rm getstartedlab</span><br></pre></td></tr></table></figure><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 初始化一个swarm集群</span><br><span class="line"></span><br><span class="line">docker swarm init --advertise-addr IP</span><br><span class="line"></span><br><span class="line">#加入到swarm集群</span><br><span class="line">docker swarm join --token &lt;token&gt; &lt;swarm manager IP&gt;:&lt;port&gt;</span><br><span class="line"></span><br><span class="line">#部署app</span><br><span class="line">docker stack deploy -c docker-compose.yml &lt;services name&gt;</span><br><span class="line">&gt; note:在所有docker服务器节点上都需要有docker-compose.yml文件和相关镜像</span><br><span class="line"></span><br><span class="line"># 查看services </span><br><span class="line">docker stack ps &lt;services name&gt;</span><br><span class="line">docker services ls</span><br><span class="line">docker stack ls</span><br><span class="line"></span><br><span class="line"># 从swarm集群中删除 services</span><br><span class="line"></span><br><span class="line">docker stack rm &lt;service name&gt;</span><br><span class="line"></span><br><span class="line"># 删除swarm集群节点</span><br><span class="line"></span><br><span class="line">docker swarm leave #worker节点</span><br><span class="line">docker swarm leave --force #manager节点</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;理解swarm集群&quot;&gt;&lt;a href=&quot;#理解swarm集群&quot; class=&quot;headerlink&quot; title=&quot;理解swarm集群&quot;&gt;&lt;/a&gt;理解swarm集群&lt;/h3&gt;&lt;p&gt;一个swarm是一组运行docker服务器的的集群,docker服务器可以是物理机也可以是虚拟机.&lt;/p&gt;
&lt;p&gt;swarm manager可以使用多种策略来运行容器.比如”emptiest node”—部署容器到压力最小的服务器上,或者”global”—确保每台服务器都只允许一个容器实例.你可以在Compose文件中指示swarm manager去选择何种策略&lt;/p&gt;
&lt;p&gt;swarm managers是swarm进群中唯一可以执行命令,或者授权其他服务器以”workers”身份加入swarm集群的服务器.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&quot;初始化swarm-加入节点&quot;&gt;&lt;a href=&quot;#初始化swarm-加入节点&quot; class=&quot;headerlink&quot; title=&quot;初始化swarm,加入节点&quot;&gt;&lt;/a&gt;初始化swarm,加入节点&lt;/h4&gt;&lt;p&gt;试验环境:&lt;/p&gt;
&lt;p&gt;1.10.0.0.50 —swarm manager&lt;br&gt;2.10.0.0.12 —worker 节点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始化swarm,并且指定通告的IP&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker网络之overlay</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0--docker%E7%BD%91%E7%BB%9C%E4%B9%8Boverlay/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习--docker网络之overlay/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T14:18:08.089Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker官网学习-7-docker网络之overlay"><a href="#docker官网学习-7-docker网络之overlay" class="headerlink" title="docker官网学习-7.docker网络之overlay"></a>docker官网学习-7.docker网络之overlay</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>overlay网卡在多个docker宿主机之间创建一个分布式的网络,允许多个容器安全通信.</p><p>当初始化一个swarm集群,或者加入docker宿主机到一个swarm集群中.Docker会在该宿主机上创建2个网络:</p><ul><li>ingress: 负责swarm集群的控制以及数据流量</li><li>docker_gwbridge:一个Bridge网络,负责连接swarm集群中的每个docker节点</li></ul><p>overlay网络的创建方式和bridge一样.也是docker network create命令</p><hr><h3 id="创建overlay网络"><a href="#创建overlay网络" class="headerlink" title="创建overlay网络"></a>创建overlay网络</h3><a id="more"></a><p><strong>创建overlay网络的前提条件</strong></p><p>1.防火墙开通以下端口</p><ul><li>TCP 2377——集群管理节点通信</li><li>TCP,UPD 7946—节点间通信</li><li>UDP 4789—overlay网络流量</li></ul><p>2.初始化docker宿主机为swarm集群的manager角色.命令为<figure class="highlight docker"><figcaption><span>swarm init```.或者使用```docker swarm join```命令加入到一个现有的swarm集群.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">这两种方式都会创建默认的ingress overlay网络.</span><br><span class="line"></span><br><span class="line">&gt; 即使你不打算使用swarm服务,但是也要这样做.然后才能创建自定义的overlay网络</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**创建overlay网络命令**</span><br></pre></td></tr></table></figure></p><p>#创建个overlay网络.名字为my-overlay<br>docker network create -d overlay my-overlay<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">如果swarm服务或者独立容器需要和其他docker宿主机上的独立容器通信.需要加上```—attachable```参数</span><br></pre></td></tr></table></figure></p><p>#创建个overlay网络,名字为my-overlay.并且和其他docker宿主机的standalone容器通信.<br>docker network create -d overlay –attachable my-overlay<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 可以自定义地址段,掩码,网关等信息.具体方法见docker network create --help</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 自定义默认Ingress网络</span><br><span class="line"></span><br><span class="line">大部分用户不需要配置ingress网络.但是docker17.05以上版本可以自定义ingress网络.如果默认的Ingress网络iP地址段和已经存在的网络有冲突,则自定义的配置可很有用.</span><br><span class="line"></span><br><span class="line">配置Ingress网络需要先删除ingress,然后重新创建.所以最好是在创建容器服务之前先定义好ingress.如果已经有暴露出端口的服务.则需要先删除服务.</span><br><span class="line"></span><br><span class="line">**自定义默认ingress网络步骤如下:**</span><br><span class="line"></span><br><span class="line">1.查看当前ingress网络.```docker network inspect ingress```.删除所有连接到ingress的容器的服务.</span><br><span class="line"></span><br><span class="line">2.移除现有的ingress网络.```docker network rm ingress</span><br></pre></td></tr></table></figure></p><p>3.使用<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p> docker network create \<br>  –driver overlay \<br>  –ingress \<br>  –subnet=10.11.0.0/16 \<br>  –gateway=10.11.0.2 \<br>  –opt com.docker.network.driver.mtu=1200 \<br>  my-ingress<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 可以用自定义名称(my-ingress)来定义ingress网络.但是只允许存在一个自定义Ingress.</span><br><span class="line"></span><br><span class="line">4.重启步骤1中的服务</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 自定义默认docker_gwbridge 接口</span><br><span class="line"></span><br><span class="line">docker_gwbridge是连接overlay网络和docker宿主机物理网卡之间的虚拟网桥.当初始化一个swarm集群,或者将docker宿主机加入到一个swarm集群时,docker会自动创建docker_gwbridge.但是docker_gwbridge不是一个docker服务,而是存在于docker宿主机的内核当中.</span><br><span class="line"></span><br><span class="line">所以需要在加入到swarm集群前先配置好docker_gwbridge.</span><br><span class="line"></span><br><span class="line">**自定义默认docker_gwbridge网络步骤如下:**</span><br><span class="line"></span><br><span class="line">1.停止docker</span><br><span class="line"></span><br><span class="line">2.删除当前```docker_gwbridge</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip link set docker_gwbridge down</span><br><span class="line"></span><br><span class="line">$ sudo ip link del dev docker_gwbridge</span><br></pre></td></tr></table></figure><p>3.开启docker.不要加入或者初始化swarm</p><p>4.手动创建<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 关于docker_gwbridge的更多参数请参考[Bridg dirver options](&lt;https://docs.docker.com/engine/reference/commandline/network_create/#bridge-driver-options&gt;)</span><br></pre></td></tr></table></figure></p><p>$ docker network create \<br>–subnet 10.11.0.0/16 \<br>–opt com.docker.network.bridge.name=docker_gwbridge \<br>–opt com.docker.network.bridge.enable_icc=false \<br>–opt com.docker.network.bridge.enable_ip_masquerade=true \<br>docker_gwbridge<br><code>`</code></p><p>5.初始化或者加入到swarm集群</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker官网学习-7-docker网络之overlay&quot;&gt;&lt;a href=&quot;#docker官网学习-7-docker网络之overlay&quot; class=&quot;headerlink&quot; title=&quot;docker官网学习-7.docker网络之overlay&quot;&gt;&lt;/a&gt;docker官网学习-7.docker网络之overlay&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;overlay网卡在多个docker宿主机之间创建一个分布式的网络,允许多个容器安全通信.&lt;/p&gt;
&lt;p&gt;当初始化一个swarm集群,或者加入docker宿主机到一个swarm集群中.Docker会在该宿主机上创建2个网络:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ingress: 负责swarm集群的控制以及数据流量&lt;/li&gt;
&lt;li&gt;docker_gwbridge:一个Bridge网络,负责连接swarm集群中的每个docker节点&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;overlay网络的创建方式和bridge一样.也是docker network create命令&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;创建overlay网络&quot;&gt;&lt;a href=&quot;#创建overlay网络&quot; class=&quot;headerlink&quot; title=&quot;创建overlay网络&quot;&gt;&lt;/a&gt;创建overlay网络&lt;/h3&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---学习Docker Stack</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0-6.%E5%AD%A6%E4%B9%A0stack/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习-6.学习stack/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:12:58.803Z</updated>
    
    <content type="html"><![CDATA[<h2 id="学习Docker-Stack"><a href="#学习Docker-Stack" class="headerlink" title="学习Docker Stack"></a>学习Docker Stack</h2><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>一个stack是一组共享依赖包的多个相关的services,并且可以编排和扩展.其实从第4小节开始,在利用compose文件部署app时,就已经开始一直使用stack.但是还只是运行在一个单一服务器的单一service.<br>现在,你可以学习在多个服务器上,运行多个相关的services.</p><hr><ul><li>使用下面的docker-compose.yml文件替换第4小节中的docker-compose.yml</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line">services:</span><br><span class="line">  web:</span><br><span class="line">    # replace username/repo:tag with your name and image details</span><br><span class="line">    image: ianch/friendlyhello:v1</span><br><span class="line">    deploy:</span><br><span class="line">      replicas: 5</span><br><span class="line">      resources:</span><br><span class="line">        limits:</span><br><span class="line">          cpus: &quot;0.1&quot;</span><br><span class="line">          memory: 50M</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;4000:80&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - webnet</span><br><span class="line">  Visualizer:</span><br><span class="line">    image: dockersamples/visualizer:stable</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8080:8080&quot;</span><br><span class="line">    volumes:</span><br><span class="line">      - &quot;/var/run/docker.sock:/var/run/docker.sock&quot;</span><br><span class="line">    deploy:</span><br><span class="line">      placement:</span><br><span class="line">        constraints: [node.role == manager]</span><br><span class="line">    networks:</span><br><span class="line">      - webnet</span><br><span class="line">networks:</span><br><span class="line">  webnet:</span><br></pre></td></tr></table></figure><a id="more"></a><p>docker-compose文件稍微做了点改动.添加一个Visualizer服务,placement指令确保这个Visualizer服务仅仅运行在swarm manager节点.</p><hr><h4 id="部署compose文件"><a href="#部署compose文件" class="headerlink" title="部署compose文件"></a>部署compose文件</h4><ul><li>初始化swarm</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker swarm init</span><br></pre></td></tr></table></figure><ul><li>第二台服务器加入swarm集群</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@php compose]$docker swarm join --token SWMTKN-1-5qr6e90o52h5licxatuvmft65kji5qf1roujebf16auoe5xgam-3d0fuzr8818r6330n88dm1fcu 10.0.0.50:2377</span><br><span class="line">This node joined a swarm as a worker.</span><br></pre></td></tr></table></figure><ul><li>部署app</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack deploy -c docker-compose.yml getstartedlab</span><br><span class="line">Creating network getstartedlab_webnet</span><br><span class="line">Creating service getstartedlab_web</span><br><span class="line">Creating service getstartedlab_Visualizer</span><br><span class="line">[root@localhost compose]$</span><br></pre></td></tr></table></figure><blockquote><p>添加了2个服务.web和Visualizer</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack ps getstartedlab</span><br><span class="line">ID                  NAME                         IMAGE                             NODE                    DESIRED STATE       CURRENT STATE                ERROR               PORTS</span><br><span class="line">ds77cn4hgd9w        getstartedlab_web.1          friendlyhello:latest              php                     Running             Running about a minute ago</span><br><span class="line">agzj3veqno63        getstartedlab_Visualizer.1   dockersamples/visualizer:stable   localhost.localdomain   Running             Running about a minute ago</span><br><span class="line">3hq0if79g3gk        getstartedlab_web.2          friendlyhello:latest              php                     Running             Running about a minute ago</span><br><span class="line">iuxh7qjfpikw        getstartedlab_web.3          friendlyhello:latest              localhost.localdomain   Running             Running 53 seconds ago</span><br><span class="line">ba9hcq5zmlbd        getstartedlab_web.4          friendlyhello:latest              php                     Running             Running about a minute ago</span><br><span class="line">o600yqmqets7        getstartedlab_web.5          friendlyhello:latest              localhost.localdomain   Running             Running 55 seconds ago</span><br><span class="line">[root@localhost compose]$</span><br></pre></td></tr></table></figure><p>访问任意一台服务器的8080端口,可以看到Visualizer服务正在运行</p><p><img src="https://docs.docker.com/get-started/images/get-started-visualizer1.png" alt=""></p><blockquote><p>这是我借用的官网的图片.</p></blockquote><p>可以看到,visualizer运行在swarm manager节点上,5个web服务运行在swarm集群上.visualizer是一个不需要任何依赖,而可以运行在任何app的独立服务.现在尝试一下创建一个具有依赖项的服务:提供访问计数器的Redis服务</p><hr><h3 id="编辑docker-compose文件"><a href="#编辑docker-compose文件" class="headerlink" title="编辑docker-compose文件"></a>编辑docker-compose文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line">services:</span><br><span class="line">  web:</span><br><span class="line">    # replace username/repo:tag with your name and image details</span><br><span class="line">    image: ianch/friendlyhello:v1</span><br><span class="line">    deploy:</span><br><span class="line">      replicas: 5</span><br><span class="line">      resources:</span><br><span class="line">        limits:</span><br><span class="line">          cpus: &quot;0.1&quot;</span><br><span class="line">          memory: 50M</span><br><span class="line">      restart_policy:</span><br><span class="line">        condition: on-failure</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;4000:80&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - webnet</span><br><span class="line">  Visualizer:</span><br><span class="line">    image: dockersamples/visualizer:stable</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8080:8080&quot;</span><br><span class="line">    volumes:</span><br><span class="line">      - &quot;/var/run/docker.sock:/var/run/docker.sock&quot;</span><br><span class="line">    deploy:</span><br><span class="line">      placement:</span><br><span class="line">        constraints: [node.role == manager]</span><br><span class="line">    networks:</span><br><span class="line">      - webnet</span><br><span class="line"></span><br><span class="line">  redis:</span><br><span class="line">    image: redis</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;6379:6379&quot;</span><br><span class="line">    volumes:</span><br><span class="line">      - &quot;/home/docker/data:/data&quot;</span><br><span class="line">    deploy:</span><br><span class="line">      placement:</span><br><span class="line">        constraints: [node.role == manager]</span><br><span class="line">    command: redis-server --appendonly yes</span><br><span class="line">    networks:</span><br><span class="line">      - webnet</span><br><span class="line">networks:</span><br><span class="line">  webnet:</span><br></pre></td></tr></table></figure><p> 这里我们添加了一个redis服务.在Docker HUB上有redis官方镜像,并且已经暴露了6379端口.所以这里只需要指定redis镜像即可..同样redis也只运行在manager节点服务器.</p><p> 这里为了持久化数据,在启动redis容器的时候指定了appendonly参数,并且挂载了本机的/home/docker/data目录映射到容器的/data.(redis容器默认保存数据路径)</p><ul><li>在manager节点创建/home/docker/data目录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$mkdir -pv /home/docker/data</span><br><span class="line">mkdir: 已创建目录 &quot;/home/docker&quot;</span><br><span class="line">mkdir: 已创建目录 &quot;/home/docker/data&quot;</span><br></pre></td></tr></table></figure><ul><li>部署compose</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker stack deploy -c docker-compose.yml getstartedlab</span><br><span class="line">Updating service getstartedlab_web (id: mtgafxttekwfh0tkhkaespa1v)</span><br><span class="line">image friendlyhello:latest could not be accessed on a registry to record</span><br><span class="line">its digest. Each node will access friendlyhello:latest independently,</span><br><span class="line">possibly leading to different nodes running different</span><br><span class="line">versions of the image.</span><br><span class="line"></span><br><span class="line">Updating service getstartedlab_Visualizer (id: zmim1kj44afsr9xay8ppxker6)</span><br><span class="line">Creating service getstartedlab_redis</span><br></pre></td></tr></table></figure><p>可以看到3个services都启动起来了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost compose]$docker service ls</span><br><span class="line">ID                  NAME                       MODE                REPLICAS            IMAGE                             PORTS</span><br><span class="line">zmim1kj44afs        getstartedlab_Visualizer   replicated          1/1                 dockersamples/visualizer:stable   *:8080-&gt;8080/tcp</span><br><span class="line">elomaiu5go9p        getstartedlab_redis        replicated          1/1                 redis:latest                      *:6379-&gt;6379/tcp</span><br><span class="line">mtgafxttekwf        getstartedlab_web          replicated          5/5                 friendlyhello:latest              *:4000-&gt;80/tcp</span><br></pre></td></tr></table></figure><p>在浏览器访问服务器的4000端口可以看到有一个访问计数器在增加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; eed56ca4164b&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 5%                                                                                                           huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 777d2cab6468&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 6%                                                                                                           huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 213e6a729c6a&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 7%                                                                                                           huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; 85ccc6b1cb18&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 8%                                                                                                           huangyong@huangyong-Macbook-Pro  ~ </span><br></pre></td></tr></table></figure><p>访问另外一台服务器也可以看到同样结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.12:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; e77e36db18be&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 10%                                                                                                          huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.50:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; eed56ca4164b&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 11%                                                                                                          huangyong@huangyong-Macbook-Pro  ~  curl http://10.0.0.12:4000</span><br><span class="line">&lt;h3&gt;Hello World!&lt;/h3&gt;&lt;b&gt;Hostname:&lt;/b&gt; eed56ca4164b&lt;br/&gt;&lt;b&gt;Visits:&lt;/b&gt; 12%</span><br></pre></td></tr></table></figure><p>访问visulizer容器的8080端口,可以看到redis服务运行</p><p><img src="https://docs.docker.com/get-started/images/visualizer-with-redis.png" alt=""></p><hr><h3 id="管理命令"><a href="#管理命令" class="headerlink" title="管理命令"></a>管理命令</h3><p>使用docker node ls 列出swarm集群的所有节点<br>使用docker service ls 列出所有服务<br>docker service ps &lt;service_name&gt; 列出某个服务的所有tasks</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]$docker node ls</span><br><span class="line">ID                            HOSTNAME                STATUS              AVAILABILITY        MANAGER STATUS      ENGINE VERSION</span><br><span class="line">056r5hr2xb6jjzmbw64m3btd2 *   localhost.localdomain   Ready               Active              Leader              18.09.2</span><br><span class="line">r58bkpsxgi4mjaxrn8octuxw1     php                     Ready               Active                                  18.09.3</span><br><span class="line">[root@localhost ~]$docker service ls</span><br><span class="line">ID                  NAME                       MODE                REPLICAS            IMAGE                             PORTS</span><br><span class="line">zmim1kj44afs        getstartedlab_Visualizer   replicated          1/1                 dockersamples/visualizer:stable   *:8080-&gt;8080/tcp</span><br><span class="line">elomaiu5go9p        getstartedlab_redis        replicated          1/1                 redis:latest                      *:6379-&gt;6379/tcp</span><br><span class="line">mtgafxttekwf        getstartedlab_web          replicated          5/5                 friendlyhello:latest              *:4000-&gt;80/tcp</span><br><span class="line"></span><br><span class="line">[root@localhost ~]$docker service ps getstartedlab_web</span><br><span class="line">ID                  NAME                  IMAGE                  NODE                    DESIRED STATE       CURRENT STATE          ERROR               PORTS</span><br><span class="line">ds77cn4hgd9w        getstartedlab_web.1   friendlyhello:latest   php                     Running             Running 14 hours ago</span><br><span class="line">3hq0if79g3gk        getstartedlab_web.2   friendlyhello:latest   php                     Running             Running 14 hours ago</span><br><span class="line">iuxh7qjfpikw        getstartedlab_web.3   friendlyhello:latest   localhost.localdomain   Running             Running 14 hours ago</span><br><span class="line">ba9hcq5zmlbd        getstartedlab_web.4   friendlyhello:latest   php                     Running             Running 14 hours ago</span><br><span class="line">o600yqmqets7        getstartedlab_web.5   friendlyhello:latest   localhost.localdomain   Running             Running 14 hours ago</span><br><span class="line">[root@localhost ~]$</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;学习Docker-Stack&quot;&gt;&lt;a href=&quot;#学习Docker-Stack&quot; class=&quot;headerlink&quot; title=&quot;学习Docker Stack&quot;&gt;&lt;/a&gt;学习Docker Stack&lt;/h2&gt;&lt;h4 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h4&gt;&lt;p&gt;一个stack是一组共享依赖包的多个相关的services,并且可以编排和扩展.其实从第4小节开始,在利用compose文件部署app时,就已经开始一直使用stack.但是还只是运行在一个单一服务器的单一service.&lt;br&gt;现在,你可以学习在多个服务器上,运行多个相关的services.&lt;/p&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;使用下面的docker-compose.yml文件替换第4小节中的docker-compose.yml&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;version: &amp;quot;3&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;services:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  web:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    # replace username/repo:tag with your name and image details&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    image: ianch/friendlyhello:v1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    deploy:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      replicas: 5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      resources:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        limits:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          cpus: &amp;quot;0.1&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          memory: 50M&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      restart_policy:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        condition: on-failure&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ports:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      - &amp;quot;4000:80&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    networks:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      - webnet&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  Visualizer:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    image: dockersamples/visualizer:stable&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ports:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      - &amp;quot;8080:8080&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    volumes:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      - &amp;quot;/var/run/docker.sock:/var/run/docker.sock&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    deploy:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      placement:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        constraints: [node.role == manager]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    networks:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;      - webnet&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;networks:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  webnet:&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker网络之bridge</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0-7.docker%E7%BD%91%E7%BB%9C%E4%B9%8Bbridge/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习-7.docker网络之bridge/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T14:08:42.118Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker官网学习–docker网络之bridge"><a href="#docker官网学习–docker网络之bridge" class="headerlink" title="docker官网学习–docker网络之bridge"></a>docker官网学习–docker网络之bridge</h2><p>本节介绍docker基础网络概念.以便能认识和利用各种不同的网络类型功能.</p><p>docker的网络支持插件化,驱动化定制.有一些网络驱动已经默认集成到docker中.docker网络主要有以下类型</p><ul><li>bridge</li><li>host</li><li>overlay</li><li>macvlan</li><li>none</li><li>其他网络插件</li></ul><hr><a id="more"></a><h4 id="bridge"><a href="#bridge" class="headerlink" title="bridge"></a>bridge</h4><p>​    bridge是docker默认的网络驱动.如果在<figure class="highlight docker"><figcaption><span>run```启动一个容器时没有指定任何网络驱动.那么默认就是bridge桥接网络.桥接网络通常适用于应用进程部署在多个独立的容器中,并且容器之间需要互相通信的场景中.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">   在docker环境中.bridge使用软件桥接允许容器之间通过同一个bridge互联.隔离没有连到这个bridge网络的其他容器网络.docker网络自动创建iptables规则阻止其他网络的容器访问.</span><br><span class="line"></span><br><span class="line">当Docker进程启动时，会在主机上创建一个名为docker0的虚拟网桥，此主机上启动的Docker容器会连接到这个虚拟网桥上。虚拟网桥的工作方式和物理交换机类似，这样主机上的所有容器就通过交换机连在了一个二层网络中。</span><br><span class="line"></span><br><span class="line">从docker0子网中分配一个IP给容器使用，并设置docker0的IP地址为容器的默认网关。在主机上创建一对虚拟网卡veth pair设备，Docker将veth pair设备的一端放在新创建的容器中，并命名为eth0（容器的网卡），另一端放在主机中，以vethxxx这样类似的名字命名，并将这个网络设备加入到docker0网桥中。可以通过brctl show命令查看。</span><br><span class="line"></span><br><span class="line">在Linux中.可以使用brctl命令查看和管理网桥(需要先安装bridge-utils软件包).例如查看本机上的网桥及其端口</span><br></pre></td></tr></table></figure></p><p>[work@docker conf.d]$sudo brctl show<br>bridge name    bridge id        STP enabled    interfaces<br>br-17ace6d9d81a        8000.024236cdd4d7    no        veth82ed0e5<br>br-d9897c225d25        8000.024237d1c9f6    no        veth6981090<br>                            veth8e29dbf<br>                            vethd511728<br>docker0        8000.0242322e2e42    no        veth0a0e27d<br>                            veth0c50104<br>                            veth3fe7f4d<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">docker 0网桥下关联了很多vethxxxxx规范命名的interfaces.每一个vethxxxx接口对应一个docker容器.在docker容器中一般是eth0的网卡</span><br></pre></td></tr></table></figure></p><p>[work@docker conf.d]$docker exec -it nginx ifconfig<br>eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 172.17.0.8  netmask 255.255.0.0  broadcast 0.0.0.0<br>        inet6 fe80::42:acff:fe11:8  prefixlen 64  scopeid 0x20<link><br>        ether 02:42:ac:11:00:08  txqueuelen 0  (Ethernet)<br>        RX packets 1428365  bytes 189142687 (180.3 MiB)<br>        RX errors 0  dropped 0  overruns 0  frame 0<br>        TX packets 1403620  bytes 287806317 (274.4 MiB)<br>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">bridge网桥就是这样通信的,docker服务端通过vethxxxx端口和容器的eth0虚拟网卡进行通信.docker容器将宿主机的docker 0虚拟网卡的IP作为它的网关:</span><br></pre></td></tr></table></figure></p><p>[work@docker conf.d]$docker exec -it nginx route -n<br>Kernel IP routing table<br>Destination     Gateway         Genmask         Flags Metric Ref    Use Iface<br>0.0.0.0         172.17.0.1      0.0.0.0         UG    0      0        0 eth0<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">bridge模式是docker的默认网络模式，不写--net参数，就是bridge模式。使用docker run -p时，docker实际是在iptables做了DNAT规则，实现端口转发功能。可以使用iptables -t nat -vnL查看。</span><br><span class="line"></span><br><span class="line">bridge网络模式如下所示:</span><br><span class="line"></span><br><span class="line">![](http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUViTWFab1lmbU9pYk56NDZwc0FMcDk0bHR1MllTNVZHMmZtNGUxTTNwM0tOUmVQN04xZVh2OHlBLzA/d3hfZm10PXBuZw==)</span><br><span class="line"></span><br><span class="line">​    bridge网桥是docker的默认网络驱动.如果用户在创建容器时自定义了Bridge网络.那么自定义Bridge要优于docker默认的Bridge</span><br><span class="line"></span><br><span class="line"> **用户定义的bridge和默认bridge的区别**</span><br><span class="line"></span><br><span class="line">* 用户定义的bridge在多个容器之间提供更好的隔离性和协调性.</span><br><span class="line"></span><br><span class="line">  连到同一个自定义的bridge的容器之间的所有端口互通.而无需通过-p参数暴露到宿主机.这让容器之间的通信更简单,而且提供更好的安全性.例如:</span><br><span class="line"></span><br><span class="line">  连到同一个自定义的bridge网络的Nginx容器和mysql容器.及时mysql容器没有暴露任何端口.nginx也可以访问mysql容器的3306端口.</span><br><span class="line"></span><br><span class="line">  而默认的Bridge网络,则需要将mysql容器通过```-p```参数暴露3306端口给宿主机.</span><br><span class="line"></span><br><span class="line">* 自定义bridge网络提供容器的主机名DNS解析</span><br><span class="line"></span><br><span class="line">​        默认的bridge网络下的容器间不能通过主机名互相访问,只能通过IP地址.(除非使用—link参数,但是这个参数已经废弃).而用户自定义的bridge网络则可以直接访问对方的主机名.</span><br><span class="line"></span><br><span class="line">* 自定义bridge网络配置更方便</span><br><span class="line"></span><br><span class="line">​        配置一个默认bridge网络,会影响到全局所有容器.而且需要重启docker服务.使用```docker network create```可以创建一个自定义bridge网络.,而且可以分别配置</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 创建和管理自定义bridge网络</span><br><span class="line"></span><br><span class="line">创建命令:</span><br></pre></td></tr></table></figure></p><p>#创建自定义网络名称为my-net<br>docker network create my-net</p><p>#还可以指定网络号,子网掩码等信息<br>[root@localhost ~]$docker network create –help</p><p>Usage:    docker network create [OPTIONS] NETWORK</p><p>Create a network</p><p>Options:<br>      –attachable           Enable manual container attachment<br>      –aux-address map      Auxiliary IPv4 or IPv6 addresses used by Network driver (default map)<br>      –config-from string   The network from which copying the configuration<br>      –config-only          Create a configuration only network<br>  -d, –driver string        Driver to manage the Network (default “bridge”)<br>      –gateway strings      IPv4 or IPv6 Gateway for the master subnet<br>      –ingress              Create swarm routing-mesh network<br>      –internal             Restrict external access to the network<br>      –ip-range strings     Allocate container ip from a sub-range<br>      –ipam-driver string   IP Address Management Driver (default “default”)<br>      –ipam-opt map         Set IPAM driver specific options (default map)<br>      –ipv6                 Enable IPv6 networking<br>      –label list           Set metadata on a network<br>  -o, –opt map              Set driver specific options (default map)<br>      –scope string         Control the network’s scope<br>      –subnet strings       Subnet in CIDR format that represents a network segment<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">删除自定义网络:</span><br></pre></td></tr></table></figure></p><p>#如果有容器正在使用该网络,需要先断开容器<br>docker network rm my-net<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 管理自定义网络下的容器</span><br><span class="line"></span><br><span class="line">创建一个自定义网络下的容器</span><br></pre></td></tr></table></figure></p><p>#在创建容器的时候指定自定义网络名</p><p>docker create –name my-nginx \<br>  –network my-net \<br>  –publish 8080:80 \<br>  nginx:latest<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">将一个正在运行的容器关联(移除)自定义网络</span><br></pre></td></tr></table></figure></p><p>#关联容器和自定义网络命令格式:<br>docker network connect</p><p>#例如,将一个正在运行的mysql容器关联到my-net网络下<br>docker network connect my-net mysql</p><p>#相反从自定义网络下移除一个容器命令:<br>docker network disconnect</p><p>#例如,将一个正在运行的mysql容器从my-net网络下移除<br>docker network disconnect my-net mysql<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 实验:</span><br><span class="line"></span><br><span class="line">* **默认的bridge网络,容器之间无法互相访问对方的主机名.只能通过iP地址通信**</span><br></pre></td></tr></table></figure></p><p>[root@localhost ~]$docker run -itd –rm –name=busybox busybox<br>b3c8be3b3b716579caf11d3852f6c6e04a41b4dc020d9478be1b1f3a4d76cf1f</p><p>[root@localhost ~]$docker run -itd –rm –name=busybox1 busybox<br>a54d962f6a692d96f0bc2fbca37e1b47e59e6b61541f42b8d6872a5008a46b87</p><p>[root@localhost ~]$docker exec busybox ping busybox1<br>ping: bad address ‘busybox1’<br>[root@localhost ~]$</p><p>#通过IP地址可以通信</p><p>[root@localhost ~]$docker exec busybox ping 172.17.0.8<br>PING 172.17.0.8 (172.17.0.8): 56 data bytes<br>64 bytes from 172.17.0.8: seq=0 ttl=64 time=0.136 ms<br>64 bytes from 172.17.0.8: seq=1 ttl=64 time=0.079 ms<br>64 bytes from 172.17.0.8: seq=2 ttl=64 time=0.131 ms<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* **自定义的Bridge网络,可以在容器之间互相访问主机名**</span><br></pre></td></tr></table></figure></p><p>#创建一个自定义网络<br>[root@localhost ~]$docker network create jesse<br>e10a936177681cbfc321f67f961f2a717079ef1790c50e82381296fa77bd7d5f</p><p>[root@localhost ~]$docker network ls | grep jesse<br>e10a93617768        jesse                  bridge              local</p><p>#创建容器,使用network参数指定网络<br>[root@localhost ~]$docker run –name busybox1 -itd –network jesse –rm busybox<br>aedf164ea1769741ae6480583abdc838022f49506761d4054daabdb7fffcd852</p><p>[root@localhost ~]$docker run –name busybox2 -itd –network jesse –rm busybox<br>523bdd54fa12423e25a8ac84f9faef1679ee6031f2d5d2a87c0cacd64f8650ad</p><p>[root@localhost ~]$docker exec busybox1 ping busybox2<br>PING busybox2 (172.20.0.3): 56 data bytes<br>64 bytes from 172.20.0.3: seq=0 ttl=64 time=0.116 ms<br>64 bytes from 172.20.0.3: seq=1 ttl=64 time=0.078 ms</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* 连接到不同的bridge网络下的容器互相之间网络隔离</span><br></pre></td></tr></table></figure><p>docker network create bridge<br>docker network create kong-net</p><p>[work@docker conf.d]$docker run –name busybox1 –network bridge -itd busybox<br>f95a229aa7eb5d7022bef4441a075b0ad37ecc50e2a02015f09790d23b28dc33</p><p>[work@docker conf.d]$docker run –name busybox2 –network kong-net -itd busybox<br>a9236a25199cc43a899285462afa851a52cdaf871776a93829897676fc7dd82c</p><p>#busybox1和busybox2不在同一个IP网段</p><p>#busybox1的IP<br>[work@docker conf.d]$docker inspect busybox1<br>172.17.0.11</p><p>#busybox2的IP<br>[work@docker conf.d]$docker exec -it busybox2 ifconfig eth0<br>eth0      Link encap:Ethernet  HWaddr 02:42:AC:12:00:05<br>          inet addr:172.18.0.5  Bcast:0.0.0.0  Mask:255.255.0.0</p><p>#主机名无法访问<br>[work@docker conf.d]$docker exec -it busybox1 ping busybox2<br>ping: bad address ‘busybox2’</p><p>#busybox1容器也无法ping busybox2容器的IP<br>[work@docker conf.d]$docker exec -it busybox1 ping 172.18.0.5<br>PING 172.18.0.5 (172.18.0.5): 56 data bytes<br>^C<br>— 172.18.0.5 ping statistics —<br>32 packets transmitted, 0 packets received, 100% packet loss<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 由于博客不能识别go模板语法,所以省去了go模板,直接用docker inspect busybox1命令来代替.实际场景中该命令无法直接获取容器IP</span><br><span class="line"></span><br><span class="line">#### 将容器从自定义bridge网络中移除</span><br></pre></td></tr></table></figure></p><p>#将jesse从jesse网络移除<br>[root@localhost ~]$docker network disconnect jesse busybox1</p><p>#此时jesse网络下只有busybox2容器<br>[root@localhost ~]$docker network inspect jesse</p><p>#此时busybox1容器的网卡被移除了<br>[root@localhost ~]$docker inspect busybox1 </p><no value=""><p>root@localhost ~]$docker exec -it busybox1 ifconfig<br>lo        Link encap:Local Loopback<br>          inet addr:127.0.0.1  Mask:255.0.0.0<br>          UP LOOPBACK RUNNING  MTU:65536  Metric:1<br>          RX packets:11 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:11 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:1000<br>          RX bytes:618 (618.0 B)  TX bytes:618 (618.0 B)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 将一个正在运行的容器加入到自定义bridge网络</span><br></pre></td></tr></table></figure></p><p>#将jesse加回到jesse网络<br>[root@localhost ~]$docker network connect jesse busybox1</p><p>#获得新的IP地址<br>[root@localhost ~]$docker inspect busybox1<br>172.20.0.2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 宿主机转发容器端口</span><br><span class="line"></span><br><span class="line">默认情况下,bridge网络不会转发外部的请求到容器.开启转发需要更改2个设置:</span><br><span class="line"></span><br><span class="line">1.修改内容,开启转发</span><br></pre></td></tr></table></figure></p><p>sysctl net.ipv4.conf.all.forwarding=1</p><p>sysctl -p<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2.更改iptables的转发的默认规则</span><br></pre></td></tr></table></figure></p><p>sudo iptables -P FORWARD ACCEPT<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 更改默认Bridge网络配置</span><br><span class="line"></span><br><span class="line">docker0网桥是在docker daemon启动时自动创建的.IP默认为172.17.0.1/16.所有连到docker 0网桥的docker容器都会在这个IP范围内选取一个未占用的IP使用.并连接到docker 0网桥上</span><br><span class="line"></span><br><span class="line">docker提供了一些参数帮助用户自定义docker0网桥的设置</span><br><span class="line"></span><br><span class="line">* —bip=CIDR: 设置Docker0的IP地址和子网范围.使用CIDR格式.例如192.168.0.1/24.需要注意的是这个参数仅仅是配置docker0的,对其他自定义的网桥无效.</span><br><span class="line">* —fixed-cidr=CIDR:限制docker容器获取iP的范围.默认情况下docker容器获取的IP范围为整个docker0网桥的IP地址段,也就是—bip指定的地址范围.此参数可以将docker容器缩小到某个子网范围.</span><br><span class="line">* —mtu=BYTES: 指定docker0的最大传输单元(MTU)</span><br></pre></td></tr></table></figure></p><p>#更改daemon.json配置文件.下面这个例子修改了docker0网络的网段地址<br>vim /etc/docker/daemon.json</p><p>{<br>  “registry-mirrors”: [“<a href="https://registry.docker-cn.com&quot;]" target="_blank" rel="noopener">https://registry.docker-cn.com&quot;]</a>,<br>  “bip”: “192.168.1.5/24”,<br>  “mtu”: 1500,<br>  “dns”: [“114.114.114.114”,”114.114.115.115”]</p><p>}</p><p>#重启docker服务<br>[root@localhost ~]$systemctl restart docker</p><p>[root@localhost ~]$ifconfig | grep -A 5 docker0<br>docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500<br>        inet 192.168.1.5  netmask 255.255.255.0  broadcast 192.168.1.255<br>        ether 02:42:89:26:d1:c9  txqueuelen 0  (Ethernet)<br>        RX packets 1072032  bytes 61915874 (59.0 MiB)<br>        RX errors 0  dropped 0  overruns 0  frame 0<br>        TX packets 1997027  bytes 1934238839 (1.8 GiB)<br><code>`</code></p><hr></no>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker官网学习–docker网络之bridge&quot;&gt;&lt;a href=&quot;#docker官网学习–docker网络之bridge&quot; class=&quot;headerlink&quot; title=&quot;docker官网学习–docker网络之bridge&quot;&gt;&lt;/a&gt;docker官网学习–docker网络之bridge&lt;/h2&gt;&lt;p&gt;本节介绍docker基础网络概念.以便能认识和利用各种不同的网络类型功能.&lt;/p&gt;
&lt;p&gt;docker的网络支持插件化,驱动化定制.有一些网络驱动已经默认集成到docker中.docker网络主要有以下类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bridge&lt;/li&gt;
&lt;li&gt;host&lt;/li&gt;
&lt;li&gt;overlay&lt;/li&gt;
&lt;li&gt;macvlan&lt;/li&gt;
&lt;li&gt;none&lt;/li&gt;
&lt;li&gt;其他网络插件&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker网络之host,Container,None网络</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0-7.docker%E7%BD%91%E7%BB%9C%E4%B9%8Bhost,Container,None%E7%BD%91%E7%BB%9C/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习-7.docker网络之host,Container,None网络/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:14:06.747Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker官网学习-7-docker网络之host-Container-None网络"><a href="#docker官网学习-7-docker网络之host-Container-None网络" class="headerlink" title="docker官网学习-7.docker网络之host,Container,None网络"></a>docker官网学习-7.docker网络之host,Container,None网络</h2><h3 id="host网络介绍"><a href="#host网络介绍" class="headerlink" title="host网络介绍"></a>host网络介绍</h3><p>如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。</p><hr><h3 id="创建host网络"><a href="#创建host网络" class="headerlink" title="创建host网络"></a>创建host网络</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">docker run -tid --net=host --name busybox busybox1</span><br><span class="line"></span><br><span class="line">#host网络下的容器没有虚拟网卡,而是和宿主机共享网络</span><br><span class="line">[root@localhost ~]$docker exec  -it busybox1 ifconfig</span><br><span class="line">docker0   Link encap:Ethernet  HWaddr 02:42:89:26:D1:C9</span><br><span class="line">          inet addr:192.168.1.5  Bcast:192.168.1.255  Mask:255.255.255.0</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</span><br><span class="line">          RX packets:1072032 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:1997027 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0</span><br><span class="line">          RX bytes:61915874 (59.0 MiB)  TX bytes:1934238839 (1.8 GiB)</span><br></pre></td></tr></table></figure><blockquote><p>注:host网络只能工作在Linux主机上.</p><p>如果容器没有暴露任何端口,那host网络没有任何效果</p></blockquote><a id="more"></a><p>host网络示意图</p><p><img src="http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVObjRZOVg0Q2tRRVZMV2dFZGt4MWljeThKY3VERXFhTGFZZUhiaDB1TWJZeVVtTjhQQ1l0bDl3LzA/d3hfZm10PXBuZw==" alt=""></p><hr><h3 id="Container网络介绍"><a href="#Container网络介绍" class="headerlink" title="Container网络介绍"></a>Container网络介绍</h3><p>这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备通信。</p><p>Container网络示意图</p><p><img src="http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVtaWM5elRoU1d1UmdSc2xVT2oyeHpmeUljZXdpY2E3VkpibE03Nnc5N01PZFRLVEl2TkdpYTBPd2cvMD93eF9mbXQ9cG5n" alt=""></p><hr><h3 id="None网络"><a href="#None网络" class="headerlink" title="None网络"></a>None网络</h3><p>使用none模式，Docker容器拥有自己的Network Namespace，但是，并不为Docker容器进行任何网络配置。也就是说，这个Docker容器没有网卡、IP、路由等信息。需要我们自己为Docker容器添加网卡、配置IP等。</p><p>Node模式示意图:</p><p><img src="http://cdn.img2.a-site.cn/pic.php?url=aHR0cDovL21tYml6LnFwaWMuY24vbW1iaXovUVAwQVk3dGRKblV4eFJNWjRRcDl0b21GaFFRMDNYVUVMeFREaWNxUVFYQ0dObVlUNFlRdVdQYkxBRk1TVmhvRFlrcUtENFVLczVXbWtqbTM1THNpY1FZUS8wP3d4X2ZtdD1wbmc=" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;docker官网学习-7-docker网络之host-Container-None网络&quot;&gt;&lt;a href=&quot;#docker官网学习-7-docker网络之host-Container-None网络&quot; class=&quot;headerlink&quot; title=&quot;docker官网学习-7.docker网络之host,Container,None网络&quot;&gt;&lt;/a&gt;docker官网学习-7.docker网络之host,Container,None网络&lt;/h2&gt;&lt;h3 id=&quot;host网络介绍&quot;&gt;&lt;a href=&quot;#host网络介绍&quot; class=&quot;headerlink&quot; title=&quot;host网络介绍&quot;&gt;&lt;/a&gt;host网络介绍&lt;/h3&gt;&lt;p&gt;如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;创建host网络&quot;&gt;&lt;a href=&quot;#创建host网络&quot; class=&quot;headerlink&quot; title=&quot;创建host网络&quot;&gt;&lt;/a&gt;创建host网络&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker run -tid --net=host --name busybox busybox1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;#host网络下的容器没有虚拟网卡,而是和宿主机共享网络&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[root@localhost ~]$docker exec  -it busybox1 ifconfig&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;docker0   Link encap:Ethernet  HWaddr 02:42:89:26:D1:C9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          inet addr:192.168.1.5  Bcast:192.168.1.255  Mask:255.255.255.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          RX packets:1072032 errors:0 dropped:0 overruns:0 frame:0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          TX packets:1997027 errors:0 dropped:0 overruns:0 carrier:0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          collisions:0 txqueuelen:0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;          RX bytes:61915874 (59.0 MiB)  TX bytes:1934238839 (1.8 GiB)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;注:host网络只能工作在Linux主机上.&lt;/p&gt;
&lt;p&gt;如果容器没有暴露任何端口,那host网络没有任何效果&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>docker学习笔记---docker使用阿里云私有仓库</title>
    <link href="https://jesse.top/2020/06/29/docker/docker%E5%AE%98%E7%BD%91%E5%AD%A6%E4%B9%A0%E2%80%943.%E4%BD%BF%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F%E5%92%8C%E4%BB%93%E5%BA%93/"/>
    <id>https://jesse.top/2020/06/29/docker/docker官网学习—3.使用阿里云国内镜像和仓库/</id>
    <published>2020-06-29T03:59:58.000Z</published>
    <updated>2020-06-29T13:11:08.185Z</updated>
    
    <content type="html"><![CDATA[<h3 id="docker使用阿里云私有仓库"><a href="#docker使用阿里云私有仓库" class="headerlink" title="docker使用阿里云私有仓库"></a>docker使用阿里云私有仓库</h3><p>注册阿里云镜像服务:</p><p>以下是我的阿里云镜像仓库链接:<br><a href="https://cr.console.aliyun.com/cn-hangzhou/repositories" target="_blank" rel="noopener">https://cr.console.aliyun.com/cn-hangzhou/repositories</a></p><p>一.使用阿里云镜像加速器<br><a href="https://cr.console.aliyun.com/cn-hangzhou/mirrors" target="_blank" rel="noopener">https://cr.console.aliyun.com/cn-hangzhou/mirrors</a></p><p>镜像加速地址:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://0w5ygvsg.mirror.aliyuncs.com</span><br></pre></td></tr></table></figure><p>如果是Centos系统,可以通过修改daemon配置文件/etc/docker/daemon.json来使用加速器:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /etc/docker</span><br><span class="line">sudo tee /etc/docker/daemon.json &lt;&lt;-&apos;EOF&apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https://0w5ygvsg.mirror.aliyuncs.com&quot;]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure><hr><a id="more"></a><h4 id="使用阿里云的镜像仓库"><a href="#使用阿里云的镜像仓库" class="headerlink" title="使用阿里云的镜像仓库"></a>使用阿里云的镜像仓库</h4><p>首先在阿里云镜像服务控制台创建镜像仓库和命令空间:</p><p><a href="https://cr.console.aliyun.com/cn-hangzhou/repositories" target="_blank" rel="noopener">https://cr.console.aliyun.com/cn-hangzhou/repositories</a></p><p>我的镜像仓库和命名空间都是:jesse_images<br>这是我的镜像仓库地址:registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images</p><p>下面演示,如何推送本地镜像到阿里云仓库</p><p>1.在本地docker服务器登陆阿里云镜像仓库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost docker_python]$docker login --username=jessehuang408 registry.cn-hangzhou.aliyuncs.com</span><br></pre></td></tr></table></figure><p>2.将本地镜像推送到仓库执行以下两条命令</p><ul><li>为本地镜像打个标签</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker tag [ImageId] registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:[镜像版本号]</span><br></pre></td></tr></table></figure><ul><li>将镜像推送到仓库</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:[镜像版本号]</span><br></pre></td></tr></table></figure><p>下面演示将friendlyhello这个镜像推送到阿里云远程仓库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@localhost docker_python]$docker images</span><br><span class="line">REPOSITORY                                                    TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">friendlyhello                                                 latest              f091d1bb803c        43 minutes ago      131MB</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker tag f091d1bb803c registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:v2.0</span><br><span class="line"></span><br><span class="line">docker push registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:v2.0</span><br></pre></td></tr></table></figure><p>在本机可以看到镜像:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@localhost docker_python]$docker images</span><br><span class="line">REPOSITORY                                                    TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">friendlyhello                                                 latest              f091d1bb803c        43 minutes ago      131MB</span><br><span class="line">registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images   v2.0                f091d1bb803c        43 minutes ago      131MB</span><br></pre></td></tr></table></figure><p>登陆阿里云的镜像服务控制台,在镜像仓库的管理界面可以看到上传上去的镜像</p><p>如果是从阿里云镜像仓库拉取镜像,执行以下命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker pull registry.cn-hangzhou.aliyuncs.com/jesse_images/jesse_images:[镜像版本号]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;docker使用阿里云私有仓库&quot;&gt;&lt;a href=&quot;#docker使用阿里云私有仓库&quot; class=&quot;headerlink&quot; title=&quot;docker使用阿里云私有仓库&quot;&gt;&lt;/a&gt;docker使用阿里云私有仓库&lt;/h3&gt;&lt;p&gt;注册阿里云镜像服务:&lt;/p&gt;
&lt;p&gt;以下是我的阿里云镜像仓库链接:&lt;br&gt;&lt;a href=&quot;https://cr.console.aliyun.com/cn-hangzhou/repositories&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://cr.console.aliyun.com/cn-hangzhou/repositories&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一.使用阿里云镜像加速器&lt;br&gt;&lt;a href=&quot;https://cr.console.aliyun.com/cn-hangzhou/mirrors&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://cr.console.aliyun.com/cn-hangzhou/mirrors&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;镜像加速地址:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;https://0w5ygvsg.mirror.aliyuncs.com&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果是Centos系统,可以通过修改daemon配置文件/etc/docker/daemon.json来使用加速器:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sudo mkdir -p /etc/docker&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo tee /etc/docker/daemon.json &amp;lt;&amp;lt;-&amp;apos;EOF&amp;apos;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;quot;registry-mirrors&amp;quot;: [&amp;quot;https://0w5ygvsg.mirror.aliyuncs.com&amp;quot;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;EOF&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo systemctl daemon-reload&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo systemctl restart docker&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="docker" scheme="https://jesse.top/categories/docker/"/>
    
    
      <category term="docker" scheme="https://jesse.top/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>kuberentes Deployment</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/kubernetes%20Deployment/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/kubernetes Deployment/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T10:02:24.946Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kuberentes-Deployment"><a href="#kuberentes-Deployment" class="headerlink" title="kuberentes Deployment"></a>kuberentes Deployment</h2><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>之前学习过使用ReplicationController和ReplicaSet实现托管和更新pod容器.<br>客户端通过Service服务访问RC或者RS托管的一组pod.这也是Kubernetes典型的应用程序运行方式</p><p>假设这个时候应用程序需要更新,从v1版本更新到v2,有以下两种方式更新pod镜像:</p><ul><li>直接删除所有现有的pod,然后创建新的pod</li><li>可以先创建新的Pod,并等待他们运行成功后,再删除旧的.</li></ul><p>这2中方法各有优缺点.第一种方法将导致服务在端在时间内不可用.第二种方法要求应用程序同时支持2个版本对外提供服务.</p><p>本文具体介绍kubernetes上述两种更新方式.然后介绍通过手动方式,replicatController的更新方式.最后再引入deployment机制的用法介绍</p><a id="more"></a><hr><h2 id="1-更新pod"><a href="#1-更新pod" class="headerlink" title="1.更新pod"></a>1.更新pod</h2><h3 id="删除旧版本pod-使用新版本pod替换"><a href="#删除旧版本pod-使用新版本pod替换" class="headerlink" title="删除旧版本pod,使用新版本pod替换"></a>删除旧版本pod,使用新版本pod替换</h3><p>这是更新pod最简单的方式,如果有一个replicationController管理一组v1的pod,可以直接修改pod模板为v2版本.然后执行下列命令删除旧rc,创建新rc来更新整个pod组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl delete -f kubia-rc.yaml</span><br><span class="line">[root@k8s-master ~]# kubectl create -f kubia-rc.yaml</span><br><span class="line">replicationcontroller/kubia created</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl get pods</span><br><span class="line">NAME                READY   STATUS        RESTARTS   AGE</span><br><span class="line">kubia-2xz92         0/1     Terminating   0          92s</span><br><span class="line">kubia-dkhsz         1/1     Running       0          3s</span><br><span class="line">kubia-kn694         1/1     Running       0          3s</span><br><span class="line">kubia-l946z         1/1     Running       0          3s</span><br><span class="line">kubia-p5bmx         0/1     Terminating   0          92s</span><br><span class="line">kubia-sjxlz         0/1     Terminating   0          92s</span><br><span class="line">kubia-tt6fz         0/1     Terminating   0          92s</span><br><span class="line">kubia-xm8js         1/1     Running       0          3s</span><br><span class="line">ssd-monitor-h7k7q   1/1     Running       0          57s</span><br></pre></td></tr></table></figure><p>此时旧版本会处于terminating终止中状态,(最后会逐渐被删除),同时创建了新的一组v2版本的Pod.</p><hr><h3 id="先新建pod-再删除旧pod"><a href="#先新建pod-再删除旧pod" class="headerlink" title="先新建pod,再删除旧pod"></a>先新建pod,再删除旧pod</h3><p>如果短暂的服务中断不可介绍,并且应用程序支持多个版本同时对外服务.那么可以先创建新的Pod,然后删除原有的pod.</p><blockquote><p>但是这会在短暂时间内同时运行2倍数量的pod,要求服务器有更多的空闲硬件资源</p></blockquote><h4 id="大致步骤"><a href="#大致步骤" class="headerlink" title="大致步骤"></a>大致步骤</h4><ol><li>创建新版本pod</li><li>一旦新版本Pod就绪,将Service服务从老版本pod关联到新版本pod</li><li>删除旧的ReplicationController</li></ol><hr><h2 id="2-使用ReplicationController实现自动的滚动升级"><a href="#2-使用ReplicationController实现自动的滚动升级" class="headerlink" title="2.使用ReplicationController实现自动的滚动升级"></a>2.使用ReplicationController实现自动的滚动升级</h2><h3 id="使用kubectl命令手动滚动式升级"><a href="#使用kubectl命令手动滚动式升级" class="headerlink" title="使用kubectl命令手动滚动式升级"></a>使用kubectl命令手动滚动式升级</h3><h4 id="1-下面创建一个ReplicationController和一个v1版本进行的pod-以及一个nodePort类型的Service-并以此为例子进行后面的演练"><a href="#1-下面创建一个ReplicationController和一个v1版本进行的pod-以及一个nodePort类型的Service-并以此为例子进行后面的演练" class="headerlink" title="1.下面创建一个ReplicationController和一个v1版本进行的pod.以及一个nodePort类型的Service.并以此为例子进行后面的演练"></a>1.下面创建一个ReplicationController和一个v1版本进行的pod.以及一个nodePort类型的Service.并以此为例子进行后面的演练</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ReplicationController</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia-dm-rc-v1</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: kubia</span><br><span class="line">      labels:</span><br><span class="line">        app: kubia</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">       - image: luksa/kubia:v1</span><br><span class="line">         name: nodejs</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">#一个yaml文件可以定义多种资源,中间用---隔开</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia-dm-svc-v1</span><br><span class="line"></span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    app: kubia</span><br><span class="line"></span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 8080</span><br><span class="line">    nodePort: 32013</span><br></pre></td></tr></table></figure><p>这个例子中创建了一个kubia-dm-rc-v1的ReplicationController.并运行了3个副本.另外还创建了一个kubia-dm-svc-v1的nodeport类型的Service</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl create -f deployment-kubia-v1.yaml</span><br><span class="line">replicationcontroller/kubia-dm-rc-v1 created</span><br><span class="line">service/kubia-dm-svc-v1 created</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl get svc</span><br><span class="line">NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">fortune-nfs-svc      ClusterIP      10.96.243.6     &lt;none&gt;        80/TCP         14d</span><br><span class="line">fortune-svc          ClusterIP      10.96.123.110   &lt;none&gt;        80/TCP         14d</span><br><span class="line">kubernetes           ClusterIP      10.96.0.1       &lt;none&gt;        443/TCP        41d</span><br><span class="line">kubia                ClusterIP      10.96.170.37    &lt;none&gt;        80/TCP         21d</span><br><span class="line">kubia-dm-svc-v1      NodePort       10.96.136.55    &lt;none&gt;        80:32013/TCP   4s</span><br><span class="line">kubia-loadbalancer   LoadBalancer   10.96.1.162     &lt;pending&gt;     80:31924/TCP   20d</span><br><span class="line">kubia-nginx          ClusterIP      10.96.154.64    &lt;none&gt;        80/TCP         21d</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl get pods</span><br><span class="line">NAME                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">kubia-dm-rc-v1-dsk5v   1/1     Running   0          9s</span><br><span class="line">kubia-dm-rc-v1-mk6hf   1/1     Running   0          9s</span><br><span class="line">kubia-dm-rc-v1-rh6qz   1/1     Running   0          9s</span><br><span class="line">[root@k8s-master ~]#</span><br></pre></td></tr></table></figure><p>访问Service的IP:10.96.136.55,或者节点物理网卡IP的32013端口访问pod容器,目前是v1版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# while true;do curl http://10.96.136.55;sleep 1;done</span><br><span class="line">This is v1 running in pod kubia-dm-rc-v1-dsk5v</span><br><span class="line">This is v1 running in pod kubia-dm-rc-v1-mk6hf</span><br><span class="line">This is v1 running in pod kubia-dm-rc-v1-rh6qz</span><br><span class="line">This is v1 running in pod kubia-dm-rc-v1-rh6qz</span><br><span class="line">This is v1 running in pod kubia-dm-rc-v1-rh6qz</span><br><span class="line">This is v1 running in pod kubia-dm-rc-v1-mk6hf</span><br></pre></td></tr></table></figure><h4 id="2-创建v2版本的应用-这里直接使用luksa-kubia-v2"><a href="#2-创建v2版本的应用-这里直接使用luksa-kubia-v2" class="headerlink" title="2.创建v2版本的应用.这里直接使用luksa/kubia:v2"></a>2.创建v2版本的应用.这里直接使用luksa/kubia:v2</h4><blockquote><p>虽然在开发的过程中,进程推送修改后的应用到同一个镜像tag,但是这种做法不推荐.如果用的是latest的tag倒还好,因为不指定容器tag,或者tag是latest,则镜像拉取策略(imagePullPolicy)默认为always.</p></blockquote><blockquote><p>如果使用了指定的tag,例如(v1),则默认的镜像拉取策略为IfNotPresent,则虽然应用程序更新了,但是由于tag不变(都是v1),所以某些已经拉取了v1版本的镜像的Pod仍然使用v1旧版本的镜像.但是之前没有运行该pod的节点拉取的是v2版本.这就造成虽然镜像名一样,但是内容不一样的意外情况</p></blockquote><h4 id="3-使用"><a href="#3-使用" class="headerlink" title="3.使用"></a>3.使用<figure class="highlight plain"><figcaption><span>rolling-update```命令开始ReplicationController的滚动升级.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt;在这之前打开一个新终端,循环访问Pod容器,观察升级过程中客户端访问情况</span><br><span class="line"></span><br><span class="line">命令格式为</span><br></pre></td></tr></table></figure></h4><p>kubectl rolling-update 旧rc名 新rc名 –image=新镜像<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl rolling-update kubia-dm-rc-v1 kubia-dm-rc-v2 –image=luksa/kubia:v2<br>Command “rolling-update” is deprecated, use “rollout” instead<br>Created kubia-dm-rc-v2</p><p>Scaling up kubia-dm-rc-v2 from 0 to 3, scaling down kubia-dm-rc-v1 from 3 to 0 (keep 3 pods available, don’t exceed 4 pods)<br>Scaling kubia-dm-rc-v2 up to 1<br>Scaling kubia-dm-rc-v1 down to 2<br>Scaling kubia-dm-rc-v2 up to 2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">该命令会自动进行滚动升级,先创建个v2版本(初始副本为0),然后v2版本副本慢慢增加到3,同时v1版本缩容慢慢缩减到0</span><br><span class="line"></span><br><span class="line">此时访问pod容器的nginx服务并没有中断,访问流量逐渐慢慢从v1过度到v2</span><br></pre></td></tr></table></figure></p><p>This is v1 running in pod kubia-dm-rc-v1-rh6qz<br>This is v1 running in pod kubia-dm-rc-v1-dsk5v<br>This is v1 running in pod kubia-dm-rc-v1-dsk5v<br>This is v2 running in pod kubia-dm-rc-v2-nvjln<br>This is v2 running in pod kubia-dm-rc-v2-nvjln<br>This is v1 running in pod kubia-dm-rc-v1-dsk5v<br>This is v1 running in pod kubia-dm-rc-v1-dsk5v<br>This is v2 running in pod kubia-dm-rc-v2-nvjln<br>This is v1 running in pod kubia-dm-rc-v1-rh6qz<br>This is v2 running in pod kubia-dm-rc-v2-nvjln<br>This is v1 running in pod kubia-dm-rc-v1-dsk5v<br>This is v2 running in pod kubia-dm-rc-v2-nvjln<br>This is v2 running in pod kubia-dm-rc-v2-nvjln<br>This is v1 running in pod kubia-dm-rc-v1-rh6qz<br>This is v2 running in pod kubia-dm-rc-v2-gx2qr<br>This is v1 running in pod kubia-dm-rc-v1-dsk5v<br>This is v1 running in pod kubia-dm-rc-v1-dsk5v<br>This is v1 running in pod kubia-dm-rc-v1-dsk5v<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">查看kubia-dm-rc-v2新的rc,发现多了一个label标签</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl describe rc kubia-dm-rc-v2<br>…..<br>  Labels:  app=kubia<br>           deployment=e765c85861762e0e8a6a68870e355478<br>…..<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">同时旧的kubia-dm-rc-v1的rc,也多了一个label标签,标签末尾有个orig</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl describe rc kubia-dm-rc-v1<br>……<br>  Labels:  app=kubia<br>           deployment=0a4a2fdf1a43e220276c99beda132b0c-orig<br>……<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">甚至正在运行的pod也修改了标签,3个正在运行的老的Pod容器副本增加了对应的旧的rc标签.</span><br><span class="line"></span><br><span class="line">同时创建了个新的pod,标签对应为新的kubia-dm-rc-v2新rc的标签</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl get po –show-labels<br>NAME                   READY   STATUS    RESTARTS   AGE   LABELS<br>kubia-dm-rc-v1-dsk5v   1/1     Running   0          18m   app=kubia,deployment=0a4a2fdf1a43e220276c99beda132b0c-orig<br>kubia-dm-rc-v1-mk6hf   1/1     Running   0          18m   app=kubia,deployment=0a4a2fdf1a43e220276c99beda132b0c-orig<br>kubia-dm-rc-v1-rh6qz   1/1     Running   0          18m   app=kubia,deployment=0a4a2fdf1a43e220276c99beda132b0c-orig<br>kubia-dm-rc-v2-nvjln   1/1     Running   0          59s   app=kubia,deployment=e765c85861762e0e8a6a68870e355478</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">最终,老的ReplicationController副本会缩容到0,导致最后一个v1 pod被删除,老的kubia-dm-rc-v1名字rc被删除.</span><br><span class="line"></span><br><span class="line">新的ReplicationController副本扩容到3,最后所有的请求都到了v2</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl get rc<br>NAME             DESIRED   CURRENT   READY   AGE<br>kubia-dm-rc-v2   3         3         3       15m<br>[root@k8s-master ~]#</p><p>[root@k8s-master ~]# while true;do curl <a href="http://10.96.136.55;sleep" target="_blank" rel="noopener">http://10.96.136.55;sleep</a> 1;done<br>This is v2 running in pod kubia-dm-rc-v2-gx2qr<br>This is v2 running in pod kubia-dm-rc-v2-gx2qr<br>This is v2 running in pod kubia-dm-rc-v2-nvjln<br>This is v2 running in pod kubia-dm-rc-v2-l44qp<br>This is v2 running in pod kubia-dm-rc-v2-gx2qr<br>^C<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"></span><br><span class="line">## 3.使用Deployment声明式升级应用</span><br><span class="line"></span><br><span class="line">使用kubectl命令并不是最优的方案,一方面这是由kubectl客户端执行任务,并不是kubernetes master执行的.领一方面,如果在kubectl执行升级时,出现网络故障,升级过程将会终端.造成意料之外情况</span><br><span class="line"></span><br><span class="line">最重要的是,通过命令行来手动的升级并不符合Kubernetes的理念和预期.直接使用期望的副本数来伸缩pod是kubernetes期望的部署和伸方式.</span><br><span class="line"></span><br><span class="line">正是这一点推动了一种称为Deployment的新资源的引入.也是现在kubernetes部署应用程序的首选方式</span><br><span class="line"></span><br><span class="line">----</span><br><span class="line"></span><br><span class="line">### 3.1 Deployment介绍</span><br><span class="line"></span><br><span class="line">Deployment是一种更高阶的资源,当创建一个Deployment时,ReplicaSet资源也会随之创建.ReplicaSet是新一代的ReplicationController,并且已经替代了后者来管理pod.</span><br><span class="line"></span><br><span class="line">在使用Deployment时,并不是由Deployment直接创建和管理pod.而是Deployment创建ReplicaSet,然后ReplicaSet创建pod</span><br></pre></td></tr></table></figure></p><p>Deployment—–&gt; ReplicaSet—–&gt;pods1,pods2,pods3……<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 为什么这么复杂,引入Deployment概念,直接ReplicaSet管理pod不是已经挺好的吗?.在上个滚动升级的例子中,我们需要协调2个ReplicationController.新的版本扩容,旧的版本缩容,所以这需要另一个资源来协调.</span><br><span class="line"></span><br><span class="line">&gt; Deployment资源就是负责处理这个问题</span><br><span class="line"></span><br><span class="line">#### 1.创建一个Deployment</span><br><span class="line"></span><br><span class="line">创建deployment和创建rc,rs并没有任何区别,也是由标签选择器,期望副本,和pod模板组成.另外还包含另一个字段,指定一个部署策略</span><br><span class="line"></span><br><span class="line">稍微修改上一个rc的配置</span><br></pre></td></tr></table></figure></p><p>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: kubia-dm-v1<br>spec:<br>  replicas: 3<br>  selector:<br>      matchLabels:<br>         app: kubia<br>  template:<br>    metadata:<br>      name: kubia<br>      labels:<br>        app: kubia<br>    spec:<br>      containers:</p><pre><code>- image: luksa/kubia:v1  name: nodejs</code></pre><hr><p>#一个yaml文件可以定义多种资源,中间用—隔开</p><p>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: kubia-dm-svc-v1</p><p>spec:<br>  type: NodePort<br>  selector:<br>    app: kubia</p><p>  ports:</p><ul><li>port: 80<br>targetPort: 8080<br>nodePort: 32014</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">deployment属于apps/v1版本.大体字段差不多,但是selector字段必须要显示指定.</span><br><span class="line"></span><br><span class="line">#### 2,创建depolyment和Service</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl create -f deployment-kubia-v2.yaml –record<br>deployment.apps/kubia-dm-v1 created<br>service/kubia-dm-svc-v1 created<br>[root@k8s-master ~]#<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; --record选项会记录历史版本号</span><br><span class="line"></span><br><span class="line">#### 3.展示Deployment滚动过程中的状态</span><br><span class="line"></span><br><span class="line">通过以下命令可以看到deployment部署的pod已经全部正常就位</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl rollout status deployment kubia-dm-v1<br>deployment “kubia-dm-v1” successfully rolled out</p><p>[root@k8s-master ~]# kubectl get pods<br>NAME                          READY   STATUS    RESTARTS   AGE<br>kubia-dm-v1-74fb644f8-6fp7n   1/1     Running   0          2m32s<br>kubia-dm-v1-74fb644f8-8hddk   1/1     Running   0          2m32s<br>kubia-dm-v1-74fb644f8-hcrck   1/1     Running   0          2m32s</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">通过deploy创建的pod命名和之前的有些稍微区别.中间的一串数字是deployment和replicaSet模板的哈希值.查看rs的信息,可以看到rs名称中也包含了pod相同的哈希值</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl get replicasets<br>NAME                    DESIRED   CURRENT   READY   AGE<br>kubia-dm-v1-74fb644f8   3         3         3       3m58s<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 3.2.deployment升级</span><br><span class="line"></span><br><span class="line">更新升级deployment相比之前的一个例子要简单,并且强大的多.只需要修改deployment资源定义模板配置文件中的镜像tag和期望副本.kubernetes会自动扩容或者缩容,以匹配期望的状态</span><br><span class="line"></span><br><span class="line">##### 不同的deployment升级策略</span><br><span class="line"></span><br><span class="line">deployment有2种升级策略</span><br><span class="line"></span><br><span class="line">* RollingUpdate(默认策略): 滚动更新.先删除一个旧的Pod,然后创建一个新的Pod,这就是蓝绿发布</span><br><span class="line">* Recreate:先全部删除旧pod,然后创建新Pod.</span><br><span class="line"></span><br><span class="line">其中RollingUpdate还有更详细的配置参数定义滚动过程动作.</span><br><span class="line"></span><br><span class="line">&gt; 在演示滚动更新之前,先回顾一下修改对象的几种不同方式.之前接触过通过```kubectl edit```命令修改,或者通过```kubectl patch```打补丁的方式修改</span><br><span class="line"></span><br><span class="line">下表中是几种不同的修改方式:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">方法  | 例子| 作用</span><br><span class="line">---|---|---</span><br><span class="line">kubectl edit | kubectl edit deployment kubia-dm-v2| 使用默认编辑器打开资源配置修改并更新</span><br><span class="line">kubectl patch | kubectl patch deployment kubia-dm-v2 -p &apos;&#123;&quot;spec&quot;:&#123;&quot;replicas&quot;:4&#125;&#125;&apos; | 通过打补丁的方式修改单个资源属性,比如修改副本数为4</span><br><span class="line">kubectl apply| kubectl apply -f kubia-dm-v2 | 应用一个完整的yaml文件,如果yaml中指定的对象不存在,则会被创建,文件中新的值会更新.</span><br><span class="line">kubectl replace|kubectl replace -f kubia-dm-v2 | 也是应用一个新的完整的yaml文件,文件中定义的值会更新原有的值,但是与apply相反,要求新的对象必须事先存在,否则会报错.</span><br><span class="line">kubectl set image| kubectl set iamge deployment kubia-dm-v2 nodejs=luksa/kubia:v2 | 修改部分资源(pod,job,rc,rs,deployment,DemonSet)的镜像</span><br><span class="line"></span><br><span class="line">----</span><br><span class="line"></span><br><span class="line">接下来演示滚动升级.首先通过patch方式修改deployment的一个属性</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl patch deployment kubia-dm-v1 -p ‘{“spec”:{“minReadySeconds”:10}}’<br>deployment.apps/kubia-dm-v1 patched<br>[root@k8s-master ~]#</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在另一个终端持续访问pod的nginx服务</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# while true;do curl <a href="http://10.96.68.186;sleep" target="_blank" rel="noopener">http://10.96.68.186;sleep</a> 1;done<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">接下来通过另外一种方式修改deployment中定义的pod镜像版本为v2</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl set image deployment kubia-dm-v1 nodejs=luksa/kubia:v2<br>deployment.apps/kubia-dm-v1 image updated<br>[root@k8s-master ~]#<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">此时,kuberntes开始执行滚动更新,直到所有的v1pod都被删除,所有的请求到v2</span><br></pre></td></tr></table></figure></p><p>This is v1 running in pod kubia-dm-v1-74fb644f8-6fp7n<br>This is v1 running in pod kubia-dm-v1-74fb644f8-6fp7n<br>This is v1 running in pod kubia-dm-v1-74fb644f8-6fp7n<br>This is v1 running in pod kubia-dm-v1-74fb644f8-6fp7n<br>This is v2 running in pod kubia-dm-v1-6c99f46f5-fj4dl<br>This is v1 running in pod kubia-dm-v1-74fb644f8-6fp7n<br>This is v2 running in pod kubia-dm-v1-6c99f46f5-fj4dl<br>This is v2 running in pod kubia-dm-v1-6c99f46f5-b55rf<br>This is v2 running in pod kubia-dm-v1-6c99f46f5-b55rf<br>This is v2 running in pod kubia-dm-v1-6c99f46f5-b55rf<br>This is v2 running in pod kubia-dm-v1-6c99f46f5-fj4dl<br>This is v2 running in pod kubia-dm-v1-6c99f46f5-b55rf<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">deploy进行滚动更新的过程和上个例子中使用的```kubectl rolling-update```命令非常相似.稍有不同的是,deployment更新完成后,创建了一个新的ReplicaSet.但是旧的ReplicaSet并不会删除.</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl get rs<br>NAME                    DESIRED   CURRENT   READY   AGE<br>kubia-dm-v1-6c99f46f5   3         3         3       5m1s<br>kubia-dm-v1-74fb644f8   0         0         0       61m<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">但是相比上个例子,使用deployment进行滚动升级明显要方便的多,用户仅仅需要管理一个单一的deployment资源.并不需要过多的关心pod和RS.</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 3.3 deployment回滚</span><br><span class="line"></span><br><span class="line">##### 1.使用v3版本的镜像(luksa/kubia:v3)升级,但是v3版本在处理第五个请求时会返回500错误,导致服务器不可用.</span><br><span class="line"></span><br><span class="line">##### 2.修改deployment资源kubia-dm-v1的镜像为v3版本,出发滚动升级</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl set image deployment kubia-dm-v1 nodejs=luksa/kubia:v3<br>deployment.apps/kubia-dm-v1 image updated<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">滚动升级完成后,全部访问到v3版本镜像,但是访问出现问题</span><br></pre></td></tr></table></figure></p><p>This is v2 running in pod kubia-dm-v1-6c99f46f5-fj4dl<br>This is v2 running in pod kubia-dm-v1-6c99f46f5-fj4dl<br>^@This is v2 running in pod kubia-dm-v1-6c99f46f5-fj4dl<br>Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-q7lr4<br>Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-q7lr4<br>This is v3 running in pod kubia-dm-v1-6bb646bc8d-p9nq5<br>Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-q7lr4<br>Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-qzwss<br>This is v3 running in pod kubia-dm-v1-6bb646bc8d-p9nq5<br>This is v2 running in pod kubia-dm-v1-6c99f46f5-fj4dl<br>This is v3 running in pod kubia-dm-v1-6bb646bc8d-p9nq5<br>Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-q7lr4<br>Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-q7lr4<br>Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-q7lr4<br>Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-q7lr4<br>This is v3 running in pod kubia-dm-v1-6bb646bc8d-p9nq5<br>Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-p9nq5<br>Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-q7lr4<br>Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-qzwss<br>Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-qzwss<br>Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-q7lr4<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##### 3.手动停止回滚升级</span><br><span class="line"></span><br><span class="line">先演示如何手动停止升级.deployment可以非常容易的回滚到先前部署的版本.它可以让Kubernetes取消最后一次部署的Deployment</span><br><span class="line"></span><br><span class="line">命令: ```kubectl rollout undo deployment DeplymentName</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl rollout undo deployment kubia-dm-v1</span><br><span class="line">deployment.apps/kubia-dm-v1 rolled back</span><br></pre></td></tr></table></figure><p>此时deployment会回滚到上一个版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-qzwss</span><br><span class="line">Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-q7lr4</span><br><span class="line">This is v2 running in pod kubia-dm-v1-6c99f46f5-pkv5f</span><br><span class="line">This is v2 running in pod kubia-dm-v1-6c99f46f5-vnglb</span><br><span class="line">Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-q7lr4</span><br><span class="line">This is v2 running in pod kubia-dm-v1-6c99f46f5-vnglb</span><br><span class="line">Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-q7lr4</span><br><span class="line">Some internal error has occurred! This is pod kubia-dm-v1-6bb646bc8d-qzwss</span><br><span class="line">This is v2 running in pod kubia-dm-v1-6c99f46f5-pkv5f</span><br></pre></td></tr></table></figure><h5 id="4-显示deployment的滚动升级历史"><a href="#4-显示deployment的滚动升级历史" class="headerlink" title="4. 显示deployment的滚动升级历史"></a>4. 显示deployment的滚动升级历史</h5><p>命令: <figure class="highlight plain"><figcaption><span>rollout history deployment DeploymentName```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl rollout history deployment kubia-dm-v1<br>deployment.apps/kubia-dm-v1<br>REVISION  CHANGE-CAUSE<br>1         kubectl create –filename=deployment-kubia-v2.yaml –record=true<br>3         kubectl create –filename=deployment-kubia-v2.yaml –record=true<br>4         kubectl create –filename=deployment-kubia-v2.yaml –record=true<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">使用--revison=REVISION编号,可以看回滚历史的具体信息,比如</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl rollout history  deployment kubia-dm-v1 –revision=3<br>deployment.apps/kubia-dm-v1 with revision #3<br>Pod Template:<br>  Labels:    app=kubia<br>    pod-template-hash=6bb646bc8d<br>  Annotations:    kubernetes.io/change-cause: kubectl create –filename=deployment-kubia-v2.yaml –record=true<br>  Containers:<br>   nodejs:<br>    Image:    luksa/kubia:v3<br>    Port:    <none><br>    Host Port:    <none><br>    Environment:    <none><br>    Mounts:    <none><br>  Volumes:    <none><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 还记得创建deployment时添加的--record参数吗?</span><br><span class="line"></span><br><span class="line">##### 5. 回滚到一个特定的deployment版本</span><br><span class="line"></span><br><span class="line">有了上面的revision版本编号,可以回滚到一个特定的deployment版本,比如回滚到第一个版本</span><br><span class="line"></span><br><span class="line">命令: ```kubectl rollout undo deployment Deployment名 --to-revision=回滚版本</span><br></pre></td></tr></table></figure></none></none></none></none></none></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl rollout undo deployment kubia-dm-v1 --to-revision=1</span><br><span class="line">deployment.apps/kubia-dm-v1 rolled back</span><br></pre></td></tr></table></figure><p>现在全部是v1版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-fckvz</span><br><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-fckvz</span><br><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-lf4fv</span><br><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-lf4fv</span><br><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-fckvz</span><br><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-lf4fv</span><br><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-lf4fv</span><br><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-lf4fv</span><br></pre></td></tr></table></figure><p>再次查看回滚历史,发现revision版本4使用的是v2版本..所以再试一下回滚到v2版本也就是revision=4</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl rollout history  deployment kubia-dm-v1 --revision=4</span><br><span class="line">deployment.apps/kubia-dm-v1 with revision #4</span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:app=kubia</span><br><span class="line">pod-template-hash=6c99f46f5</span><br><span class="line">  Annotations:kubernetes.io/change-cause: kubectl create --filename=deployment-kubia-v2.yaml --record=true</span><br><span class="line">  Containers:</span><br><span class="line">   nodejs:</span><br><span class="line">    Image:luksa/kubia:v2</span><br><span class="line">    Port:&lt;none&gt;</span><br><span class="line">    Host Port:&lt;none&gt;</span><br><span class="line">    Environment:&lt;none&gt;</span><br><span class="line">    Mounts:&lt;none&gt;</span><br><span class="line">  Volumes:&lt;none&gt;</span><br></pre></td></tr></table></figure><p> 回滚到revision=4</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> [root@k8s-master ~]# kubectl rollout undo deployment kubia-dm-v1 --to-revision=4</span><br><span class="line">deployment.apps/kubia-dm-v1 rolled back</span><br></pre></td></tr></table></figure><p>于是,开始回滚到v2版本镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-fckvz</span><br><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-lf4fv</span><br><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-lf4fv</span><br><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-fckvz</span><br><span class="line">This is v2 running in pod kubia-dm-v1-6c99f46f5-458p7</span><br><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-fckvz</span><br><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-bmwx7</span><br><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-lf4fv</span><br><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-fckvz</span><br><span class="line">This is v1 running in pod kubia-dm-v1-74fb644f8-bmwx7</span><br><span class="line">This is v2 running in pod kubia-dm-v1-6c99f46f5-458p7</span><br><span class="line">This is v2 running in pod kubia-dm-v1-6c99f46f5-cvbfd</span><br></pre></td></tr></table></figure><h5 id="6-历史ReplicaSet版本记录"><a href="#6-历史ReplicaSet版本记录" class="headerlink" title="6. 历史ReplicaSet版本记录"></a>6. 历史ReplicaSet版本记录</h5><p>查看rs信息,可以发现保留了3个RS版本,一份是正在使用的(v2),一份是上一次回滚的版本(v1),还有一份是最早的版本(v3))</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get rs</span><br><span class="line">NAME                     DESIRED   CURRENT   READY   AGE</span><br><span class="line">kubia-dm-v1-6bb646bc8d   0         0         0       22m</span><br><span class="line">kubia-dm-v1-6c99f46f5    3         3         3       20h</span><br><span class="line">kubia-dm-v1-74fb644f8    0         0         0       20h</span><br></pre></td></tr></table></figure><p>通过回滚历史中的pod-template-hash的号码,可以定位到某个rs名字对应的Pod镜像版本以及rs对应的回滚版本</p><p>从下面的命令中可以看到6bb646bc8d的HASH值对应的是revision=3这个版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl rollout history deployment kubia-dm-v1 --revision=3</span><br><span class="line">deployment.apps/kubia-dm-v1 with revision #3</span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:app=kubia</span><br><span class="line">pod-template-hash=6bb646bc8d</span><br><span class="line">  Annotations:kubernetes.io/change-cause: kubectl create --filename=deployment-kubia-v2.yaml --record=true</span><br><span class="line">  Containers:</span><br><span class="line">   nodejs:</span><br><span class="line">    Image:luksa/kubia:v3</span><br><span class="line">    Port:&lt;none&gt;</span><br><span class="line">    Host Port:&lt;none&gt;</span><br><span class="line">    Environment:&lt;none&gt;</span><br><span class="line">    Mounts:&lt;none&gt;</span><br><span class="line">  Volumes:&lt;none&gt;</span><br></pre></td></tr></table></figure><blockquote><p>旧版本过多会导致ReplicaSet列表管理混乱,可以通过Deployment的revisionHistoryLimit属性来限制历史版本数量,默认是3</p></blockquote><hr><h3 id="4-控制滚动升级速率"><a href="#4-控制滚动升级速率" class="headerlink" title="4 控制滚动升级速率"></a>4 控制滚动升级速率</h3><p>在上面的例子中,进行deployment升级时会看到第一个新pod被创建(等待就绪后),然后旧pod被删除,然后又一个新pod被创建,然后又一个旧pod被删除,直到新pod创建完毕,旧pod全部删除完</p><p>在此期间有2个滚动策略的属性决定一次性替换多少个Pod.这就是maxSurge字段和maxUnavailable字段.</p><p>这2个字段的属性和用法可以通过下面的命令查看</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl explain deployment.spec.strategy.rollingUpdate</span><br></pre></td></tr></table></figure><ul><li><p>maxSurge: 该字段决定了deployment期望的副本之外,最多运行超出的Pod实例数量.默认为25%.也就是pod实例最多可以比期望数量多25%.也可以是一个绝对值.如果期望副本数是4.那么不会同时运行超过5个pod.</p></li><li><p>maxUnavailable: 该字段决定了在滚动升级期间,<strong>相对于期望副本数</strong>有多少个Pod实例可以处于不可用状态.默认值也是25%.所以可用的Pod实例数不能低于期望副本数的75%.也可以是一个绝对值.如果副本数是4,那么最多只允许有一个Pod处于不可用状态.</p></li></ul><blockquote><p>需要注意的是maxUnavailable是相对期望副本数而言的.并不是绝对值.例如期望副本数是3,maxSurge和maxUnavailable都设置成1.那么第一步,先删除1个旧Pod,然后创建2个新pod,此时一共4个Pod,但是由于新Pod还未就绪,所以一共只有2个旧pod提供服务..虽然maxUnavailable是1.但是有2个pod不可用.但是由于期望副本数是3.所以相对副本数来说,仍然是只有1个pod不可用</p></blockquote><h3 id="5-滚动升级暂停-金丝雀发布"><a href="#5-滚动升级暂停-金丝雀发布" class="headerlink" title="5.滚动升级暂停(金丝雀发布)"></a>5.滚动升级暂停(金丝雀发布)</h3><p>上一个v3版本出现了Bug,如果采用这种升级方式,势必会带来糟糕的体验.比较好的方式是采用金丝雀发布,在现有的版本之上运行一个新版本pod.并查看一小部分用户请求的情况.如果一旦没问题.就可以用新的pod替换所有的旧的pod</p><p>deploy可以使用暂停滚动升级来实现这一需求,在触发滚动升级刚开始几秒后,马上暂停滚动更新.下面用v4版本的镜像来演示这个例子.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl set image deployment kubia-dm-v1  nodejs=luksa/kubia:v4</span><br><span class="line">deployment.apps/kubia-dm-v1 image updated</span><br></pre></td></tr></table></figure><p>使用命令<figure class="highlight plain"><figcaption><span>rollout pause deployment deploymentName```暂停</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl rollout pause deployment kubia-dm-v1<br>deployment.apps/kubia-dm-v1 paused<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">此时增加了一个新版本的Pod,只有一个v4 pod正在运行,</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl get pods<br>NAME                           READY   STATUS    RESTARTS   AGE<br>kubia-dm-v1-6c99f46f5-458p7    1/1     Running   0          4h43m<br>kubia-dm-v1-6c99f46f5-cvbfd    1/1     Running   0          4h43m<br>kubia-dm-v1-6c99f46f5-q4lr6    1/1     Running   0          4h42m<br>kubia-dm-v1-8585b844df-5rnx7   1/1     Running   0          93s<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line">This is v2 running in pod kubia-dm-v1-6c99f46f5-458p7</span><br><span class="line">This is v2 running in pod kubia-dm-v1-6c99f46f5-cvbfd</span><br><span class="line">This is v2 running in pod kubia-dm-v1-6c99f46f5-cvbfd</span><br><span class="line">This is v4 running in pod kubia-dm-v1-8585b844df-5rnx7</span><br><span class="line">This is v2 running in pod kubia-dm-v1-6c99f46f5-cvbfd</span><br></pre></td></tr></table></figure></p><h4 id="恢复滚动升级"><a href="#恢复滚动升级" class="headerlink" title="恢复滚动升级"></a>恢复滚动升级</h4><p>一旦确认v4版本pod没有问题,就可以恢复升级了,使用命令<figure class="highlight plain"><figcaption><span>rollout resume deployment deploymentName```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl rollout resume deployment kubia-dm-v1<br>deployment.apps/kubia-dm-v1 resumed<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 在滚动升级的过程中,想要在一个确切的位置暂停滚动升级,目前还无法做到,目前想要实现金丝雀发布的正确方式是使用2个不同的deployment,并同时调整他们对应的pod数量</span><br><span class="line"></span><br><span class="line">&gt; 暂停滚动升级的另一个用处是,先暂停滚动升级,然后可以反复多次修改deployment配置文件,从而不会触发自动升级</span><br><span class="line"></span><br><span class="line">### 6. minReadySeconds用处</span><br><span class="line"></span><br><span class="line">之前演示过使用minReadySeconds参数.这个属性是指新创建的pod至少要成功运行多久后,才能视为可用状态,从而继续删除旧Pod,新建新pod.配合就绪探针的使用,当达到这个时间,且pod就绪探针正常时,pod才被标记就绪状态.如果探针返回失败,新的Pod运行出错,那么新版本的滚动升级将被阻止.</span><br><span class="line"></span><br><span class="line">使用这个属性可以减缓滚动升级的过程,同时能有效的组织升级继续进行.在生产中,需要设置为较高的值.以确保pod在开始接收流量后持续保持就绪状态</span><br><span class="line"></span><br><span class="line">----</span><br><span class="line"></span><br><span class="line">### 7.配合就绪探针阻止有Bug版本的滚动部署</span><br><span class="line"></span><br><span class="line">再次部署v3版本,但是这次配合就绪探针和minReadySeconds参数来演示阻止滚动升级.</span><br><span class="line"></span><br><span class="line">下面定义deployment-kubia-with-readinesscheck.yaml文件来更新deployment</span><br></pre></td></tr></table></figure></p><p>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: kubia-dm-v1<br>spec:<br>  replicas: 3</p><p>  #Pod就绪等待时间<br>  minReadySeconds: 10</p><p>  #滚动更新策略<br>  strategy:<br>    rollingUpdate:<br>       maxSurge: 1<br>       maxUnavailable: 0 #设置为0.确保pod在升级过程中被挨个替代<br>    type: RollingUpdate<br>  selector:<br>      matchLabels:<br>         app: kubia<br>  template:<br>    metadata:<br>      name: kubia<br>      labels:<br>        app: kubia<br>    spec:<br>      containers:</p><pre><code>- image: luksa/kubia:v3  name: nodejs  readinessProbe: #httpGet类型的就绪探针     periodSeconds: 1 #每秒探测一次     httpGet: #get访问8080端口的根路径        path: /        port: 8080</code></pre><hr><p>#一个yaml文件可以定义多种资源,中间用—隔开</p><p>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: kubia-dm-svc-v1</p><p>spec:<br>  type: NodePort<br>  selector:<br>    app: kubia</p><p>  ports:</p><ul><li>port: 80<br>targetPort: 8080<br>nodePort: 32014</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">更新deployment配置文件</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl apply -f deployment-kubia-with-readinesscheck.yaml<br>deployment.apps/kubia-dm-v1 configured<br>service/kubia-dm-svc-v1 unchanged<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">查看更新状态,发现一直停留在等待第一个Pod就绪</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl rollout status deployment kubia-dm-v1<br>Waiting for deployment “kubia-dm-v1” rollout to finish: 1 out of 3 new replicas have been updated…</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">查看pods状态,发现3个旧版本的pod继续运行,新版本的Pod还未就绪</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl get pods<br>NAME                           READY   STATUS    RESTARTS   AGE<br>kubia-dm-v1-645b94954b-2gkw9   0/1     Running   0          49s<br>kubia-dm-v1-8585b844df-5rnx7   1/1     Running   0          53m<br>kubia-dm-v1-8585b844df-sz65w   1/1     Running   0          2m21s<br>kubia-dm-v1-8585b844df-ztd7x   1/1     Running   0          2m32s<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 因为maxUnavailable属性设置为0,所以不会删除任何旧版本的Pod.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">默认情况下,在10分钟之内不能完成滚动升级的话,此次滚动升级被视为失败.</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl rollout status deployment kubia-dm-v1<br>Waiting for deployment “kubia-dm-v1” rollout to finish: 1 out of 3 new replicas have been updated…<br>error: deployment “kubia-dm-v1” exceeded its progress deadline<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">通过```kubectl describe deploy kubia-dm-v1```可以查看deployment状态.ProgressDeadlineExceeded表示升级超时</span><br></pre></td></tr></table></figure></p><p>Conditions:<br>  Type           Status  Reason</p><hr><p>  Available      True    MinimumReplicasAvailable<br>  Progressing    False   ProgressDeadlineExceeded</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&gt; 可以通过设定deployment.sepc.progressDeadlineSeconds来指定超时时间</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##### 取消出错版本的滚动升级</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl rollout undo deployment kubia-dm-v1<br>deployment.apps/kubia-dm-v1 rolled back<br>[root@k8s-master ~]#<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## 命令总结</span><br></pre></td></tr></table></figure></p><p>#手动升级ReplicationController<br>kubectl rolling-update 当前rc名 新rc名 –image=新镜像名</p><p>#创建deployment资源,记录版本信息<br>kubectl create -f deployment配置文件 –record</p><p>#查看deployment滚动升级状态<br>kubectl rollout status deployment deployment名字</p><p>#修改deployment资源配置<br>kubectl patch deployment deployment名字 -p ‘{}’</p><p>#修改deployment配置的镜像信息<br>kubectl set image deployment deployment名字 镜像名=新镜像</p><p>#deployment回滚到上一个版本.在滚动升级超时时,用于取消滚动升级任务<br>kubectl rollout undo deployment deployment名字</p><p>#显示deployment回滚历史<br>kubectl rollout history deployment deployment名字</p><p>#deployment回滚到指定版本<br>kubectl rollout undo deployment deployment名字 –to-revision=版本号</p><p>#deployment暂停滚动升级<br>kubectl rollout pause deployment deployment名字</p><p>#deployment恢复滚动升级<br>kubectl rollout resume deployment deployment名字</p><p>#等待pod就绪时间<br>minReadySeconds</p><p><code>`</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kuberentes-Deployment&quot;&gt;&lt;a href=&quot;#kuberentes-Deployment&quot; class=&quot;headerlink&quot; title=&quot;kuberentes Deployment&quot;&gt;&lt;/a&gt;kuberentes Deployment&lt;/h2&gt;&lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;之前学习过使用ReplicationController和ReplicaSet实现托管和更新pod容器.&lt;br&gt;客户端通过Service服务访问RC或者RS托管的一组pod.这也是Kubernetes典型的应用程序运行方式&lt;/p&gt;
&lt;p&gt;假设这个时候应用程序需要更新,从v1版本更新到v2,有以下两种方式更新pod镜像:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;直接删除所有现有的pod,然后创建新的pod&lt;/li&gt;
&lt;li&gt;可以先创建新的Pod,并等待他们运行成功后,再删除旧的.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这2中方法各有优缺点.第一种方法将导致服务在端在时间内不可用.第二种方法要求应用程序同时支持2个版本对外提供服务.&lt;/p&gt;
&lt;p&gt;本文具体介绍kubernetes上述两种更新方式.然后介绍通过手动方式,replicatController的更新方式.最后再引入deployment机制的用法介绍&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>docker安装最新版Kong(v1.0)+konga</title>
    <link href="https://jesse.top/2020/06/26/Linux-Web/docker%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0%E7%89%88Kong(v1.0)%20konga/"/>
    <id>https://jesse.top/2020/06/26/Linux-Web/docker安装最新版Kong(v1.0) konga/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T07:32:53.722Z</updated>
    
    <content type="html"><![CDATA[<h3 id="docker安装最新版Kong-v1-0-konga"><a href="#docker安装最新版Kong-v1-0-konga" class="headerlink" title="docker安装最新版Kong(v1.0)+konga"></a>docker安装最新版Kong(v1.0)+konga</h3><p>参考以下文档:</p><p><a href="https://docs.konghq.com/install/docker/?_ga=2.167535422.1288669860.1553147426-917309945.1539077269" target="_blank" rel="noopener">Kong installation</a></p><p><a href="https://github.com/pantsel/konga#installation" target="_blank" rel="noopener">konga github</a></p><hr><h4 id="docker安装kong-postgresql"><a href="#docker安装kong-postgresql" class="headerlink" title="docker安装kong+postgresql"></a>docker安装kong+postgresql</h4><p>1.创建一个docker网络用于docker,postgresql和konga容器间通信</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network create kong-net</span><br></pre></td></tr></table></figure><p>2.启动posgtresql容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name kong-database \</span><br><span class="line">               --network=kong-net \</span><br><span class="line">               -p 5432:5432 \</span><br><span class="line">               -e &quot;POSTGRES_USER=kong&quot; \</span><br><span class="line">               -e &quot;POSTGRES_DB=kong&quot; \</span><br><span class="line">               postgres:9.6</span><br></pre></td></tr></table></figure><p>3.初始化postgresql数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm \</span><br><span class="line">    --network=kong-net \</span><br><span class="line">    -e &quot;KONG_DATABASE=postgres&quot; \</span><br><span class="line">    -e &quot;KONG_PG_HOST=kong-database&quot; \</span><br><span class="line">    -e &quot;KONG_CASSANDRA_CONTACT_POINTS=kong-database&quot; \</span><br><span class="line">    kong:latest kong migrations bootstrap</span><br></pre></td></tr></table></figure><blockquote><p>注意两点:</p><p>1.最好是先删除本地的kong镜像.因为本地的Kong:lastest镜像不一定就是最新版</p><p>2.如果本地的kong:latest镜像地域0.15版本,则不支持bootstrap命令.可以将bootstrap命令替换成up</p></blockquote><a id="more"></a><p>4.启动kong容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name kong \</span><br><span class="line">     --network=kong-net \</span><br><span class="line">     -v /data/logs/kong:/var/log/kong \</span><br><span class="line">     -v /data/apps/kong/plugins/:/usr/local/share/lua/5.1/kong/plugins/ \</span><br><span class="line">     -e &quot;KONG_DATABASE=postgres&quot; \</span><br><span class="line">     -e &quot;KONG_PG_HOST=kong-database&quot; \</span><br><span class="line">     -e &quot;KONG_CASSANDRA_CONTACT_POINTS=kong-database&quot; \</span><br><span class="line">     -e &quot;KONG_PROXY_ACCESS_LOG=/var/log/kong/access.log&quot; \</span><br><span class="line">     -e &quot;KONG_ADMIN_ACCESS_LOG=/var/log/kong/admin_access.log&quot; \</span><br><span class="line">     -e &quot;KONG_PROXY_ERROR_LOG=/var/log/kong/error.log&quot; \</span><br><span class="line">     -e &quot;KONG_ADMIN_ERROR_LOG=/var/log/kong/admin_error.log&quot; \</span><br><span class="line">     -e &quot;KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl&quot; \</span><br><span class="line">     -p 8000:8000 \</span><br><span class="line">     -p 8443:8443 \</span><br><span class="line">     -p 8001:8001 \</span><br><span class="line">     -p 8444:8444 \</span><br><span class="line">     kong:latest</span><br></pre></td></tr></table></figure><blockquote><p>这里我映射了kong的插件目录和日志目录. </p></blockquote><ul><li>注意:要先吧kong容器里的/usr/local/share/lua/5.1/kong/plugins/目录下内容复制到宿主机的/data/apps/kong/plugins/目录下.否则宿主机的空目录会覆盖容器的插件目录,导致容器无法启动.</li></ul><p>kong容器目录拷贝到宿主机方法如下:</p><p>1.先不挂载目录启动kong容器</p><p>2.执行命令拷贝kong容器的/usr/local/share/lua/5.1/kong/plugins/ 目录到宿主机/data/apps/kong/plugins/目录下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[work@docker ~]$docker cp kong:/usr/local/share/lua/5.1/kong/plugins/  /data/apps/kong/plugins/</span><br></pre></td></tr></table></figure><blockquote><p>如果不需要将容器的kong插件目录映射到宿主机的话,这一步可以不需要做</p></blockquote><p>容器已经成功启动:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[work@docker conf.d]$docker ps | grep -E &quot;kong|postgre&quot;</span><br><span class="line">10fc881cca4b        kong:latest                                 &quot;/docker-entrypoin...&quot;   About an hour ago   Up About an hour    0.0.0.0:8000-8001-&gt;8000-8001/tcp, 0.0.0.0:8443-8444-&gt;8443-8444/tcp                                             kong</span><br><span class="line"></span><br><span class="line">afd1487e29a0        postgres:9.6                                &quot;docker-entrypoint...&quot;   3 hours ago         Up 3 hours          0.0.0.0:5432-&gt;5432/tcp                                                                                         kong-database</span><br></pre></td></tr></table></figure><hr><h4 id="安装konga"><a href="#安装konga" class="headerlink" title="安装konga"></a>安装konga</h4><p>konga是管理kong的一个dashboard界面.</p><p>1.先初始化数据库.这里也是用后端的postgresql数据库.官方命令如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm pantsel/konga:latest -c prepare -a &#123;&#123;adapter&#125;&#125; -u &#123;&#123;connection-uri&#125;&#125;</span><br></pre></td></tr></table></figure><table><thead><tr><th>argument</th><th>description</th><th>default</th></tr></thead><tbody><tr><td>-c</td><td>command</td><td>-</td></tr><tr><td>-a</td><td>adapter (can be <code>postgres</code> or <code>mysql</code>)</td><td>-</td></tr><tr><td>-u</td><td>full database connection url</td></tr></tbody></table><p>执行以下命令,初始化数据库:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm pantsel/konga:latest -c prepare -a postgres -u  postgres://kong@10.0.0.250:5432/konga</span><br></pre></td></tr></table></figure><blockquote><p>这里稍微有点疑问的是数据库的connection url ..完整的connection url地址是: postgres://user:password@host:port/konga</p><p> postgres:<a href="mailto://kong@10.0.0.250" target="_blank" rel="noopener">//kong@10.0.0.250</a>:5432/konga —— 这里kong代表用户名,由于没有密码所以没有指定密码.10.0.0.250是postgresql的host主机名.konga表示初始化一个数据库</p></blockquote><p>执行结果如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[work@docker ~]$docker run --rm pantsel/konga:latest -c prepare -a postgres -u  postgres://kong@10.0.0.250:5432/konga</span><br><span class="line">debug: Preparing database...</span><br><span class="line">Using postgres DB Adapter.</span><br><span class="line">Database `konga` does not exist. Creating...</span><br><span class="line">Database `konga` created! Continue...</span><br><span class="line">debug: Hook:api_health_checks:process() called</span><br><span class="line">debug: Hook:health_checks:process() called</span><br><span class="line">debug: Hook:start-scheduled-snapshots:process() called</span><br><span class="line">debug: Hook:upstream_health_checks:process() called</span><br><span class="line">debug: Hook:user_events_hook:process() called</span><br><span class="line">debug: Seeding User...</span><br><span class="line">debug: User seed planted</span><br><span class="line">debug: Seeding Kongnode...</span><br><span class="line">debug: Kongnode seed planted</span><br><span class="line">debug: Seeding Emailtransport...</span><br><span class="line">debug: Emailtransport seed planted</span><br><span class="line">debug: Database migrations completed!</span><br></pre></td></tr></table></figure><ol start="2"><li>启动konga</li></ol><p>命令格式如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -p 1337:1337 </span><br><span class="line">             --network &#123;&#123;kong-network&#125;&#125; \ // optional</span><br><span class="line">             -e &quot;TOKEN_SECRET=&#123;&#123;somerandomstring&#125;&#125;&quot; \</span><br><span class="line">             -e &quot;DB_ADAPTER=the-name-of-the-adapter&quot; \ // &apos;mongo&apos;,&apos;postgres&apos;,&apos;sqlserver&apos;  or &apos;mysql&apos;</span><br><span class="line">             -e &quot;DB_HOST=your-db-hostname&quot; \</span><br><span class="line">             -e &quot;DB_PORT=your-db-port&quot; \ // Defaults to the default db port</span><br><span class="line">             -e &quot;DB_USER=your-db-user&quot; \ // Omit if not relevant</span><br><span class="line">             -e &quot;DB_PASSWORD=your-db-password&quot; \ // Omit if not relevant</span><br><span class="line">             -e &quot;DB_DATABASE=your-db-name&quot; \ // Defaults to &apos;konga_database&apos;</span><br><span class="line">             -e &quot;DB_PG_SCHEMA=my-schema&quot;\ // Optionally define a schema when integrating with prostgres</span><br><span class="line">             -e &quot;NODE_ENV=production&quot; \ // or &apos;development&apos; | defaults to &apos;development&apos;</span><br><span class="line">             --name konga \</span><br><span class="line">             pantsel/konga</span><br></pre></td></tr></table></figure><p>执行如下命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 1337:1337 -d \</span><br><span class="line">             --network=kong-net \</span><br><span class="line">             -e &quot;DB_ADAPTER=postgres&quot; \</span><br><span class="line">             -e &quot;DB_HOST=10.0.0.250&quot; \</span><br><span class="line">             -e &quot;DB_PORT=5432&quot; \</span><br><span class="line">             -e &quot;DB_USER=kong&quot; \</span><br><span class="line">             -e &quot;DB_DATABASE=konga&quot; \</span><br><span class="line">             -e &quot;NODE_ENV=production&quot; \</span><br><span class="line">             --name konga \</span><br><span class="line">             pantsel/konga</span><br></pre></td></tr></table></figure><p>容器成功启动:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beb70407b417        pantsel/konga                               &quot;/app/start.sh&quot;          2 hours ago         Up 2 hours          0.0.0.0:1337-&gt;1337/tcp                                                                                         konga</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;docker安装最新版Kong-v1-0-konga&quot;&gt;&lt;a href=&quot;#docker安装最新版Kong-v1-0-konga&quot; class=&quot;headerlink&quot; title=&quot;docker安装最新版Kong(v1.0)+konga&quot;&gt;&lt;/a&gt;docker安装最新版Kong(v1.0)+konga&lt;/h3&gt;&lt;p&gt;参考以下文档:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.konghq.com/install/docker/?_ga=2.167535422.1288669860.1553147426-917309945.1539077269&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kong installation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/pantsel/konga#installation&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;konga github&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&quot;docker安装kong-postgresql&quot;&gt;&lt;a href=&quot;#docker安装kong-postgresql&quot; class=&quot;headerlink&quot; title=&quot;docker安装kong+postgresql&quot;&gt;&lt;/a&gt;docker安装kong+postgresql&lt;/h4&gt;&lt;p&gt;1.创建一个docker网络用于docker,postgresql和konga容器间通信&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker network create kong-net&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;2.启动posgtresql容器&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;docker run -d --name kong-database \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               --network=kong-net \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               -p 5432:5432 \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               -e &amp;quot;POSTGRES_USER=kong&amp;quot; \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               -e &amp;quot;POSTGRES_DB=kong&amp;quot; \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               postgres:9.6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;3.初始化postgresql数据库&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ docker run --rm \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    --network=kong-net \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    -e &amp;quot;KONG_DATABASE=postgres&amp;quot; \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    -e &amp;quot;KONG_PG_HOST=kong-database&amp;quot; \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    -e &amp;quot;KONG_CASSANDRA_CONTACT_POINTS=kong-database&amp;quot; \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    kong:latest kong migrations bootstrap&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;注意两点:&lt;/p&gt;
&lt;p&gt;1.最好是先删除本地的kong镜像.因为本地的Kong:lastest镜像不一定就是最新版&lt;/p&gt;
&lt;p&gt;2.如果本地的kong:latest镜像地域0.15版本,则不支持bootstrap命令.可以将bootstrap命令替换成up&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Linux-Web" scheme="https://jesse.top/categories/Linux-Web/"/>
    
      <category term="kong" scheme="https://jesse.top/categories/Linux-Web/kong/"/>
    
    
      <category term="kong" scheme="https://jesse.top/tags/kong/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes 高级调度</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/kubernetes%20%E9%AB%98%E7%BA%A7%E8%B0%83%E5%BA%A6/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/kubernetes 高级调度/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-29T14:41:57.002Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kubernetes-高级调度"><a href="#kubernetes-高级调度" class="headerlink" title="kubernetes 高级调度"></a>kubernetes 高级调度</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>本章主要介绍pod的高级调度功能.主要涵盖2方面调度特性:</p><ul><li>节点污染和pod容忍度</li><li>节点亲缘性</li></ul><hr><h3 id="一-节点污染和pod容忍度"><a href="#一-节点污染和pod容忍度" class="headerlink" title="一.节点污染和pod容忍度"></a>一.节点污染和pod容忍度</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p><strong>节点污点</strong>: 该特性用于限制哪些Pod可以被调度到某一个节点.</p><p><strong>pod容忍度</strong>:当一个Pod容忍某个节点的污点,这个pod才能被调度到该节点</p><p>默认情况下k8s集群中的master主节点就设置了污点,.这样才能保证只有控制面板等系统组件才能部署在主节点上.应用pod只能被调度到工作节点</p><hr><h4 id="显示节点污点信息"><a href="#显示节点污点信息" class="headerlink" title="显示节点污点信息"></a>显示节点污点信息</h4><a id="more"></a><p>通过<code>kubectl describe node</code>可以查看节点的污点信息.例如下面是master节点污点信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl describe node k8s-master</span></span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoSchedule</span><br></pre></td></tr></table></figure><p>master节点包含一个污点(taints).污点包含一个key,value,effect.格式为:<key>=<value>:<effect></effect></value></key></p><p>上面的污点信息包含一个<code>node-role.kubernetes.io/master</code>的key.一个空的value,以及值为<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">这个污点将阻止pod调度到这个节点上,除非有pod能容忍这个污点.通常能容忍这个污点的Pod都是kubernetes系统级别的Pod.</span><br><span class="line"></span><br><span class="line">例如观察一个Kube-system名称空间下的coredns系统级别的pod信息:</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl describe po coredns-7f9c544f75-9sh28 -n kube-system</span><br><span class="line"></span><br><span class="line">Tolerations:     CriticalAddonsOnly</span><br><span class="line">                 node-role.kubernetes.io/master:NoSchedule</span><br><span class="line">                 node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br></pre></td></tr></table></figure></p><p>该pod容忍度(tolerations)包含了4个容忍度,其中<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##### 污点效果</span><br><span class="line"></span><br><span class="line">上面列出的pod展示了2种污点effect(污点效果):```NoSchedule```和```NoExecute```.每一个污点都可以关联一个effect.下面介绍各污点效果.</span><br><span class="line"></span><br><span class="line">* **NoSechedule**: 如果pod没有容忍这些污点,这pod不能被调度到包含这个污点的节点</span><br><span class="line">* **PreferNoSechedule** NoSechedule的宽松版本,表示尽量阻止pod被调度到这个节点.但是如果没有其他节点可以调度,pod依然会被调度到这个节点</span><br><span class="line">* **NoExecute** 上面两者只在调度期间起作用,而NoExecute也会影响正在节点上运行中的pod.如果在一个节点上添加了NoExecute污点.那么在这个节点上正在运行的pod如果没有容忍这个污点,会被这个节点驱除</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">#### 添加自定义污点</span><br><span class="line"></span><br><span class="line">如果一个单独K8s集群上同时有生产环境和非生产环境的应用.那么可以在生产环境上添加自定义污点来防止其他环境的Pod调度到生产节点.</span><br><span class="line"></span><br><span class="line">添加污点命令:```kubectl taint</span><br></pre></td></tr></table></figure></p><p>命令格式: <figure class="highlight plain"><figcaption><span>taint node NodeName key</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">例如下面命令将node1的节点添加一个key为```node-type```.value为```production```.效果为```NoSchedule```的污点</span><br></pre></td></tr></table></figure></p><p>root@k8s-master ~]# kubectl taint node k8s-node1 node-type=production:NoSchedule<br>node/k8s-node1 tainted<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">下面部署一个常规的多副本pod.此时所有pod都被调度到k8s-node2节点上.无法调度到node1</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl run test --image busybox --replicas 5 -- sleep 999</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl get pods -o wide  </span><br><span class="line">test-69c6778cfb-2jgbn           1/1     Running   0          4m38s   10.100.169.188   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">test-69c6778cfb-7lbnw           1/1     Running   0          4m38s   10.100.169.187   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">test-69c6778cfb-c4vrx           1/1     Running   0          4m38s   10.100.169.190   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">test-69c6778cfb-grkhc           1/1     Running   0          4m38s   10.100.169.185   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">test-69c6778cfb-xc2sp           1/1     Running   0          4m38s   10.100.169.130   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure></p><hr><h4 id="在pod上添加污点容忍度"><a href="#在pod上添加污点容忍度" class="headerlink" title="在pod上添加污点容忍度"></a>在pod上添加污点容忍度</h4><p>如果想让pod部署到node1这个已经打了<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```yaml</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: prod</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">       node-type: production</span><br><span class="line">  replicas: 5</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">       name: taint-kubia-v1</span><br><span class="line">       labels:</span><br><span class="line">         node-type: production</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">           - image: busybox</span><br><span class="line">             command: [&quot;sleep&quot;]</span><br><span class="line">             args: [&quot;999999&quot;]</span><br><span class="line">             name: kubia-busybox</span><br><span class="line">      tolerations:</span><br><span class="line">        - key: node-type</span><br><span class="line">          operator: Equal</span><br><span class="line">          value: production</span><br><span class="line">          effect: NoSchedule</span><br></pre></td></tr></table></figure></p><p>在pod的<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl get pods -o wide</span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE    IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">prod-59b4554db6-d24d5           1/1     Running   0          24s    10.100.169.191   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-59b4554db6-kmzj2           1/1     Running   0          24s    10.100.36.114    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-59b4554db6-nlc8s           1/1     Running   0          24s    10.100.36.116    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-59b4554db6-pvzrw           1/1     Running   0          24s    10.100.36.115    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-59b4554db6-t8n5c           1/1     Running   0          24s    10.100.169.131   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure></p><hr><h4 id="在node节点添加污点-驱逐正在该节点上运行的pod"><a href="#在node节点添加污点-驱逐正在该节点上运行的pod" class="headerlink" title="在node节点添加污点,驱逐正在该节点上运行的pod"></a>在node节点添加污点,驱逐正在该节点上运行的pod</h4><p>当前在node2节点上运行了以下pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide  | grep k8s-node2</span></span><br><span class="line">cloud-busybox-99c65b774-6bbbc   1/1     Running   80         3d8h    10.100.169.183   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">cloud-busybox-99c65b774-zcw78   1/1     Running   80         3d8h    10.100.169.182   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">curl-custome-sa                 2/2     Running   0          8d      10.100.169.177   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-59b4554db6-d24d5           1/1     Running   4          9m49s   10.100.169.191   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-59b4554db6-t8n5c           1/1     Running   4          9m49s   10.100.169.131   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-1          1/1     Running   0          10d     10.100.169.172   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-2jgbn           1/1     Running   80         22h     10.100.169.188   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-7lbnw           1/1     Running   80         22h     10.100.169.187   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-c4vrx           1/1     Running   80         22h     10.100.169.190   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-grkhc           1/1     Running   80         22h     10.100.169.185   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-xc2sp           1/1     Running   80         22h     10.100.169.130   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>如果想要在node2节点上打上污点信息,只允许non-production环境的pod才能调度到该节点上.可以直接打上污点标签</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl taint node k8s-node2 node-type=non-production:NoExecute</span></span><br><span class="line">node/k8s-node2 tainted</span><br><span class="line"></span><br><span class="line"><span class="comment">#此时该节点上的所有pod都被终止</span></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide  | grep k8s-node2</span></span><br><span class="line">cloud-busybox-99c65b774-6bbbc   1/1     Terminating         80         3d8h   10.100.169.183   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">cloud-busybox-99c65b774-zcw78   1/1     Terminating         80         3d8h   10.100.169.182   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">curl-custome-sa                 2/2     Terminating         0          8d     10.100.169.177   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-1          1/1     Terminating         0          10d    10.100.169.172   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-2jgbn           1/1     Terminating         80         22h    10.100.169.188   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-7lbnw           1/1     Terminating         80         22h    10.100.169.187   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-c4vrx           1/1     Terminating         80         22h    10.100.169.190   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-grkhc           1/1     Terminating         80         22h    10.100.169.185   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"><span class="built_in">test</span>-69c6778cfb-xc2sp           1/1     Terminating         80         22h    10.100.169.130   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">#所有pod都被调度到node1节点.(注意有一个运行在Node2上的statefulset类型的Pod被终止了,由于statefulset的特性,该pod不会被自动调度到其他节点,除非手动强制删除)</span></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide</span></span><br><span class="line">NAME                            READY   STATUS        RESTARTS   AGE     IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">cloud-busybox-99c65b774-j5ggn   0/1     Pending       0          144m    &lt;none&gt;           &lt;none&gt;      &lt;none&gt;           &lt;none&gt;</span><br><span class="line">cloud-busybox-99c65b774-ksj8x   1/1     Running       82         3d10h   10.100.36.103    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">cloud-busybox-99c65b774-qtzv9   0/1     Pending       0          144m    &lt;none&gt;           &lt;none&gt;      &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-6d8fdb7654-9xlqq           1/1     Running       0          7m23s   10.100.36.109    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-6d8fdb7654-cwsfp           1/1     Running       0          7m23s   10.100.36.108    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-6d8fdb7654-gbc79           1/1     Running       0          7m10s   10.100.36.119    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-6d8fdb7654-lsjc4           1/1     Running       0          7m15s   10.100.36.110    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">prod-6d8fdb7654-mcmt7           1/1     Running       0          7m23s   10.100.36.111    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-0          1/1     Running       0          10d     10.100.36.101    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-1          0/1     Terminating   0          10d     10.100.169.172   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-2          1/1     Running       0          10d     10.100.36.97     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-3          1/1     Running       0          10d     10.100.36.99     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><hr><h4 id="了解污点和污点容忍度使用场景"><a href="#了解污点和污点容忍度使用场景" class="headerlink" title="了解污点和污点容忍度使用场景"></a>了解污点和污点容忍度使用场景</h4><p>​       节点可以拥有多个污点信息,而pod也可以有多个污点容忍度.污点容忍度可以通过操作符<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 二.节点亲缘性(node affinity)</span><br><span class="line"></span><br><span class="line">污点可以让pod远离特定的节点.节点亲缘性是一种更新的机制.这种机制允许Kubernetes将pod只调度到某个节点上面</span><br><span class="line"></span><br><span class="line">与节点选择器类似,每个pod可以定义自己的节点亲缘性规则,这些规则可以允许你指定硬性限制或者偏好.如果指定一种偏好的话,你将告知Kubernetes对于某个特定的pod.它更倾向于调度到某些节点上.如果没法实现的话,pod将被调度到其他节点.</span><br><span class="line"></span><br><span class="line">##### 检查默认node节点标签:</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl describe node k8s-node2</span><br><span class="line">Name:               k8s-node2</span><br><span class="line">Roles:              &lt;none&gt;</span><br><span class="line">#Labels表示该节点拥有的默认的标签.</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/arch=amd64</span><br><span class="line">                    kubernetes.io/hostname=k8s-node2</span><br><span class="line">                    kubernetes.io/os=linux</span><br><span class="line">Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock</span><br><span class="line">                    node.alpha.kubernetes.io/ttl: 0</span><br><span class="line">                    projectcalico.org/IPv4Address: 172.16.20.253/24</span><br><span class="line">                    projectcalico.org/IPv4IPIPTunnelAddr: 10.100.169.128</span><br><span class="line">                    volumes.kubernetes.io/controller-managed-attach-detach: true</span><br></pre></td></tr></table></figure></p><h5 id="给节点打一个标签"><a href="#给节点打一个标签" class="headerlink" title="给节点打一个标签."></a>给节点打一个标签.</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#给node1节点打一个标签.disk=ssd</span><br><span class="line">[root@k8s-master ~]# kubectl label node k8s-node1 disk=ssd</span><br></pre></td></tr></table></figure><h5 id="回顾nodeSelector节点选择器"><a href="#回顾nodeSelector节点选择器" class="headerlink" title="回顾nodeSelector节点选择器"></a>回顾nodeSelector节点选择器</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下面是一个deployment的配置文件,Pod指定了标签选择器</span></span><br><span class="line"><span class="string">[root@k8s-master</span> <span class="string">~]#</span> <span class="string">cat</span> <span class="string">node-affinity-nodeslector.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kubia-nodeselector-ssd</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">4</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">     matchLabels:</span></span><br><span class="line"><span class="attr">        app:</span>  <span class="string">kubia-nodeselector</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">kubia-nodeselector-ssd</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">         app:</span>  <span class="string">kubia-nodeselector</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      nodeSelector:</span></span><br><span class="line"><span class="attr">         disk:</span> <span class="string">ssd</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - image:</span> <span class="string">luksa/kubia</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">kubia-nodeselector-ssd</span></span><br></pre></td></tr></table></figure><p>所有节点都调度到符合这个标签的node1节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide | grep kubia-nodeselector-ssd</span></span><br><span class="line">kubia-nodeselector-ssd-7d5b94f9bd-cqmsv   1/1     Running       0          2m2s    10.100.36.124    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-nodeselector-ssd-7d5b94f9bd-djkxr   1/1     Running       0          2m2s    10.100.36.125    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-nodeselector-ssd-7d5b94f9bd-htggw   1/1     Running       0          2m2s    10.100.36.126    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-nodeselector-ssd-7d5b94f9bd-vm2rv   1/1     Running       0          2m2s    10.100.36.127    k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><hr><h4 id="节点亲缘性配置"><a href="#节点亲缘性配置" class="headerlink" title="节点亲缘性配置"></a>节点亲缘性配置</h4><p>将上面的节点标签选择器换成用节点亲缘性来表达</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-master</span> <span class="string">~]#</span> <span class="string">cat</span>  <span class="string">node-affinity-disk-ssd.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">kubia-nodeaffinity-ssd</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">4</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">     matchLabels:</span></span><br><span class="line"><span class="attr">        app:</span>  <span class="string">kubia-nodeaffinity</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">kubia-nodeaffinity</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">         app:</span>  <span class="string">kubia-nodeaffinity</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        nodeAffinity:</span></span><br><span class="line"><span class="attr">          requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line"><span class="attr">             nodeSelectorTerms:</span></span><br><span class="line"><span class="attr">               - matchExpressions:</span></span><br><span class="line"><span class="attr">                    - key:</span> <span class="string">disk</span></span><br><span class="line"><span class="attr">                      operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">                      values:</span></span><br><span class="line"><span class="bullet">                         -</span> <span class="string">ssd</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - image:</span> <span class="string">luksa/kubia</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">kubia-nodeselector-ssd</span></span><br></pre></td></tr></table></figure><p>第一印象是节点亲缘性写法比节点选择器要复杂的多(这是因为节点亲缘性的表达能力更强).</p><h5 id="节点亲缘性属性"><a href="#节点亲缘性属性" class="headerlink" title="节点亲缘性属性"></a>节点亲缘性属性</h5><p>pod的<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* **nodeAffinity** 顾名思义,节点亲缘性调度规则</span><br><span class="line">* **podAffinity**  Pod亲缘性调度规则,例如某类节点调度到同一个节点,地域,可用区</span><br><span class="line">* **podAntiAffinity** 和podAffinity完全相反,pod反亲缘性调度规则,</span><br><span class="line"></span><br><span class="line">在这个例子中石油了**nodeAffinity**表示节点亲缘性规则.在这个规则下有一个极其长的属性.把这个属性的名字分成两段,然后分别看下他们的含义:</span><br><span class="line"></span><br><span class="line">* **requiredDuringScheduling** 表明该字段下定义的规则.为了让pod能够调度到该节点,明确指出该节点必须包含的标签.重点包含以下2个条件.也就是说在pod调度期间,节点必须要包含标签:</span><br><span class="line">  - **required**(必须),</span><br><span class="line">  - **DuringScheduling**(在pod调度期间)</span><br><span class="line">* ...**IgnoredDuringExecution** 表示该规则不会影响已经在节点上运行的pod.重点包含以下2个条件.也就是说无论节点是否具备某个标签,或者无论pod怎么调度,都不会影响已经运行的Pod</span><br><span class="line">  - **Ignored**(忽略)</span><br><span class="line">  - **DuringExecution**(在pod已运行期间)</span><br><span class="line"></span><br><span class="line">接下来的字段属性比较容易理解,</span><br><span class="line"></span><br><span class="line">* **nodeSelectorTerms**: 对象类型,表示一组节点选择器项列表</span><br><span class="line">* **matchExpressions**: 匹配表达式,也是一个对象类型.包含以下属性</span><br><span class="line">  - **key**: 必要字段.</span><br><span class="line">  - **operator**: 必要字段,key和value关系的表达式,有**In**,**NotIn**,**Exists**,**DoesNotExist**,**Gt**,**Lt**</span><br><span class="line">  - **values**: key的值,是一个字符串的数组格式.</span><br><span class="line"></span><br><span class="line">&gt; 通过```kubectl explain pods.spec.affinity```可以查询属性字段的含义以及配置用法</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### pod调度节点优先</span><br><span class="line"></span><br><span class="line">节点亲缘性还有一个属性```preferredDuringSchedulingIgnoredDuringExecution```这个属性字段指定调度器优先考虑哪些节点.</span><br><span class="line"></span><br><span class="line">想象一下如果你在阿里云同一地域下的多个可用区都有部署节点服务器,你想要将Pod优先部署在zone1,并且是部署在专有服务器上.如果zone1下没有足够的服务器资源,那么部署到其他可用区(zone2)也是可以接受的.那么节点亲缘性就可以实现这种功能.</span><br><span class="line"></span><br><span class="line">##### 给节点加上可用区等信息标签</span><br><span class="line"></span><br><span class="line">下面的配置中给2个节点加上标签,模拟他们在不同的可用区服务器,以及属于不同的服务器类型(专用和共享)</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">[root@k8s-master ~]# kubectl label node k8s-node1 zone=zone1</span><br><span class="line">node/k8s-node1 labeled</span><br><span class="line">[root@k8s-master ~]# kubectl label node k8s-node1 share-type=dedicated</span><br><span class="line">node/k8s-node1 labeled</span><br><span class="line">[root@k8s-master ~]# kubectl label node k8s-node2 share-type=shared</span><br><span class="line">node/k8s-node2 labeled</span><br><span class="line">[root@k8s-master ~]# kubectl label node k8s-node2 zone=zone2</span><br><span class="line">node/k8s-node2 labeled</span><br><span class="line">[root@k8s-master ~]#</span><br></pre></td></tr></table></figure></p><h5 id="指定优先级节点亲缘性规则"><a href="#指定优先级节点亲缘性规则" class="headerlink" title="指定优先级节点亲缘性规则"></a>指定优先级节点亲缘性规则</h5><p>下面创建一个deployment资源,pod节点指定了优先调度到zone1区域(权重80),次优先调度到dedicated服务器(权重20)</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">node-affinity-weight</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">10</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">       app:</span> <span class="string">node-affinity-weight</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">node-affinity-weight</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">         app:</span> <span class="string">node-affinity-weight</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - image:</span> <span class="string">luksa/kubia</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">kubia</span></span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line"><span class="attr">        nodeAffinity:</span></span><br><span class="line">          <span class="comment">#优先调度到某个节点,注意不是必须要调度到该节点</span></span><br><span class="line"><span class="attr">          preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">             <span class="comment">#权重越高,优先级越高,优先调度到zone1</span></span><br><span class="line"><span class="attr">              - weight:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">                preference:</span></span><br><span class="line"><span class="attr">                  matchExpressions:</span></span><br><span class="line"><span class="attr">                   - key:</span> <span class="string">zone</span></span><br><span class="line"><span class="attr">                     operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">                     values:</span></span><br><span class="line"><span class="bullet">                        -</span> <span class="string">zone1</span></span><br><span class="line"></span><br><span class="line">             <span class="comment">#如果以上需求不能满足,则调度到满足以下标签的节点</span></span><br><span class="line"><span class="attr">              - weight:</span> <span class="number">20</span></span><br><span class="line"><span class="attr">                preference:</span></span><br><span class="line"><span class="attr">                  matchExpressions:</span></span><br><span class="line"><span class="attr">                    - key:</span> <span class="string">share-type</span></span><br><span class="line"><span class="attr">                      operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">                      values:</span></span><br><span class="line"><span class="bullet">                        -</span> <span class="string">dedicated</span></span><br></pre></td></tr></table></figure><p>此时大部分Pod都优先部署到node1节点.之所以不是所有Pod都被调度到Node1节点是因为除了节点亲缘性优先级外还有其他的优先级函数来决定节点被调度到哪.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide</span></span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">node-affinity-weight-7777f464d6-8h9qq   1/1     Running   0          33s   10.100.36.75     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-8xchv   1/1     Running   0          33s   10.100.169.132   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-bnsvs   1/1     Running   0          33s   10.100.36.79     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-bxbpb   1/1     Running   0          33s   10.100.36.71     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-bznfw   1/1     Running   0          33s   10.100.36.78     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-cgdfq   1/1     Running   0          33s   10.100.36.77     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-flvcf   1/1     Running   0          33s   10.100.169.135   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-pb666   1/1     Running   0          33s   10.100.169.134   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-tqd5k   1/1     Running   0          33s   10.100.36.74     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-affinity-weight-7777f464d6-vpknb   1/1     Running   0          33s   10.100.36.76     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><hr><h3 id="pod硬亲缘性"><a href="#pod硬亲缘性" class="headerlink" title="pod硬亲缘性"></a>pod硬亲缘性</h3><p>上一节已经了解节点亲缘性规则是如何影响Pod被调度到哪个节点的.这些规则只影响了Pod和节点之前的亲缘性.但是有时候也希望有能力指定pod自身之间的亲缘性.</p><p>例如,有一个前端pod,和一个后端Pod,我们期望将前端Pod部署到和后端pod同一个节点,降低前后端pod之间的访问延时,提高应用性能.使用Pod亲缘性可以满足这种场景的需求.</p><blockquote><p> pod亲缘性和节点亲缘性区别是.pod亲缘性不关心pod被调度到具体哪个节点,而是只要求被调度到其他某组pod同一个节点上.</p></blockquote><p>在下面的例子中部署一个1后端pod.和5个前端pod.使用节点亲缘性将5个前端Pod部署到后端pod相同的节点上</p><h5 id="部署一个后端pod"><a href="#部署一个后端pod" class="headerlink" title="部署一个后端pod"></a>部署一个后端pod</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#启动一个backend的pod.指定一个标签</span><br><span class="line">[root@k8s-master ~]# kubectl run backend -l app=backend --image busybox -- sleep 999999</span><br></pre></td></tr></table></figure><h5 id="部署前端Pod亲缘性"><a href="#部署前端Pod亲缘性" class="headerlink" title="部署前端Pod亲缘性"></a>部署前端Pod亲缘性</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">pod-affinity</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">       app:</span> <span class="string">frontend</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">luksa/kubia</span></span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line">        <span class="comment">#节点亲缘性</span></span><br><span class="line"><span class="attr">        podAffinity:</span></span><br><span class="line">         <span class="comment">#一个required强制性要求而不是prefered</span></span><br><span class="line"><span class="attr">          requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">               <span class="comment">#此pod必须被调度到满足以下条件的节点.该节点运行了app:backend标签的Pod</span></span><br><span class="line"><span class="attr">             - topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">               <span class="comment">#满足以下标签选择器的pod</span></span><br><span class="line"><span class="attr">               labelSelector:</span></span><br><span class="line"><span class="attr">                  matchLabels:</span></span><br><span class="line"><span class="attr">                      app:</span> <span class="string">backend</span></span><br></pre></td></tr></table></figure><p>代码清单显示要求将pod调度到和其他包含app=backend标签的Pod所在的相同节点上(通过topologyKey字段指定)</p><blockquote><p>除了使用matchLabels字段外,也可以使用表达能力更强的matchExpressions字段</p></blockquote><p>此时所有的前端pod都被部署到后端Pod同一个节点,没有被部署到其他节点(node1)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide</span></span><br><span class="line">NAME                            READY   STATUS    RESTARTS   AGE     IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">backend-7d4c66b6b5-4mhpl        1/1     Running   0          43m     10.100.169.133   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-694cf9455c-6h2dq   1/1     Running   0          2m46s   10.100.169.139   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-694cf9455c-jtmd8   1/1     Running   0          2m46s   10.100.169.140   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-694cf9455c-p6m7d   1/1     Running   0          2m46s   10.100.169.141   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-694cf9455c-qbr4c   1/1     Running   0          2m46s   10.100.169.137   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-694cf9455c-vfp84   1/1     Running   0          2m46s   10.100.169.138   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><blockquote><p>有趣的是,如果现在删除了后端Pod,调度器会将后端pod还是调度到node2.即便后端pod没有定义任何Pod亲缘性规则.</p><p>这种设置很合理,因为如何后端Pod被调度到其他节点,前端pod的亲缘性规则就被打破了</p></blockquote><h5 id="topologyKey"><a href="#topologyKey" class="headerlink" title="topologyKey"></a>topologyKey</h5><p>在上面的例子中topology属性为kubernetes.io/hostname.除此之外还有其他属性:</p><ul><li><strong>failure-domain.beta.kubernetes.io/zone</strong> 云服务提供商不同的不同可用区</li><li><strong>failure-domain.beta.kubernetes.io/region</strong> 云服务提供商不同的不同地域</li><li>自定义键</li></ul><hr><h3 id="pod软亲缘性"><a href="#pod软亲缘性" class="headerlink" title="pod软亲缘性"></a>pod软亲缘性</h3><p>pod软亲缘性稍微的区别就是没有硬性要求一定要部署到某个节点.拿前面一个例子来说,软亲缘性只是优先将前端pod调度到和后端Pod相同的节点,但是如果条件不满足要求,也可以调度到其他节点</p><p>软亲缘性的配置文件和硬亲缘性的主要区别就在于.<strong>required</strong>开头的属性是硬性要求.<strong>preferred</strong>是软要求</p><p>下面将之前的前端pod的deployment删除,然后部署下面的软podAffinity</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">pod-affinity-frontend-preferred</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">       app:</span> <span class="string">frontend</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">     metadata:</span></span><br><span class="line"><span class="attr">       name:</span> <span class="string">pod-affinity-frontend-preferred</span></span><br><span class="line"><span class="attr">       labels:</span></span><br><span class="line"><span class="attr">         app:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">     spec:</span></span><br><span class="line"><span class="attr">       containers:</span></span><br><span class="line"><span class="attr">         - name:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">           image:</span> <span class="string">luksa/kubia</span></span><br><span class="line"><span class="attr">       affinity:</span></span><br><span class="line"><span class="attr">          podAffinity:</span></span><br><span class="line"><span class="attr">             preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line"><span class="attr">                - weight:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">                  podAffinityTerm:</span></span><br><span class="line"><span class="attr">                      topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line"><span class="attr">                      labelSelector:</span></span><br><span class="line"><span class="attr">                        matchExpressions:</span></span><br><span class="line"><span class="attr">                            - key:</span> <span class="string">app</span></span><br><span class="line"><span class="attr">                              operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">                              values:</span></span><br><span class="line"><span class="bullet">                                -</span> <span class="string">backend</span></span><br></pre></td></tr></table></figure><p>此时看到几乎所有的前端pod都部署到和后端Pod相同的节点上.但是其中还是有一个pod被部署到node1.</p><p>正如<strong>nodeAffinity</strong>的软亲缘性一样.kubernetes不会将所有的Pod都完全按照要求部署到同一个节点,还有其他优先调度函数在起作用,将pod调度到其他节点.</p><p>这样的设置是有道理的,因为Kubernets考虑到如果该节点出现宕机故障会导致整个应用不可用.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide</span></span><br><span class="line">NAME                                               READY   STATUS    RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">backend-7d4c66b6b5-hvkph                           1/1     Running   0          41m   10.100.169.142   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-frontend-preferred-7b7b9fb784-4jc9r   1/1     Running   0          17s   10.100.36.80     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-frontend-preferred-7b7b9fb784-bd97q   1/1     Running   0          17s   10.100.169.146   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-frontend-preferred-7b7b9fb784-rfgp7   1/1     Running   0          17s   10.100.169.145   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-frontend-preferred-7b7b9fb784-s7qtc   1/1     Running   0          17s   10.100.169.143   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-affinity-frontend-preferred-7b7b9fb784-skhhq   1/1     Running   0          17s   10.100.169.144   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><hr><h3 id="pod非亲缘性"><a href="#pod非亲缘性" class="headerlink" title="pod非亲缘性"></a>pod非亲缘性</h3><p>和pod亲缘性完全相反,pod非亲缘性让两组pod远离彼此,不被调度到同一个节点.这种场景比较常见.例如当2个集合的pod调度到同一个节点上会影响彼此的性能.</p><p>pod非亲缘性和pod亲缘性配置几乎一模一样,唯一的区别就是podAntiAffinity替代podAffinity</p><p>删除之前的例子,使用下面的Pod配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">pod-antiaffinity</span></span><br><span class="line"></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">5</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">       app:</span> <span class="string">frontend</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">luksa/kubia</span></span><br><span class="line"><span class="attr">      affinity:</span></span><br><span class="line">        <span class="comment">#节点亲缘性</span></span><br><span class="line"><span class="attr">        podAntiAffinity:</span></span><br><span class="line">         <span class="comment">#一个required强制性要求而不是prefered</span></span><br><span class="line"><span class="attr">          requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">               <span class="comment">#此pod必须不被调度到运行了app:backend标签Pod的节点</span></span><br><span class="line"><span class="attr">             - topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line">               <span class="comment">#满足以下标签选择器的pod</span></span><br><span class="line"><span class="attr">               labelSelector:</span></span><br><span class="line"><span class="attr">                  matchLabels:</span></span><br><span class="line"><span class="attr">                      app:</span> <span class="string">backend</span></span><br></pre></td></tr></table></figure><p>前端的所有pod都被调度到和后端pod不一样的节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide</span></span><br><span class="line">NAME                                READY   STATUS    RESTARTS   AGE    IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">backend-7d4c66b6b5-hvkph            1/1     Running   0          168m   10.100.169.142   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-antiaffinity-5b9fd9846c-k5flw   1/1     Running   0          61s    10.100.36.81     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-antiaffinity-5b9fd9846c-t4l8z   1/1     Running   0          61s    10.100.36.89     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-antiaffinity-5b9fd9846c-wp5mf   1/1     Running   0          61s    10.100.36.83     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-antiaffinity-5b9fd9846c-xkqhm   1/1     Running   0          61s    10.100.36.82     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-antiaffinity-5b9fd9846c-xzg7t   1/1     Running   0          61s    10.100.36.84     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>pod软非亲缘性就不再介绍了</p><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>如果节点上添加了1个污点信息.除非Pod容忍这些污点,否则pod不会被调度到该节点</li><li>有3种类型的污点:<code>Noschdule</code>完全阻止pod调度,<code>PreferNoSchedule</code>不完全阻止,<code>NoExecute</code>将已经在运行的Pod从节点驱逐</li><li><code>NoExecute</code>污点的节点可以设置等待时间,当节点不可用时,pod重新调度时,最长等待时间</li><li>节点亲缘性允许指定pod应该被调度到哪些节点.有硬性(<strong>required</strong>)要求,也有软性优先级(<strong>preferred</strong>)</li><li>pod亲缘性用于将pod调度到和另一个pod相同的一个节点.(基于pod的标签)</li><li>pod亲缘性的topologyKey属性表示了被调度的pod和另一组pod的距离.(在同一个节点,同一个机柜,同一个可用区,或者同一个地域)</li><li>pod非亲缘性和亲缘性相反,用于将pod调度到远离某组pod的节点</li><li>无论是节点亲缘性还是pod亲缘性可以设置是硬性要求还是软性优选选择</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kubernetes-高级调度&quot;&gt;&lt;a href=&quot;#kubernetes-高级调度&quot; class=&quot;headerlink&quot; title=&quot;kubernetes 高级调度&quot;&gt;&lt;/a&gt;kubernetes 高级调度&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;p&gt;本章主要介绍pod的高级调度功能.主要涵盖2方面调度特性:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;节点污染和pod容忍度&lt;/li&gt;
&lt;li&gt;节点亲缘性&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&quot;一-节点污染和pod容忍度&quot;&gt;&lt;a href=&quot;#一-节点污染和pod容忍度&quot; class=&quot;headerlink&quot; title=&quot;一.节点污染和pod容忍度&quot;&gt;&lt;/a&gt;一.节点污染和pod容忍度&lt;/h3&gt;&lt;h4 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;节点污点&lt;/strong&gt;: 该特性用于限制哪些Pod可以被调度到某一个节点.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;pod容忍度&lt;/strong&gt;:当一个Pod容忍某个节点的污点,这个pod才能被调度到该节点&lt;/p&gt;
&lt;p&gt;默认情况下k8s集群中的master主节点就设置了污点,.这样才能保证只有控制面板等系统组件才能部署在主节点上.应用pod只能被调度到工作节点&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&quot;显示节点污点信息&quot;&gt;&lt;a href=&quot;#显示节点污点信息&quot; class=&quot;headerlink&quot; title=&quot;显示节点污点信息&quot;&gt;&lt;/a&gt;显示节点污点信息&lt;/h4&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>kubernets探针</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/kubernetes%E6%8E%A2%E9%92%88/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/kubernetes探针/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T10:05:50.556Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kubernets探针"><a href="#kubernets探针" class="headerlink" title="kubernets探针"></a>kubernets探针</h2><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>kubernetes一共有2种探针:</p><p><strong>存活探针</strong></p><p><strong>就绪探针</strong></p><hr><h2 id="就绪探针"><a href="#就绪探针" class="headerlink" title="就绪探针"></a>就绪探针</h2><h3 id="就绪探针工作介绍"><a href="#就绪探针工作介绍" class="headerlink" title="就绪探针工作介绍"></a>就绪探针工作介绍</h3><p>就绪探针会定期调用,检查特定的pod是否准备就绪接收客户端的请求.当容器启动时,会等待一个时间,然后执行第一次准备就绪检查.如果某个Pod没有通过探针探测,则会从服务中删除该pod,如果pod再次准备就绪,则会重新添加到Service</p><p>就绪探针确保客户端只与正常的Pod交互.</p><hr><h3 id="就绪探针的类型"><a href="#就绪探针的类型" class="headerlink" title="就绪探针的类型"></a>就绪探针的类型</h3><ul><li>Exec探针 . 使用command命令,如果命令结果返回0,则说明pod准备就绪.</li><li>HTTP GET探针. 向容器发送HTTP GET.通过响应状态码判断pod容器是否准备好</li><li>TCP socket探针.尝试连接Pod容器的TCP端口.判断pod容器的某个端口是否正常工作</li></ul><hr><a id="more"></a><h3 id="向pod容器添加探针"><a href="#向pod容器添加探针" class="headerlink" title="向pod容器添加探针"></a>向pod容器添加探针</h3><p>编辑kubia的rc配置文件,在spec.containers下添加readinessProbe配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ReplicationController</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia</span><br><span class="line">spec:</span><br><span class="line">  replicas: 4</span><br><span class="line">  selector:</span><br><span class="line">      app: kubia</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: kubia</span><br><span class="line"></span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - name: kubia</span><br><span class="line">          image: luksa/kubia</span><br><span class="line">          ports:</span><br><span class="line">          - containerPort: 8080</span><br><span class="line">          readinessProbe:</span><br><span class="line">            exec: #存活探针类型</span><br><span class="line">              command: #探针命令</span><br><span class="line">                - ls</span><br><span class="line">                - /var/ready</span><br></pre></td></tr></table></figure><blockquote><p>重新编辑rc文件后,存活探针并不会对已经存在的Pod生效.删除所有pod,等待rc重新创建</p></blockquote><p>让yaml文件生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl apply -f kubia-rc.yaml</span><br><span class="line">replicationcontroller/kubia unchanged</span><br></pre></td></tr></table></figure><p>删除Pods,重新创建后,查看pods的状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get pods</span><br><span class="line">NAME                READY   STATUS    RESTARTS   AGE</span><br><span class="line">kubia-6gvhg         0/1     Running   0          61s</span><br><span class="line">kubia-qlpdt         0/1     Running   0          61s</span><br><span class="line">kubia-w8c82         0/1     Running   0          61s</span><br><span class="line">kubia-w9spj         0/1     Running   0          61s</span><br></pre></td></tr></table></figure><p>pods的READY是0.表示pod容器虽然正在运行中,但是未准备就绪</p><p>向其中一个pod容器创建/var/ready文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl exec kubia-6gvhg -- touch /var/ready</span><br></pre></td></tr></table></figure><p>但是容器并不会马上就绪.这是因为pod默认每隔10s探测一次.通过<figure class="highlight plain"><figcaption><span>describe pod kubia-6gvhg```可以看到就绪探针的策略</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl describe pods kubia-6gvhg</p><p>Readiness:      exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1 #failure=3<br>Warning  Unhealthy  2m53s (x30 over 7m43s)  kubelet, k8s-node2  Readiness probe failed: ls: cannot access /var/ready: No such file or directory<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">10s后,可以看到就只有一个pod准备就绪</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl get pods<br>NAME                READY   STATUS    RESTARTS   AGE<br>kubia-6gvhg         1/1     Running   0          9m45s<br>kubia-qlpdt         0/1     Running   0          9m45s<br>kubia-w8c82         0/1     Running   0          9m45s<br>kubia-w9spj         0/1     Running   0          9m45s<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">通过测试发现,客户端的请求永远只转发到已经准备就绪的Pod容器内,而其他容器则不接受请求</span><br></pre></td></tr></table></figure></p><p>[work@k8s-node1 ~]$ while true;do curl <a href="http://10.96.170.37;sleep" target="_blank" rel="noopener">http://10.96.170.37;sleep</a> 1;done<br>You’ve hit kubia-6gvhg<br>You’ve hit kubia-6gvhg<br>You’ve hit kubia-6gvhg<br>You’ve hit kubia-6gvhg<br>You’ve hit kubia-6gvhg<br>You’ve hit kubia-6gvhg<br>You’ve hit kubia-6gvhg<br><code>`</code></p><p>其他两种探测类型的是否方法也类似</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kubernets探针&quot;&gt;&lt;a href=&quot;#kubernets探针&quot; class=&quot;headerlink&quot; title=&quot;kubernets探针&quot;&gt;&lt;/a&gt;kubernets探针&lt;/h2&gt;&lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;kubernetes一共有2种探针:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;存活探针&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;就绪探针&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;就绪探针&quot;&gt;&lt;a href=&quot;#就绪探针&quot; class=&quot;headerlink&quot; title=&quot;就绪探针&quot;&gt;&lt;/a&gt;就绪探针&lt;/h2&gt;&lt;h3 id=&quot;就绪探针工作介绍&quot;&gt;&lt;a href=&quot;#就绪探针工作介绍&quot; class=&quot;headerlink&quot; title=&quot;就绪探针工作介绍&quot;&gt;&lt;/a&gt;就绪探针工作介绍&lt;/h3&gt;&lt;p&gt;就绪探针会定期调用,检查特定的pod是否准备就绪接收客户端的请求.当容器启动时,会等待一个时间,然后执行第一次准备就绪检查.如果某个Pod没有通过探针探测,则会从服务中删除该pod,如果pod再次准备就绪,则会重新添加到Service&lt;/p&gt;
&lt;p&gt;就绪探针确保客户端只与正常的Pod交互.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;就绪探针的类型&quot;&gt;&lt;a href=&quot;#就绪探针的类型&quot; class=&quot;headerlink&quot; title=&quot;就绪探针的类型&quot;&gt;&lt;/a&gt;就绪探针的类型&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Exec探针 . 使用command命令,如果命令结果返回0,则说明pod准备就绪.&lt;/li&gt;
&lt;li&gt;HTTP GET探针. 向容器发送HTTP GET.通过响应状态码判断pod容器是否准备好&lt;/li&gt;
&lt;li&gt;TCP socket探针.尝试连接Pod容器的TCP端口.判断pod容器的某个端口是否正常工作&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes headless Service</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/kubernetes%20headless%20Service/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/kubernetes headless Service/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T08:48:46.557Z</updated>
    
    <content type="html"><![CDATA[<hr><h3 id="kubernetes-headless-Service"><a href="#kubernetes-headless-Service" class="headerlink" title="kubernetes headless Service"></a>kubernetes headless Service</h3><p>​     我们以前学习过,Service是Kubernetes项目中用来将一组Pod暴露给外界访问的一种机制.外部客户端通过Service地址可以随机访问到某个具体的Pod</p><p>​     之前学过几种Service类型,包括nodeport,loadbalancer等等.所有这类Service都有一个VIP(虚拟IP),访问Service VIP,Service会将请求转发到后端的Pod上,</p><p>​     还有一种Service是Headless Service(无头服务),这类Service自身不需要VIP,当DNS解析该Service时,会解析出Service后端的Pod地址.这样设置的好处是Kubernetes项目为Pod分配唯一的”可解析身份”,只要知道一个pod的名字和对应的Headless Service名字,就可以通过这条DNS访问到后端的Pod</p><a id="more"></a><hr><h3 id="持久存储"><a href="#持久存储" class="headerlink" title="持久存储"></a>持久存储</h3><p>我们知道通过headless Service使Pod有一个稳定的网络标识,那么存储呢?有状态的应用必须有自己独立的存储,即便这个pod被删除,新创建出来的pod(新pod与旧pod拥有相同的网络表示)也必须挂载相同的存储.</p><p>​      之前在学习kubernetes的存储时,我们学习过PV,PVC存储卷,通过pod模板关联一个持久卷声明就可以为pod提供一个持久卷存储.因为持久卷声明(PVC)和持久卷(PV)是一对一关系.但是之前接触过的ReplicationController,ReplicaSet,Deployment等资源创建的pod是同一个模板创建的,所以共享的是同一个持久卷存储.而StatefulSet要求每个pod都需要有独立的持久卷声明和存储.所以StatefulSet要求关联到一个或多个不同的持久卷声明模板.这些持久卷声明会在pod创建之前准备就绪,并且关联到每个pod中.</p><h3 id="持久卷的创建和删除"><a href="#持久卷的创建和删除" class="headerlink" title="持久卷的创建和删除"></a>持久卷的创建和删除</h3><p>​      扩容一个StatefulSet副本时,会创建2个或者多个对象: pod实例已经与之关联的一个或者多个持久卷声明.但是当StatefulSet缩容时,只会删除一个Pod,而留下持久卷声明.这就意味着删除Pod时,与pod关联的持久卷存储数据并不会被删除.如果持久卷声明被手动删除,那么持久卷上的数据则会消失.</p><p>​     因为缩容会保留持久卷声明,所以在随后的扩容操作中,新的pod实例会使用绑定在持久卷上相同的声明和其上的数据.所以如果因为误操作而缩容一个StatefulSet副本后,可以做一次扩容操作,新的pod实例会运行到与之前完全一致的状态,甚至连pod名字也是一样的</p><hr><h3 id="部署StatefulSet应用"><a href="#部署StatefulSet应用" class="headerlink" title="部署StatefulSet应用"></a>部署StatefulSet应用</h3><p>部署StatefulSet应用之前,需要创建几个不同类型的对象.</p><ul><li><p>一个演示用的docker镜像</p></li><li><p>存储数据文件的持久卷(PV)</p></li><li><p>一个Headless Service服务实例</p></li><li><p>Statefulset模板</p></li></ul><h4 id="准备一个docker镜像"><a href="#准备一个docker镜像" class="headerlink" title="准备一个docker镜像"></a>准备一个docker镜像</h4><p>这里使用书上提供的luksa/kubia-pet镜像,这个镜像是一个Node应用,当应用接收到一个POST请求时,将请求中的body写入到某个文件,当接收到一个GET请求时,返回pod主机名以及改文件中的内容.</p><h4 id="创建持久化存储卷-pv"><a href="#创建持久化存储卷-pv" class="headerlink" title="创建持久化存储卷(pv)"></a>创建持久化存储卷(pv)</h4><p>因为稍后会调度StatefulSet创建3个副本.所以这里需要3个持久卷.如果计划调度更多的副本,则需要创建更多的持久卷..</p><p>之前在学习存储知识的时候介绍过存储卷,所以具体不演示,以下是创建3个PV持久卷的配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-master</span> <span class="string">~]#</span> <span class="string">cat</span> <span class="string">statefulset-kubia-pv.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="comment">#创建一个List列表资源,List的items下列出每个PV的配置</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">List</span></span><br><span class="line"><span class="attr">items:</span></span><br><span class="line"><span class="attr">  - apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">    kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">pv-1</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">       capacity:</span></span><br><span class="line"><span class="attr">         storage:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">       accessModes:</span></span><br><span class="line"><span class="bullet">         -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">       persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span></span><br><span class="line"><span class="attr">       storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">       nfs:</span></span><br><span class="line"><span class="attr">         path:</span> <span class="string">/data/k8s/pv-1</span></span><br><span class="line"><span class="attr">         server:</span> <span class="number">172.16</span><span class="number">.20</span><span class="number">.1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  - apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">    kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">pv-2</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">       capacity:</span></span><br><span class="line"><span class="attr">         storage:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">       accessModes:</span></span><br><span class="line"><span class="bullet">         -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">       persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span></span><br><span class="line"><span class="attr">       storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">       nfs:</span></span><br><span class="line"><span class="attr">         path:</span> <span class="string">/data/k8s/pv-2</span></span><br><span class="line"><span class="attr">         server:</span> <span class="number">172.16</span><span class="number">.20</span><span class="number">.1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  - apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">    kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">pv-3</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">       capacity:</span></span><br><span class="line"><span class="attr">         storage:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">       accessModes:</span></span><br><span class="line"><span class="bullet">         -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">       persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span></span><br><span class="line"><span class="attr">       storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">       nfs:</span></span><br><span class="line"><span class="attr">         path:</span> <span class="string">/data/k8s/pv-3</span></span><br><span class="line"><span class="attr">         server:</span> <span class="number">172.16</span><span class="number">.20</span><span class="number">.1</span></span><br></pre></td></tr></table></figure><blockquote><p>以前接触过在yaml文件中添加—3个横杠使的在一个文件中可以区分定义多个资源,这次定义一个List对象,然后把各个资源作为List对象的各个项目.这2种方法均可以在一个YAML文件中定义多个资源</p></blockquote><p>现在已经定义了个3个底层的PV持久卷</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get pv</span><br><span class="line">NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE</span><br><span class="line">mypv1   1Gi        RWO,ROX        Recycle          Available           nfs                     12d</span><br><span class="line">pv-1    1Gi        RWO            Recycle          Available           nfs                     4s</span><br><span class="line">pv-2    1Gi        RWO            Recycle          Available           nfs                     4s</span><br><span class="line">pv-3    1Gi        RWO            Recycle          Available           nfs                     4s</span><br></pre></td></tr></table></figure><hr><h3 id="创建Headless-Service"><a href="#创建Headless-Service" class="headerlink" title="创建Headless Service"></a>创建Headless Service</h3><p>下面是headless service的配置文件,唯一需要注意的是该类型服务的clusterIP属性必须为None</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">statefulset-kubia-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  clusterIP:</span> <span class="string">None</span></span><br><span class="line">  <span class="comment">#所有标签为statefulset-kubia的Pod都属于这个Service</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">statefulset-kubia</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">      port:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p>创建服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get svc</span><br><span class="line">NAME                    TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">statefulset-kubia-svc   ClusterIP      None            &lt;none&gt;        80/TCP         3s</span><br></pre></td></tr></table></figure><hr><h3 id="创建Statefuleset"><a href="#创建Statefuleset" class="headerlink" title="创建Statefuleset"></a>创建Statefuleset</h3><p>​      statefulset资源的配置和RS,deployment等没有太大的区别,这里使用了一个新的组件volumeClaimTemplates.其中定义了一个持久卷声明.该组件会为每个Pod创建一个独立的持久卷声明.</p><p>​      这个组件是在statefulset资源的spec全局对象下,虽然在pod的template模板中并没有创建持久卷声明(而是直接通过volumeMounts属性来挂在).但是Statefulset在创建时,会自动将volumeClaimTemplate定义的持久卷声明关联到pod中.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-master</span> <span class="string">~]#</span> <span class="string">cat</span> <span class="string">statefulset-kubia.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">statefulset-kubia-v1</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  serviceName:</span> <span class="string">statefulset-kubia-v1</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">       app:</span> <span class="string">statefulset-kubia</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">statefulset-kubia</span></span><br><span class="line"></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">statefulset-kubia</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">luksa/kubia-pet</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">            containerPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">data</span></span><br><span class="line"><span class="attr">              mountPath:</span> <span class="string">/var/data</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  volumeClaimTemplates:</span></span><br><span class="line"><span class="attr">       - metadata:</span></span><br><span class="line"><span class="attr">           name:</span> <span class="string">data</span></span><br><span class="line"><span class="attr">         spec:</span></span><br><span class="line"><span class="attr">           resources:</span></span><br><span class="line"><span class="attr">              requests:</span></span><br><span class="line"><span class="attr">                 storage:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">           storageClassName:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">           accessModes:</span></span><br><span class="line"><span class="bullet">             -</span> <span class="string">ReadWriteOnce</span></span><br></pre></td></tr></table></figure><blockquote><p>注意:volumeClaimTemplates组件一定要声明存储类型storageClassName,如果没有声明这一点则Pod一直处于Pending状态.并且会有以下报错信息</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl describe po statefulset-kubia-v1-0</span><br><span class="line"></span><br><span class="line">  Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  error while running &quot;VolumeBinding&quot; filter plugin for pod &quot;statefulset-kubia-v1-0&quot;: pod has unbound immediate PersistentVolumeClaims</span><br></pre></td></tr></table></figure><p>查看PVC提示没有找到PV</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl describe pvc data-statefulset-kubia-v1-0</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason         Age                  From                         Message</span><br><span class="line">  ----    ------         ----                 ----                         -------</span><br><span class="line">  Normal  FailedBinding  55s (x182 over 45m)  persistentvolume-controller  no persistent volumes available for this claim and no storage class is set</span><br></pre></td></tr></table></figure><p>   创建statefulset资源,列出pod资源.和rs,rc,deployment不同的是,他们会一次性创建完所有的pod,而statefulset会在每一个pod完全就绪后,才会创建第二个.</p><p>​    statefulset这样做是因为:状态明确的集群应用对同事有2个集群成员启动引起的竞争情况是非常敏感的.所以依次启动每个成员是比较安全可靠的.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">[root@k8s-master</span> <span class="string">~]#</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">pods</span></span><br><span class="line"><span class="string">NAME</span>                     <span class="string">READY</span>   <span class="string">STATUS</span>    <span class="string">RESTARTS</span>   <span class="string">AGE</span></span><br><span class="line"><span class="string">statefulset-kubia-v1-0</span>   <span class="number">0</span><span class="string">/1</span>     <span class="string">Pending</span>   <span class="number">0</span>          <span class="number">74</span><span class="string">s</span></span><br></pre></td></tr></table></figure><hr><p>现在3个Pod副本都已经被创建完成.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get pods</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Running   0          12m</span><br><span class="line">statefulset-kubia-v1-1   1/1     Running   0          12m</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running   0          12m</span><br></pre></td></tr></table></figure><p>statefulset自动创建了3个PVC,并且各自与3个pv自动关联</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pvc</span></span><br><span class="line">NAME                          STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">data-statefulset-kubia-v1-0   Bound    pv-2     1Gi        RWO            nfs            11m</span><br><span class="line">data-statefulset-kubia-v1-1   Bound    pv-3     1Gi        RWO            nfs            11m</span><br><span class="line">data-statefulset-kubia-v1-2   Bound    pv-1     1Gi        RWO            nfs            11m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pv</span></span><br><span class="line">NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                 STORAGECLASS   REASON   AGE</span><br><span class="line">pv-1   1Gi        RWO            Recycle          Bound    default/data-statefulset-kubia-v1-2   nfs                     5h18m</span><br><span class="line">pv-2   1Gi        RWO            Recycle          Bound    default/data-statefulset-kubia-v1-0   nfs                     5h18m</span><br><span class="line">pv-3   1Gi        RWO            Recycle          Bound    default/data-statefulset-kubia-v1-1   nfs                     5h18m</span><br><span class="line">[root@k8s-master ~]<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>​       和RS,RC,Deployment等资源不同的是,Statefulset部署的pod名称并非是随机的,而是pod模板名加上一个序号,这个序号从0开始,依次增加.</p><p>​       PVC的名称格式是PVC的名字+pod名.每个pod自动创建一个PVC,并且该PVC自动关联到一个后端的PV持久卷</p><hr><h3 id="访问POD"><a href="#访问POD" class="headerlink" title="访问POD"></a>访问POD</h3><p>​     由于创建的Service类型是Headless service模式,所以不能通过它来访问pod,而是需要直接连接到每个后端单独的pod.(或者是创建一个普通的Service,但是这样也不允许访问指定的pod)</p><p>​    这次介绍如何通过API服务器与pod通信.API服务器可以通过代理直接连接到指定的pod.可以通过如下的URL</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;apiServerHost&gt;:&lt;port&gt;/api/v1/namespaces/default/pods/pods名称/proxy/&lt;path&gt;</span><br></pre></td></tr></table></figure><p> 在k8s的master节点运行下面命令,下面命令运行一个kubectl proxy.从而可以让proxy去API服务器通信,而不必使用麻烦的授权和SSL证书来直接与API服务器通信</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl proxy</span><br></pre></td></tr></table></figure><p>现在就可以直接访问Pod了.开启另一个master服务器终端.通过curl访问某个Pod.比如访问statefulset-kubia-v1-0这个Pod容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-0/proxy/</span><br><span class="line">You&apos;ve hit statefulset-kubia-v1-0</span><br><span class="line">Data stored on this pod: No data posted yet</span><br></pre></td></tr></table></figure><p>这种访问方式经过了2层的中间代理:</p><h5 id="1-curl命令发送给kubectl-proxy"><a href="#1-curl命令发送给kubectl-proxy" class="headerlink" title="1.curl命令发送给kubectl proxy"></a>1.curl命令发送给kubectl proxy</h5><h5 id="2-kubectl-proxy-带上认证TOKEN转发给API服务器"><a href="#2-kubectl-proxy-带上认证TOKEN转发给API服务器" class="headerlink" title="2. kubectl proxy 带上认证TOKEN转发给API服务器"></a>2. kubectl proxy 带上认证TOKEN转发给API服务器</h5><h5 id="3-API服务器再通过pod容器的实际IP地址将请求转发到后端的Pod"><a href="#3-API服务器再通过pod容器的实际IP地址将请求转发到后端的Pod" class="headerlink" title="3. API服务器再通过pod容器的实际IP地址将请求转发到后端的Pod"></a>3. API服务器再通过pod容器的实际IP地址将请求转发到后端的Pod</h5><p>下面是发送一个post请求到statefulset-kubia-v1-0的例子</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# curl -X POST -d &quot;Hey There ! This greeting was submitted to statefulset-kubia-v1-0&quot; \</span><br><span class="line">&gt; localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-0/proxy/</span><br><span class="line">Data stored on pod statefulset-kubia-v1-0</span><br><span class="line"></span><br><span class="line">#再次用GET请求,就可以返回刚才POST提交的数据</span><br><span class="line">[root@k8s-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-0/proxy/</span><br><span class="line">You&apos;ve hit statefulset-kubia-v1-0</span><br><span class="line">Data stored on this pod: Hey There ! This greeting was submitted to statefulset-kubia-v1-0</span><br></pre></td></tr></table></figure><p>当我们访问其他的pod容器时,并没有返回写入的数据,这和期望的一致,说明每个节点都有各自独立的存储状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-1/proxy/</span><br><span class="line">You&apos;ve hit statefulset-kubia-v1-1</span><br><span class="line">Data stored on this pod: No data posted yet</span><br></pre></td></tr></table></figure><hr><h3 id="删除pod-重新调度"><a href="#删除pod-重新调度" class="headerlink" title="删除pod,重新调度"></a>删除pod,重新调度</h3><p>之前我们在statefulset-kubia-v1-0这个pod节点写入了一条数据,这次我们删除这个Pod,等它被重新调度,然后检查它是否还会返回与之前一致的数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl delete po statefulset-kubia-v1-0</span><br><span class="line">pod &quot;statefulset-kubia-v1-0&quot; deleted</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# kubectl get po</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Running   0          2m</span><br><span class="line">statefulset-kubia-v1-1   1/1     Running   0          17h</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running   0          17h</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-0/proxy/</span><br><span class="line">You&apos;ve hit statefulset-kubia-v1-0</span><br><span class="line">Data stored on this pod: Hey There ! This greeting was submitted to statefulset-kubia-v1-0</span><br><span class="line">[root@k8s-master ~]#</span><br></pre></td></tr></table></figure><blockquote><p>删除一个Pod,当Pod重新被调度时不一定是原节点,有可能会调度到另外一个节点</p></blockquote><p>从上面的实验中可以得出2个结论:</p><ul><li>statefulset的pod被重新调度时,会新创建一个和之前一模一样的Pod(包括主机名称,pod名,存储)</li><li>当pod被删除,重新调度后持久化数据与之前一模一样.</li></ul><hr><h3 id="statefulSet滚动更新"><a href="#statefulSet滚动更新" class="headerlink" title="statefulSet滚动更新"></a>statefulSet滚动更新</h3><h4 id="1-7版本之前默认的On-Delete更新策略"><a href="#1-7版本之前默认的On-Delete更新策略" class="headerlink" title="1.7版本之前默认的On Delete更新策略"></a>1.7版本之前默认的On Delete更新策略</h4><p>​     statefulset在1.7版本开始支持滚动更新..在1.7版本之前默认的更新测量是<figure class="highlight plain"><figcaption><span>Delete```.这种侧列和ReplicaSet类似.当更新了配置文件后,旧的pod并不会被自动删除,而是需要手动删除.</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">​    下面这个例子中,将镜像更换为luksa/kubia-pet-peers.副本数从3个增加到4个.(为此,我们需要提前再创建一个pv-4</span><br><span class="line"></span><br><span class="line">```shell</span><br><span class="line">#编辑pv配置文件,增加pv-4(前提是nfs服务器上实现存在/data/k8s/pv-4目录</span><br><span class="line">[root@k8s-master ~]# vim statefulset-kubia-pv.yaml</span><br><span class="line"></span><br><span class="line">#更新pv配置文件</span><br><span class="line">[root@k8s-master ~]# kubectl apply -f statefulset-kubia-pv.yaml</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">persistentvolume/pv-1 configured</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">persistentvolume/pv-2 configured</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">persistentvolume/pv-3 configured</span><br><span class="line">persistentvolume/pv-4 created</span><br><span class="line"></span><br><span class="line">#查看PV.</span><br><span class="line">[root@k8s-master ~]# kubectl get pv</span><br><span class="line">NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                 STORAGECLASS   REASON   AGE</span><br><span class="line">pv-1   1Gi        RWO            Recycle          Bound       default/data-statefulset-kubia-v1-2   nfs                     23h</span><br><span class="line">pv-2   1Gi        RWO            Recycle          Bound       default/data-statefulset-kubia-v1-0   nfs                     23h</span><br><span class="line">pv-3   1Gi        RWO            Recycle          Bound       default/data-statefulset-kubia-v1-1   nfs                     23h</span><br><span class="line">pv-4   1Gi        RWO            Recycle          Available                                         nfs                     9s</span><br><span class="line">[root@k8s-master ~]#</span><br></pre></td></tr></table></figure></p><p>更新statefulset配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#更新配置文件</span><br><span class="line">[root@k8s-master ~]# vim statefulset-kubia.yaml</span><br><span class="line">#应用配置文件</span><br><span class="line">[root@k8s-master ~]# kubectl apply -f statefulset-kubia.yaml</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">statefulset.apps/statefulset-kubia-v1 configured</span><br><span class="line"></span><br><span class="line">#查看Pod</span><br><span class="line">[root@k8s-master ~]# kubectl get pods</span><br><span class="line">NAME                     READY   STATUS              RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Running             0          70m</span><br><span class="line">statefulset-kubia-v1-1   1/1     Running             0          18h</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running             0          18h</span><br><span class="line">statefulset-kubia-v1-3   0/1     ContainerCreating   0          5s</span><br><span class="line">[root@k8s-master ~]#</span><br></pre></td></tr></table></figure><p>通过Pod的存活字段可以看到之前旧版本的Pod并没有被自动删除,而是新增了一个副本.这和ReplicaSet的机制类似.</p><hr><h4 id="自动滚动更新策略"><a href="#自动滚动更新策略" class="headerlink" title="自动滚动更新策略"></a>自动滚动更新策略</h4><p>编辑statefulset配置文件,将镜像版本改回到luksa/kubia-pet</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  serviceName:</span> <span class="string">statefulset-kubia-v1</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">4</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">       app:</span> <span class="string">statefulset-kubia</span></span><br><span class="line">  <span class="comment">#在spec字段配置更新策略,默认的type是On Delete,修改为RollingUpdate</span></span><br><span class="line"><span class="attr">  updateStrategy:</span></span><br><span class="line"><span class="attr">     type:</span> <span class="string">RollingUpdate</span></span><br></pre></td></tr></table></figure><p>应用新的配置文件,此时会触发自动更新</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl apply -f statefulset-kubia.yaml</span></span><br><span class="line">statefulset.apps/statefulset-kubia-v1 configured</span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods</span></span><br><span class="line">NAME                     READY   STATUS        RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Running       0          5m22s</span><br><span class="line">statefulset-kubia-v1-1   1/1     Running       0          6m12s</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running       0          6m57s</span><br><span class="line">statefulset-kubia-v1-3   1/1     Terminating   0          7m39s</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods</span></span><br><span class="line">NAME                     READY   STATUS        RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Running       0          6m55s</span><br><span class="line">statefulset-kubia-v1-1   1/1     Terminating   0          7m45s</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running       0          14s</span><br><span class="line">statefulset-kubia-v1-3   1/1     Running       0          54s</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods</span></span><br><span class="line">NAME                     READY   STATUS        RESTARTS   AGE</span><br><span class="line">statefulset-kubia-v1-0   1/1     Terminating   0          7m27s</span><br><span class="line">statefulset-kubia-v1-1   1/1     Running       0          7s</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running       0          46s</span><br><span class="line">statefulset-kubia-v1-3   1/1     Running       0          86s</span><br></pre></td></tr></table></figure><p>发现了什么? 当滚动更新时,kubectl会以倒序的方式,从最末尾一个pod开始依次更新.</p><blockquote><p>StatefulSet的滚动更新策略不同于Deployment可以指定maxSuge参数指定一次同时更新的pod数量,而是只能单个方式进行依次更新</p></blockquote><blockquote><p>StatefulSet还支持partition(分区)的更新策略,具体可以查看官网</p></blockquote><p>无论是何种更新策略.Pod的数据(包括主机名,存储)都会持久化.再次访问第0个pod,存储数据依然存在</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/statefulset-kubia-v1-0/proxy/</span><br><span class="line">You&apos;ve hit statefulset-kubia-v1-0</span><br><span class="line">Data stored on this pod: Hey There ! This greeting was submitted to statefulset-kubia-v1-0</span><br></pre></td></tr></table></figure><hr><h3 id="StatefulSet-如何处理节点失效"><a href="#StatefulSet-如何处理节点失效" class="headerlink" title="StatefulSet 如何处理节点失效"></a>StatefulSet 如何处理节点失效</h3><p>在node2上关闭网卡来模拟这台服务器掉线,观察statefulSet处理节点失效的情况</p><blockquote><p>注意关闭节点网卡前请确保可以通过控制台连接服务器,因为这意味着无法ssh远程登录</p></blockquote><p>node2节点已经关闭,状态为notready</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get node</span><br><span class="line">NAME         STATUS     ROLES    AGE   VERSION</span><br><span class="line">k8s-master   Ready      master   49d   v1.17.3</span><br><span class="line">k8s-node1    Ready      &lt;none&gt;   49d   v1.17.3</span><br><span class="line">k8s-node2    NotReady   &lt;none&gt;   49d   v1.17.3</span><br></pre></td></tr></table></figure><p>过一段时间后,所有node2节点上的Pod为Terminating终止状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl get pods -o wide</span></span><br><span class="line">NAME                     READY   STATUS        RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES</span><br><span class="line">statefulset-kubia-v1-0   1/1     Terminating   0          33m   10.100.169.174   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-1   1/1     Terminating   0          34m   10.100.169.173   k8s-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-2   1/1     Running       0          34m   10.100.36.97     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">statefulset-kubia-v1-3   1/1     Running       0          35m   10.100.36.99     k8s-node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h5 id="删除不健康的Pod"><a href="#删除不健康的Pod" class="headerlink" title="删除不健康的Pod"></a>删除不健康的Pod</h5><p>当尝试手动删除pod时,发现永远都无法删除</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl delete po statefulset-kubia-v1-0</span><br><span class="line">pod &quot;statefulset-kubia-v1-0&quot; deleted</span><br><span class="line">^@</span><br><span class="line">^@</span><br></pre></td></tr></table></figure><p>在另一个终端上查看该pod.发现虽然pod被Terminating挂起,但是容器仍然处于运行状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]<span class="comment"># kubectl describe pods statefulset-kubia-v1-0</span></span><br><span class="line">Name:                      statefulset-kubia-v1-0</span><br><span class="line">Namespace:                 default</span><br><span class="line">Priority:                  0</span><br><span class="line">Node:                      k8s-node2/172.16.20.253</span><br><span class="line">Start Time:                Sun, 03 May 2020 10:54:17 +0800</span><br><span class="line">Labels:                    app=statefulset-kubia</span><br><span class="line">                           controller-revision-hash=statefulset-kubia-v1-74b44bc68b</span><br><span class="line">                           statefulset.kubernetes.io/pod-name=statefulset-kubia-v1-0</span><br><span class="line">Annotations:               cni.projectcalico.org/podIP: 10.100.169.174/32</span><br><span class="line">Status:                    Terminating (lasts 12m)</span><br><span class="line">Termination Grace Period:  30s</span><br><span class="line">IP:                        10.100.169.174</span><br><span class="line">IPs:</span><br><span class="line">  IP:           10.100.169.174</span><br><span class="line">Controlled By:  StatefulSet/statefulset-kubia-v1</span><br><span class="line">Containers:</span><br><span class="line">  statefulset-kubia:</span><br><span class="line">    Container ID:   docker://099628b95ded3644752a3de799ef338794704aaa4ebe4db5a966b821b2e9a71a</span><br><span class="line">    Image:          luksa/kubia-pet</span><br><span class="line">    Image ID:       docker-pullable://luksa/kubia-pet@sha256:4263bc375d3ae2f73fe7486818cab64c07f9cd4a645a7c71a07c1365a6e1a4d2</span><br><span class="line">    Port:           8080/TCP</span><br><span class="line">    Host Port:      0/TCP</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Sun, 03 May 2020 10:54:21 +0800</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/data from data (rw)</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jfrqr (ro)</span><br></pre></td></tr></table></figure><h5 id="强制删除"><a href="#强制删除" class="headerlink" title="强制删除"></a>强制删除</h5><p>带上参数<figure class="highlight plain"><figcaption><span>--grace-period 0```可以强制删除一个pod</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl delete po statefulset-kubia-v1-0 –force –grace-period 0<br>warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.<br>pod “statefulset-kubia-v1-0” force deleted</p><p>[root@k8s-master ~]# kubectl get pods<br>NAME                     READY   STATUS              RESTARTS   AGE<br>statefulset-kubia-v1-0   0/1     ContainerCreating   0          5s<br>statefulset-kubia-v1-1   1/1     Terminating         0          44m<br>statefulset-kubia-v1-2   1/1     Running             0          44m<br>statefulset-kubia-v1-3   1/1     Running             0          45m</p><p>[root@k8s-master ~]# kubectl get pods -o wide<br>NAME                     READY   STATUS        RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES<br>statefulset-kubia-v1-0   1/1     Running       0          12s   10.100.36.101    k8s-node1   <none>           <none><br>statefulset-kubia-v1-1   1/1     Terminating   0          44m   10.100.169.173   k8s-node2   <none>           <none><br>statefulset-kubia-v1-2   1/1     Running       0          44m   10.100.36.97     k8s-node1   <none>           <none><br>statefulset-kubia-v1-3   1/1     Running       0          45m   10.100.36.99     k8s-node1   <none>           <none><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">此时第0个pod已经重新创建,并且运行到node1节点,而另一个pod依然处于Terminating状态</span><br><span class="line"></span><br><span class="line">##### node2节点恢复正常</span><br><span class="line"></span><br><span class="line">当节点恢复正常后,很快node和pod都全部恢复正常.此时第0个pod依然还是挂在在node1节点.第1个pod的状态迅速从Terminating状态变为Running状态</span><br></pre></td></tr></table></figure></none></none></none></none></none></none></none></none></p><p>[root@k8s-master ~]# kubectl get pods -o wide<br>NAME                     READY   STATUS    RESTARTS   AGE     IP               NODE        NOMINATED NODE   READINESS GATES<br>statefulset-kubia-v1-0   1/1     Running   0          3m17s   10.100.36.101    k8s-node1   <none>           <none><br>statefulset-kubia-v1-1   1/1     Running   0          5s      10.100.169.172   k8s-node2   <none>           <none><br>statefulset-kubia-v1-2   1/1     Running   0          47m     10.100.36.97     k8s-node1   <none>           <none><br>statefulset-kubia-v1-3   1/1     Running   0          48m     10.100.36.99     k8s-node1   <none>           <none><br><code>`</code></none></none></none></none></none></none></none></none></p><hr><h3 id="本章总结"><a href="#本章总结" class="headerlink" title="本章总结"></a>本章总结</h3><p>Stateful和RS,deployment的用法总体没有太大区别,下面是这2种资源的对比</p><table><thead><tr><th style="text-align:center">特性</th><th style="text-align:center">Deployment</th><th style="text-align:center">StatefulSet</th></tr></thead><tbody><tr><td style="text-align:center">是否暴露到外网</td><td style="text-align:center">可以</td><td style="text-align:center">一般不</td></tr><tr><td style="text-align:center">请求面向的对象</td><td style="text-align:center">ServiceName</td><td style="text-align:center">指定pod的域名</td></tr><tr><td style="text-align:center">灵活性</td><td style="text-align:center">通过Service(名称或者IP)访问后端Pod</td><td style="text-align:center">可以访问任意一个pod</td></tr><tr><td style="text-align:center">易用性</td><td style="text-align:center">只需要关心Service信息即可</td><td style="text-align:center">需要知道访问pod的名称,Headless Service名称</td></tr><tr><td style="text-align:center">PV/PVC稳定性</td><td style="text-align:center">无法保障绑定关系</td><td style="text-align:center">可以保障</td></tr><tr><td style="text-align:center">pod名称稳定性</td><td style="text-align:center">使用一个随机的名称后缀,重启后会随机生成另外一个.名称不重复</td><td style="text-align:center">稳定,每次都一样</td></tr><tr><td style="text-align:center">升级更新顺序</td><td style="text-align:center">随机启动.如果pod宕机重启,也是随机分配一个Node节点重新启动</td><td style="text-align:center">pod按顺序依次启动,如果pod宕机,依然使用相同的Node节点和名称</td></tr><tr><td style="text-align:center">停止顺序</td><td style="text-align:center">随机停止</td><td style="text-align:center">倒序停止</td></tr><tr><td style="text-align:center">集群内部服务发现</td><td style="text-align:center">只能通过Service访问随机的一个Pod</td><td style="text-align:center">可以打通pod之间的通信</td></tr><tr><td style="text-align:center">性能开销</td><td style="text-align:center">无需维护pod与node,pvc等关系</td><td style="text-align:center">需要维护额外的关系信息</td></tr></tbody></table><p>通过对比发现</p><ul><li>如果不需要额外数据依赖或者状态维护的部署,优先选择Deployment</li><li>如果单纯要做数据持久化,方式pod宕机数据丢失,直接使用PV/PVC就可以</li><li>如果是有多个副本,且每个副本挂载的PV存储数据不同,并且pod宕机重启后仍然关联到之前的PVC,并且数据需要持久化,考虑使用StatefulSet</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;h3 id=&quot;kubernetes-headless-Service&quot;&gt;&lt;a href=&quot;#kubernetes-headless-Service&quot; class=&quot;headerlink&quot; title=&quot;kubernetes headless Service&quot;&gt;&lt;/a&gt;kubernetes headless Service&lt;/h3&gt;&lt;p&gt;​     我们以前学习过,Service是Kubernetes项目中用来将一组Pod暴露给外界访问的一种机制.外部客户端通过Service地址可以随机访问到某个具体的Pod&lt;/p&gt;
&lt;p&gt;​     之前学过几种Service类型,包括nodeport,loadbalancer等等.所有这类Service都有一个VIP(虚拟IP),访问Service VIP,Service会将请求转发到后端的Pod上,&lt;/p&gt;
&lt;p&gt;​     还有一种Service是Headless Service(无头服务),这类Service自身不需要VIP,当DNS解析该Service时,会解析出Service后端的Pod地址.这样设置的好处是Kubernetes项目为Pod分配唯一的”可解析身份”,只要知道一个pod的名字和对应的Headless Service名字,就可以通过这条DNS访问到后端的Pod&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>kubernetes volume</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/kubernetes%20volume%E5%AD%98%E5%82%A8%E5%8D%B7/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/kubernetes volume存储卷/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T10:05:07.431Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kubernetes-volume"><a href="#kubernetes-volume" class="headerlink" title="kubernetes volume"></a>kubernetes volume</h2><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在学习docker时,学习了如何将宿主机上的文件系统挂载到容器当中,实现持久化存储,以及多个容器共享同一个文件目录.</p><p>在k8s中,我们也希望pod容器中的数据能够持久化存储.当一个容器因为故障或者其他原因需要删除时,我们希望新的容器能够在上一个容器结束的位置继续运行.</p><p>但是和docker一样,容器默认情况下删除后,所有该容器产生的数据都会消失.并且,虽然容器和宿主机共享CPU,网络,内存等资源,但是并不会共享文件存储.甚至同一个Pod中每个容器都有自己独立的文件系统,彼此互相隔离.</p><p>和docker容器一样,这就需要有一种存储卷能够挂载到Pod容器中,并脱离Pod的生命周期之外,将容器运行产生的数据保存在宿主机或者独立的外部存储卷中</p><a id="more"></a><hr><h2 id="存储卷类型"><a href="#存储卷类型" class="headerlink" title="存储卷类型"></a>存储卷类型</h2><ul><li>emptyDir——-用于存储容器临时数据的空目录</li></ul><p>emptyDir类型的存储卷和pod容器的生命周期相关联当pod容器删除时,emtpyDir卷的数据就会丢失</p><ul><li>hostPath——–和docker类似,将宿主机的某个目录挂载到pod容器.</li></ul><p>但是本地节点运行的Pod容器和其他节点服务器上的同一组pod容器之间无法共享数据.只能本地持久化,无法实现集群存储</p><ul><li>nfs,iscsi,FC SAN——-网络存储</li></ul><p>网络存储设备,挂载一个独立的第三方存储设备</p><ul><li><p>AWS EBK,Azure Disk等—— 云厂商虚拟存储</p></li><li><p>cephfs,glusterfs等—– 分布式集群存储</p></li></ul><blockquote><p>使用<figure class="highlight plain"><figcaption><span>explain pods.spec.volumes```命令可以查看k8s支持的许多存储卷类型</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">----</span><br><span class="line"></span><br><span class="line">## 一. emptyDir卷</span><br><span class="line"></span><br><span class="line">为了演示emtpyDir卷,现在创建2个简单的pod容器:</span><br><span class="line"></span><br><span class="line">nginx-alpine容器.</span><br><span class="line"></span><br><span class="line">fortune容器定期输出字符串到/var/hotdocs/index.html文件下.然后Nginx容器展示.</span><br><span class="line"></span><br><span class="line">这2个容器需要共享同一个目录.</span><br></pre></td></tr></table></figure></p></blockquote><p>#furtune容器镜像主要是运行一个while循环脚本,该脚本每隔10秒随机想/var/htdocs/index.html文件写入一段名人名言</p><p>#!/bin/bash<br>trap ”exit” SIGINT<br>mkdir /var/htdocs<br>while :<br>do<br>echo $(date) Writing fortune to /var/htdocs/index.html<br>/usr/games/fortune &gt; /var/htdocs/index.html<br>sleep 10<br>done</p><p>然后编译Dockerfile的文件，其中包含以下内容:</p><p>FROM ubuntu:latest<br>RUN apt-get update;apt-get -y install fortune<br>ADD fortuneloop.sh /bin/fortuneloop.sh<br>ENTRYPOINT /bin/fortuneloop.sh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">使用```kubectl explain pod.volumes```命令可以查看k8s支持哪些存储卷类型</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">在pod定义的sepc.containers.volumeMounts字段中详细介绍了挂载磁盘卷的配置参数.</span><br><span class="line"></span><br><span class="line">使用命令 ``` kubectl explian pod.spec.containers.volumeMounts```可以看到支持以下主要挂载参数</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl explain pod.spec.containers.volumeMounts<br>KIND:     Pod<br>VERSION:  v1  #pod对象属于哪个版本</p><p>#volumeMounts参数的值是一个列表对象<br>RESOURCE: volumeMounts &lt;[]Object&gt;</p><p>FIELDS:</p><p>#挂载路径.必要字段<br>   mountPath    <string> -required-<br>     Path within the container at which the volume should be mounted. Must not<br>     contain ‘:’.</string></p><p>#挂载的volume卷名,必要字段<br>   name    <string> -required-<br>     This must match the Name of a Volume.</string></p><p>#是否只读<br>   readOnly    <boolean><br>     Mounted read-only if true, read-write otherwise (false or unspecified).<br>     Defaults to false.</boolean></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">为了演示实验是否正常,同时复习之前学到的replicaSet和Service对象,我们来创建一个rs,和svc对象</span><br><span class="line"></span><br><span class="line">####  1. 创建replicaSet对象.包含一个副本,以及上面配置的2个pod容器</span><br></pre></td></tr></table></figure><p>apiVersion: apps/v1<br>kind: ReplicaSet<br>metadata:<br>    name: fortune-rs</p><p>spec:<br>   replicas: 1<br>   selector:<br>       matchLabels:<br>           app: fortune</p><p>   template:<br>      metadata:<br>         name: fortune<br>         labels:<br>           app: fortune<br>      spec:<br>         containers:</p><pre><code>    - image: luksa/fortune      name: html-generator      #磁盘卷挂载配置      volumeMounts:           - name: html  #卷名             mountPath: /var/htdocs #挂载在容器的哪个目录下    - image: nginx:alpine      name: web-servier      volumeMounts:          - name: html            mountPath: /usr/share/nginx/html            readOnly: true      ports:          - name: http            containerPort: 80#磁盘卷定义volumes:#卷名   - name: html     #卷类型     emptyDir: {}</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">pod包含2个容器,他们挂载同一个公用的存储卷,当html-generator容器启动时,每10秒启动一次fortune命令输出到/var/htdocs/index.html文件中,因为卷同时也被web-server容器挂载,所以后者能访问到html-generator容器生成的数据</span><br><span class="line"></span><br><span class="line">如果要将emptyDir卷存储在内存上而非磁盘,可以声明medium配置如下配置:</span><br></pre></td></tr></table></figure><p> volumes:</p><pre><code>#卷名   - name: html     #卷类型     emptyDir:         #类型        medium: Memory</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 2.为此ReplicaSet创建一个Service对象.内容如下</span><br></pre></td></tr></table></figure><p>apiVersion: v1<br>kind: Service<br>metadata:<br>   name: fortune-svc</p><p>spec:<br>   selector:<br>     app: fortune</p><p>   ports:</p><pre><code>- port: 80  targetPort: 80</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 3.分别创建svc和rs对象</span><br></pre></td></tr></table></figure><p>kubectl create -f fortune-rs.yaml</p><p>kubectl create -f fortune-svc.yaml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 4.查看资源</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl get rs<br>NAME         DESIRED   CURRENT   READY   AGE<br>fortune-rs   1         1         1       11h</p><p>[root@k8s-master ~]# kubectl get svc<br>NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE<br>fortune-svc          ClusterIP      10.96.123.110   <none>        80/TCP         7m9s</none></p><p>[root@k8s-master ~]# kubectl get pods<br>NAME                READY   STATUS    RESTARTS   AGE<br>fortune-rs-4df5q    2/2     Running   0          11m<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 5.访问Service</span><br></pre></td></tr></table></figure></p><p>[work@k8s-node1 ~]$ curl <a href="http://10.96.123.110" target="_blank" rel="noopener">http://10.96.123.110</a><br>You never have to change anything you got up in the middle of the night<br>to write.<br>        – Saul Bellow<br>[work@k8s-node1 ~]$ curl <a href="http://10.96.123.110" target="_blank" rel="noopener">http://10.96.123.110</a><br>Q:    How much does it cost to ride the Unibus?<br>A:    2 bits.<br>[work@k8s-node1 ~]$ curl <a href="http://10.96.123.110" target="_blank" rel="noopener">http://10.96.123.110</a><br>While you recently had your problems on the run, they’ve regrouped and<br>are making another attack.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">可以看到,每10秒钟访问的内容不一样</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## 二 .hostPath卷</span><br><span class="line"></span><br><span class="line">hostPath卷可以实现持久性存储,如果删除了一个Pod,并且新的pod使用了相同的主机路径的hostPath卷,则新pod会发现上一个pod留下来的数据,但是前提是必须和前一个pod是同一个节点</span><br><span class="line"></span><br><span class="line">这也解释了为什么使用hostPath卷不是一个好主意,因为当Pod被调度到另外一个节点上时,会找不到数据.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 三.NFS卷</span><br><span class="line"></span><br><span class="line">NFS卷可以实现多个节点之间共享存储.但是生产中不建议这样做,因为NFS共享存储传输效率低,稳定性和安全性不高.</span><br><span class="line"></span><br><span class="line">仍然使用上面的例子</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# cp fortune-svc.yaml fortune-nfs-svc.yaml<br>[root@k8s-master ~]# cp fortune-rs.yaml fortune-nfs-rs.yaml<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">修改各资源的标签,和rs配置文件中的卷信息:</span><br></pre></td></tr></table></figure></p><p>apiVersion: apps/v1<br>kind: ReplicaSet<br>metadata:<br>    name: fortune-nfs-rs</p><p>spec:<br>   replicas: 1<br>   selector:<br>       matchLabels:<br>           app: fortune-nfs</p><p>   template:<br>      metadata:<br>         name: fortune-nfs<br>         labels:<br>           app: fortune-nfs<br>      spec:<br>         containers:</p><pre><code>    - image: luksa/fortune      name: html-generator      #磁盘卷挂载配置      volumeMounts:           - name: html  #卷名             mountPath: /var/htdocs #挂载在容器的哪个目录下    - image: nginx:alpine      name: web-servier      volumeMounts:          - name: html            mountPath: /usr/share/nginx/html            readOnly: true      ports:          - name: http            containerPort: 80#磁盘卷定义volumes:#卷名   - name: html     #卷类型     nfs:   #NFS服务器地址      server: 172.16.20.2   #NFS服务端共享目录      path: /data/apps/k8s</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">启动rs后,在nfs服务器上已经看到pod写入的数据:</span><br></pre></td></tr></table></figure><p>[work@idc-beta-cron ~]$ cat /data/apps/k8s/index.html<br>Tonight’s the night: Sleep in a eucalyptus tree.</p><p>[work@idc-beta-cron ~]$ cat /data/apps/k8s/index.html<br>Fortune: You will be attacked next Wednesday at 3:15 p.m. by six samurai<br>sword wielding purple fish glued to Harley-Davidson motorcycles.</p><p>Oh, and have a nice day!<br>        – Bryce Nesbitt ‘84</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"></span><br><span class="line">## PV和PVC</span><br><span class="line"></span><br><span class="line">PV和PVC用于kubenetes将存储和pod解耦,存储管理人员配置好PV,然后开发人员只需要声明需要多大的存储卷(pvc)即可,无需关系底层的存储实现方式.</span><br><span class="line"></span><br><span class="line">**实现逻辑大致如下:**</span><br><span class="line"></span><br><span class="line">1.存储管理员配置底层存储方案(NFS,SAN,FC SAN,ceph等)</span><br><span class="line"></span><br><span class="line">2.k8s集群管理员创建一个PV(Psersistent volume,持久卷)</span><br><span class="line"></span><br><span class="line">3.开发人员(或者用户)创建一个PVC(Persistent volume claim,持久卷声明),将PVC和PV绑定</span><br><span class="line"></span><br><span class="line">4.开发人员(或者用户)在pod中引用PVC</span><br><span class="line"></span><br><span class="line">下面用NFS底层存储来演示PV和PVC的工作过程</span><br><span class="line"></span><br><span class="line">#### 1.创建PV</span><br></pre></td></tr></table></figure><p>apiVersion: v1<br>kind: PersistentVolume<br>metadata:<br>  name: mypv1</p><p>spec:<br>   capacity:<br>      storage: 1Gi   #定义PV卷的大小</p><p>   accessModes:</p><pre><code>- ReadWriteOnce   #PV可以读写模式被挂载到单个节点- ReadOnlyMany    #PV以只读模式被挂载到多个节点</code></pre><p>   persistentVolumeReclaimPolicy: Retain  #PV的回收策略.Retain表示PVC释放后,PV会继续保留<br>   storageClassName: nfs   #指定nfs类型<br>   nfs:<br>     path: /data/k8s<br>     server: 172.16.20.1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* accessModes支持ReadWriteOnce ,ReadOnlyMany ,ReadWriteMany </span><br><span class="line"></span><br><span class="line">可以直接使用RWO,ROM,RWM缩写.</span><br><span class="line"></span><br><span class="line">* PersistentVolumeReclaimPolicy回收策略支持:</span><br><span class="line">  - Retain:  PV一直保留,直到管理员手动回收</span><br><span class="line">  - Recycle: 清除PV中的数据</span><br><span class="line">  - Delete:  清除存储上的资源</span><br><span class="line"></span><br><span class="line">* storageClassName: PV的类型</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl create -f nfs-pv.yaml<br>persistentvolume/mypv1 created<br>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Retain           Available           nfs                     5s<br>[root@k8s-master ~]#</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 2.持久卷声明(PVC)</span><br></pre></td></tr></table></figure><p>apiVersion: v1<br>kind: PersistentVolumeClaim<br>metadata:<br>  name: mypvc1</p><p>spec:<br>  accessModes: #访问模式</p><pre><code>- ReadWriteOnce</code></pre><p>  resources: #定义需要的存储空间大小<br>     requests:<br>       storage: 1Gi<br>  storageClassName: nfs #存储类型<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">创建PVC</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl create -f nfs-pvc.yaml<br>persistentvolumeclaim/mypvc1 created<br>[root@k8s-master ~]# kubectl get pvc<br>NAME     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE<br>mypvc1   Bound    mypv1    1Gi        RWO,ROX        nfs            10s<br>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Retain           Bound    default/mypvc1   nfs                     22m<br>[root@k8s-master ~]#<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">default/mypvc1 表示default命名空间的PVC</span><br><span class="line"></span><br><span class="line">Status的Bound表示PV和PVC已经成功绑定</span><br><span class="line"></span><br><span class="line">&gt; 持久卷PV不属于任何名称空间,但是PVC和pod有名称空间概念.PV可以被所有名称空间下的PVC绑定.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#### 3.创建pod.在pod中调用刚才创建的PVC</span><br></pre></td></tr></table></figure></p><p>apiVersion: apps/v1<br>kind: ReplicaSet<br>metadata:<br>    name: fortune-nfs-pvc-rs</p><p>spec:<br>   replicas: 1<br>   selector:<br>       matchLabels:<br>           app: fortune-pvc-nfs</p><p>   template:<br>      metadata:<br>         name: fortune-pvc-nfs<br>         labels:<br>           app: fortune-pvc-nfs<br>      spec:<br>         containers:</p><pre><code>    - image: luksa/fortune      name: html-generator      #磁盘卷挂载配置      volumeMounts:           - name: html  #卷名             mountPath: /var/htdocs #挂载在容器的哪个目录下    - image: nginx:alpine      name: web-servier      volumeMounts:          - name: html            mountPath: /usr/share/nginx/html            readOnly: true      ports:          - name: http            containerPort: 80#磁盘卷定义volumes:#卷名   - name: html     #卷类型     persistentVolumeClaim:        claimName: mypvc1</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">和empty dir以及Hostpath类似,在引用pvc的时候只是修改一下卷类型.引用mypvc1这个刚才创建的pvc</span><br><span class="line"></span><br><span class="line">创建pod</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl create -f fortune-nfs-pvc.yaml<br>replicaset.apps/fortune-nfs-pvc-rs created</p><p>[root@k8s-master ~]# kubectl get pods<br>NAME                       READY   STATUS    RESTARTS   AGE<br>fortune-nfs-pvc-rs-zlnk2   2/2     Running   0          26s<br>kubia-5452q                0/1     Running   0          5d19h<br>kubia-6mghh                0/1     Running   0          5d19h<br>kubia-nl6rd                0/1     Running   0          5d19h<br>kubia-pvs5z                0/1     Running   0          5d19h<br>ssd-monitor-5vxbn          1/1     Running   0          5d19h<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#### 4.验证</span><br><span class="line"></span><br><span class="line">在nfs服务器上的共享目录/data/k8s上确实看到了Pod容器写入的数据</span><br></pre></td></tr></table></figure></p><p>[work@idc-beta-docker ~]$ ll /data/k8s/<br>total 4<br>-rw-r–r– 1 root root 276 Apr 19 10:24 index.html<br>[work@idc-beta-docker ~]$ cat /data/k8s/index.html<br>Mind!  I don’t mean to say that I know, of my own knowledge, what there is<br>particularly dead about a door-nail.  I might have been inclined, myself,<br>to regard a coffin-nail as the deadest piece of ironmongery in the trade.<br>But the wisdom of our ancestors is in the simile; and my unhallowed hands<br>shall not disturb it, or the Country’s done for.  You will therefore permit<br>me to repeat, emphatically, that Marley was as dead as a door-nail.<br>        – Charles Dickens, “A Christmas Carol”<br>[work@idc-beta-docker ~]$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">-------</span><br><span class="line"></span><br><span class="line">## pv的手动回收</span><br><span class="line"></span><br><span class="line">手动删除Pod,pvc</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl delete pvc mypvc1<br>persistentvolumeclaim “mypvc1” deleted</p><p>[root@k8s-master ~]# kubectl get pvc<br>No resources found in default namespace.</p><p>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM            STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Retain           Released   default/mypvc1   nfs                     11h<br>[root@k8s-master ~]#<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">删除pvc后,pv是Released状态,不可绑定PVC.删除PV,然后重新创建.</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl delete pv mypv1<br>persistentvolume “mypv1” deleted</p><p>[work@idc-beta-docker ~]$ ll /data/k8s/<br>total 4<br>-rw-r–r– 1 root root 100 Apr 19 10:38 index.html</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">删除PV卷,并不会删除存储卷中的数据</span><br><span class="line"></span><br><span class="line">重新创建PV.存储卷中的数据仍然存在,并且PV的状态为Available</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl create -f nfs-pv.yaml<br>persistentvolume/mypv1 created</p><p>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Retain           Available           nfs                     6s</p><p>[work@idc-beta-docker ~]$ ll /data/k8s/<br>total 4<br>-rw-r–r– 1 root root 100 Apr 19 10:38 index.html<br>[work@idc-beta-docker ~]$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"></span><br><span class="line">## PV自动回收</span><br><span class="line"></span><br><span class="line">在pv的配置中修改如下字段</span><br></pre></td></tr></table></figure><p>persistentVolumeReclaimPolicy: Recycle<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">创建PV.Reclaim policy是Recycle</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl create -f nfs-pv.yaml<br>persistentvolume/mypv1 created<br>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Recycle          Available           nfs                     3s<br>[root@k8s-master ~]#</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">创建PVC,绑定到PV,并且创建pod</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl create -f nfs-pvc.yaml<br>persistentvolumeclaim/mypvc1 created</p><p>[root@k8s-master ~]# kubectl create -f fortune-nfs-pvc.yaml<br>replicaset.apps/fortune-nfs-pvc-rs created</p><p>[root@k8s-master ~]# kubectl get pvc<br>NAME     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE<br>mypvc1   Bound    mypv1    1Gi        RWO,ROX        nfs            26s</p><p>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Recycle          Bound    default/mypvc1   nfs                     101s</p><p>[root@k8s-master ~]# kubectl get pods<br>NAME                       READY   STATUS    RESTARTS   AGE<br>fortune-nfs-pvc-rs-9p2xz   2/2     Running   0          16s</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">nfs底层存储卷上数据已经更新</span><br></pre></td></tr></table></figure><p>[work@idc-beta-docker ~]$ cat /data/k8s/index.html<br>Alimony and bribes will engage a large share of your wealth.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### 删除Pod,pvc</span><br></pre></td></tr></table></figure><p>[root@k8s-master ~]# kubectl delete pvc mypvc1<br>persistentvolumeclaim “mypvc1” deleted</p><p>[root@k8s-master ~]# kubectl get pvc<br>No resources found in default namespace.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">再次查看PV发现状态又变成了Available</span><br></pre></td></tr></table></figure></p><p>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM            STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Recycle          Released   default/mypvc1   nfs                     5m19s<br>[root@k8s-master ~]# kubectl get pv<br>NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE<br>mypv1   1Gi        RWO,ROX        Recycle          Available           nfs                     5m21s<br><code>`</code></p><blockquote><p>NFS不支持delete回收策略,所以就不演示</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kubernetes-volume&quot;&gt;&lt;a href=&quot;#kubernetes-volume&quot; class=&quot;headerlink&quot; title=&quot;kubernetes volume&quot;&gt;&lt;/a&gt;kubernetes volume&lt;/h2&gt;&lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;在学习docker时,学习了如何将宿主机上的文件系统挂载到容器当中,实现持久化存储,以及多个容器共享同一个文件目录.&lt;/p&gt;
&lt;p&gt;在k8s中,我们也希望pod容器中的数据能够持久化存储.当一个容器因为故障或者其他原因需要删除时,我们希望新的容器能够在上一个容器结束的位置继续运行.&lt;/p&gt;
&lt;p&gt;但是和docker一样,容器默认情况下删除后,所有该容器产生的数据都会消失.并且,虽然容器和宿主机共享CPU,网络,内存等资源,但是并不会共享文件存储.甚至同一个Pod中每个容器都有自己独立的文件系统,彼此互相隔离.&lt;/p&gt;
&lt;p&gt;和docker容器一样,这就需要有一种存储卷能够挂载到Pod容器中,并脱离Pod的生命周期之外,将容器运行产生的数据保存在宿主机或者独立的外部存储卷中&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>kuberentes Ingress</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/kubernetes%20Ingress/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/kubernetes Ingress/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-26T10:04:38.304Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kuberentes-Ingress"><a href="#kuberentes-Ingress" class="headerlink" title="kuberentes Ingress"></a>kuberentes Ingress</h2><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><a href="https://segmentfault.com/a/1190000019908991" target="_blank" rel="noopener">K8s ingress 学习博客</a></p><p>ingress是暴露服务给外部客户端的另外一种方法.(前面我们学习过2种服务类型可以实现同样的目的:NodePort和LoadBalance)</p><p>但是这几种Service都有局限性:</p><ul><li>ClusterIP的默认Serivce方式只能在集群内部访问。</li><li><p>NodePort:当有几十上百的服务在集群中运行时，NodePort的端口管理是灾难。</p></li><li><p>LoadBalance方式受限于云平台，且通常在云平台部署ELB还需要额外的费用。</p></li></ul><p>所幸k8s还提供了一种集群维度暴露服务的方式，也就是ingress。ingress可以简单理解为service的service，他通过独立的ingress对象来制定请求转发的规则，把请求路由到一个或多个service中。这样就把服务与请求规则解耦了，可以从业务维度统一考虑业务的暴露，而不用为每个service单独考虑。</p><p>Ingress可以为多个Service提供服务,可以根据请求的HOST和PATH,将请求代理到相关的Service中.而且Ingres工作在HTTP层,可以实现7层代理特性,诸如:coockies和会话亲和力</p><a id="more"></a><hr><h2 id="ingress与ingress-controller"><a href="#ingress与ingress-controller" class="headerlink" title="ingress与ingress-controller"></a>ingress与ingress-controller</h2><p>要理解ingress，需要区分两个概念，ingress和ingress-controller：</p><ul><li>ingress对象：</li></ul><p>指的是k8s中的一个api对象，一般用yaml配置。作用是定义请求如何转发到service的规则，可以理解为配置模板。</p><ul><li>ingress-controller：</li></ul><p>具体实现反向代理及负载均衡的程序，对ingress定义的规则进行解析，根据配置的规则来实现请求转发。</p><p>简单来说，ingress-controller才是负责具体转发的组件，通过各种方式将它暴露在集群入口，外部对集群的请求流量会先到ingress-controller，而ingress对象是用来告诉ingress-controller该如何转发请求，比如哪些域名哪些path要转发到哪些服务等等。</p><hr><h2 id="ingress-controller"><a href="#ingress-controller" class="headerlink" title="ingress-controller"></a>ingress-controller</h2><p>ingress-controller并不是k8s自带的组件，实际上ingress-controller只是一个统称，用户可以选择不同的ingress-controller实.</p><p>目前，由k8s维护的ingress-controller只有google云的GCE与ingress-nginx两个，其他还有很多第三方维护的ingress-controller，具体可以参考官方文档。</p><p>但是不管哪一种ingress-controller，实现的机制都大同小异，只是在具体配置上有差异。一般来说，ingress-controller的形式都是一个pod，里面跑着daemon程序和反向代理程序。daemon负责不断监控集群的变化，根据ingress对象生成配置并应用新配置到反向代理，比如nginx-ingress就是动态生成nginx配置，动态更新upstream，并在需要的时候reload程序应用新配置。</p><p>为了方便，后面的例子都以k8s官方维护的nginx-ingress为例。</p><hr><h2 id="ingress"><a href="#ingress" class="headerlink" title="ingress"></a>ingress</h2><p>ingress是一个API对象，和其他对象一样，通过yaml文件来配置。ingress通过http或https暴露集群内部service，给service提供外部URL、负载均衡、SSL/TLS能力以及基于host的方向代理。ingress要依靠ingress-controller来具体实现以上功能。</p><hr><h2 id="ingress的部署"><a href="#ingress的部署" class="headerlink" title="ingress的部署"></a>ingress的部署</h2><p>ingress的部署，需要考虑两个方面：</p><p>1.ingress-controller是作为pod来运行的，以什么方式部署比较好</p><p>2.ingress解决了把如何请求路由到集群内部，那它自己怎么暴露给外部比较好</p><p>下面列举一些目前常见的部署和暴露方式，具体使用哪种方式还是得根据实际需求来考虑决定。</p><p><strong>Deployment+LoadBalancer模式的Service</strong></p><p>如果要把ingress部署在公有云，那用这种方式比较合适。用Deployment部署ingress-controller，创建一个type为LoadBalancer的service关联这组pod。大部分公有云，都会为LoadBalancer的service自动创建一个负载均衡器，通常还绑定了公网地址。只要把域名解析指向该地址，就实现了集群服务的对外暴露。</p><p><strong>Deployment+NodePort模式的Service</strong></p><p>同样用deployment模式部署ingress-controller，并创建对应的服务，但是type为NodePort。这样，ingress就会暴露在集群节点ip的特定端口上。由于nodeport暴露的端口是随机端口，一般会在前面再搭建一套负载均衡器来转发请求。该方式一般用于宿主机是相对固定的环境ip地址不变的场景。<br>NodePort方式暴露ingress虽然简单方便，但是NodePort多了一层NAT，在请求量级很大时可能对性能会有一定影响。</p><p><strong>DaemonSet+HostNetwork+nodeSelector</strong></p><p>用DaemonSet结合nodeselector来部署ingress-controller到特定的node上，然后使用HostNetwork直接把该pod与宿主机node的网络打通，直接使用宿主机的80/433端口就能访问服务。这时，ingress-controller所在的node机器就很类似传统架构的边缘节点，比如机房入口的nginx服务器。该方式整个请求链路最简单，性能相对NodePort模式更好。缺点是由于直接利用宿主机节点的网络和端口，一个node只能部署一个ingress-controller pod。比较适合大并发的生产环境使用。</p><hr><h2 id="ingress-controller部署"><a href="#ingress-controller部署" class="headerlink" title="ingress-controller部署"></a>ingress-controller部署</h2><p>参考文档: <a href="https://www.kuboard.cn/install/install-k8s.html#%E5%AE%89%E8%A3%85-ingress-controller" target="_blank" rel="noopener">kubernetes集群部署</a></p><hr><h2 id="配置一个简单的Ingress对象"><a href="#配置一个简单的Ingress对象" class="headerlink" title="配置一个简单的Ingress对象"></a>配置一个简单的Ingress对象</h2><p>编辑一个简单的yaml配置文件如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# cat kubia-ingress.yaml</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">    - host: kubia.example.com </span><br><span class="line">      http:</span><br><span class="line">        paths:</span><br><span class="line">        - path: /</span><br><span class="line">          backend:</span><br><span class="line">            #将访问请求转发到上一节中创建的kubia-svc-nodeport的Serivce上</span><br><span class="line">            serviceName: kubia-svc-nodeport</span><br><span class="line">            #kubia-svc-nodeport的源端口</span><br><span class="line">            servicePort: 80</span><br></pre></td></tr></table></figure><p>接下来,可以将kubia.exanple.com域名解析到任意一个Node服务器节点的IP地址,然后请求这个域名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl http://kubia.example.com</span><br><span class="line">You&apos;ve hit kubia-hd429</span><br><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl http://kubia.example.com</span><br><span class="line">You&apos;ve hit kubia-fcc59</span><br><span class="line"> huangyong@huangyong-Macbook-Pro  ~  curl http://kubia.example.com</span><br><span class="line">You&apos;ve hit kubia-rm9qf</span><br></pre></td></tr></table></figure><hr><h2 id="ingress关联多个Service"><a href="#ingress关联多个Service" class="headerlink" title="ingress关联多个Service"></a>ingress关联多个Service</h2><p>ingress的rules字段可以包含多个转发规则,可以将不同的访问URL或者HOST代理到相关联的不同的Service中.参考下面的例子</p><ul><li>不同的PATH转发到不同的Service</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia-ingress-kubia</span><br><span class="line"></span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">    - host: kubia.example.com</span><br><span class="line">      paths:</span><br><span class="line">       - path: /foo</span><br><span class="line">         backend:</span><br><span class="line">            serviceName: kubia-svc-foo</span><br><span class="line">            servicePort: 80</span><br><span class="line"></span><br><span class="line">       - path: /bar</span><br><span class="line">         backend:</span><br><span class="line">            serviceName: kubia-svc-bar</span><br><span class="line">            servicePort: 80</span><br></pre></td></tr></table></figure><ul><li>不同的Host转发到不同的Service</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia-ingress-kubia2</span><br><span class="line"></span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">    - host: foo.example.com</span><br><span class="line">      paths:</span><br><span class="line">       - path: /</span><br><span class="line">         backend:</span><br><span class="line">            serviceName: kubia-svc-foo</span><br><span class="line">            servicePort: 80</span><br><span class="line">    - host: bar.example.com</span><br><span class="line">      paths:</span><br><span class="line">       - path: /</span><br><span class="line">         backend:</span><br><span class="line">            serviceName: kubia-svc-bar</span><br><span class="line">            servicePort: 80</span><br></pre></td></tr></table></figure><hr><h2 id="Ingress处理https-TLS传输"><a href="#Ingress处理https-TLS传输" class="headerlink" title="Ingress处理https TLS传输"></a>Ingress处理https TLS传输</h2><p>当用户使用TLS链接时,ingress负责处理用户的TLS请求,但是和后端的Pod仍然是以http连接,要实现这一点,Https的证书和秘钥必须挂载在ingress控制器上.</p><p>证书和秘钥可以保存在kubernetes的secret资源对象中.然后在ingress的mainfest资源中引用.</p><p>下面先创建一个自签名的证书和私钥</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# openssl genrsa -out tls.key 2048</span><br><span class="line"></span><br><span class="line">[root@k8s-master ~]#openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj /CN=kubia.example.com</span><br></pre></td></tr></table></figure><p>创建Secret</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key</span><br><span class="line">secret/tls-secret created</span><br></pre></td></tr></table></figure><h3 id="创建https的ingress"><a href="#创建https的ingress" class="headerlink" title="创建https的ingress"></a>创建https的ingress</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia</span><br><span class="line">spec:</span><br><span class="line">  tls:</span><br><span class="line">   - hosts:</span><br><span class="line">     - kubia.example.com</span><br><span class="line">     secretName: tls-secret</span><br><span class="line"></span><br><span class="line">  rules:</span><br><span class="line">   - host: kubia.example.com</span><br><span class="line">     http:</span><br><span class="line">       paths:</span><br><span class="line">         - path: /</span><br><span class="line">           backend:</span><br><span class="line">              serviceName: kubia</span><br><span class="line">              servicePort: 80</span><br></pre></td></tr></table></figure><p>更新ingress资源kubia</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl apply -f kubia-ingress-tls.yaml</span><br><span class="line">ingress.extensions/kubia configured</span><br></pre></td></tr></table></figure><p>ingress支持443端口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-master ~]# kubectl get ing</span><br><span class="line">NAME    HOSTS               ADDRESS   PORTS     AGE</span><br><span class="line">kubia   kubia.example.com             80, 443   6h4m</span><br></pre></td></tr></table></figure><p>现在客户端可以https访问kubia.expample.com</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> ✘ huangyong@huangyong-Macbook-Pro  ~  curl -k https://kubia.example.com</span><br><span class="line">You&apos;ve hit kubia-xgnkq</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kuberentes-Ingress&quot;&gt;&lt;a href=&quot;#kuberentes-Ingress&quot; class=&quot;headerlink&quot; title=&quot;kuberentes Ingress&quot;&gt;&lt;/a&gt;kuberentes Ingress&lt;/h2&gt;&lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://segmentfault.com/a/1190000019908991&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;K8s ingress 学习博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ingress是暴露服务给外部客户端的另外一种方法.(前面我们学习过2种服务类型可以实现同样的目的:NodePort和LoadBalance)&lt;/p&gt;
&lt;p&gt;但是这几种Service都有局限性:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ClusterIP的默认Serivce方式只能在集群内部访问。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;NodePort:当有几十上百的服务在集群中运行时，NodePort的端口管理是灾难。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;LoadBalance方式受限于云平台，且通常在云平台部署ELB还需要额外的费用。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所幸k8s还提供了一种集群维度暴露服务的方式，也就是ingress。ingress可以简单理解为service的service，他通过独立的ingress对象来制定请求转发的规则，把请求路由到一个或多个service中。这样就把服务与请求规则解耦了，可以从业务维度统一考虑业务的暴露，而不用为每个service单独考虑。&lt;/p&gt;
&lt;p&gt;Ingress可以为多个Service提供服务,可以根据请求的HOST和PATH,将请求代理到相关的Service中.而且Ingres工作在HTTP层,可以实现7层代理特性,诸如:coockies和会话亲和力&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s挂载NFS网络磁盘</title>
    <link href="https://jesse.top/2020/06/26/kubernetes/k8s%E6%8C%82%E8%BD%BDNFS%E7%BD%91%E7%BB%9C%E7%A3%81%E7%9B%98/"/>
    <id>https://jesse.top/2020/06/26/kubernetes/k8s挂载NFS网络磁盘/</id>
    <published>2020-06-26T03:59:58.000Z</published>
    <updated>2020-06-29T14:45:54.747Z</updated>
    
    <content type="html"><![CDATA[<h2 id="k8s挂载NFS网络磁盘"><a href="#k8s挂载NFS网络磁盘" class="headerlink" title="k8s挂载NFS网络磁盘"></a>k8s挂载NFS网络磁盘</h2><p>按照<kubernetes in="" action="">这本书NFS做持久化存储的例子,发现了一个坑.,启动pod失败,报如下错误</kubernetes></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown: changing ownership of &apos;/data/db&apos;: Operation not permitted</span><br></pre></td></tr></table></figure><p>网上也有人遇到这个问题.可以参考这篇文档: <a href="https://blog.csdn.net/herhun_chen/article/details/90247123" target="_blank" rel="noopener">Kubernetes 集群挂载NFS Volume</a></p><a id="more"></a><p>以下是pod的yaml文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@k8s-m01 ~]# cat mongodb-pod-nfs.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: mongodb</span><br><span class="line">spec:</span><br><span class="line">  volumes:</span><br><span class="line">     - name: mongodb-data</span><br><span class="line">       nfs:</span><br><span class="line">         server: 10.111.5.184</span><br><span class="line">         path: /data/k8s/</span><br><span class="line"></span><br><span class="line">  containers:</span><br><span class="line">     - image: mongo</span><br><span class="line">       name: mongodb</span><br><span class="line">       volumeMounts:</span><br><span class="line">         - name: mongodb-data</span><br><span class="line">           mountPath: /data/db</span><br><span class="line">       ports:</span><br><span class="line">         - containerPort: 27017</span><br><span class="line">           protocol: TCP</span><br></pre></td></tr></table></figure><p>出现这种<code>Operation not permitted</code>的权限类问题,肯定是NFS的挂载有问题.但是在所有k8s的节点上往NFS共享磁盘写文件,又是正常的.</p><p>解决这个问题需要在NFS服务器的<code>/etx/exports</code>配置文件修改成如下配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[work@hsq-beta-rpc ~]$cat /etc/exports</span><br><span class="line">/data/k8s/ 10.111.0.0/16(rw,fsid=0,async,no_subtree_check,no_auth_nlm,insecure,no_root_squash)</span><br></pre></td></tr></table></figure><blockquote><p>注意,以上配置其实是一行.没有换行</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;k8s挂载NFS网络磁盘&quot;&gt;&lt;a href=&quot;#k8s挂载NFS网络磁盘&quot; class=&quot;headerlink&quot; title=&quot;k8s挂载NFS网络磁盘&quot;&gt;&lt;/a&gt;k8s挂载NFS网络磁盘&lt;/h2&gt;&lt;p&gt;按照&lt;kubernetes in=&quot;&quot; action=&quot;&quot;&gt;这本书NFS做持久化存储的例子,发现了一个坑.,启动pod失败,报如下错误&lt;/kubernetes&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;chown: changing ownership of &amp;apos;/data/db&amp;apos;: Operation not permitted&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;网上也有人遇到这个问题.可以参考这篇文档: &lt;a href=&quot;https://blog.csdn.net/herhun_chen/article/details/90247123&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kubernetes 集群挂载NFS Volume&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="kubernetes" scheme="https://jesse.top/categories/kubernetes/"/>
    
    
      <category term="k8s" scheme="https://jesse.top/tags/k8s/"/>
    
  </entry>
  
</feed>
